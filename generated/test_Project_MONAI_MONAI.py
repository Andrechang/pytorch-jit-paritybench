import sys
_module = sys.modules[__name__]
del sys
conf = _module
densenet_evaluation_array = _module
densenet_evaluation_dict = _module
densenet_training_array = _module
densenet_training_dict = _module
densenet_training_array = _module
densenet_training_dict = _module
unet_evaluation_array = _module
unet_evaluation_dict = _module
unet_training_array = _module
unet_training_dict = _module
monai = _module
_version = _module
application = _module
config = _module
deviceconfig = _module
data = _module
csv_saver = _module
dataset = _module
grid_dataset = _module
nifti_reader = _module
nifti_saver = _module
nifti_writer = _module
png_saver = _module
png_writer = _module
synthetic = _module
utils = _module
engines = _module
evaluator = _module
multi_gpu_supervised_trainer = _module
trainer = _module
workflow = _module
handlers = _module
checkpoint_loader = _module
classification_saver = _module
lr_schedule_handler = _module
mean_dice = _module
metric_logger = _module
roc_auc = _module
segmentation_saver = _module
stats_handler = _module
tensorboard_handlers = _module
validation_handler = _module
inferers = _module
inferer = _module
utils = _module
losses = _module
dice = _module
focal_loss = _module
tversky = _module
metrics = _module
meandice = _module
rocauc = _module
networks = _module
blocks = _module
convolutions = _module
layers = _module
convutils = _module
factories = _module
simplelayers = _module
nets = _module
densenet = _module
highresnet = _module
unet = _module
utils = _module
transforms = _module
adaptors = _module
compose = _module
croppad = _module
array = _module
dictionary = _module
intensity = _module
io = _module
post = _module
spatial = _module
array = _module
dictionary = _module
utility = _module
aliases = _module
decorators = _module
misc = _module
module = _module
visualize = _module
img2tensorboard = _module
setup = _module
tests = _module
test_adaptors = _module
test_add_channeld = _module
test_adjust_contrast = _module
test_adjust_contrastd = _module
test_affine = _module
test_affine_grid = _module
test_arraydataset = _module
test_as_channel_first = _module
test_as_channel_firstd = _module
test_as_channel_last = _module
test_as_channel_lastd = _module
test_cachedataset = _module
test_cachedataset_parallel = _module
test_cast_to_type = _module
test_cast_to_typed = _module
test_center_spatial_crop = _module
test_center_spatial_cropd = _module
test_compose = _module
test_compute_meandice = _module
test_compute_roc_auc = _module
test_convolutions = _module
test_create_grid_and_affine = _module
test_crop_foreground = _module
test_crop_foregroundd = _module
test_csv_saver = _module
test_data_stats = _module
test_data_statsd = _module
test_dataset = _module
test_delete_keys = _module
test_densenet = _module
test_dice_loss = _module
test_flip = _module
test_flipd = _module
test_focal_loss = _module
test_gaussian_filter = _module
test_generalized_dice_loss = _module
test_generate_pos_neg_label_crop_centers = _module
test_generate_spatial_bounding_box = _module
test_handler_classification_saver = _module
test_handler_lr_scheduler = _module
test_handler_mean_dice = _module
test_handler_rocauc = _module
test_handler_segmentation_saver = _module
test_handler_stats = _module
test_handler_tb_image = _module
test_handler_tb_stats = _module
test_handler_validation = _module
test_header_correct = _module
test_highresnet = _module
test_img2tensorboard = _module
test_integration_classification_2d = _module
test_integration_determinism = _module
test_integration_segmentation_3d = _module
test_integration_sliding_window = _module
test_integration_unet_2d = _module
test_list_data_collate = _module
test_load_nifti = _module
test_load_niftid = _module
test_load_png = _module
test_load_pngd = _module
test_load_spacing_orientation = _module
test_map_transform = _module
test_nifti_header_revise = _module
test_nifti_rw = _module
test_nifti_saver = _module
test_normalize_intensity = _module
test_normalize_intensityd = _module
test_orientation = _module
test_orientationd = _module
test_parallel_execution = _module
test_persistentdataset = _module
test_plot_2d_or_3d_image = _module
test_png_rw = _module
test_png_saver = _module
test_query_memory = _module
test_rand_adjust_contrast = _module
test_rand_adjust_contrastd = _module
test_rand_affine = _module
test_rand_affine_grid = _module
test_rand_affined = _module
test_rand_crop_by_pos_neg_labeld = _module
test_rand_deform_grid = _module
test_rand_elastic_2d = _module
test_rand_elastic_3d = _module
test_rand_elasticd_2d = _module
test_rand_elasticd_3d = _module
test_rand_flip = _module
test_rand_flipd = _module
test_rand_gaussian_noise = _module
test_rand_gaussian_noised = _module
test_rand_rotate = _module
test_rand_rotate90 = _module
test_rand_rotate90d = _module
test_rand_rotated = _module
test_rand_scale_intensity = _module
test_rand_scale_intensityd = _module
test_rand_shift_intensity = _module
test_rand_shift_intensityd = _module
test_rand_spatial_crop = _module
test_rand_spatial_cropd = _module
test_rand_zoom = _module
test_rand_zoomd = _module
test_randomizable = _module
test_repeat_channel = _module
test_repeat_channeld = _module
test_resampler = _module
test_resize = _module
test_resized = _module
test_rotate = _module
test_rotate90 = _module
test_rotate90d = _module
test_rotated = _module
test_scale_intensity = _module
test_scale_intensity_range = _module
test_scale_intensity_ranged = _module
test_scale_intensityd = _module
test_seg_loss_integration = _module
test_shift_intensity = _module
test_shift_intensityd = _module
test_simulatedelay = _module
test_simulatedelayd = _module
test_sliding_window_inference = _module
test_spacing = _module
test_spacingd = _module
test_spatial_crop = _module
test_spatial_cropd = _module
test_spatial_pad = _module
test_spatial_padd = _module
test_split_channel = _module
test_split_channeld = _module
test_squeezedim = _module
test_squeezedimd = _module
test_threshold_intensity = _module
test_threshold_intensityd = _module
test_to_onehot = _module
test_tversky_loss = _module
test_unet = _module
test_zipdataset = _module
test_zoom = _module
test_zoom_affine = _module
test_zoomd = _module
versioneer = _module

from _paritybench_helpers import _mock_config
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
open = mock_open()
logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import logging


import numpy as np


import torch


from torch.utils.data import DataLoader


from torch.utils.tensorboard import SummaryWriter


import torch.nn.functional as F


import warnings


from torch.nn.modules.loss import _Loss


from torch.nn.modules.loss import _WeightedLoss


import torch.nn as nn


from typing import Callable


from collections import OrderedDict


import torch.nn.functional as f


import scipy.ndimage


import torch.optim as optim


def one_hot(labels, num_classes):
    """
    For a tensor `labels` of dimensions B1[spatial_dims], return a tensor of dimensions `BN[spatial_dims]`
    for `num_classes` N number of classes.

    Example:

        For every value v = labels[b,1,h,w], the value in the result at [b,v,h,w] will be 1 and all others 0.
        Note that this will include the background label, thus a binary mask should be treated as having 2 classes.
    """
    num_dims = labels.dim()
    if num_dims > 1:
        assert labels.shape[1
            ] == 1, 'labels should have a channel with length equals to one.'
        labels = torch.squeeze(labels, 1)
    labels = f.one_hot(labels.long(), num_classes)
    new_axes = [0, -1] + list(range(1, num_dims - 1))
    labels = labels.permute(*new_axes)
    if not labels.is_contiguous():
        return labels.contiguous()
    return labels


class DiceLoss(_Loss):
    """
    Compute average Dice loss between two tensors. It can support both multi-classes and multi-labels tasks.
    Input logits `input` (BNHW[D] where N is number of classes) is compared with ground truth `target` (BNHW[D]).
    Axis N of `input` is expected to have logit predictions for each class rather than being image channels,
    while the same axis of `target` can be 1 or N (one-hot format). The `smooth` parameter is a value added to the
    intersection and union components of the inter-over-union calculation to smooth results and prevent divide by 0,
    this value should be small. The `include_background` class attribute can be set to False for an instance of
    DiceLoss to exclude the first category (channel index 0) which is by convention assumed to be background.
    If the non-background segmentations are small compared to the total image size they can get overwhelmed by
    the signal from the background so excluding it in such cases helps convergence.
    """

    def __init__(self, include_background=True, to_onehot_y=False,
        do_sigmoid=False, do_softmax=False, squared_pred=False, jaccard=
        False, reduction='mean'):
        """
        Args:
            include_background (bool): If False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y (bool): whether to convert `y` into the one-hot format. Defaults to False.
            do_sigmoid (bool): If True, apply a sigmoid function to the prediction.
            do_softmax (bool): If True, apply a softmax function to the prediction.
            squared_pred (bool): use squared versions of targets and predictions in the denominator or not.
            jaccard (bool): compute Jaccard Index (soft IoU) instead of dice or not.
            reduction (`none|mean|sum`): Specifies the reduction to apply to the output:
                ``'none'``: no reduction will be applied,
                ``'mean'``: the sum of the output will be divided by the number of elements in the output,
                ``'sum'``: the output will be summed.
                Default: ``'mean'``.
        """
        super().__init__(reduction=reduction)
        self.include_background = include_background
        self.to_onehot_y = to_onehot_y
        if do_sigmoid and do_softmax:
            raise ValueError(
                'do_sigmoid=True and do_softmax=True are not compatible.')
        self.do_sigmoid = do_sigmoid
        self.do_softmax = do_softmax
        self.squared_pred = squared_pred
        self.jaccard = jaccard

    def forward(self, input, target, smooth=1e-05):
        """
        Args:
            input (tensor): the shape should be BNH[WD].
            target (tensor): the shape should be BNH[WD].
            smooth (float): a small constant to avoid nan.
        """
        if self.do_sigmoid:
            input = torch.sigmoid(input)
        n_pred_ch = input.shape[1]
        if n_pred_ch == 1:
            if self.do_softmax:
                warnings.warn(
                    'single channel prediction, `do_softmax=True` ignored.')
            if self.to_onehot_y:
                warnings.warn(
                    'single channel prediction, `to_onehot_y=True` ignored.')
            if not self.include_background:
                warnings.warn(
                    'single channel prediction, `include_background=False` ignored.'
                    )
        else:
            if self.do_softmax:
                input = torch.softmax(input, 1)
            if self.to_onehot_y:
                target = one_hot(target, n_pred_ch)
            if not self.include_background:
                target = target[:, 1:]
                input = input[:, 1:]
        assert target.shape == input.shape, f'ground truth has differing shape ({target.shape}) from input ({input.shape})'
        reduce_axis = list(range(2, len(input.shape)))
        intersection = torch.sum(target * input, reduce_axis)
        if self.squared_pred:
            target = torch.pow(target, 2)
            input = torch.pow(input, 2)
        ground_o = torch.sum(target, reduce_axis)
        pred_o = torch.sum(input, reduce_axis)
        denominator = ground_o + pred_o
        if self.jaccard:
            denominator -= intersection
        f = 1.0 - (2.0 * intersection + smooth) / (denominator + smooth)
        if self.reduction == 'sum':
            return f.sum()
        if self.reduction == 'none':
            return f
        if self.reduction == 'mean':
            return f.mean()
        raise ValueError(f'reduction={self.reduction} is invalid.')


class GeneralizedDiceLoss(_Loss):
    """
    Compute the generalised Dice loss defined in:

        Sudre, C. et. al. (2017) Generalised Dice overlap as a deep learning
        loss function for highly unbalanced segmentations. DLMIA 2017.

    Adapted from:
        https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/layer/loss_segmentation.py#L279
    """

    def __init__(self, include_background=True, to_onehot_y=False,
        do_sigmoid=False, do_softmax=False, w_type='square', reduction='mean'):
        """
        Args:
            include_background (bool): If False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y (bool): whether to convert `y` into the one-hot format. Defaults to False.
            do_sigmoid (bool): If True, apply a sigmoid function to the prediction.
            do_softmax (bool): If True, apply a softmax function to the prediction.
            w_type ('square'|'simple'|'uniform'): type of function to transform ground truth volume to a weight factor.
                Default: `'square'`
            reduction (`none|mean|sum`): Specifies the reduction to apply to the output:
                ``'none'``: no reduction will be applied,
                ``'mean'``: the sum of the output will be divided by the batch size in the output,
                ``'sum'``: the output will be summed over the batch dim.
                Default: ``'mean'``.
        """
        super().__init__(reduction=reduction)
        self.include_background = include_background
        self.to_onehot_y = to_onehot_y
        if do_sigmoid and do_softmax:
            raise ValueError(
                'do_sigmoid=True and do_softmax=True are not compatible.')
        self.do_sigmoid = do_sigmoid
        self.do_softmax = do_softmax
        self.w_func = torch.ones_like
        if w_type == 'simple':
            self.w_func = torch.reciprocal
        elif w_type == 'square':
            self.w_func = lambda x: torch.reciprocal(x * x)

    def forward(self, input, target, smooth=1e-05):
        """
        Args:
            input (tensor): the shape should be BNH[WD].
            target (tensor): the shape should be BNH[WD].
            smooth (float): a small constant to avoid nan.
        """
        if self.do_sigmoid:
            input = torch.sigmoid(input)
        n_pred_ch = input.shape[1]
        if n_pred_ch == 1:
            if self.do_softmax:
                warnings.warn(
                    'single channel prediction, `do_softmax=True` ignored.')
            if self.to_onehot_y:
                warnings.warn(
                    'single channel prediction, `to_onehot_y=True` ignored.')
            if not self.include_background:
                warnings.warn(
                    'single channel prediction, `include_background=False` ignored.'
                    )
        else:
            if self.do_softmax:
                input = torch.softmax(input, 1)
            if self.to_onehot_y:
                target = one_hot(target, n_pred_ch)
            if not self.include_background:
                target = target[:, 1:]
                input = input[:, 1:]
        assert target.shape == input.shape, f'ground truth has differing shape ({target.shape}) from input ({input.shape})'
        reduce_axis = list(range(2, len(input.shape)))
        intersection = torch.sum(target * input, reduce_axis)
        ground_o = torch.sum(target, reduce_axis)
        pred_o = torch.sum(input, reduce_axis)
        denominator = ground_o + pred_o
        w = self.w_func(ground_o.float())
        for b in w:
            infs = torch.isinf(b)
            b[infs] = 0.0
            b[infs] = torch.max(b)
        f = 1.0 - (2.0 * (intersection * w).sum(1) + smooth) / ((
            denominator * w).sum(1) + smooth)
        if self.reduction == 'sum':
            return f.sum()
        if self.reduction == 'none':
            return f
        if self.reduction == 'mean':
            return f.mean()
        raise ValueError(f'reduction={self.reduction} is invalid.')


class FocalLoss(_WeightedLoss):
    """
    PyTorch implementation of the Focal Loss.
    [1] "Focal Loss for Dense Object Detection", T. Lin et al., ICCV 2017
    """

    def __init__(self, gamma=2.0, weight=None, reduction='mean'):
        """
        Args:
            gamma (float): value of the exponent gamma in the definition of the Focal loss.
            weight (tensor): weights to apply to the voxels of each class. If None no weights are applied.
                This corresponds to the weights `lpha` in [1].
            reduction (`none|mean|sum`): Specifies the reduction to apply to the output:
                ``'none'``: no reduction will be applied,
                ``'mean'``: the sum of the output will be divided by the batch size in the output,
                ``'sum'``: the output will be summed over the batch dim.
                Default: ``'mean'``.

        Example:
            .. code-block:: python

                import torch
                from monai.losses import FocalLoss

                pred = torch.tensor([[1, 0], [0, 1], [1, 0]], dtype=torch.float32)
                grnd = torch.tensor([[0], [1], [0]], dtype=torch.int64)
                fl = FocalLoss()
                fl(pred, grnd)

        """
        super(FocalLoss, self).__init__(weight=weight, reduction=reduction)
        self.gamma = gamma

    def forward(self, input, target):
        """
        Args:
            input: (tensor): the shape should be BCH[WD].
                where C is the number of classes.
            target: (tensor): the shape should be B1H[WD].
                The target that this loss expects should be a class index in the range
                [0, C-1] where C is the number of classes.
        """
        i = input
        t = target
        if i.ndim != t.ndim:
            raise ValueError(
                f'input and target must have the same number of dimensions, got {i.ndim} and {t.ndim}'
                )
        if target.shape[1] != 1:
            raise ValueError(
                f'target must have one channel, and should be a class index in the range [0, C-1] '
                 +
                f"where C is the number of classes inferred from 'input': C={i.shape[1]}."
                )
        if input.dim() > 2:
            i = i.view(i.size(0), i.size(1), -1)
            t = t.view(t.size(0), t.size(1), -1)
        else:
            i = i.unsqueeze(2)
            t = t.unsqueeze(2)
        logpt = F.log_softmax(i, dim=1)
        logpt = logpt.gather(1, t.long())
        logpt = torch.squeeze(logpt, dim=1)
        pt = torch.exp(logpt)
        if self.weight is not None:
            self.weight = self.weight
            at = self.weight[(None), :, (None)]
            at = at.expand((t.size(0), -1, t.size(2)))
            at = at.gather(1, t.long())
            at = torch.squeeze(at, dim=1)
            logpt = logpt * at
        weight = torch.pow(-pt + 1.0, self.gamma)
        loss = torch.mean(-weight * logpt, dim=1)
        if self.reduction == 'sum':
            return loss.sum()
        if self.reduction == 'none':
            return loss
        if self.reduction == 'mean':
            return loss.mean()
        raise ValueError(f'reduction={self.reduction} is invalid.')


class TverskyLoss(_Loss):
    """
    Compute the Tversky loss defined in:

        Sadegh et al. (2017) Tversky loss function for image segmentation
        using 3D fully convolutional deep networks. (https://arxiv.org/abs/1706.05721)

    Adapted from:
        https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/layer/loss_segmentation.py#L631

    """

    def __init__(self, include_background=True, to_onehot_y=False,
        do_sigmoid=False, do_softmax=False, alpha=0.5, beta=0.5, reduction=
        'mean'):
        """
        Args:
            include_background (bool): If False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y (bool): whether to convert `y` into the one-hot format. Defaults to False.
            do_sigmoid (bool): If True, apply a sigmoid function to the prediction.
            do_softmax (bool): If True, apply a softmax function to the prediction.
            alpha (float): weight of false positives
            beta  (float): weight of false negatives
            reduction (`none|mean|sum`): Specifies the reduction to apply to the output:
                ``'none'``: no reduction will be applied,
                ``'mean'``: the sum of the output will be divided by the number of elements in the output,
                ``'sum'``: the output will be summed.
                Default: ``'mean'``.

        """
        super().__init__(reduction=reduction)
        self.include_background = include_background
        self.to_onehot_y = to_onehot_y
        if do_sigmoid and do_softmax:
            raise ValueError(
                'do_sigmoid=True and do_softmax=True are not compatible.')
        self.do_sigmoid = do_sigmoid
        self.do_softmax = do_softmax
        self.alpha = alpha
        self.beta = beta

    def forward(self, input, target, smooth=1e-05):
        """
        Args:
            input (tensor): the shape should be BNH[WD].
            target (tensor): the shape should be BNH[WD].
            smooth (float): a small constant to avoid nan.
        """
        if self.do_sigmoid:
            input = torch.sigmoid(input)
        n_pred_ch = input.shape[1]
        if n_pred_ch == 1:
            if self.do_softmax:
                warnings.warn(
                    'single channel prediction, `do_softmax=True` ignored.')
            if self.to_onehot_y:
                warnings.warn(
                    'single channel prediction, `to_onehot_y=True` ignored.')
            if not self.include_background:
                warnings.warn(
                    'single channel prediction, `include_background=False` ignored.'
                    )
        else:
            if self.do_softmax:
                input = torch.softmax(input, 1)
            if self.to_onehot_y:
                target = one_hot(target, n_pred_ch)
            if not self.include_background:
                target = target[:, 1:]
                input = input[:, 1:]
        assert target.shape == input.shape, f'ground truth has differing shape ({target.shape}) from input ({input.shape})'
        p0 = input
        p1 = 1 - p0
        g0 = target
        g1 = 1 - g0
        reduce_axis = list(range(2, len(input.shape)))
        tp = torch.sum(p0 * g0, reduce_axis)
        fp = self.alpha * torch.sum(p0 * g1, reduce_axis)
        fn = self.beta * torch.sum(p1 * g0, reduce_axis)
        numerator = tp + smooth
        denominator = tp + fp + fn + smooth
        score = 1.0 - numerator / denominator
        if self.reduction == 'sum':
            return score.sum()
        if self.reduction == 'none':
            return score
        if self.reduction == 'mean':
            return score.mean()
        raise ValueError(f'reduction={self.reduction} is invalid.')


class LayerFactory:
    """
    Factory object for creating layers, this uses given factory functions to actually produce the types or constructing
    callables. These functions are referred to by name and can be added at any time.
    """

    def __init__(self):
        self.factories = {}

    @property
    def names(self):
        """
        Produces all factory names.
        """
        return tuple(self.factories)

    def add_factory_callable(self, name, func):
        """
        Add the factory function to this object under the given name.
        """
        self.factories[name.upper()] = func

    def factory_function(self, name):
        """
        Decorator for adding a factory function with the given name.
        """

        def _add(func):
            self.add_factory_callable(name, func)
            return func
        return _add

    def get_constructor(self, factory_name, *args):
        """
        Get the constructor for the given factory name and arguments.
        """
        if not isinstance(factory_name, str):
            raise ValueError('Factories must be selected by name')
        fact = self.factories[factory_name.upper()]
        return fact(*args)

    def __getitem__(self, args):
        """
        Get the given name or name/arguments pair. If `args` is a callable it is assumed to be the constructor
        itself and is returned, otherwise it should be the factory name or a pair containing the name and arguments.
        """
        if callable(args):
            return args
        if isinstance(args, str):
            name_obj, args = args, ()
        else:
            name_obj, *args = args
        return self.get_constructor(name_obj, *args)

    def __getattr__(self, key):
        """
        If `key` is a factory name, return it, otherwise behave as inherited. This allows referring to factory names
        as if they were constants, eg. `Fact.FOO` for a factory Fact with factory function foo.
        """
        if key in self.factories:
            return key
        return super().__getattr__(key)


Act = LayerFactory()


Conv = LayerFactory()


Dropout = LayerFactory()


Norm = LayerFactory()


def same_padding(kernel_size, dilation=1):
    """
    Return the padding value needed to ensure a convolution using the given kernel size produces an output of the same
    shape as the input for a stride of 1, otherwise ensure a shape of the input divided by the stride rounded down.
    """
    kernel_size = np.atleast_1d(kernel_size)
    padding = (kernel_size - 1) // 2 + (dilation - 1)
    padding = tuple(int(p) for p in padding)
    return tuple(padding) if len(padding) > 1 else padding[0]


def split_args(args):
    """
    Split arguments in a way to be suitable for using with the factory types. If `args` is a name it's interpreted
    """
    if isinstance(args, str):
        return args, {}
    else:
        name_obj, args = args
        if not isinstance(name_obj, (str, Callable)) or not isinstance(args,
            dict):
            msg = (
                'Layer specifiers must be single strings or pairs of the form (name/object-types, argument dict)'
                )
            raise ValueError(msg)
        return name_obj, args


class SkipConnection(nn.Module):
    """Concats the forward pass input with the result from the given submodule."""

    def __init__(self, submodule, cat_dim=1):
        super().__init__()
        self.submodule = submodule
        self.cat_dim = cat_dim

    def forward(self, x):
        return torch.cat([x, self.submodule(x)], self.cat_dim)


class Flatten(nn.Module):
    """Flattens the given input in the forward pass to be [B,-1] in shape."""

    def forward(self, x):
        return x.view(x.size(0), -1)


def issequenceiterable(obj):
    """
    Determine if the object is an iterable sequence and is not a string
    """
    return isinstance(obj, Iterable) and not isinstance(obj, str)


def ensure_tuple_rep(tup, dim):
    """
    Returns a copy of `tup` with `dim` values by either shortened or duplicated input.
    """
    if not issequenceiterable(tup):
        return (tup,) * dim
    elif len(tup) == dim:
        return tuple(tup)
    raise ValueError(f'sequence must have length {dim}, got length {len(tup)}.'
        )


def gaussian_1d(sigma, truncated=4.0):
    """
    one dimensional gaussian kernel.

    Args:
        sigma: std of the kernel
        truncated: tail length

    Returns:
        1D numpy array
    """
    if sigma <= 0:
        raise ValueError('sigma must be positive')
    tail = int(sigma * truncated + 0.5)
    sigma2 = sigma * sigma
    x = np.arange(-tail, tail + 1)
    out = np.exp(-0.5 / sigma2 * x ** 2)
    out /= out.sum()
    return out


class GaussianFilter(nn.Module):

    def __init__(self, spatial_dims, sigma, truncated=4.0):
        """
        Args:
            spatial_dims (int): number of spatial dimensions of the input image.
                must have shape (Batch, channels, H[, W, ...]).
            sigma (float or sequence of floats): std.
            truncated (float): spreads how many stds.
        """
        super().__init__()
        self.spatial_dims = int(spatial_dims)
        _sigma = ensure_tuple_rep(sigma, self.spatial_dims)
        self.kernel = [torch.nn.Parameter(torch.as_tensor(gaussian_1d(s,
            truncated), dtype=torch.float32), False) for s in _sigma]
        self.padding = [same_padding(k.size()[0]) for k in self.kernel]
        self.conv_n = [F.conv1d, F.conv2d, F.conv3d][spatial_dims - 1]
        for idx, param in enumerate(self.kernel):
            self.register_parameter(f'kernel_{idx}', param)

    def forward(self, x):
        """
        Args:
            x (tensor): in shape [Batch, chns, H, W, D].
        """
        chns = x.shape[1]
        sp_dim = self.spatial_dims
        x = torch.as_tensor(x).contiguous()

        def _conv(input_, d):
            if d < 0:
                return input_
            s = [1] * (sp_dim + 2)
            s[d + 2] = -1
            kernel = self.kernel[d].reshape(s)
            kernel = kernel.repeat([chns, 1] + [1] * sp_dim)
            padding = [0] * sp_dim
            padding[d] = self.padding[d]
            return self.conv_n(input=_conv(input_, d - 1), weight=kernel,
                padding=padding, groups=chns)
        return _conv(x, sp_dim - 1)


class _DenseLayer(nn.Sequential):

    def __init__(self, spatial_dims, in_channels, growth_rate, bn_size,
        dropout_prob):
        super(_DenseLayer, self).__init__()
        out_channels = bn_size * growth_rate
        conv_type = Conv[Conv.CONV, spatial_dims]
        norm_type = Norm[Norm.BATCH, spatial_dims]
        dropout_type = Dropout[Dropout.DROPOUT, spatial_dims]
        self.add_module('norm1', norm_type(in_channels))
        self.add_module('relu1', nn.ReLU(inplace=True))
        self.add_module('conv1', conv_type(in_channels, out_channels,
            kernel_size=1, bias=False))
        self.add_module('norm2', norm_type(out_channels))
        self.add_module('relu2', nn.ReLU(inplace=True))
        self.add_module('conv2', conv_type(out_channels, growth_rate,
            kernel_size=3, padding=1, bias=False))
        if dropout_prob > 0:
            self.add_module('dropout', dropout_type(dropout_prob))

    def forward(self, x):
        new_features = super(_DenseLayer, self).forward(x)
        return torch.cat([x, new_features], 1)


class _DenseBlock(nn.Sequential):

    def __init__(self, spatial_dims, layers, in_channels, bn_size,
        growth_rate, dropout_prob):
        super(_DenseBlock, self).__init__()
        for i in range(layers):
            layer = _DenseLayer(spatial_dims, in_channels, growth_rate,
                bn_size, dropout_prob)
            in_channels += growth_rate
            self.add_module('denselayer%d' % (i + 1), layer)


Pool = LayerFactory()


class _Transition(nn.Sequential):

    def __init__(self, spatial_dims, in_channels, out_channels):
        super(_Transition, self).__init__()
        conv_type = Conv[Conv.CONV, spatial_dims]
        norm_type = Norm[Norm.BATCH, spatial_dims]
        pool_type = Pool[Pool.AVG, spatial_dims]
        self.add_module('norm', norm_type(in_channels))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv', conv_type(in_channels, out_channels,
            kernel_size=1, bias=False))
        self.add_module('pool', pool_type(kernel_size=2, stride=2))


class DenseNet(nn.Module):
    """
    Densenet based on: "Densely Connected Convolutional Networks" https://arxiv.org/pdf/1608.06993.pdf
    Adapted from PyTorch Hub 2D version:
    https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py

    Args:
        spatial_dims (Int): number of spatial dimensions of the input image.
        in_channels (Int): number of the input channel.
        out_channels (Int): number of the output classes.
        init_features (Int) number of filters in the first convolution layer.
        growth_rate (Int): how many filters to add each layer (k in paper).
        block_config (tuple): how many layers in each pooling block.
        bn_size (Int) multiplicative factor for number of bottle neck layers.
                      (i.e. bn_size * k features in the bottleneck layer)
        dropout_prob (Float): dropout rate after each dense layer.
    """

    def __init__(self, spatial_dims, in_channels, out_channels,
        init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),
        bn_size=4, dropout_prob=0):
        super(DenseNet, self).__init__()
        conv_type = Conv[Conv.CONV, spatial_dims]
        norm_type = Norm[Norm.BATCH, spatial_dims]
        pool_type = Pool[Pool.MAX, spatial_dims]
        avg_pool_type = Pool[Pool.ADAPTIVEAVG, spatial_dims]
        self.features = nn.Sequential(OrderedDict([('conv0', conv_type(
            in_channels, init_features, kernel_size=7, stride=2, padding=3,
            bias=False)), ('norm0', norm_type(init_features)), ('relu0', nn
            .ReLU(inplace=True)), ('pool0', pool_type(kernel_size=3, stride
            =2, padding=1))]))
        in_channels = init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(spatial_dims=spatial_dims, layers=
                num_layers, in_channels=in_channels, bn_size=bn_size,
                growth_rate=growth_rate, dropout_prob=dropout_prob)
            self.features.add_module('denseblock%d' % (i + 1), block)
            in_channels += num_layers * growth_rate
            if i == len(block_config) - 1:
                self.features.add_module('norm5', norm_type(in_channels))
            else:
                _out_channels = in_channels // 2
                trans = _Transition(spatial_dims, in_channels=in_channels,
                    out_channels=_out_channels)
                self.features.add_module('transition%d' % (i + 1), trans)
                in_channels = _out_channels
        self.class_layers = nn.Sequential(OrderedDict([('relu', nn.ReLU(
            inplace=True)), ('norm', avg_pool_type(1)), ('flatten', nn.
            Flatten(1)), ('class', nn.Linear(in_channels, out_channels))]))
        for m in self.modules():
            if isinstance(m, conv_type):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, norm_type):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.features(x)
        x = self.class_layers(x)
        return x


SUPPORTED_ACTI = {'relu': nn.ReLU, 'prelu': nn.PReLU, 'relu6': nn.ReLU6}


SUPPORTED_NORM = {'batch': lambda spatial_dims: Norm[Norm.BATCH,
    spatial_dims], 'instance': lambda spatial_dims: Norm[Norm.INSTANCE,
    spatial_dims]}


class ConvNormActi(nn.Module):

    def __init__(self, spatial_dims, in_channels, out_channels, kernel_size,
        norm_type=None, acti_type=None, dropout_prob=None):
        super(ConvNormActi, self).__init__()
        layers = nn.ModuleList()
        conv_type = Conv[Conv.CONV, spatial_dims]
        padding_size = same_padding(kernel_size)
        conv = conv_type(in_channels, out_channels, kernel_size, padding=
            padding_size)
        layers.append(conv)
        if norm_type is not None:
            layers.append(SUPPORTED_NORM[norm_type](spatial_dims)(out_channels)
                )
        if acti_type is not None:
            layers.append(SUPPORTED_ACTI[acti_type](inplace=True))
        if dropout_prob is not None:
            dropout_type = Dropout[Dropout.DROPOUT, spatial_dims]
            layers.append(dropout_type(p=dropout_prob))
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)


class HighResBlock(nn.Module):

    def __init__(self, spatial_dims, in_channels, out_channels, kernels=(3,
        3), dilation=1, norm_type='instance', acti_type='relu',
        channel_matching='pad'):
        """
        Args:
            kernels (list of int): each integer k in `kernels` corresponds to a convolution layer with kernel size k.
            channel_matching ('pad'|'project'): handling residual branch and conv branch channel mismatches
                with either zero padding ('pad') or a trainable conv with kernel size 1 ('project').
        """
        super(HighResBlock, self).__init__()
        conv_type = Conv[Conv.CONV, spatial_dims]
        self.project, self.pad = None, None
        if in_channels != out_channels:
            if channel_matching not in ('pad', 'project'):
                raise ValueError(
                    f'channel matching must be pad or project, got {channel_matching}.'
                    )
            if channel_matching == 'project':
                self.project = conv_type(in_channels, out_channels,
                    kernel_size=1)
            if channel_matching == 'pad':
                if in_channels > out_channels:
                    raise ValueError(
                        'in_channels > out_channels is incompatible with `channel_matching=pad`.'
                        )
                pad_1 = (out_channels - in_channels) // 2
                pad_2 = out_channels - in_channels - pad_1
                pad = [0, 0] * spatial_dims + [pad_1, pad_2] + [0, 0]
                self.pad = lambda input: F.pad(input, pad)
        layers = nn.ModuleList()
        _in_chns, _out_chns = in_channels, out_channels
        for kernel_size in kernels:
            layers.append(SUPPORTED_NORM[norm_type](spatial_dims)(_in_chns))
            layers.append(SUPPORTED_ACTI[acti_type](inplace=True))
            layers.append(conv_type(_in_chns, _out_chns, kernel_size,
                padding=same_padding(kernel_size, dilation), dilation=dilation)
                )
            _in_chns = _out_chns
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        x_conv = self.layers(x)
        if self.project is not None:
            return x_conv + self.project(x)
        if self.pad is not None:
            return x_conv + self.pad(x)
        return x_conv + x


DEFAULT_LAYER_PARAMS_3D = {'name': 'conv_0', 'n_features': 16, 'kernel_size': 3
    }, {'name': 'res_1', 'n_features': 16, 'kernels': (3, 3), 'repeat': 3}, {
    'name': 'res_2', 'n_features': 32, 'kernels': (3, 3), 'repeat': 3}, {'name'
    : 'res_3', 'n_features': 64, 'kernels': (3, 3), 'repeat': 3}, {'name':
    'conv_1', 'n_features': 80, 'kernel_size': 1}, {'name': 'conv_2',
    'kernel_size': 1}


class HighResNet(nn.Module):
    """
    Reimplementation of highres3dnet based on
    Li et al., "On the compactness, efficiency, and representation of 3D
    convolutional networks: Brain parcellation as a pretext task", IPMI '17

    Adapted from:
    https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/network/highres3dnet.py
    https://github.com/fepegar/highresnet

    Args:
        spatial_dims (int): number of spatial dimensions of the input image.
        in_channels (int): number of input channels.
        out_channels (int): number of output channels.
        norm_type ('batch'|'instance'): feature normalisation with batchnorm or instancenorm.
        acti_type ('relu'|'prelu'|'relu6'): non-linear activation using ReLU or PReLU.
        dropout_prob (float): probability of the feature map to be zeroed
            (only applies to the penultimate conv layer).
        layer_params (a list of dictionaries): specifying key parameters of each layer/block.
    """

    def __init__(self, spatial_dims=3, in_channels=1, out_channels=1,
        norm_type='batch', acti_type='relu', dropout_prob=None,
        layer_params=DEFAULT_LAYER_PARAMS_3D):
        super(HighResNet, self).__init__()
        blocks = nn.ModuleList()
        params = layer_params[0]
        _in_chns, _out_chns = in_channels, params['n_features']
        blocks.append(ConvNormActi(spatial_dims, _in_chns, _out_chns,
            kernel_size=params['kernel_size'], norm_type=norm_type,
            acti_type=acti_type, dropout_prob=None))
        for idx, params in enumerate(layer_params[1:-2]):
            _in_chns, _out_chns = _out_chns, params['n_features']
            _dilation = 2 ** idx
            for _ in range(params['repeat']):
                blocks.append(HighResBlock(spatial_dims, _in_chns,
                    _out_chns, params['kernels'], dilation=_dilation,
                    norm_type=norm_type, acti_type=acti_type))
                _in_chns = _out_chns
        params = layer_params[-2]
        _in_chns, _out_chns = _out_chns, params['n_features']
        blocks.append(ConvNormActi(spatial_dims, _in_chns, _out_chns,
            kernel_size=params['kernel_size'], norm_type=norm_type,
            acti_type=acti_type, dropout_prob=dropout_prob))
        params = layer_params[-1]
        _in_chns = _out_chns
        blocks.append(ConvNormActi(spatial_dims, _in_chns, out_channels,
            kernel_size=params['kernel_size'], norm_type=norm_type,
            acti_type=None, dropout_prob=None))
        self.blocks = nn.Sequential(*blocks)

    def forward(self, x):
        return self.blocks(x)


GlobalAliases = {}


import torch
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile

class Test_Project_MONAI_MONAI(_paritybench_base):
    pass
    @_fails_compile()
    def test_000(self):
        self._check(DiceLoss(*[], **{}), [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})

    def test_001(self):
        self._check(Flatten(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_002(self):
        self._check(GeneralizedDiceLoss(*[], **{}), [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})

    def test_003(self):
        self._check(SkipConnection(*[], **{'submodule': ReLU()}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_004(self):
        self._check(TverskyLoss(*[], **{}), [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})

