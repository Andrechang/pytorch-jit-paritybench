import sys
_module = sys.modules[__name__]
del sys
_init_paths = _module
Evaluation = _module
eval_classification = _module
eval_detection = _module
eval_kinetics = _module
eval_proposal = _module
get_classification_performance = _module
get_detection_performance = _module
get_kinetics_performance = _module
get_proposal_performance = _module
utils = _module
activitynet_log_analysis = _module
TH14evalDet_Updated = _module
eval_thumos14 = _module
thumos14_log_analysis = _module
datasets = _module
coco = _module
ds_utils = _module
factory = _module
imagenet = _module
imdb = _module
pascal_voc = _module
pascal_voc_rbg = _module
mcg_munge = _module
vg = _module
vg_eval = _module
voc_eval = _module
model = _module
nms = _module
_ext = _module
nms = _module
build = _module
nms_cpu = _module
nms_gpu = _module
nms_wrapper = _module
roi_temporal_pooling = _module
roi_temporal_pooling = _module
build = _module
functions = _module
roi_temporal_pool = _module
modules = _module
roi_temporal_pool = _module
rpn = _module
anchor_target_layer = _module
generate_anchors = _module
proposal_layer = _module
proposal_target_layer_cascade = _module
resnet = _module
rpn = _module
twin_transform = _module
tdcnn = _module
c3d = _module
eco = _module
i3d = _module
resnet = _module
tdcnn = _module
vgg16 = _module
blob = _module
config = _module
logger = _module
net_utils = _module
non_local_dot_product = _module
transforms = _module
roi_data_layer = _module
minibatch = _module
roibatchLoader = _module
setup = _module
test_tdcnn = _module
C3DRes18 = _module
layer_factory = _module
pytorch_load = _module
ECO = _module
layer_factory = _module
pytorch_load = _module
ECOfull = _module
layer_factory = _module
pytorch_load = _module
tf_model_zoo = _module
bninception = _module
caffe_pb2 = _module
layer_factory = _module
parse_caffe = _module
pytorch_load = _module
inceptionresnetv2 = _module
pytorch_load = _module
tensorflow_dump = _module
inceptionv4 = _module
pytorch_load = _module
AdditiveGaussianNoiseAutoencoderRunner = _module
AutoencoderRunner = _module
MaskingNoiseAutoencoderRunner = _module
Utils = _module
VariationalAutoencoderRunner = _module
autoencoder = _module
Autoencoder = _module
DenoisingAutoencoder = _module
VariationalAutoencoder = _module
autoencoder_models = _module
decoder = _module
encoder = _module
msssim = _module
differential_privacy = _module
dp_mnist = _module
dp_optimizer = _module
dp_pca = _module
sanitizer = _module
per_example_gradients = _module
aggregation = _module
analysis = _module
deep_cnn = _module
input = _module
metrics = _module
train_student = _module
train_teachers = _module
gaussian_moments = _module
accountant = _module
configuration = _module
build_mscoco_data = _module
evaluate = _module
caption_generator = _module
caption_generator_test = _module
inference_wrapper_base = _module
vocabulary = _module
inference_wrapper = _module
image_embedding = _module
image_embedding_test = _module
image_processing = _module
inputs = _module
run_inference = _module
show_and_tell_model = _module
show_and_tell_model_test = _module
train = _module
build_image_data = _module
build_imagenet_data = _module
preprocess_imagenet_validation_data = _module
process_bounding_boxes = _module
dataset = _module
flowers_data = _module
flowers_eval = _module
flowers_train = _module
imagenet_data = _module
imagenet_distributed_train = _module
imagenet_eval = _module
imagenet_train = _module
inception_distributed_train = _module
inception_eval = _module
inception_model = _module
inception_train = _module
collections_test = _module
inception_test = _module
losses = _module
losses_test = _module
ops = _module
ops_test = _module
scopes = _module
scopes_test = _module
slim = _module
variables = _module
variables_test = _module
data_utils = _module
lm_1b_eval = _module
neural_gpu = _module
neural_gpu_trainer = _module
nn_utils = _module
parameters = _module
wiki_data = _module
cifar_input = _module
resnet_main = _module
resnet_model = _module
cifar10 = _module
dataset_factory = _module
dataset_utils = _module
download_and_convert_cifar10 = _module
download_and_convert_flowers = _module
download_and_convert_mnist = _module
flowers = _module
mnist = _module
deployment = _module
model_deploy = _module
model_deploy_test = _module
download_and_convert_data = _module
eval_image_classifier = _module
nets = _module
alexnet = _module
alexnet_test = _module
cifarnet = _module
inception = _module
inception_resnet_v2 = _module
inception_resnet_v2_test = _module
inception_utils = _module
inception_v1 = _module
inception_v1_test = _module
inception_v2 = _module
inception_v2_test = _module
inception_v3 = _module
inception_v3_test = _module
inception_v4 = _module
inception_v4_test = _module
lenet = _module
nets_factory = _module
nets_factory_test = _module
overfeat = _module
overfeat_test = _module
resnet_utils = _module
resnet_v1 = _module
resnet_v1_test = _module
resnet_v2 = _module
resnet_v2_test = _module
vgg = _module
vgg_test = _module
preprocessing = _module
cifarnet_preprocessing = _module
inception_preprocessing = _module
lenet_preprocessing = _module
preprocessing_factory = _module
vgg_preprocessing = _module
train_image_classifier = _module
decoder_test = _module
errorcounter = _module
errorcounter_test = _module
nn_ops = _module
shapes = _module
shapes_test = _module
vgsl_eval = _module
vgsl_input = _module
vgsl_model = _module
vgsl_model_test = _module
vgsl_train = _module
vgslspecs = _module
vgslspecs_test = _module
glove_to_shards = _module
nearest = _module
prep = _module
swivel = _module
text2bin = _module
vecs = _module
wordsim = _module
beam_reader_ops_test = _module
conll2tree = _module
graph_builder = _module
graph_builder_test = _module
lexicon_builder_test = _module
load_parser_ops = _module
parser_eval = _module
parser_trainer = _module
reader_ops_test = _module
structured_graph_builder = _module
text_formats_test = _module
batch_reader = _module
beam_search = _module
data = _module
data_convert_example = _module
seq2seq_attention = _module
seq2seq_attention_decode = _module
seq2seq_attention_model = _module
seq2seq_lib = _module
cluttered_mnist = _module
example = _module
spatial_transformer = _module
tf_utils = _module
lstm_ops = _module
prediction_input = _module
prediction_model = _module
prediction_train = _module
download_video = _module
generate_frames = _module
generate_roidb_training = _module
generate_roidb_validation = _module
util = _module
test_net = _module
trainval_net = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, numbers, numpy, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


import numpy as np


from torch.autograd import Function


from torch.nn.modules.module import Module


import torch.nn as nn


import numpy.random as npr


import math


import torch.nn.functional as F


from torch.autograd import Variable


import time


from functools import partial


import random


import torchvision.models as models


from torch import nn


from torch.nn import functional as F


import torchvision


import numbers


import torch.utils.data as data


import torch.utils.model_zoo as model_zoo


from torch.nn.init import constant_


from torch.nn.init import xavier_uniform_


import collections


import tensorflow as tf


import torch.backends.cudnn as cudnn


import torch.optim as optim


import torchvision.transforms as transforms


from torch.utils.data.sampler import Sampler


class RoITemporalPoolFunction(Function):

    def __init__(ctx, pooled_length, pooled_height, pooled_width, temporal_scale, ctx_ratio):
        ctx.pooled_length = pooled_length
        ctx.pooled_width = pooled_width
        ctx.pooled_height = pooled_height
        ctx.temporal_scale = temporal_scale
        ctx.ctx_ratio = ctx_ratio
        ctx.feature_size = None

    def forward(ctx, features, rois):
        ctx.feature_size = features.size()
        batch_size, num_channels, data_length, data_height, data_width = ctx.feature_size
        num_rois = rois.size(0)
        output = features.new(num_rois, num_channels, ctx.pooled_length, ctx.pooled_height, ctx.pooled_width).zero_()
        ctx.argmax = features.new(num_rois, num_channels, ctx.pooled_length, ctx.pooled_height, ctx.pooled_width).zero_().int()
        ctx.rois = rois
        if not features.is_cuda:
            _features = features.permute(0, 2, 3, 4, 1)
            roi_temporal_pooling.roi_temporal_pooling_forward(ctx.pooled_length, ctx.pooled_height, ctx.pooled_width, ctx.temporal_scale, ctx.ctx_ratio, _features, rois, output)
        else:
            roi_temporal_pooling.roi_temporal_pooling_forward_cuda(ctx.pooled_length, ctx.pooled_height, ctx.pooled_width, ctx.temporal_scale, ctx.ctx_ratio, features, rois, output, ctx.argmax)
        return output

    def backward(ctx, grad_output):
        batch_size, num_channels, data_length, data_height, data_width = ctx.feature_size
        grad_input = grad_output.new(batch_size, num_channels, data_length, data_height, data_width).zero_()
        roi_temporal_pooling.roi_temporal_pooling_backward_cuda(ctx.pooled_length, ctx.pooled_height, ctx.pooled_width, ctx.temporal_scale, ctx.ctx_ratio, grad_output, ctx.rois, grad_input, ctx.argmax)
        return grad_input, None


class _RoITemporalPooling(Module):

    def __init__(self, pooled_length, pooled_height, pooled_width, temporal_scale, ctx_ratio=1.0):
        super(_RoITemporalPooling, self).__init__()
        self.pooled_width = int(pooled_width)
        self.pooled_height = int(pooled_height)
        self.pooled_length = int(pooled_length)
        self.temporal_scale = float(temporal_scale)
        self.ctx_ratio = float(ctx_ratio)

    def forward(self, features, rois):
        return RoITemporalPoolFunction(self.pooled_length, self.pooled_height, self.pooled_width, self.temporal_scale, self.ctx_ratio)(features, rois)


def twin_transform_batch(ex_rois, gt_rois):
    if ex_rois.dim() == 2:
        ex_lengths = ex_rois[:, (1)] - ex_rois[:, (0)] + 1.0
        ex_ctr_x = ex_rois[:, (0)] + 0.5 * ex_lengths
        gt_lengths = gt_rois[:, :, (1)] - gt_rois[:, :, (0)] + 1.0
        gt_ctr_x = gt_rois[:, :, (0)] + 0.5 * gt_lengths
        targets_dx = (gt_ctr_x - ex_ctr_x.view(1, -1).expand_as(gt_ctr_x)) / ex_lengths
        targets_dl = torch.log(gt_lengths / ex_lengths.view(1, -1).expand_as(gt_lengths))
    elif ex_rois.dim() == 3:
        ex_lengths = ex_rois[:, :, (1)] - ex_rois[:, :, (0)] + 1.0
        ex_ctr_x = ex_rois[:, :, (0)] + 0.5 * ex_lengths
        gt_lengths = gt_rois[:, :, (1)] - gt_rois[:, :, (0)] + 1.0
        gt_ctr_x = gt_rois[:, :, (0)] + 0.5 * gt_lengths
        targets_dx = (gt_ctr_x - ex_ctr_x) / ex_lengths
        targets_dl = torch.log(gt_lengths / ex_lengths)
    else:
        raise ValueError('ex_roi input dimension is not correct.')
    targets = torch.stack((targets_dx, targets_dl), 2)
    return targets


def _compute_targets_batch(ex_rois, gt_rois):
    """Compute bounding-box regression targets for an video."""
    return twin_transform_batch(ex_rois, gt_rois[:, :, :2])


def _unmap(data, count, inds, batch_size, fill=0):
    """ Unmap a subset of item (data) back to the original set of items (of
    size count) """
    if data.dim() == 2:
        ret = data.new(batch_size, count).fill_(fill)
        ret[:, (inds)] = data
    else:
        ret = data.new(batch_size, count, data.size(2)).fill_(fill)
        ret[:, (inds), :] = data
    return ret


def _mkanchors(ls, x_ctr):
    """
    Given a vector of lengths (ls) around a center
    (x_ctr), output a set of anchors (windows).
    """
    ls = ls[:, (np.newaxis)]
    anchors = np.hstack((x_ctr - 0.5 * (ls - 1), x_ctr + 0.5 * (ls - 1)))
    return anchors


def _whctrs(anchor):
    """
    Return width, height, x center, and y center for an anchor (window).
    """
    l = anchor[1] - anchor[0] + 1
    x_ctr = anchor[0] + 0.5 * (l - 1)
    return l, x_ctr


def _scale_enum(anchor, scales):
    """
    Enumerate a set of anchors for each scale wrt an anchor.
    """
    l, x_ctr = _whctrs(anchor)
    ls = l * scales
    anchors = _mkanchors(ls, x_ctr)
    return anchors


def generate_anchors(base_size=8, scales=2 ** np.arange(3, 6)):
    """
    Generate anchor (reference) windows by enumerating aspect 
    scales wrt a reference (0, 7) window.
    """
    base_anchor = np.array([1, base_size]) - 1
    anchors = _scale_enum(base_anchor, scales)
    return anchors


def twins_overlaps_batch(anchors, gt_twins):
    """
    anchors: 
        For RPN: (N, 2) ndarray of float or (batch_size, N, 2) ndarray of float
        For TDCNN: (batch_size, N, 3) ndarray of float
    gt_twins: (batch_size, K, 3) ndarray of float, (x1, x2, class_id)
    overlaps: (batch_size, N, K) ndarray of overlap between twins and query_twins
    """
    batch_size = gt_twins.size(0)
    if anchors.dim() == 2:
        N = anchors.size(0)
        K = gt_twins.size(1)
        anchors = anchors.view(1, N, 2).expand(batch_size, N, 2).contiguous()
        gt_twins = gt_twins[:, :, :2].contiguous()
        gt_twins_x = gt_twins[:, :, (1)] - gt_twins[:, :, (0)] + 1
        gt_twins_len = gt_twins_x.view(batch_size, 1, K)
        anchors_twins_x = anchors[:, :, (1)] - anchors[:, :, (0)] + 1
        anchors_len = anchors_twins_x.view(batch_size, N, 1)
        gt_len_zero = gt_twins_x == 1
        anchors_len_zero = anchors_twins_x == 1
        twins = anchors.view(batch_size, N, 1, 2).expand(batch_size, N, K, 2)
        query_twins = gt_twins.view(batch_size, 1, K, 2).expand(batch_size, N, K, 2)
        ilen = torch.min(twins[:, :, :, (1)], query_twins[:, :, :, (1)]) - torch.max(twins[:, :, :, (0)], query_twins[:, :, :, (0)]) + 1
        ilen[ilen < 0] = 0
        ua = anchors_len + gt_twins_len - ilen
        overlaps = ilen / ua
        overlaps.masked_fill_(gt_len_zero.view(batch_size, 1, K).expand(batch_size, N, K), 0)
        overlaps.masked_fill_(anchors_len_zero.view(batch_size, N, 1).expand(batch_size, N, K), -1)
    elif anchors.dim() == 3:
        N = anchors.size(1)
        K = gt_twins.size(1)
        if anchors.size(2) == 2:
            anchors = anchors[:, :, :2].contiguous()
        else:
            anchors = anchors[:, :, 1:3].contiguous()
        gt_twins = gt_twins[:, :, :2].contiguous()
        gt_twins_x = gt_twins[:, :, (1)] - gt_twins[:, :, (0)] + 1
        gt_twins_len = gt_twins_x.view(batch_size, 1, K)
        anchors_twins_x = anchors[:, :, (1)] - anchors[:, :, (0)] + 1
        anchors_len = anchors_twins_x.view(batch_size, N, 1)
        gt_len_zero = gt_twins_x == 1
        anchors_len_zero = anchors_twins_x == 1
        twins = anchors.view(batch_size, N, 1, 2).expand(batch_size, N, K, 2)
        query_twins = gt_twins.view(batch_size, 1, K, 2).expand(batch_size, N, K, 2)
        ilen = torch.min(twins[:, :, :, (1)], query_twins[:, :, :, (1)]) - torch.max(twins[:, :, :, (0)], query_twins[:, :, :, (0)]) + 1
        ilen[ilen < 0] = 0
        ua = anchors_len + gt_twins_len - ilen
        overlaps = ilen / ua
        overlaps.masked_fill_(gt_len_zero.view(batch_size, 1, K).expand(batch_size, N, K), 0)
        overlaps.masked_fill_(anchors_len_zero.view(batch_size, N, 1).expand(batch_size, N, K), -1)
    else:
        raise ValueError('anchors input dimension is not correct.')
    return overlaps


class _AnchorTargetLayer(nn.Module):
    """
        Assign anchors to ground-truth targets. Produces anchor classification
        labels and bounding-box regression targets.
    """

    def __init__(self, feat_stride, scales):
        super(_AnchorTargetLayer, self).__init__()
        self._feat_stride = feat_stride
        self._anchors = torch.from_numpy(generate_anchors(base_size=feat_stride, scales=np.array(scales))).float()
        self._num_anchors = self._anchors.size(0)
        self._allowed_border = 0

    def forward(self, input):
        rpn_cls_score = input[0]
        gt_twins = input[1]
        batch_size = gt_twins.size(0)
        length, height, width = rpn_cls_score.shape[-3:]
        shifts = np.arange(0, length) * self._feat_stride
        shifts = torch.from_numpy(shifts.astype(float))
        shifts = shifts.contiguous().type_as(rpn_cls_score)
        A = self._num_anchors
        K = shifts.shape[0]
        self._anchors = self._anchors.type_as(rpn_cls_score)
        all_anchors = self._anchors.view((1, A, 2)) + shifts.view(K, 1, 1)
        all_anchors = all_anchors.view(K * A, 2)
        total_anchors = int(K * A)
        keep = (all_anchors[:, (0)] >= -self._allowed_border) & (all_anchors[:, (1)] < long(length * self._feat_stride) + self._allowed_border)
        inds_inside = torch.nonzero(keep).view(-1)
        anchors = all_anchors[(inds_inside), :]
        labels = gt_twins.new(batch_size, inds_inside.size(0)).fill_(-1)
        twin_inside_weights = gt_twins.new(batch_size, inds_inside.size(0)).zero_()
        twin_outside_weights = gt_twins.new(batch_size, inds_inside.size(0)).zero_()
        overlaps = twins_overlaps_batch(anchors, gt_twins)
        max_overlaps, argmax_overlaps = torch.max(overlaps, 2)
        gt_max_overlaps, _ = torch.max(overlaps, 1)
        if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:
            labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0
        gt_max_overlaps[gt_max_overlaps == 0] = 1e-05
        keep = torch.sum(overlaps.eq(gt_max_overlaps.view(batch_size, 1, -1).expand_as(overlaps)), 2)
        if torch.sum(keep) > 0:
            labels[keep > 0] = 1
        labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1
        if cfg.TRAIN.RPN_CLOBBER_POSITIVES:
            labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0
        num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)
        sum_fg = torch.sum((labels == 1).int(), 1)
        sum_bg = torch.sum((labels == 0).int(), 1)
        for i in range(batch_size):
            if sum_fg[i] > num_fg:
                fg_inds = torch.nonzero(labels[i] == 1).view(-1)
                rand_num = torch.from_numpy(np.random.permutation(fg_inds.size(0))).type_as(gt_twins).long()
                disable_inds = fg_inds[rand_num[:fg_inds.size(0) - num_fg]]
                labels[i][disable_inds] = -1
            num_bg = cfg.TRAIN.RPN_BATCHSIZE - torch.sum((labels == 1).int(), 1)[i]
            if sum_bg[i] > num_bg:
                bg_inds = torch.nonzero(labels[i] == 0).view(-1)
                rand_num = torch.from_numpy(np.random.permutation(bg_inds.size(0))).type_as(gt_twins).long()
                disable_inds = bg_inds[rand_num[:bg_inds.size(0) - num_bg]]
                labels[i][disable_inds] = -1
        offset = torch.arange(0, batch_size) * gt_twins.size(1)
        argmax_overlaps = argmax_overlaps + offset.view(batch_size, 1).type_as(argmax_overlaps)
        twin_targets = _compute_targets_batch(anchors, gt_twins.view(-1, 3)[(argmax_overlaps.view(-1)), :].view(batch_size, -1, 3))
        twin_inside_weights[labels == 1] = cfg.TRAIN.RPN_TWIN_INSIDE_WEIGHTS[0]
        if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:
            num_examples = torch.sum(labels[i] >= 0)
            positive_weights = 1.0 / num_examples.float()
            negative_weights = 1.0 / num_examples.float()
        else:
            assert (cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) & (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1)
            positive_weights = cfg.TRAIN.RPN_POSITIVE_WEIGHT
            negative_weights = 1 - positive_weights
        twin_outside_weights[labels == 1] = positive_weights
        twin_outside_weights[labels == 0] = negative_weights
        labels = _unmap(labels, total_anchors, inds_inside, batch_size, fill=-1)
        twin_targets = _unmap(twin_targets, total_anchors, inds_inside, batch_size, fill=0)
        twin_inside_weights = _unmap(twin_inside_weights, total_anchors, inds_inside, batch_size, fill=0)
        twin_outside_weights = _unmap(twin_outside_weights, total_anchors, inds_inside, batch_size, fill=0)
        outputs = []
        labels = labels.view(batch_size, length, height, width, A).permute(0, 4, 1, 2, 3).contiguous()
        labels = labels.view(batch_size, 1, A * length, height, width)
        outputs.append(labels)
        twin_targets = twin_targets.view(batch_size, length, height, width, A * 2).permute(0, 4, 1, 2, 3).contiguous()
        outputs.append(twin_targets)
        anchors_count = twin_inside_weights.size(1)
        twin_inside_weights = twin_inside_weights.view(batch_size, anchors_count, 1).expand(batch_size, anchors_count, 2)
        twin_inside_weights = twin_inside_weights.contiguous().view(batch_size, length, height, width, 2 * A).permute(0, 4, 1, 2, 3).contiguous()
        outputs.append(twin_inside_weights)
        twin_outside_weights = twin_outside_weights.view(batch_size, anchors_count, 1).expand(batch_size, anchors_count, 2)
        twin_outside_weights = twin_outside_weights.contiguous().view(batch_size, length, height, width, 2 * A).permute(0, 4, 1, 2, 3).contiguous()
        outputs.append(twin_outside_weights)
        return outputs

    def backward(self, top, propagate_down, bottom):
        """This layer does not propagate gradients."""
        pass

    def reshape(self, bottom, top):
        """Reshaping happens during the call to forward."""
        pass


DEBUG = False


def clip_twins(wins, video_length, batch_size):
    """
    Clip wins to video boundaries.
    """
    wins.clamp_(0, video_length - 1)
    return wins


def nms_cpu(dets, thresh):
    dets = dets.numpy()
    x1 = dets[:, (0)]
    x2 = dets[:, (1)]
    scores = dets[:, (2)]
    length = x2 - x1 + 1
    order = scores.argsort()[::-1]
    keep = []
    while order.size > 0:
        i = order.item(0)
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        inter = np.maximum(0.0, xx2 - xx1 + 1)
        ovr = inter / (length[i] + length[order[1:]] - inter)
        inds = np.where(ovr < thresh)[0]
        order = order[inds + 1]
    return torch.IntTensor(keep)


def nms_gpu(dets, thresh):
    keep = dets.new(dets.size(0), 1).zero_().int()
    num_out = dets.new(1).zero_().int()
    nms.nms_cuda(keep, dets, num_out, thresh)
    keep = keep[:num_out[0]]
    return keep


def nms(dets, thresh, force_cpu=False):
    """Dispatch to either CPU or GPU NMS implementations."""
    if dets.shape[0] == 0:
        return []
    return nms_gpu(dets, thresh) if force_cpu == False else nms_cpu(dets, thresh)


def twin_transform_inv(wins, deltas, batch_size):
    lengths = wins[:, :, (1)] - wins[:, :, (0)] + 1.0
    ctr_x = wins[:, :, (0)] + 0.5 * lengths
    dx = deltas[:, :, 0::2]
    dl = deltas[:, :, 1::2]
    pred_ctr_x = dx * lengths.unsqueeze(2) + ctr_x.unsqueeze(2)
    pred_l = torch.exp(dl) * lengths.unsqueeze(2)
    pred_wins = deltas.clone()
    pred_wins[:, :, 0::2] = pred_ctr_x - 0.5 * pred_l
    pred_wins[:, :, 1::2] = pred_ctr_x + 0.5 * pred_l
    return pred_wins


class _ProposalLayer(nn.Module):
    """
    Outputs object detection proposals by applying estimated bounding-box
    transformations to a set of regular twins (called "anchors").
    """

    def __init__(self, feat_stride, scales, out_scores=False):
        super(_ProposalLayer, self).__init__()
        self._feat_stride = feat_stride
        self._anchors = torch.from_numpy(generate_anchors(base_size=feat_stride, scales=np.array(scales))).float()
        self._num_anchors = self._anchors.size(0)
        self._out_scores = out_scores

    def forward(self, input):
        scores = input[0][:, self._num_anchors:, :, :, :]
        twin_deltas = input[1]
        cfg_key = input[2]
        pre_nms_topN = cfg[cfg_key].RPN_PRE_NMS_TOP_N
        post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N
        nms_thresh = cfg[cfg_key].RPN_NMS_THRESH
        min_size = cfg[cfg_key].RPN_MIN_SIZE
        length, height, width = scores.shape[-3:]
        if DEBUG:
            None
        batch_size = twin_deltas.size(0)
        shifts = np.arange(0, length) * self._feat_stride
        shifts = torch.from_numpy(shifts.astype(float))
        shifts = shifts.contiguous().type_as(scores)
        A = self._num_anchors
        K = shifts.shape[0]
        self._anchors = self._anchors.type_as(scores)
        anchors = self._anchors.view(1, A, 2) + shifts.view(K, 1, 1)
        anchors = anchors.view(1, K * A, 2).expand(batch_size, K * A, 2)
        twin_deltas = twin_deltas.permute(0, 2, 3, 4, 1).contiguous()
        twin_deltas = twin_deltas.view(batch_size, -1, 2)
        scores = scores.permute(0, 2, 3, 4, 1).contiguous()
        scores = scores.view(batch_size, -1)
        proposals = twin_transform_inv(anchors, twin_deltas, batch_size)
        proposals = clip_twins(proposals, length * self._feat_stride, batch_size)
        no_keep = self._filter_twins_reverse(proposals, min_size)
        scores[no_keep] = 0
        scores_keep = scores
        proposals_keep = proposals
        _, order = torch.sort(scores_keep, 1, True)
        output = scores.new(batch_size, post_nms_topN, 3).zero_()
        if self._out_scores:
            output_score = scores.new(batch_size, post_nms_topN, 2).zero_()
        for i in range(batch_size):
            proposals_single = proposals_keep[i]
            scores_single = scores_keep[i]
            order_single = order[i]
            if pre_nms_topN > 0 and pre_nms_topN < scores_keep.numel():
                order_single = order_single[:pre_nms_topN]
            proposals_single = proposals_single[(order_single), :]
            scores_single = scores_single[order_single].view(-1, 1)
            keep_idx_i = nms(torch.cat((proposals_single, scores_single), 1), nms_thresh, force_cpu=not cfg.USE_GPU_NMS)
            keep_idx_i = keep_idx_i.long().view(-1)
            if post_nms_topN > 0:
                keep_idx_i = keep_idx_i[:post_nms_topN]
            proposals_single = proposals_single[(keep_idx_i), :]
            scores_single = scores_single[(keep_idx_i), :]
            num_proposal = proposals_single.size(0)
            output[(i), :, (0)] = i
            output[(i), :num_proposal, 1:] = proposals_single
            if self._out_scores:
                output_score[(i), :, (0)] = i
                output_score[(i), :num_proposal, (1)] = scores_single
        if self._out_scores:
            return output, output_score
        else:
            return output

    def backward(self, top, propagate_down, bottom):
        """This layer does not propagate gradients."""
        pass

    def reshape(self, bottom, top):
        """Reshaping happens during the call to forward."""
        pass

    def _filter_twins_reverse(self, twins, min_size):
        """get the keep index of all twins with length smaller than min_size. 
        twins will be (batch_size, C, 2), keep will be (batch_size, C)"""
        ls = twins[:, :, (1)] - twins[:, :, (0)] + 1
        no_keep = ls < min_size
        return no_keep


class _ProposalTargetLayer(nn.Module):
    """
    Assign object detection proposals to ground-truth targets. Produces proposal
    classification labels and bounding-box regression targets.
    """

    def __init__(self, nclasses):
        super(_ProposalTargetLayer, self).__init__()
        self._num_classes = nclasses
        self.TWIN_NORMALIZE_MEANS = torch.FloatTensor(cfg.TRAIN.TWIN_NORMALIZE_MEANS)
        self.TWIN_NORMALIZE_STDS = torch.FloatTensor(cfg.TRAIN.TWIN_NORMALIZE_STDS)
        self.TWIN_INSIDE_WEIGHTS = torch.FloatTensor(cfg.TRAIN.TWIN_INSIDE_WEIGHTS)

    def forward(self, all_rois, gt_twins):
        self.TWIN_NORMALIZE_MEANS = self.TWIN_NORMALIZE_MEANS.type_as(gt_twins)
        self.TWIN_NORMALIZE_STDS = self.TWIN_NORMALIZE_STDS.type_as(gt_twins)
        self.TWIN_INSIDE_WEIGHTS = self.TWIN_INSIDE_WEIGHTS.type_as(gt_twins)
        gt_twins_append = gt_twins.new(gt_twins.size()).zero_()
        gt_twins_append[:, :, 1:3] = gt_twins[:, :, :2]
        all_rois = torch.cat([all_rois, gt_twins_append], 1)
        num_videos = 1
        rois_per_video = int(cfg.TRAIN.BATCH_SIZE / num_videos)
        fg_rois_per_video = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_video))
        fg_rois_per_video = 1 if fg_rois_per_video == 0 else fg_rois_per_video
        labels, rois, twin_targets, twin_inside_weights = self._sample_rois_pytorch(all_rois, gt_twins, fg_rois_per_video, rois_per_video, self._num_classes)
        twin_outside_weights = (twin_inside_weights > 0).float()
        return rois, labels, twin_targets, twin_inside_weights, twin_outside_weights

    def backward(self, top, propagate_down, bottom):
        """This layer does not propagate gradients."""
        pass

    def reshape(self, bottom, top):
        """Reshaping happens during the call to forward."""
        pass

    def _get_twin_regression_labels_pytorch(self, twin_target_data, labels_batch, num_classes):
        """Bounding-box regression targets (twin_target_data) are stored in a
        compact form b x N x (tx, tl)

        This function expands those targets into the 2-of-2*K representation used
        by the network (i.e. only one class has non-zero targets).

        Returns:
            twin_target (ndarray): b x N x 2K blob of regression targets
            twin_inside_weights (ndarray): b x N x 2K blob of loss weights
        """
        batch_size = labels_batch.size(0)
        rois_per_video = labels_batch.size(1)
        clss = labels_batch
        twin_targets = twin_target_data.new(batch_size, rois_per_video, 2).zero_()
        twin_inside_weights = twin_target_data.new(twin_targets.size()).zero_()
        for b in range(batch_size):
            if clss[b].sum() == 0:
                continue
            inds = torch.nonzero(clss[b] > 0).view(-1)
            for i in range(inds.numel()):
                ind = inds[i]
                twin_targets[(b), (ind), :] = twin_target_data[(b), (ind), :]
                twin_inside_weights[(b), (ind), :] = self.TWIN_INSIDE_WEIGHTS
        return twin_targets, twin_inside_weights

    def _compute_targets_pytorch(self, ex_rois, gt_rois):
        """Compute bounding-box regression targets for an video."""
        assert ex_rois.size(1) == gt_rois.size(1)
        assert ex_rois.size(2) == 2
        assert gt_rois.size(2) == 2
        batch_size = ex_rois.size(0)
        rois_per_video = ex_rois.size(1)
        targets = twin_transform_batch(ex_rois, gt_rois)
        if cfg.TRAIN.TWIN_NORMALIZE_TARGETS_PRECOMPUTED:
            targets = (targets - self.TWIN_NORMALIZE_MEANS.expand_as(targets)) / self.TWIN_NORMALIZE_STDS.expand_as(targets)
        return targets

    def _sample_rois_pytorch(self, all_rois, gt_twins, fg_rois_per_video, rois_per_video, num_classes):
        """Generate a random sample of RoIs comprising foreground and background
        examples.
        """
        overlaps = twins_overlaps_batch(all_rois, gt_twins)
        max_overlaps, gt_assignment = torch.max(overlaps, 2)
        batch_size = overlaps.size(0)
        num_proposal = overlaps.size(1)
        num_twins_per_video = overlaps.size(2)
        offset = torch.arange(0, batch_size) * gt_twins.size(1)
        offset = offset.view(-1, 1).type_as(gt_assignment) + gt_assignment
        labels = gt_twins[:, :, (2)].contiguous().view(-1)[offset.view(-1)].view(batch_size, -1)
        labels_batch = labels.new(batch_size, rois_per_video).zero_()
        rois_batch = all_rois.new(batch_size, rois_per_video, 3).zero_()
        gt_rois_batch = all_rois.new(batch_size, rois_per_video, 3).zero_()
        for i in range(batch_size):
            fg_inds = torch.nonzero(max_overlaps[i] >= cfg.TRAIN.FG_THRESH).view(-1)
            fg_num_rois = fg_inds.numel()
            bg_inds = torch.nonzero((max_overlaps[i] < cfg.TRAIN.BG_THRESH_HI) & (max_overlaps[i] >= cfg.TRAIN.BG_THRESH_LO)).view(-1)
            bg_num_rois = bg_inds.numel()
            if DEBUG:
                None
            if fg_num_rois > 0 and bg_num_rois > 0:
                fg_rois_per_this_video = min(fg_rois_per_video, fg_num_rois)
                rand_num = torch.from_numpy(np.random.permutation(fg_num_rois)).type_as(gt_twins).long()
                fg_inds = fg_inds[rand_num[:fg_rois_per_this_video]]
                bg_rois_per_this_video = rois_per_video - fg_rois_per_this_video
                rand_num = np.floor(np.random.rand(bg_rois_per_this_video) * bg_num_rois)
                rand_num = torch.from_numpy(rand_num).type_as(gt_twins).long()
                bg_inds = bg_inds[rand_num]
            elif fg_num_rois > 0 and bg_num_rois == 0:
                rand_num = np.floor(np.random.rand(rois_per_video) * fg_num_rois)
                rand_num = torch.from_numpy(rand_num).type_as(gt_twins).long()
                fg_inds = fg_inds[rand_num]
                fg_rois_per_this_video = rois_per_video
                bg_rois_per_this_video = 0
            elif bg_num_rois > 0 and fg_num_rois == 0:
                rand_num = np.floor(np.random.rand(rois_per_video) * bg_num_rois)
                rand_num = torch.from_numpy(rand_num).type_as(gt_twins).long()
                bg_inds = bg_inds[rand_num]
                bg_rois_per_this_video = rois_per_video
                fg_rois_per_this_video = 0
            else:
                raise ValueError('bg_num_rois = 0 and fg_num_rois = 0, this should not happen!')
            keep_inds = torch.cat([fg_inds, bg_inds], 0)
            labels_batch[i].copy_(labels[i][keep_inds])
            if fg_rois_per_this_video < rois_per_video:
                labels_batch[i][fg_rois_per_this_video:] = 0
            rois_batch[i] = all_rois[i][keep_inds]
            rois_batch[(i), :, (0)] = i
            gt_rois_batch[i] = gt_twins[i][gt_assignment[i][keep_inds]]
        twin_target_data = self._compute_targets_pytorch(rois_batch[:, :, 1:3], gt_rois_batch[:, :, :2])
        twin_targets, twin_inside_weights = self._get_twin_regression_labels_pytorch(twin_target_data, labels_batch, num_classes)
        return labels_batch, rois_batch, twin_targets, twin_inside_weights


def _smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):
    sigma_2 = sigma ** 2
    box_diff = bbox_pred - bbox_targets
    in_box_diff = bbox_inside_weights * box_diff
    abs_in_box_diff = torch.abs(in_box_diff)
    smoothL1_sign = (abs_in_box_diff < 1.0 / sigma_2).detach().float()
    in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.0) * smoothL1_sign + (abs_in_box_diff - 0.5 / sigma_2) * (1.0 - smoothL1_sign)
    out_loss_box = bbox_outside_weights * in_loss_box
    loss_box = out_loss_box
    for i in sorted(dim, reverse=True):
        loss_box = loss_box.sum(i)
    loss_box = loss_box.mean()
    return loss_box


class _RPN(nn.Module):
    """ region proposal network """

    def __init__(self, din, out_scores=False):
        super(_RPN, self).__init__()
        self.din = din
        self.anchor_scales = cfg.ANCHOR_SCALES
        self.feat_stride = cfg.FEAT_STRIDE[0]
        self.out_scores = out_scores
        self.mask_upsample_rate = 1
        self.RPN_Conv1 = nn.Conv3d(self.din, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=True)
        self.RPN_Conv2 = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=True)
        self.RPN_output_pool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))
        self.nc_score_out = len(self.anchor_scales) * 2
        self.RPN_cls_score = nn.Conv3d(512, self.nc_score_out, 1, 1, 0)
        self.nc_twin_out = len(self.anchor_scales) * 2
        self.RPN_twin_pred = nn.Conv3d(512, self.nc_twin_out, 1, 1, 0)
        self.RPN_proposal = _ProposalLayer(self.feat_stride, self.anchor_scales, self.out_scores)
        self.RPN_anchor_target = _AnchorTargetLayer(self.feat_stride, self.anchor_scales)
        self.rpn_loss_cls = 0
        self.rpn_loss_twin = 0
        self.rpn_loss_mask = 0

    @staticmethod
    def reshape(x, d):
        input_shape = x.size()
        x = x.view(input_shape[0], int(d), int(float(input_shape[1] * input_shape[2]) / float(d)), input_shape[3], input_shape[4])
        return x

    def forward(self, base_feat, gt_twins):
        batch_size = base_feat.size(0)
        rpn_conv1 = F.relu(self.RPN_Conv1(base_feat), inplace=True)
        rpn_conv2 = F.relu(self.RPN_Conv2(rpn_conv1), inplace=True)
        rpn_output_pool = self.RPN_output_pool(rpn_conv2)
        rpn_cls_score = self.RPN_cls_score(rpn_output_pool)
        rpn_cls_score_reshape = self.reshape(rpn_cls_score, 2)
        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, dim=1)
        rpn_cls_prob = self.reshape(rpn_cls_prob_reshape, self.nc_score_out)
        rpn_twin_pred = self.RPN_twin_pred(rpn_output_pool)
        cfg_key = 'TRAIN' if self.training else 'TEST'
        if self.out_scores:
            rois, rois_score = self.RPN_proposal((rpn_cls_prob.data, rpn_twin_pred.data, cfg_key))
        else:
            rois = self.RPN_proposal((rpn_cls_prob.data, rpn_twin_pred.data, cfg_key))
        self.rpn_loss_cls = 0
        self.rpn_loss_twin = 0
        self.rpn_loss_mask = 0
        self.rpn_label = None
        if self.training:
            assert gt_twins is not None
            rpn_data = self.RPN_anchor_target((rpn_cls_score.data, gt_twins))
            rpn_cls_score = rpn_cls_score_reshape.permute(0, 2, 3, 4, 1).contiguous().view(batch_size, -1, 2)
            self.rpn_label = rpn_data[0].view(batch_size, -1)
            rpn_keep = Variable(self.rpn_label.view(-1).ne(-1).nonzero().view(-1))
            rpn_cls_score = torch.index_select(rpn_cls_score.view(-1, 2), 0, rpn_keep)
            self.rpn_label = torch.index_select(self.rpn_label.view(-1), 0, rpn_keep.data)
            self.rpn_label = Variable(self.rpn_label.long())
            self.rpn_loss_cls = F.cross_entropy(rpn_cls_score, self.rpn_label)
            fg_cnt = torch.sum(self.rpn_label.data.ne(0))
            rpn_twin_targets, rpn_twin_inside_weights, rpn_twin_outside_weights = rpn_data[1:]
            rpn_twin_inside_weights = Variable(rpn_twin_inside_weights)
            rpn_twin_outside_weights = Variable(rpn_twin_outside_weights)
            rpn_twin_targets = Variable(rpn_twin_targets)
            self.rpn_loss_twin = _smooth_l1_loss(rpn_twin_pred, rpn_twin_targets, rpn_twin_inside_weights, rpn_twin_outside_weights, sigma=3, dim=[1, 2, 3, 4])
        if self.out_scores:
            return rois, rois_score, rpn_cls_prob, rpn_twin_pred, self.rpn_loss_cls, self.rpn_loss_twin, self.rpn_label, self.rpn_loss_mask
        else:
            return rois, rpn_cls_prob, rpn_twin_pred, self.rpn_loss_cls, self.rpn_loss_twin, self.rpn_label, self.rpn_loss_mask

    def init_weights(self):

        def normal_init(m, mean, stddev, truncated=False):
            """
            weight initalizer: truncated normal and random normal.
            """
            if truncated:
                m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)
            else:
                m.weight.data.normal_(mean, stddev)
                m.bias.data.zero_()
        normal_init(self.RPN_Conv1, 0, 0.01, cfg.TRAIN.TRUNCATED)
        normal_init(self.RPN_Conv2, 0, 0.01, cfg.TRAIN.TRUNCATED)
        normal_init(self.RPN_cls_score, 0, 0.01, cfg.TRAIN.TRUNCATED)
        normal_init(self.RPN_twin_pred, 0, 0.01, cfg.TRAIN.TRUNCATED)

    def create_architecture(self):
        self._init_modules()
        self.init_weights()

    def generate_mask_label(self, gt_twins, feat_len):
        """ 
        gt_twins will be (batch_size, n, 3), where each gt will be (x1, x2, class_id)
        # feat_len is the length of mask-task features, self.feat_stride * feat_len = video_len
        # according: self.feat_stride, and upsample_rate
        # mask will be (batch_size, feat_len), -1 -- ignore, 1 -- fg, 0 -- bg
        """
        batch_size = gt_twins.size(0)
        mask_label = torch.zeros(batch_size, feat_len).type_as(gt_twins)
        for b in range(batch_size):
            single_gt_twins = gt_twins[b]
            single_gt_twins[:, :2] = (single_gt_twins[:, :2] / self.feat_stride).int()
            twins_start = single_gt_twins[:, (0)]
            _, indices = torch.sort(twins_start)
            single_gt_twins = torch.index_select(single_gt_twins, 0, indices).long().cpu().numpy()
            starts = np.minimum(np.maximum(0, single_gt_twins[:, (0)]), feat_len - 1)
            ends = np.minimum(np.maximum(0, single_gt_twins[:, (1)]), feat_len)
            for x in zip(starts, ends):
                mask_label[(b), x[0]:x[1] + 1] = 1
        return mask_label


def make_layers(cfg, batch_norm=False):
    layers = []
    in_channels = 3
    maxpool_count = 0
    for v in cfg:
        if v == 'M':
            maxpool_count += 1
            if maxpool_count == 1:
                layers += [nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))]
            elif maxpool_count == 5:
                layers += [nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))]
            else:
                layers += [nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))]
        else:
            conv3d = nn.Conv3d(in_channels, v, kernel_size=(3, 3, 3), padding=(1, 1, 1))
            if batch_norm:
                layers += [conv3d, nn.BatchNorm3d(v), nn.ReLU(inplace=True)]
            else:
                layers += [conv3d, nn.ReLU(inplace=True)]
            in_channels = v
    return nn.Sequential(*layers)


class C3D(nn.Module):
    """
    The C3D network as described in [1].
        References
        ----------
       [1] Tran, Du, et al. "Learning spatiotemporal features with 3d convolutional networks."
       Proceedings of the IEEE international conference on computer vision. 2015.
    """

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2.0 / n))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm3d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                m.bias.data.zero_()

    def __init__(self):
        super(C3D, self).__init__()
        self.features = make_layers(cfg['A'], batch_norm=False)
        self.classifier = nn.Sequential(nn.Linear(512 * 1 * 4 * 4, 4096), nn.ReLU(True), nn.Dropout(inplace=False), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(inplace=False), nn.Linear(4096, 487))
        self._initialize_weights()

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x


class BasicConv3d(nn.Module):

    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):
        super(BasicConv3d, self).__init__()
        self.conv = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
        self.bn = nn.BatchNorm3d(out_planes, eps=0.001, momentum=0.001, affine=True)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


class Mixed_3b(nn.Module):

    def __init__(self):
        super(Mixed_3b, self).__init__()
        self.branch0 = nn.Sequential(BasicConv3d(192, 64, kernel_size=1, stride=1))
        self.branch1 = nn.Sequential(BasicConv3d(192, 96, kernel_size=1, stride=1), BasicConv3d(96, 128, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv3d(192, 16, kernel_size=1, stride=1), BasicConv3d(16, 32, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=1, padding=1), BasicConv3d(192, 32, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Mixed_3c(nn.Module):

    def __init__(self):
        super(Mixed_3c, self).__init__()
        self.branch0 = nn.Sequential(BasicConv3d(256, 128, kernel_size=1, stride=1))
        self.branch1 = nn.Sequential(BasicConv3d(256, 128, kernel_size=1, stride=1), BasicConv3d(128, 192, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv3d(256, 32, kernel_size=1, stride=1), BasicConv3d(32, 96, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=1, padding=1), BasicConv3d(256, 64, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Mixed_4b(nn.Module):

    def __init__(self):
        super(Mixed_4b, self).__init__()
        self.branch0 = nn.Sequential(BasicConv3d(480, 192, kernel_size=1, stride=1))
        self.branch1 = nn.Sequential(BasicConv3d(480, 96, kernel_size=1, stride=1), BasicConv3d(96, 208, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv3d(480, 16, kernel_size=1, stride=1), BasicConv3d(16, 48, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=1, padding=1), BasicConv3d(480, 64, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Mixed_4c(nn.Module):

    def __init__(self):
        super(Mixed_4c, self).__init__()
        self.branch0 = nn.Sequential(BasicConv3d(512, 160, kernel_size=1, stride=1))
        self.branch1 = nn.Sequential(BasicConv3d(512, 112, kernel_size=1, stride=1), BasicConv3d(112, 224, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv3d(512, 24, kernel_size=1, stride=1), BasicConv3d(24, 64, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=1, padding=1), BasicConv3d(512, 64, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Mixed_4d(nn.Module):

    def __init__(self):
        super(Mixed_4d, self).__init__()
        self.branch0 = nn.Sequential(BasicConv3d(512, 128, kernel_size=1, stride=1))
        self.branch1 = nn.Sequential(BasicConv3d(512, 128, kernel_size=1, stride=1), BasicConv3d(128, 256, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv3d(512, 24, kernel_size=1, stride=1), BasicConv3d(24, 64, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=1, padding=1), BasicConv3d(512, 64, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Mixed_4e(nn.Module):

    def __init__(self):
        super(Mixed_4e, self).__init__()
        self.branch0 = nn.Sequential(BasicConv3d(512, 112, kernel_size=1, stride=1))
        self.branch1 = nn.Sequential(BasicConv3d(512, 144, kernel_size=1, stride=1), BasicConv3d(144, 288, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv3d(512, 32, kernel_size=1, stride=1), BasicConv3d(32, 64, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=1, padding=1), BasicConv3d(512, 64, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Mixed_4f(nn.Module):

    def __init__(self):
        super(Mixed_4f, self).__init__()
        self.branch0 = nn.Sequential(BasicConv3d(528, 256, kernel_size=1, stride=1))
        self.branch1 = nn.Sequential(BasicConv3d(528, 160, kernel_size=1, stride=1), BasicConv3d(160, 320, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv3d(528, 32, kernel_size=1, stride=1), BasicConv3d(32, 128, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=1, padding=1), BasicConv3d(528, 128, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class BasicConv2d(nn.Module):

    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
        self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


class Mixed_5b(nn.Module):

    def __init__(self):
        super(Mixed_5b, self).__init__()
        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)
        self.branch1 = nn.Sequential(BasicConv2d(192, 48, kernel_size=1, stride=1), BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2))
        self.branch2 = nn.Sequential(BasicConv2d(192, 64, kernel_size=1, stride=1), BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1), BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False), BasicConv2d(192, 64, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Mixed_5c(nn.Module):

    def __init__(self):
        super(Mixed_5c, self).__init__()
        self.branch0 = nn.Sequential(BasicConv3d(832, 384, kernel_size=1, stride=1))
        self.branch1 = nn.Sequential(BasicConv3d(832, 192, kernel_size=1, stride=1), BasicConv3d(192, 384, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv3d(832, 48, kernel_size=1, stride=1), BasicConv3d(48, 128, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=1, padding=1), BasicConv3d(832, 128, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class I3D(nn.Module):

    def __init__(self, num_classes=400, dropout_keep_prob=1, input_channel=3, spatial_squeeze=True):
        super(I3D, self).__init__()
        self.features = nn.Sequential(BasicConv3d(input_channel, 64, kernel_size=7, stride=2, padding=3), nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)), BasicConv3d(64, 64, kernel_size=1, stride=1), BasicConv3d(64, 192, kernel_size=3, stride=1, padding=1), nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)), Mixed_3b(), Mixed_3c(), nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)), Mixed_4b(), Mixed_4c(), Mixed_4d(), Mixed_4e(), Mixed_4f(), nn.MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=(0, 0, 0)), Mixed_5b(), Mixed_5c(), nn.AvgPool3d(kernel_size=(2, 7, 7), stride=1), nn.Dropout3d(dropout_keep_prob), nn.Conv3d(1024, num_classes, kernel_size=1, stride=1, bias=True))
        self.spatial_squeeze = spatial_squeeze
        self.softmax = nn.Softmax()

    def forward(self, x):
        logits = self.features(x)
        if self.spatial_squeeze:
            logits = logits.squeeze(3)
            logits = logits.squeeze(3)
        averaged_logits = torch.mean(logits, 2)
        predictions = self.softmax(averaged_logits)
        return predictions, averaged_logits


def conv3x3x3(in_planes, out_planes, stride=1):
    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm3d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3x3(planes, planes)
        self.bn2 = nn.BatchNorm3d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm3d(planes)
        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm3d(planes)
        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm3d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out


def downsample_basic_block(x, planes, stride):
    out = F.avg_pool3d(x, kernel_size=1, stride=stride)
    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)).zero_()
    if isinstance(out.data, torch.FloatTensor):
        zero_pads = zero_pads
    out = Variable(torch.cat([out.data, zero_pads], dim=1))
    return out


class ResNet(nn.Module):

    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type='B', num_classes=400):
        self.inplanes = 64
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)
        self.bn1 = nn.BatchNorm3d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)
        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)
        last_duration = int(math.ceil(sample_duration / 16))
        last_size = int(math.ceil(sample_size / 32))
        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)
        self.fc = nn.Linear(512 * block.expansion, num_classes)
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')
            elif isinstance(m, nn.BatchNorm3d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            if shortcut_type == 'A':
                downsample = partial(downsample_basic_block, planes=planes * block.expansion, stride=stride)
            else:
                downsample = nn.Sequential(nn.Conv3d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm3d(planes * block.expansion))
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


class _NonLocalBlockND(nn.Module):

    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):
        super(_NonLocalBlockND, self).__init__()
        assert dimension in [1, 2, 3]
        self.dimension = dimension
        self.sub_sample = sub_sample
        self.in_channels = in_channels
        self.inter_channels = inter_channels
        if self.inter_channels is None:
            self.inter_channels = in_channels // 2
            if self.inter_channels == 0:
                self.inter_channels = 1
        if dimension == 3:
            conv_nd = nn.Conv3d
            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))
            bn = nn.BatchNorm3d
        elif dimension == 2:
            conv_nd = nn.Conv2d
            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))
            bn = nn.BatchNorm2d
        else:
            conv_nd = nn.Conv1d
            max_pool_layer = nn.MaxPool1d(kernel_size=2)
            bn = nn.BatchNorm1d
        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)
        if bn_layer:
            self.W = nn.Sequential(conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0), bn(self.in_channels))
            nn.init.constant_(self.W[1].weight, 0)
            nn.init.constant_(self.W[1].bias, 0)
        else:
            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0)
            nn.init.constant_(self.W.weight, 0)
            nn.init.constant_(self.W.bias, 0)
        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)
        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)
        if sub_sample:
            self.g = nn.Sequential(self.g, max_pool_layer)
            self.phi = nn.Sequential(self.phi, max_pool_layer)

    def forward(self, x):
        """
        :param x: (b, c, t, h, w)
        :return:
        """
        batch_size = x.size(0)
        g_x = self.g(x).view(batch_size, self.inter_channels, -1)
        g_x = g_x.permute(0, 2, 1)
        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)
        theta_x = theta_x.permute(0, 2, 1)
        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)
        f = torch.matmul(theta_x, phi_x)
        N = f.size(-1)
        f_div_C = f / N
        y = torch.matmul(f_div_C, g_x)
        y = y.permute(0, 2, 1).contiguous()
        y = y.view(batch_size, self.inter_channels, *x.size()[2:])
        W_y = self.W(y)
        z = W_y + x
        return z


class NONLocalBlock3D(_NonLocalBlockND):

    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):
        super(NONLocalBlock3D, self).__init__(in_channels, inter_channels=inter_channels, dimension=3, sub_sample=sub_sample, bn_layer=bn_layer)


class _TDCNN(nn.Module):
    """ faster RCNN """

    def __init__(self):
        super(_TDCNN, self).__init__()
        self.n_classes = cfg.NUM_CLASSES
        self.RCNN_loss_cls = 0
        self.RCNN_loss_twin = 0
        self.RCNN_rpn = _RPN(self.dout_base_model)
        self.RCNN_proposal_target = _ProposalTargetLayer(self.n_classes)
        self.RCNN_roi_temporal_pool = _RoITemporalPooling(cfg.POOLING_LENGTH, cfg.POOLING_HEIGHT, cfg.POOLING_WIDTH, cfg.DEDUP_TWINS)
        if cfg.USE_ATTENTION:
            self.RCNN_attention = NONLocalBlock3D(self.dout_base_model, inter_channels=self.dout_base_model)

    def prepare_data(self, video_data):
        return video_data

    def forward(self, video_data, gt_twins):
        batch_size = video_data.size(0)
        gt_twins = gt_twins.data
        video_data = self.prepare_data(video_data)
        base_feat = self.RCNN_base(video_data)
        rois, _, _, rpn_loss_cls, rpn_loss_twin, _, _ = self.RCNN_rpn(base_feat, gt_twins)
        if self.training:
            roi_data = self.RCNN_proposal_target(rois, gt_twins)
            rois, rois_label, rois_target, rois_inside_ws, rois_outside_ws = roi_data
            rois_label = Variable(rois_label.view(-1).long())
            rois_target = Variable(rois_target.view(-1, rois_target.size(2)))
            rois_inside_ws = Variable(rois_inside_ws.view(-1, rois_inside_ws.size(2)))
            rois_outside_ws = Variable(rois_outside_ws.view(-1, rois_outside_ws.size(2)))
        else:
            rois_label = None
            rois_target = None
            rois_inside_ws = None
            rois_outside_ws = None
            rpn_loss_cls = 0
            rpn_loss_twin = 0
        rois = Variable(rois)
        if cfg.POOLING_MODE == 'pool':
            pooled_feat = self.RCNN_roi_temporal_pool(base_feat, rois.view(-1, 3))
        if cfg.USE_ATTENTION:
            pooled_feat = self.RCNN_attention(pooled_feat)
        pooled_feat = self._head_to_tail(pooled_feat)
        twin_pred = self.RCNN_twin_pred(pooled_feat)
        if self.training:
            twin_pred_view = twin_pred.view(twin_pred.size(0), int(twin_pred.size(1) / 2), 2)
            twin_pred_select = torch.gather(twin_pred_view, 1, rois_label.view(rois_label.size(0), 1, 1).expand(rois_label.size(0), 1, 2))
            twin_pred = twin_pred_select.squeeze(1)
        cls_score = self.RCNN_cls_score(pooled_feat)
        cls_prob = F.softmax(cls_score, dim=1)
        if DEBUG:
            None
            None
            None
            None
            None
        RCNN_loss_cls = 0
        RCNN_loss_twin = 0
        if self.training:
            RCNN_loss_cls = F.cross_entropy(cls_score, rois_label)
            RCNN_loss_twin = _smooth_l1_loss(twin_pred, rois_target, rois_inside_ws, rois_outside_ws)
            rpn_loss_cls = torch.unsqueeze(rpn_loss_cls, 0)
            rpn_loss_twin = torch.unsqueeze(rpn_loss_twin, 0)
            RCNN_loss_cls = torch.unsqueeze(RCNN_loss_cls, 0)
            RCNN_loss_twin = torch.unsqueeze(RCNN_loss_twin, 0)
        cls_prob = cls_prob.view(batch_size, rois.size(1), -1)
        twin_pred = twin_pred.view(batch_size, rois.size(1), -1)
        if self.training:
            return rois, cls_prob, twin_pred, rpn_loss_cls, rpn_loss_twin, RCNN_loss_cls, RCNN_loss_twin, rois_label
        else:
            return rois, cls_prob, twin_pred

    def _init_weights(self):

        def normal_init(m, mean, stddev, truncated=False):
            """
            weight initalizer: truncated normal and random normal.
            """
            if truncated:
                m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)
            else:
                m.weight.data.normal_(mean, stddev)
                m.bias.data.zero_()
        self.RCNN_rpn.init_weights()
        normal_init(self.RCNN_cls_score, 0, 0.01, cfg.TRAIN.TRUNCATED)
        normal_init(self.RCNN_twin_pred, 0, 0.001, cfg.TRAIN.TRUNCATED)

    def create_architecture(self):
        self._init_modules()
        self._init_weights()


class NONLocalBlock1D(_NonLocalBlockND):

    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):
        super(NONLocalBlock1D, self).__init__(in_channels, inter_channels=inter_channels, dimension=1, sub_sample=sub_sample, bn_layer=bn_layer)


class NONLocalBlock2D(_NonLocalBlockND):

    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):
        super(NONLocalBlock2D, self).__init__(in_channels, inter_channels=inter_channels, dimension=2, sub_sample=sub_sample, bn_layer=bn_layer)


class test_PSRoIPooling(Module):

    def __init__(self, output_dim, group_size, spatial_scale):
        super(test_PSRoIPooling, self).__init__()
        self.psroi_pool = _PSRoIPooling(output_dim, group_size, spatial_scale)

    def forward(self, feature, rois):
        return self.psroi_pool(feature, rois)


LAYER_BUILDER_DICT = dict()


def parse_expr(expr):
    parts = expr.split('<=')
    return parts[0].split(','), parts[1], parts[2].split(',')


def get_basic_layer(info, channels=None, conv_bias=False):
    id = info['id']
    attr = info['attrs'] if 'attrs' in info else list()
    out, op, in_vars = parse_expr(info['expr'])
    assert len(out) == 1
    assert len(in_vars) == 1
    mod, out_channel = LAYER_BUILDER_DICT[op](attr, channels, conv_bias)
    return id, out[0], mod, out_channel, in_vars[0]


class C3DRes18(nn.Module):

    def __init__(self, model_path='tf_model_zoo/C3DRes18/C3DRes18.yaml', num_classes=101, num_segments=4, pretrained_parts='both'):
        super(C3DRes18, self).__init__()
        self.num_segments = num_segments
        self.pretrained_parts = pretrained_parts
        manifest = yaml.load(open(model_path))
        layers = manifest['layers']
        self._channel_dict = dict()
        self._op_list = list()
        for l in layers:
            out_var, op, in_var = parse_expr(l['expr'])
            if op != 'Concat' and op != 'Eltwise':
                id, out_name, module, out_channel, in_name = get_basic_layer(l, 3 if len(self._channel_dict) == 0 else self._channel_dict[in_var[0]], conv_bias=True if op == 'Conv3d' else True, num_segments=num_segments)
                self._channel_dict[out_name] = out_channel
                setattr(self, id, module)
                self._op_list.append((id, op, out_name, in_name))
            elif op == 'Concat':
                self._op_list.append((id, op, out_var[0], in_var))
                channel = sum([self._channel_dict[x] for x in in_var])
                self._channel_dict[out_var[0]] = channel
            else:
                self._op_list.append((id, op, out_var[0], in_var))
                channel = self._channel_dict[in_var[0]]
                self._channel_dict[out_var[0]] = channel

    def forward(self, input):
        data_dict = dict()
        data_dict[self._op_list[0][-1]] = input

        def get_hook(name):

            def hook(m, grad_in, grad_out):
                None
            return hook
        for op in self._op_list:
            if op[1] != 'Concat' and op[1] != 'InnerProduct' and op[1] != 'Eltwise':
                if op[0] == 'adasa':
                    inception_3c_output = data_dict['inception_3c_double_3x3_1_bn']
                    inception_3c_transpose_output = torch.transpose(inception_3c_output.view((-1, self.num_segments) + inception_3c_output.size()[1:]), 1, 2)
                    data_dict[op[2]] = getattr(self, op[0])(inception_3c_transpose_output)
                else:
                    data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])
            elif op[1] == 'InnerProduct':
                x = data_dict[op[-1]]
                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))
            elif op[1] == 'Eltwise':
                try:
                    data_dict[op[2]] = torch.add(data_dict[op[-1][0]], 1, data_dict[op[-1][1]])
                except:
                    for x in op[-1]:
                        None
                    raise
            else:
                try:
                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)
                except:
                    for x in op[-1]:
                        None
                    raise
        return data_dict[self._op_list[-1][2]]


class Identity(nn.Module):

    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, input):
        return input


class ECO(nn.Module):

    def __init__(self, model_path='tf_model_zoo/ECO/ECO.yaml', num_classes=101, num_segments=4, pretrained_parts='both'):
        super(ECO, self).__init__()
        self.num_segments = num_segments
        self.pretrained_parts = pretrained_parts
        manifest = yaml.load(open(model_path))
        layers = manifest['layers']
        self._channel_dict = dict()
        self._op_list = list()
        for l in layers:
            out_var, op, in_var = parse_expr(l['expr'])
            if len(self._channel_dict) == 0:
                if op == 'Identity':
                    in_channel = 256
                else:
                    in_channel = 3
            else:
                in_channel = self._channel_dict[in_var[0]]
            if op != 'Concat' and op != 'Eltwise':
                id, out_name, module, out_channel, in_name = get_basic_layer(l, in_channel, conv_bias=False if op == 'Conv3d' else True, num_segments=num_segments)
                self._channel_dict[out_name] = out_channel
                setattr(self, id, module)
                self._op_list.append((id, op, out_name, in_name))
            elif op == 'Concat':
                self._op_list.append((id, op, out_var[0], in_var))
                channel = sum([self._channel_dict[x] for x in in_var])
                self._channel_dict[out_var[0]] = channel
            else:
                self._op_list.append((id, op, out_var[0], in_var))
                channel = self._channel_dict[in_var[0]]
                self._channel_dict[out_var[0]] = channel

    def forward(self, input):
        data_dict = dict()
        data_dict[self._op_list[0][-1]] = input

        def get_hook(name):

            def hook(m, grad_in, grad_out):
                None
            return hook
        for op in self._op_list:
            if op[1] != 'Concat' and op[1] != 'InnerProduct' and op[1] != 'Eltwise':
                if op[0] == 'res3a_2':
                    inception_3c_output = data_dict['inception_3c_double_3x3_1_bn']
                    inception_3c_transpose_output = torch.transpose(inception_3c_output.view((-1, self.num_segments) + inception_3c_output.size()[1:]), 1, 2)
                    data_dict[op[2]] = getattr(self, op[0])(inception_3c_transpose_output)
                else:
                    data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])
            elif op[1] == 'InnerProduct':
                x = data_dict[op[-1]]
                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))
            elif op[1] == 'Eltwise':
                try:
                    data_dict[op[2]] = torch.add(data_dict[op[-1][0]], 1, data_dict[op[-1][1]])
                except:
                    for x in op[-1]:
                        None
                    raise
            else:
                try:
                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)
                except:
                    for x in op[-1]:
                        None
                    raise
        return data_dict[self._op_list[-1][2]]


class ECOfull(nn.Module):

    def __init__(self, model_path='tf_model_zoo/ECOfull/ECOfull.yaml', num_classes=101, num_segments=4, pretrained_parts='both'):
        super(ECOfull, self).__init__()
        self.num_segments = num_segments
        self.pretrained_parts = pretrained_parts
        manifest = yaml.load(open(model_path))
        layers = manifest['layers']
        self._channel_dict = dict()
        self._op_list = list()
        for l in layers:
            out_var, op, in_var = parse_expr(l['expr'])
            if op != 'Concat' and op != 'Eltwise':
                id, out_name, module, out_channel, in_name = get_basic_layer(l, 3 if len(self._channel_dict) == 0 else self._channel_dict[in_var[0]], conv_bias=True if op == 'Conv3d' else True, num_segments=num_segments)
                self._channel_dict[out_name] = out_channel
                setattr(self, id, module)
                self._op_list.append((id, op, out_name, in_name))
            elif op == 'Concat':
                self._op_list.append((id, op, out_var[0], in_var))
                channel = sum([self._channel_dict[x] for x in in_var])
                self._channel_dict[out_var[0]] = channel
            else:
                self._op_list.append((id, op, out_var[0], in_var))
                channel = self._channel_dict[in_var[0]]
                self._channel_dict[out_var[0]] = channel

    def forward(self, input):
        data_dict = dict()
        data_dict[self._op_list[0][-1]] = input

        def get_hook(name):

            def hook(m, grad_in, grad_out):
                None
            return hook
        for op in self._op_list:
            if op[1] != 'Concat' and op[1] != 'InnerProduct' and op[1] != 'Eltwise':
                if op[0] == 'res3a_2' or op[0] == 'global_pool2D_reshape_consensus':
                    layer_output = data_dict[op[-1]]
                    layer_transpose_output = torch.transpose(layer_output.view((-1, self.num_segments) + layer_output.size()[1:]), 1, 2)
                    data_dict[op[2]] = getattr(self, op[0])(layer_transpose_output)
                else:
                    data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])
            elif op[1] == 'InnerProduct':
                x = data_dict[op[-1]]
                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))
            elif op[1] == 'Eltwise':
                try:
                    data_dict[op[2]] = torch.add(data_dict[op[-1][0]], 1, data_dict[op[-1][1]])
                except:
                    for x in op[-1]:
                        None
                    raise
            else:
                try:
                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)
                except:
                    for x in op[-1]:
                        None
                    raise
        return data_dict[self._op_list[-1][2]]


class BNInception(nn.Module):

    def __init__(self, model_path='tf_model_zoo/bninception/bn_inception.yaml', num_classes=101, weight_url='https://yjxiong.blob.core.windows.net/models/bn_inception-9f5701afb96c8044.pth', pretrained=True):
        super(BNInception, self).__init__()
        manifest = yaml.load(open(model_path))
        layers = manifest['layers']
        self._channel_dict = dict()
        self._op_list = list()
        for l in layers:
            out_var, op, in_var = parse_expr(l['expr'])
            if op != 'Concat':
                id, out_name, module, out_channel, in_name = get_basic_layer(l, 3 if len(self._channel_dict) == 0 else self._channel_dict[in_var[0]], conv_bias=True)
                self._channel_dict[out_name] = out_channel
                setattr(self, id, module)
                self._op_list.append((id, op, out_name, in_name))
            else:
                self._op_list.append((id, op, out_var[0], in_var))
                channel = sum([self._channel_dict[x] for x in in_var])
                self._channel_dict[out_var[0]] = channel
        if pretrained:
            pretrained_model = torch.utils.model_zoo.load_url(weight_url)
            if 'state_dict' in pretrained_model.keys():
                pretrained_model = pretrained_model['state_dict']
            self.load_state_dict(pretrained_model)

    def forward(self, input):
        data_dict = dict()
        data_dict[self._op_list[0][-1]] = input

        def get_hook(name):

            def hook(m, grad_in, grad_out):
                None
            return hook
        for op in self._op_list:
            if op[1] != 'Concat' and op[1] != 'InnerProduct':
                data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])
            elif op[1] == 'InnerProduct':
                x = data_dict[op[-1]]
                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))
            else:
                try:
                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)
                except:
                    for x in op[-1]:
                        None
                    raise
        return data_dict[self._op_list[-1][2]]


class InceptionV3(BNInception):

    def __init__(self, model_path='model_zoo/bninception/inceptionv3.yaml', num_classes=101, weight_url='https://yjxiong.blob.core.windows.net/models/inceptionv3-cuhk-0e09b300b493bc74c.pth'):
        super(InceptionV3, self).__init__(model_path=model_path, weight_url=weight_url, num_classes=num_classes)


class Block35(nn.Module):

    def __init__(self, scale=1.0):
        super(Block35, self).__init__()
        self.scale = scale
        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)
        self.branch1 = nn.Sequential(BasicConv2d(320, 32, kernel_size=1, stride=1), BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv2d(320, 32, kernel_size=1, stride=1), BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1), BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1))
        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)
        self.relu = nn.ReLU(inplace=False)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        out = torch.cat((x0, x1, x2), 1)
        out = self.conv2d(out)
        out = out * self.scale + x
        out = self.relu(out)
        return out


class Mixed_6a(nn.Module):

    def __init__(self):
        super(Mixed_6a, self).__init__()
        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)
        self.branch1 = nn.Sequential(BasicConv2d(320, 256, kernel_size=1, stride=1), BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1), BasicConv2d(256, 384, kernel_size=3, stride=2))
        self.branch2 = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        out = torch.cat((x0, x1, x2), 1)
        return out


class Block17(nn.Module):

    def __init__(self, scale=1.0):
        super(Block17, self).__init__()
        self.scale = scale
        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)
        self.branch1 = nn.Sequential(BasicConv2d(1088, 128, kernel_size=1, stride=1), BasicConv2d(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3)), BasicConv2d(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)))
        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)
        self.relu = nn.ReLU(inplace=False)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        out = torch.cat((x0, x1), 1)
        out = self.conv2d(out)
        out = out * self.scale + x
        out = self.relu(out)
        return out


class Mixed_7a(nn.Module):

    def __init__(self):
        super(Mixed_7a, self).__init__()
        self.branch0 = nn.Sequential(BasicConv2d(1088, 256, kernel_size=1, stride=1), BasicConv2d(256, 384, kernel_size=3, stride=2))
        self.branch1 = nn.Sequential(BasicConv2d(1088, 256, kernel_size=1, stride=1), BasicConv2d(256, 288, kernel_size=3, stride=2))
        self.branch2 = nn.Sequential(BasicConv2d(1088, 256, kernel_size=1, stride=1), BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1), BasicConv2d(288, 320, kernel_size=3, stride=2))
        self.branch3 = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Block8(nn.Module):

    def __init__(self, scale=1.0, noReLU=False):
        super(Block8, self).__init__()
        self.scale = scale
        self.noReLU = noReLU
        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)
        self.branch1 = nn.Sequential(BasicConv2d(2080, 192, kernel_size=1, stride=1), BasicConv2d(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1)), BasicConv2d(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0)))
        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)
        if not self.noReLU:
            self.relu = nn.ReLU(inplace=False)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        out = torch.cat((x0, x1), 1)
        out = self.conv2d(out)
        out = out * self.scale + x
        if not self.noReLU:
            out = self.relu(out)
        return out


class InceptionResnetV2(nn.Module):

    def __init__(self, num_classes=1001):
        super(InceptionResnetV2, self).__init__()
        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)
        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)
        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.maxpool_3a = nn.MaxPool2d(3, stride=2)
        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)
        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)
        self.maxpool_5a = nn.MaxPool2d(3, stride=2)
        self.mixed_5b = Mixed_5b()
        self.repeat = nn.Sequential(Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17))
        self.mixed_6a = Mixed_6a()
        self.repeat_1 = nn.Sequential(Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1))
        self.mixed_7a = Mixed_7a()
        self.repeat_2 = nn.Sequential(Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2))
        self.block8 = Block8(noReLU=True)
        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)
        self.avgpool_1a = nn.AvgPool2d(8, count_include_pad=False)
        self.classif = nn.Linear(1536, num_classes)

    def forward(self, x):
        x = self.conv2d_1a(x)
        x = self.conv2d_2a(x)
        x = self.conv2d_2b(x)
        x = self.maxpool_3a(x)
        x = self.conv2d_3b(x)
        x = self.conv2d_4a(x)
        x = self.maxpool_5a(x)
        x = self.mixed_5b(x)
        x = self.repeat(x)
        x = self.mixed_6a(x)
        x = self.repeat_1(x)
        x = self.mixed_7a(x)
        x = self.repeat_2(x)
        x = self.block8(x)
        x = self.conv2d_7b(x)
        x = self.avgpool_1a(x)
        x = x.view(x.size(0), -1)
        x = self.classif(x)
        return x


class Mixed_3a(nn.Module):

    def __init__(self):
        super(Mixed_3a, self).__init__()
        self.maxpool = nn.MaxPool2d(3, stride=2)
        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)

    def forward(self, x):
        x0 = self.maxpool(x)
        x1 = self.conv(x)
        out = torch.cat((x0, x1), 1)
        return out


class Mixed_4a(nn.Module):

    def __init__(self):
        super(Mixed_4a, self).__init__()
        self.branch0 = nn.Sequential(BasicConv2d(160, 64, kernel_size=1, stride=1), BasicConv2d(64, 96, kernel_size=3, stride=1))
        self.branch1 = nn.Sequential(BasicConv2d(160, 64, kernel_size=1, stride=1), BasicConv2d(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)), BasicConv2d(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)), BasicConv2d(64, 96, kernel_size=(3, 3), stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        out = torch.cat((x0, x1), 1)
        return out


class Mixed_5a(nn.Module):

    def __init__(self):
        super(Mixed_5a, self).__init__()
        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)
        self.maxpool = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.conv(x)
        x1 = self.maxpool(x)
        out = torch.cat((x0, x1), 1)
        return out


class Inception_A(nn.Module):

    def __init__(self):
        super(Inception_A, self).__init__()
        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)
        self.branch1 = nn.Sequential(BasicConv2d(384, 64, kernel_size=1, stride=1), BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1))
        self.branch2 = nn.Sequential(BasicConv2d(384, 64, kernel_size=1, stride=1), BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1), BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1))
        self.branch3 = nn.Sequential(nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False), BasicConv2d(384, 96, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Reduction_A(nn.Module):

    def __init__(self):
        super(Reduction_A, self).__init__()
        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)
        self.branch1 = nn.Sequential(BasicConv2d(384, 192, kernel_size=1, stride=1), BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1), BasicConv2d(224, 256, kernel_size=3, stride=2))
        self.branch2 = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        out = torch.cat((x0, x1, x2), 1)
        return out


class Inception_B(nn.Module):

    def __init__(self):
        super(Inception_B, self).__init__()
        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)
        self.branch1 = nn.Sequential(BasicConv2d(1024, 192, kernel_size=1, stride=1), BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)), BasicConv2d(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0)))
        self.branch2 = nn.Sequential(BasicConv2d(1024, 192, kernel_size=1, stride=1), BasicConv2d(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)), BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)), BasicConv2d(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0)), BasicConv2d(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)))
        self.branch3 = nn.Sequential(nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False), BasicConv2d(1024, 128, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Reduction_B(nn.Module):

    def __init__(self):
        super(Reduction_B, self).__init__()
        self.branch0 = nn.Sequential(BasicConv2d(1024, 192, kernel_size=1, stride=1), BasicConv2d(192, 192, kernel_size=3, stride=2))
        self.branch1 = nn.Sequential(BasicConv2d(1024, 256, kernel_size=1, stride=1), BasicConv2d(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)), BasicConv2d(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0)), BasicConv2d(320, 320, kernel_size=3, stride=2))
        self.branch2 = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        out = torch.cat((x0, x1, x2), 1)
        return out


class Inception_C(nn.Module):

    def __init__(self):
        super(Inception_C, self).__init__()
        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)
        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)
        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))
        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))
        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)
        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0))
        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1))
        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))
        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))
        self.branch3 = nn.Sequential(nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False), BasicConv2d(1536, 256, kernel_size=1, stride=1))

    def forward(self, x):
        x0 = self.branch0(x)
        x1_0 = self.branch1_0(x)
        x1_1a = self.branch1_1a(x1_0)
        x1_1b = self.branch1_1b(x1_0)
        x1 = torch.cat((x1_1a, x1_1b), 1)
        x2_0 = self.branch2_0(x)
        x2_1 = self.branch2_1(x2_0)
        x2_2 = self.branch2_2(x2_1)
        x2_3a = self.branch2_3a(x2_2)
        x2_3b = self.branch2_3b(x2_2)
        x2 = torch.cat((x2_3a, x2_3b), 1)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class InceptionV4(nn.Module):

    def __init__(self, num_classes=1001):
        super(InceptionV4, self).__init__()
        self.features = nn.Sequential(BasicConv2d(3, 32, kernel_size=3, stride=2), BasicConv2d(32, 32, kernel_size=3, stride=1), BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1), Mixed_3a(), Mixed_4a(), Mixed_5a(), Inception_A(), Inception_A(), Inception_A(), Inception_A(), Reduction_A(), Inception_B(), Inception_B(), Inception_B(), Inception_B(), Inception_B(), Inception_B(), Inception_B(), Reduction_B(), Inception_C(), Inception_C(), Inception_C(), nn.AvgPool2d(8, count_include_pad=False))
        self.classif = nn.Linear(1536, num_classes)

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classif(x)
        return x


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (BasicBlock,
     lambda: ([], {'inplanes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 64, 64, 64])], {}),
     True),
    (BasicConv2d,
     lambda: ([], {'in_planes': 4, 'out_planes': 4, 'kernel_size': 4, 'stride': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BasicConv3d,
     lambda: ([], {'in_planes': 4, 'out_planes': 4, 'kernel_size': 4, 'stride': 1}),
     lambda: ([torch.rand([4, 4, 64, 64, 64])], {}),
     True),
    (Block17,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1088, 64, 64])], {}),
     False),
    (Block35,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 320, 64, 64])], {}),
     False),
    (Block8,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 2080, 64, 64])], {}),
     False),
    (Identity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (InceptionResnetV2,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 512, 512])], {}),
     False),
    (InceptionV4,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 512, 512])], {}),
     False),
    (Inception_A,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 384, 64, 64])], {}),
     False),
    (Inception_B,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1024, 64, 64])], {}),
     False),
    (Inception_C,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1536, 64, 64])], {}),
     False),
    (Mixed_3a,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 64, 64, 64])], {}),
     False),
    (Mixed_4a,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 160, 64, 64])], {}),
     False),
    (Mixed_5a,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 192, 64, 64])], {}),
     False),
    (Mixed_5b,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 192, 64, 64])], {}),
     False),
    (Mixed_6a,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 320, 64, 64])], {}),
     False),
    (Mixed_7a,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1088, 64, 64])], {}),
     False),
    (NONLocalBlock1D,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 64])], {}),
     False),
    (NONLocalBlock2D,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (NONLocalBlock3D,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 64, 8, 8])], {}),
     False),
    (Reduction_A,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 384, 64, 64])], {}),
     False),
    (Reduction_B,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1024, 64, 64])], {}),
     False),
    (_NonLocalBlockND,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 64, 8, 8])], {}),
     False),
]

class Test_sunnyxiaohu_R_C3D_pytorch(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

