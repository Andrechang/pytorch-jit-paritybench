import sys
_module = sys.modules[__name__]
del sys
setup = _module
conf = _module
gbdt_selector_test = _module
benchmark_test = _module
sklearn_test = _module
test_memory = _module
test_time = _module
BNN_quantizer_cifar10 = _module
DoReFaQuantizer_torch_mnist = _module
L1_torch_cifar10 = _module
QAT_torch_quantizer = _module
fpgm_tf_mnist = _module
knowledge_distill = _module
lottery_torch_mnist_fc = _module
model_prune_torch = _module
model_speedup = _module
vgg = _module
pruning_kd = _module
slim_torch_cifar10 = _module
aux_head = _module
config = _module
cifar = _module
data_utils = _module
imagenet = _module
genotypes = _module
model = _module
ops = _module
retrain = _module
search = _module
utils = _module
mnist = _module
datasets = _module
model = _module
ops = _module
retrain = _module
search = _module
macro = _module
micro = _module
macro = _module
micro = _module
ops = _module
search = _module
train = _module
train = _module
search = _module
main = _module
model = _module
ops = _module
putils = _module
retrain = _module
blocks = _module
dataloader = _module
network = _module
scratch = _module
supernet = _module
tester = _module
tuner = _module
utils = _module
model = _module
ops = _module
retrain = _module
search = _module
utils = _module
main = _module
models = _module
densenet = _module
dpn = _module
googlenet = _module
lenet = _module
mobilenet = _module
mobilenetv2 = _module
pnasnet = _module
preact_resnet = _module
resnet = _module
resnext = _module
senet = _module
shufflenet = _module
vgg = _module
utils = _module
attention = _module
data = _module
evaluate = _module
graph = _module
graph_to_tf = _module
rnn = _module
train_model = _module
trial = _module
util = _module
augmentation = _module
focal_loss = _module
loader = _module
lovasz_losses = _module
metrics = _module
models = _module
postprocessing = _module
predict = _module
preprocess = _module
settings = _module
train = _module
dist_mnist = _module
mnist = _module
mnist = _module
mnist_before = _module
FashionMNIST_keras = _module
FashionMNIST_pytorch = _module
FashionMNIST = _module
utils = _module
cifar10 = _module
cifar10_keras = _module
cifar10_pytorch = _module
utils = _module
ga_customer_tuner = _module
customer_tuner = _module
dummy_advisor = _module
mnist_keras = _module
random_nas_tuner = _module
assessor = _module
dummy_assessor = _module
dummy_tuner = _module
mockedTrial = _module
nnicli = _module
nni_client = _module
nni = _module
__main__ = _module
_graph_utils = _module
batch_tuner = _module
bohb_advisor = _module
config_generator = _module
common = _module
compression = _module
tensorflow = _module
builtin_pruners = _module
builtin_quantizers = _module
compressor = _module
default_layers = _module
torch = _module
compressor = _module
pruning = _module
agp = _module
apply_compression = _module
constants = _module
finegrained_pruning = _module
lottery_ticket = _module
one_shot = _module
structured_pruning = _module
weight_masker = _module
quantization = _module
quantizers = _module
speedup = _module
compress_modules = _module
infer_shape = _module
config_validation = _module
curvefitting_assessor = _module
curvefunctions = _module
model_factory = _module
env_vars = _module
evolution_tuner = _module
feature_engineering = _module
feature_selector = _module
gbdt_selector = _module
gradient_selector = _module
fginitialize = _module
fgtrain = _module
learnability = _module
syssettings = _module
gp_tuner = _module
target_space = _module
gridsearch_tuner = _module
hyperband_advisor = _module
hyperopt_tuner = _module
medianstop_assessor = _module
test = _module
CreateModel = _module
Selection = _module
Regression_GMM = _module
OutlierDetection = _module
Prediction = _module
Regression_GP = _module
metis_tuner = _module
lib_acquisition_function = _module
lib_constraint_summation = _module
lib_data = _module
msg_dispatcher = _module
msg_dispatcher_base = _module
nas = _module
pytorch = _module
base_mutator = _module
base_trainer = _module
callbacks = _module
cdarts = _module
mutator = _module
trainer = _module
classic_nas = _module
mutator = _module
darts = _module
mutator = _module
trainer = _module
enas = _module
mutator = _module
trainer = _module
fixed = _module
mutables = _module
pdarts = _module
proxylessnas = _module
mutator = _module
trainer = _module
utils = _module
random = _module
mutator = _module
spos = _module
evolution = _module
mutator = _module
trainer = _module
trainer = _module
nas_utils = _module
networkmorphism_tuner = _module
bayesian = _module
graph = _module
graph_transformer = _module
layer_transformer = _module
layers = _module
nn = _module
parameter_expressions = _module
pbt_tuner = _module
platform = _module
local = _module
standalone = _module
ppo_tuner = _module
distri = _module
policy = _module
protocol = _module
recoverable = _module
smac_tuner = _module
convert_ss_to_scenario = _module
smartparam = _module
tests = _module
pytorch_models = _module
layer_choice_only = _module
mutable_scope = _module
naive = _module
nested = _module
test_assessor = _module
test_builtin_tuners = _module
test_compressor = _module
test_curvefitting_assessor = _module
test_evolution_tuner = _module
test_graph_utils = _module
test_hyperopt_tuner = _module
test_model_speedup = _module
test_msg_dispatcher = _module
test_nas = _module
test_networkmorphism_tuner = _module
test_protocol = _module
test_pruners = _module
test_smartparam = _module
test_trial = _module
test_utils = _module
simple_tuner = _module
multi_phase = _module
multi_thread_trial = _module
multi_thread_tuner = _module
naive_assessor = _module
naive_trial = _module
naive_tuner = _module
trial_choices = _module
nnitest = _module
foreground = _module
generate_ts_config = _module
naive_test = _module
remote_docker = _module
run_tests = _module
validators = _module
nni_annotation = _module
code_generator = _module
mnist_generated = _module
mnist_with_annotation = _module
mnist_without_annotation = _module
search_space_generator = _module
specific_code_generator = _module
test_annotation = _module
simple = _module
handwrite = _module
bar = _module
foo = _module
nni_cmd = _module
command_utils = _module
common_utils = _module
config_schema = _module
config_utils = _module
launcher = _module
launcher_utils = _module
nnictl = _module
nnictl_utils = _module
package_management = _module
rest_utils = _module
ssh_utils = _module
tensorboard_utils = _module
updater = _module
url_utils = _module
nni_gpu_tool = _module
gpu_metrics_collector = _module
nni_trial_tool = _module
hdfsClientUtility = _module
log_utils = _module
test_hdfsClientUtility = _module
trial_keeper = _module

from _paritybench_helpers import _mock_config
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
open = mock_open()
logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import torch


import torch.nn as nn


import torch.nn.functional as F


import math


import logging


import torch.utils.data


import torch.optim as optim


from torch.utils.data import DataLoader


import random


import numpy as np


import torch.distributed as dist


from torch.utils.data import Sampler


from collections import namedtuple


from collections import OrderedDict


from torch.utils.tensorboard import SummaryWriter


from torch import nn as nn


import re


from itertools import cycle


from torch import nn


from torch import optim


import torch.nn.functional as Func


import torch.backends.cudnn as cudnn


import torch.nn.init as init


from torch.autograd import Variable


from torch.nn import functional as F


from torch.optim.lr_scheduler import CosineAnnealingLR


from torch.optim.lr_scheduler import ReduceLROnPlateau


from math import ceil


from random import Random


from collections import defaultdict


from torch.utils.tensorboard._pytorch_graph import NodePy


from torch.utils.tensorboard._pytorch_graph import NodePyIO


from torch.utils.tensorboard._pytorch_graph import NodePyOP


from torch.utils.tensorboard._pytorch_graph import GraphPy


import types


import copy


import scipy.special


import warnings


from abc import abstractmethod


from collections.abc import Iterable


from copy import deepcopy


from copy import copy


from torch.nn import functional


import uuid


class VGG_Cifar10(nn.Module):

    def __init__(self, num_classes=1000):
        super(VGG_Cifar10, self).__init__()
        self.features = nn.Sequential(nn.Conv2d(3, 128, kernel_size=3,
            padding=1, bias=False), nn.BatchNorm2d(128, eps=0.0001,
            momentum=0.1), nn.Hardtanh(inplace=True), nn.Conv2d(128, 128,
            kernel_size=3, padding=1, bias=False), nn.MaxPool2d(kernel_size
            =2, stride=2), nn.BatchNorm2d(128, eps=0.0001, momentum=0.1),
            nn.Hardtanh(inplace=True), nn.Conv2d(128, 256, kernel_size=3,
            padding=1, bias=False), nn.BatchNorm2d(256, eps=0.0001,
            momentum=0.1), nn.Hardtanh(inplace=True), nn.Conv2d(256, 256,
            kernel_size=3, padding=1, bias=False), nn.MaxPool2d(kernel_size
            =2, stride=2), nn.BatchNorm2d(256, eps=0.0001, momentum=0.1),
            nn.Hardtanh(inplace=True), nn.Conv2d(256, 512, kernel_size=3,
            padding=1, bias=False), nn.BatchNorm2d(512, eps=0.0001,
            momentum=0.1), nn.Hardtanh(inplace=True), nn.Conv2d(512, 512,
            kernel_size=3, padding=1, bias=False), nn.MaxPool2d(kernel_size
            =2, stride=2), nn.BatchNorm2d(512, eps=0.0001, momentum=0.1),
            nn.Hardtanh(inplace=True))
        self.classifier = nn.Sequential(nn.Linear(512 * 4 * 4, 1024, bias=
            False), nn.BatchNorm1d(1024), nn.Hardtanh(inplace=True), nn.
            Linear(1024, 1024, bias=False), nn.BatchNorm1d(1024), nn.
            Hardtanh(inplace=True), nn.Linear(1024, num_classes), nn.
            BatchNorm1d(num_classes, affine=False))

    def forward(self, x):
        x = self.features(x)
        x = x.view(-1, 512 * 4 * 4)
        x = self.classifier(x)
        return x


class Mnist(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)
        self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)
        self.fc1 = torch.nn.Linear(4 * 4 * 50, 500)
        self.fc2 = torch.nn.Linear(500, 10)
        self.relu1 = torch.nn.ReLU6()
        self.relu2 = torch.nn.ReLU6()
        self.relu3 = torch.nn.ReLU6()

    def forward(self, x):
        x = self.relu1(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = self.relu2(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4 * 4 * 50)
        x = self.relu3(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class Mnist(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)
        self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)
        self.fc1 = torch.nn.Linear(4 * 4 * 50, 500)
        self.fc2 = torch.nn.Linear(500, 10)
        self.relu1 = torch.nn.ReLU6()
        self.relu2 = torch.nn.ReLU6()
        self.relu3 = torch.nn.ReLU6()

    def forward(self, x):
        x = self.relu1(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = self.relu2(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4 * 4 * 50)
        x = self.relu3(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class fc1(nn.Module):

    def __init__(self, num_classes=10):
        super(fc1, self).__init__()
        self.classifier = nn.Sequential(nn.Linear(28 * 28, 300), nn.ReLU(
            inplace=True), nn.Linear(300, 100), nn.ReLU(inplace=True), nn.
            Linear(100, num_classes))

    def forward(self, x):
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


class NaiveModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)
        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)
        self.fc1 = nn.Linear(4 * 4 * 50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


defaultcfg = {(11): [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 
    512], (13): [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 
    512, 512], (16): [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 
    512, 512, 'M', 512, 512, 512], (19): [64, 64, 'M', 128, 128, 'M', 256, 
    256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]}


class VGG(nn.Module):

    def __init__(self, depth=16):
        super(VGG, self).__init__()
        cfg = defaultcfg[depth]
        self.cfg = cfg
        self.feature = self.make_layers(cfg, True)
        num_classes = 10
        self.classifier = nn.Sequential(nn.Linear(cfg[-1], 512), nn.
            BatchNorm1d(512), nn.ReLU(inplace=True), nn.Linear(512,
            num_classes))
        self._initialize_weights()

    def make_layers(self, cfg, batch_norm=False):
        layers = []
        in_channels = 3
        for v in cfg:
            if v == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1,
                    bias=False)
                if batch_norm:
                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)
                        ]
                else:
                    layers += [conv2d, nn.ReLU(inplace=True)]
                in_channels = v
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.feature(x)
        x = nn.AvgPool2d(2)(x)
        x = x.view(x.size(0), -1)
        y = self.classifier(x)
        return y

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2.0 / n))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(0.5)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                m.bias.data.zero_()


class DistillHeadCIFAR(nn.Module):

    def __init__(self, C, size, num_classes, bn_affine=False):
        """assuming input size 8x8 or 16x16"""
        super(DistillHeadCIFAR, self).__init__()
        self.features = nn.Sequential(nn.ReLU(), nn.AvgPool2d(size, stride=
            2, padding=0, count_include_pad=False), nn.Conv2d(C, 128, 1,
            bias=False), nn.BatchNorm2d(128, affine=bn_affine), nn.ReLU(),
            nn.Conv2d(128, 768, 2, bias=False), nn.BatchNorm2d(768, affine=
            bn_affine), nn.ReLU())
        self.classifier = nn.Linear(768, num_classes)
        self.gap = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        x = self.features(x)
        x = self.gap(x)
        x = self.classifier(x.view(x.size(0), -1))
        return x


class DistillHeadImagenet(nn.Module):

    def __init__(self, C, size, num_classes, bn_affine=False):
        """assuming input size 7x7 or 14x14"""
        super(DistillHeadImagenet, self).__init__()
        self.features = nn.Sequential(nn.ReLU(), nn.AvgPool2d(size, stride=
            2, padding=0, count_include_pad=False), nn.Conv2d(C, 128, 1,
            bias=False), nn.BatchNorm2d(128, affine=bn_affine), nn.ReLU(),
            nn.Conv2d(128, 768, 2, bias=False), nn.BatchNorm2d(768, affine=
            bn_affine), nn.ReLU())
        self.classifier = nn.Linear(768, num_classes)
        self.gap = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        x = self.features(x)
        x = self.gap(x)
        x = self.classifier(x.view(x.size(0), -1))
        return x


class AuxiliaryHeadCIFAR(nn.Module):

    def __init__(self, C, size=5, num_classes=10):
        """assuming input size 8x8"""
        super(AuxiliaryHeadCIFAR, self).__init__()
        self.features = nn.Sequential(nn.ReLU(inplace=True), nn.AvgPool2d(5,
            stride=3, padding=0, count_include_pad=False), nn.Conv2d(C, 128,
            1, bias=False), nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.
            Conv2d(128, 768, 2, bias=False), nn.BatchNorm2d(768), nn.ReLU(
            inplace=True))
        self.classifier = nn.Linear(768, num_classes)

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x.view(x.size(0), -1))
        return x


class AuxiliaryHeadImageNet(nn.Module):

    def __init__(self, C, size=5, num_classes=1000):
        """assuming input size 7x7"""
        super(AuxiliaryHeadImageNet, self).__init__()
        self.features = nn.Sequential(nn.ReLU(inplace=True), nn.AvgPool2d(
            size, stride=2, padding=0, count_include_pad=False), nn.Conv2d(
            C, 128, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(inplace=
            True), nn.Conv2d(128, 768, 2, bias=False), nn.ReLU(inplace=True))
        self.classifier = nn.Linear(768, num_classes)

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x.view(x.size(0), -1))
        return x


class Node(nn.Module):

    def __init__(self, node_id, num_prev_nodes, channels,
        num_downsample_connect):
        super().__init__()
        self.ops = nn.ModuleList()
        choice_keys = []
        for i in range(num_prev_nodes):
            stride = 2 if i < num_downsample_connect else 1
            choice_keys.append('{}_p{}'.format(node_id, i))
            self.ops.append(mutables.LayerChoice([ops.OPS[k](channels,
                stride, False) for k in ops.PRIMITIVES], key=choice_keys[-1]))
        self.drop_path = ops.DropPath()
        self.input_switch = mutables.InputChoice(choose_from=choice_keys,
            n_chosen=2, key='{}_switch'.format(node_id))

    def forward(self, prev_nodes):
        assert len(self.ops) == len(prev_nodes)
        out = [op(node) for op, node in zip(self.ops, prev_nodes)]
        out = [(self.drop_path(o) if o is not None else None) for o in out]
        return self.input_switch(out)


class Cell(nn.Module):

    def __init__(self, n_nodes, channels_pp, channels_p, channels,
        reduction_p, reduction):
        super().__init__()
        self.reduction = reduction
        self.n_nodes = n_nodes
        if reduction_p:
            self.preproc0 = ops.FactorizedReduce(channels_pp, channels,
                affine=False)
        else:
            self.preproc0 = ops.StdConv(channels_pp, channels, 1, 1, 0,
                affine=False)
        self.preproc1 = ops.StdConv(channels_p, channels, 1, 1, 0, affine=False
            )
        self.mutable_ops = nn.ModuleList()
        for depth in range(2, self.n_nodes + 2):
            self.mutable_ops.append(Node('{}_n{}'.format('reduce' if
                reduction else 'normal', depth), depth, channels, 2 if
                reduction else 0))

    def forward(self, s0, s1):
        tensors = [self.preproc0(s0), self.preproc1(s1)]
        for node in self.mutable_ops:
            cur_tensor = node(tensors)
            tensors.append(cur_tensor)
        output = torch.cat(tensors[2:], dim=1)
        return output


Genotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')


PRIMITIVES = ['max_pool_3x3', 'avg_pool_3x3', 'skip_connect',
    'sep_conv_3x3', 'sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_5x5']


def parse_results(results, n_nodes):
    concat = range(2, 2 + n_nodes)
    normal_gene = []
    reduction_gene = []
    for i in range(n_nodes):
        normal_node = []
        reduction_node = []
        for j in range(2 + i):
            normal_key = 'normal_n{}_p{}'.format(i + 2, j)
            reduction_key = 'reduce_n{}_p{}'.format(i + 2, j)
            normal_op = results[normal_key].cpu().numpy()
            reduction_op = results[reduction_key].cpu().numpy()
            if sum(normal_op == 1):
                normal_index = np.argmax(normal_op)
                normal_node.append((PRIMITIVES[normal_index], j))
            if sum(reduction_op == 1):
                reduction_index = np.argmax(reduction_op)
                reduction_node.append((PRIMITIVES[reduction_index], j))
        normal_gene.append(normal_node)
        reduction_gene.append(reduction_node)
    genotypes = Genotype(normal=normal_gene, normal_concat=concat, reduce=
        reduction_gene, reduce_concat=concat)
    return genotypes


class Model(nn.Module):

    def __init__(self, dataset, n_layers, in_channels=3, channels=16,
        n_nodes=4, retrain=False, shared_modules=None):
        super().__init__()
        assert dataset in ['cifar10', 'imagenet']
        self.dataset = dataset
        self.input_size = 32 if dataset == 'cifar' else 224
        self.in_channels = in_channels
        self.channels = channels
        self.n_nodes = n_nodes
        self.aux_size = {(2 * n_layers // 3): self.input_size // 4}
        if dataset == 'cifar10':
            self.n_classes = 10
            self.aux_head_class = (AuxiliaryHeadCIFAR if retrain else
                DistillHeadCIFAR)
            if not retrain:
                self.aux_size = {(n_layers // 3): 6, (2 * n_layers // 3): 6}
        elif dataset == 'imagenet':
            self.n_classes = 1000
            self.aux_head_class = (AuxiliaryHeadImageNet if retrain else
                DistillHeadImagenet)
            if not retrain:
                self.aux_size = {(n_layers // 3): 6, (2 * n_layers // 3): 5}
        self.n_layers = n_layers
        self.aux_head = nn.ModuleDict()
        self.ensemble_param = nn.Parameter(torch.rand(len(self.aux_size) + 
            1) / (len(self.aux_size) + 1)) if not retrain else None
        stem_multiplier = 3 if dataset == 'cifar' else 1
        c_cur = stem_multiplier * self.channels
        self.shared_modules = {}
        if shared_modules is not None:
            self.stem = shared_modules['stem']
        else:
            self.stem = nn.Sequential(nn.Conv2d(in_channels, c_cur, 3, 1, 1,
                bias=False), nn.BatchNorm2d(c_cur))
            self.shared_modules['stem'] = self.stem
        channels_pp, channels_p, c_cur = c_cur, c_cur, channels
        self.cells = nn.ModuleList()
        reduction_p, reduction = False, False
        aux_head_count = 0
        for i in range(n_layers):
            reduction_p, reduction = reduction, False
            if i in [n_layers // 3, 2 * n_layers // 3]:
                c_cur *= 2
                reduction = True
            cell = Cell(n_nodes, channels_pp, channels_p, c_cur,
                reduction_p, reduction)
            self.cells.append(cell)
            c_cur_out = c_cur * n_nodes
            if i in self.aux_size:
                if shared_modules is not None:
                    self.aux_head[str(i)] = shared_modules['aux' + str(
                        aux_head_count)]
                else:
                    self.aux_head[str(i)] = self.aux_head_class(c_cur_out,
                        self.aux_size[i], self.n_classes)
                    self.shared_modules['aux' + str(aux_head_count)
                        ] = self.aux_head[str(i)]
                aux_head_count += 1
            channels_pp, channels_p = channels_p, c_cur_out
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.linear = nn.Linear(channels_p, self.n_classes)

    def forward(self, x):
        s0 = s1 = self.stem(x)
        outputs = []
        for i, cell in enumerate(self.cells):
            s0, s1 = s1, cell(s0, s1)
            if str(i) in self.aux_head:
                outputs.append(self.aux_head[str(i)](s1))
        out = self.gap(s1)
        out = out.view(out.size(0), -1)
        logits = self.linear(out)
        outputs.append(logits)
        if self.ensemble_param is None:
            assert len(outputs) == 2
            return outputs[1], outputs[0]
        else:
            em_output = torch.cat([(e * o) for e, o in zip(F.softmax(self.
                ensemble_param, dim=0), outputs)], 0)
            return logits, em_output

    def drop_path_prob(self, p):
        for module in self.modules():
            if isinstance(module, ops.DropPath):
                module.p = p

    def plot_genotype(self, results, logger):
        genotypes = parse_results(results, self.n_nodes)
        logger.info(genotypes)
        return genotypes


class DropPath(nn.Module):

    def __init__(self, p=0.0):
        """
        Drop path with probability.

        Parameters
        ----------
        p : float
            Probability of an path to be zeroed.
        """
        super().__init__()
        self.p = p

    def forward(self, x):
        if self.training and self.p > 0.0:
            keep_prob = 1.0 - self.p
            mask = torch.zeros((x.size(0), 1, 1, 1), device=x.device
                ).bernoulli_(keep_prob)
            return x / keep_prob * mask
        return x


class PoolWithoutBN(nn.Module):
    """
    AvgPool or MaxPool with BN. `pool_type` must be `max` or `avg`.
    """

    def __init__(self, pool_type, C, kernel_size, stride, padding, affine=True
        ):
        super().__init__()
        if pool_type.lower() == 'max':
            self.pool = nn.MaxPool2d(kernel_size, stride, padding)
        elif pool_type.lower() == 'avg':
            self.pool = nn.AvgPool2d(kernel_size, stride, padding,
                count_include_pad=False)
        else:
            raise NotImplementedError(
                "Pool doesn't support pooling type other than max and avg.")

    def forward(self, x):
        out = self.pool(x)
        return out


class StdConv(nn.Module):
    """
    Standard conv: ReLU - Conv - BN
    """

    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
        super().__init__()
        self.net = nn.Sequential(nn.ReLU(), nn.Conv2d(C_in, C_out,
            kernel_size, stride, padding, bias=False), nn.BatchNorm2d(C_out,
            affine=affine))

    def forward(self, x):
        return self.net(x)


class FacConv(nn.Module):
    """
    Factorized conv: ReLU - Conv(Kx1) - Conv(1xK) - BN
    """

    def __init__(self, C_in, C_out, kernel_length, stride, padding, affine=True
        ):
        super().__init__()
        self.net = nn.Sequential(nn.ReLU(), nn.Conv2d(C_in, C_in, (
            kernel_length, 1), stride, padding, bias=False), nn.Conv2d(C_in,
            C_out, (1, kernel_length), stride, padding, bias=False), nn.
            BatchNorm2d(C_out, affine=affine))

    def forward(self, x):
        return self.net(x)


class DilConv(nn.Module):
    """
    (Dilated) depthwise separable conv.
    ReLU - (Dilated) depthwise separable - Pointwise - BN.
    If dilation == 2, 3x3 conv => 5x5 receptive field, 5x5 conv => 9x9 receptive field.
    """

    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation,
        affine=True):
        super().__init__()
        self.net = nn.Sequential(nn.ReLU(), nn.Conv2d(C_in, C_in,
            kernel_size, stride, padding, dilation=dilation, groups=C_in,
            bias=False), nn.Conv2d(C_in, C_out, 1, stride=1, padding=0,
            bias=False), nn.BatchNorm2d(C_out, affine=affine))

    def forward(self, x):
        return self.net(x)


class SepConv(nn.Module):
    """
    Depthwise separable conv.
    DilConv(dilation=1) * 2.
    """

    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
        super().__init__()
        self.net = nn.Sequential(DilConv(C_in, C_in, kernel_size, stride,
            padding, dilation=1, affine=affine), DilConv(C_in, C_out,
            kernel_size, 1, padding, dilation=1, affine=affine))

    def forward(self, x):
        return self.net(x)


class FactorizedReduce(nn.Module):
    """
    Reduce feature map size by factorized pointwise (stride=2).
    """

    def __init__(self, C_in, C_out, affine=True):
        super().__init__()
        self.relu = nn.ReLU()
        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0,
            bias=False)
        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0,
            bias=False)
        self.bn = nn.BatchNorm2d(C_out, affine=affine)

    def forward(self, x):
        x = self.relu(x)
        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)
        out = self.bn(out)
        return out


class CrossEntropyLabelSmooth(nn.Module):

    def __init__(self, num_classes, epsilon):
        super(CrossEntropyLabelSmooth, self).__init__()
        self.num_classes = num_classes
        self.epsilon = epsilon
        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, inputs, targets):
        log_probs = self.logsoftmax(inputs)
        targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze
            (1), 1)
        targets = (1 - self.epsilon
            ) * targets + self.epsilon / self.num_classes
        loss = (-targets * log_probs).mean(0).sum()
        return loss


_logger = logging.getLogger(__name__)


def global_mutable_counting():
    """
    A program level counter starting from 1.
    """
    global _counter
    _counter += 1
    return _counter


class Mutable(Model):

    def __init__(self, key=None):
        super().__init__()
        if key is None:
            self._key = '{}_{}'.format(type(self).__name__,
                global_mutable_counting())
        elif isinstance(key, str):
            self._key = key
        else:
            self._key = str(key)
            _logger.warning('Key "%s" is not string, converted to string.', key
                )
        self.init_hook = None
        self.forward_hook = None

    def __deepcopy__(self, memodict=None):
        raise NotImplementedError("Deep copy doesn't work for mutables.")

    def __call__(self, *args, **kwargs):
        self._check_built()
        return super().__call__(*args, **kwargs)

    def set_mutator(self, mutator):
        if 'mutator' in self.__dict__:
            raise RuntimeError(
                '`set_mutator is called more than once. Did you parse the search space multiple times? Or did you apply multiple fixed architectures?'
                )
        self.__dict__['mutator'] = mutator

    def call(self, *inputs):
        raise NotImplementedError('Method `call` of Mutable must be overridden'
            )

    @property
    def key(self):
        return self._key

    @property
    def name(self):
        return self._name if hasattr(self, '_name') else self._key

    @name.setter
    def name(self, name):
        self._name = name

    def _check_built(self):
        if not hasattr(self, 'mutator'):
            raise ValueError(
                'Mutator not set for {}. You might have forgotten to initialize and apply your mutator. Or did you initialize a mutable on the fly in forward pass? Move to `__init__` so that trainer can locate all your mutables. See NNI docs for more details.'
                .format(self))

    def __repr__(self):
        return '{} ({})'.format(self.name, self.key)


class LayerChoice(Mutable):
    """
    Layer choice selects one of the ``op_candidates``, then apply it on inputs and return results.
    In rare cases, it can also select zero or many.

    Layer choice does not allow itself to be nested.

    Parameters
    ----------
    op_candidates : list of nn.Module or OrderedDict
        A module list to be selected from.
    reduction : str
        ``mean``, ``concat``, ``sum`` or ``none``. Policy if multiples are selected.
        If ``none``, a list is returned. ``mean`` returns the average. ``sum`` returns the sum.
        ``concat`` concatenate the list at dimension 1.
    return_mask : bool
        If ``return_mask``, return output tensor and a mask. Otherwise return tensor only.
    key : str
        Key of the input choice.

    Attributes
    ----------
    length : int
        Deprecated. Number of ops to choose from. ``len(layer_choice)`` is recommended.
    names : list of str
        Names of candidates.
    choices : list of Module
        Deprecated. A list of all candidate modules in the layer choice module.
        ``list(layer_choice)`` is recommended, which will serve the same purpose.

    Notes
    -----
    ``op_candidates`` can be a list of modules or a ordered dict of named modules, for example,

    .. code-block:: python

        self.op_choice = LayerChoice(OrderedDict([
            ("conv3x3", nn.Conv2d(3, 16, 128)),
            ("conv5x5", nn.Conv2d(5, 16, 128)),
            ("conv7x7", nn.Conv2d(7, 16, 128))
        ]))

    Elements in layer choice can be modified or deleted. Use ``del self.op_choice["conv5x5"]`` or
    ``self.op_choice[1] = nn.Conv3d(...)``. Adding more choices is not supported yet.
    """

    def __init__(self, op_candidates, reduction='sum', return_mask=False,
        key=None):
        super().__init__(key=key)
        self.names = []
        if isinstance(op_candidates, OrderedDict):
            for name, module in op_candidates.items():
                assert name not in ['length', 'reduction', 'return_mask',
                    '_key', 'key', 'names'
                    ], "Please don't use a reserved name '{}' for your module.".format(
                    name)
                self.add_module(name, module)
                self.names.append(name)
        elif isinstance(op_candidates, list):
            for i, module in enumerate(op_candidates):
                self.add_module(str(i), module)
                self.names.append(str(i))
        else:
            raise TypeError('Unsupported op_candidates type: {}'.format(
                type(op_candidates)))
        self.reduction = reduction
        self.return_mask = return_mask

    def __getitem__(self, idx):
        if isinstance(idx, str):
            return self._modules[idx]
        return list(self)[idx]

    def __setitem__(self, idx, module):
        key = idx if isinstance(idx, str) else self.names[idx]
        return setattr(self, key, module)

    def __delitem__(self, idx):
        if isinstance(idx, slice):
            for key in self.names[idx]:
                delattr(self, key)
        else:
            if isinstance(idx, str):
                key, idx = idx, self.names.index(idx)
            else:
                key = self.names[idx]
            delattr(self, key)
        del self.names[idx]

    @property
    def length(self):
        warnings.warn(
            'layer_choice.length is deprecated. Use `len(layer_choice)` instead.'
            , DeprecationWarning)
        return len(self)

    def __len__(self):
        return len(self.names)

    def __iter__(self):
        return map(lambda name: self._modules[name], self.names)

    @property
    def choices(self):
        warnings.warn(
            'layer_choice.choices is deprecated. Use `list(layer_choice)` instead.'
            , DeprecationWarning)
        return list(self)

    def forward(self, *args, **kwargs):
        """
        Returns
        -------
        tuple of tensors
            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.
        """
        out, mask = self.mutator.on_forward_layer_choice(self, *args, **kwargs)
        if self.return_mask:
            return out, mask
        return out


class InputChoice(Mutable):
    """
    Input choice selects ``n_chosen`` inputs from ``choose_from`` (contains ``n_candidates`` keys). For beginners,
    use ``n_candidates`` instead of ``choose_from`` is a safe option. To get the most power out of it, you might want to
    know about ``choose_from``.

    The keys in ``choose_from`` can be keys that appear in past mutables, or ``NO_KEY`` if there are no suitable ones.
    The keys are designed to be the keys of the sources. To help mutators make better decisions,
    mutators might be interested in how the tensors to choose from come into place. For example, the tensor is the
    output of some operator, some node, some cell, or some module. If this operator happens to be a mutable (e.g.,
    ``LayerChoice`` or ``InputChoice``), it has a key naturally that can be used as a source key. If it's a
    module/submodule, it needs to be annotated with a key: that's where a :class:`MutableScope` is needed.

    In the example below, ``input_choice`` is a 4-choose-any. The first 3 is semantically output of cell1, output of cell2,
    output of cell3 with respectively. Notice that an extra max pooling is followed by cell1, indicating x1 is not
    "actually" the direct output of cell1.

    .. code-block:: python

        class Cell(MutableScope):
            pass

        class Net(nn.Module):
            def __init__(self):
                self.cell1 = Cell("cell1")
                self.cell2 = Cell("cell2")
                self.op = LayerChoice([conv3x3(), conv5x5()], key="op")
                self.input_choice = InputChoice(choose_from=["cell1", "cell2", "op", InputChoice.NO_KEY])

            def forward(self, x):
                x1 = max_pooling(self.cell1(x))
                x2 = self.cell2(x)
                x3 = self.op(x)
                x4 = torch.zeros_like(x)
                return self.input_choice([x1, x2, x3, x4])

    Parameters
    ----------
    n_candidates : int
        Number of inputs to choose from.
    choose_from : list of str
        List of source keys to choose from. At least of one of ``choose_from`` and ``n_candidates`` must be fulfilled.
        If ``n_candidates`` has a value but ``choose_from`` is None, it will be automatically treated as ``n_candidates``
        number of empty string.
    n_chosen : int
        Recommended inputs to choose. If None, mutator is instructed to select any.
    reduction : str
        ``mean``, ``concat``, ``sum`` or ``none``. See :class:`LayerChoice`.
    return_mask : bool
        If ``return_mask``, return output tensor and a mask. Otherwise return tensor only.
    key : str
        Key of the input choice.
    """
    NO_KEY = ''

    def __init__(self, n_candidates=None, choose_from=None, n_chosen=None,
        reduction='sum', return_mask=False, key=None):
        super().__init__(key=key)
        assert n_candidates is not None or choose_from is not None, 'At least one of `n_candidates` and `choose_from`must be not None.'
        if choose_from is not None and n_candidates is None:
            n_candidates = len(choose_from)
        elif choose_from is None and n_candidates is not None:
            choose_from = [self.NO_KEY] * n_candidates
        assert n_candidates == len(choose_from
            ), 'Number of candidates must be equal to the length of `choose_from`.'
        assert n_candidates > 0, 'Number of candidates must be greater than 0.'
        assert n_chosen is None or 0 <= n_chosen <= n_candidates, 'Expected selected number must be None or no more than number of candidates.'
        self.n_candidates = n_candidates
        self.choose_from = choose_from.copy()
        self.n_chosen = n_chosen
        self.reduction = reduction
        self.return_mask = return_mask

    def forward(self, optional_inputs):
        """
        Forward method of LayerChoice.

        Parameters
        ----------
        optional_inputs : list or dict
            Recommended to be a dict. As a dict, inputs will be converted to a list that follows the order of
            ``choose_from`` in initialization. As a list, inputs must follow the semantic order that is the same as
            ``choose_from``.

        Returns
        -------
        tuple of tensors
            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.
        """
        optional_input_list = optional_inputs
        if isinstance(optional_inputs, dict):
            optional_input_list = [optional_inputs[tag] for tag in self.
                choose_from]
        assert isinstance(optional_input_list, list
            ), 'Optional input list must be a list, not a {}.'.format(type(
            optional_input_list))
        assert len(optional_inputs
            ) == self.n_candidates, 'Length of the input list must be equal to number of candidates.'
        out, mask = self.mutator.on_forward_input_choice(self,
            optional_input_list)
        if self.return_mask:
            return out, mask
        return out


class Net(nn.Module):

    def __init__(self, hidden_size):
        super(Net, self).__init__()
        self.conv1 = LayerChoice(OrderedDict([('conv5x5', nn.Conv2d(1, 20, 
            5, 1)), ('conv3x3', nn.Conv2d(1, 20, 3, 1))]), key='first_conv')
        self.mid_conv = LayerChoice([nn.Conv2d(20, 20, 3, 1, padding=1), nn
            .Conv2d(20, 20, 5, 1, padding=2)], key='mid_conv')
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.fc1 = nn.Linear(4 * 4 * 50, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 10)
        self.input_switch = InputChoice(n_candidates=2, n_chosen=1, key='skip')

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        old_x = x
        x = F.relu(self.mid_conv(x))
        zero_x = torch.zeros_like(old_x)
        skip_x = self.input_switch([zero_x, old_x])
        x = torch.add(x, skip_x)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4 * 4 * 50)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class AuxiliaryHead(nn.Module):
    """ Auxiliary head in 2/3 place of network to let the gradient flow well """

    def __init__(self, input_size, C, n_classes):
        """ assuming input size 7x7 or 8x8 """
        assert input_size in [7, 8]
        super().__init__()
        self.net = nn.Sequential(nn.ReLU(inplace=True), nn.AvgPool2d(5,
            stride=input_size - 5, padding=0, count_include_pad=False), nn.
            Conv2d(C, 128, kernel_size=1, bias=False), nn.BatchNorm2d(128),
            nn.ReLU(inplace=True), nn.Conv2d(128, 768, kernel_size=2, bias=
            False), nn.BatchNorm2d(768), nn.ReLU(inplace=True))
        self.linear = nn.Linear(768, n_classes)

    def forward(self, x):
        out = self.net(x)
        out = out.view(out.size(0), -1)
        logits = self.linear(out)
        return logits


class Node(nn.Module):

    def __init__(self, node_id, num_prev_nodes, channels,
        num_downsample_connect):
        super().__init__()
        self.ops = nn.ModuleList()
        choice_keys = []
        for i in range(num_prev_nodes):
            stride = 2 if i < num_downsample_connect else 1
            choice_keys.append('{}_p{}'.format(node_id, i))
            self.ops.append(mutables.LayerChoice(OrderedDict([('maxpool',
                ops.PoolBN('max', channels, 3, stride, 1, affine=False)), (
                'avgpool', ops.PoolBN('avg', channels, 3, stride, 1, affine
                =False)), ('skipconnect', nn.Identity() if stride == 1 else
                ops.FactorizedReduce(channels, channels, affine=False)), (
                'sepconv3x3', ops.SepConv(channels, channels, 3, stride, 1,
                affine=False)), ('sepconv5x5', ops.SepConv(channels,
                channels, 5, stride, 2, affine=False)), ('dilconv3x3', ops.
                DilConv(channels, channels, 3, stride, 2, 2, affine=False)),
                ('dilconv5x5', ops.DilConv(channels, channels, 5, stride, 4,
                2, affine=False))]), key=choice_keys[-1]))
        self.drop_path = ops.DropPath()
        self.input_switch = mutables.InputChoice(choose_from=choice_keys,
            n_chosen=2, key='{}_switch'.format(node_id))

    def forward(self, prev_nodes):
        assert len(self.ops) == len(prev_nodes)
        out = [op(node) for op, node in zip(self.ops, prev_nodes)]
        out = [(self.drop_path(o) if o is not None else None) for o in out]
        return self.input_switch(out)


class Cell(nn.Module):

    def __init__(self, n_nodes, channels_pp, channels_p, channels,
        reduction_p, reduction):
        super().__init__()
        self.reduction = reduction
        self.n_nodes = n_nodes
        if reduction_p:
            self.preproc0 = ops.FactorizedReduce(channels_pp, channels,
                affine=False)
        else:
            self.preproc0 = ops.StdConv(channels_pp, channels, 1, 1, 0,
                affine=False)
        self.preproc1 = ops.StdConv(channels_p, channels, 1, 1, 0, affine=False
            )
        self.mutable_ops = nn.ModuleList()
        for depth in range(2, self.n_nodes + 2):
            self.mutable_ops.append(Node('{}_n{}'.format('reduce' if
                reduction else 'normal', depth), depth, channels, 2 if
                reduction else 0))

    def forward(self, s0, s1):
        tensors = [self.preproc0(s0), self.preproc1(s1)]
        for node in self.mutable_ops:
            cur_tensor = node(tensors)
            tensors.append(cur_tensor)
        output = torch.cat(tensors[2:], dim=1)
        return output


class CNN(nn.Module):

    def __init__(self, input_size, in_channels, channels, n_classes,
        n_layers, n_nodes=4, stem_multiplier=3, auxiliary=False):
        super().__init__()
        self.in_channels = in_channels
        self.channels = channels
        self.n_classes = n_classes
        self.n_layers = n_layers
        self.aux_pos = 2 * n_layers // 3 if auxiliary else -1
        c_cur = stem_multiplier * self.channels
        self.stem = nn.Sequential(nn.Conv2d(in_channels, c_cur, 3, 1, 1,
            bias=False), nn.BatchNorm2d(c_cur))
        channels_pp, channels_p, c_cur = c_cur, c_cur, channels
        self.cells = nn.ModuleList()
        reduction_p, reduction = False, False
        for i in range(n_layers):
            reduction_p, reduction = reduction, False
            if i in [n_layers // 3, 2 * n_layers // 3]:
                c_cur *= 2
                reduction = True
            cell = Cell(n_nodes, channels_pp, channels_p, c_cur,
                reduction_p, reduction)
            self.cells.append(cell)
            c_cur_out = c_cur * n_nodes
            channels_pp, channels_p = channels_p, c_cur_out
            if i == self.aux_pos:
                self.aux_head = AuxiliaryHead(input_size // 4, channels_p,
                    n_classes)
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.linear = nn.Linear(channels_p, n_classes)

    def forward(self, x):
        s0 = s1 = self.stem(x)
        aux_logits = None
        for i, cell in enumerate(self.cells):
            s0, s1 = s1, cell(s0, s1)
            if i == self.aux_pos and self.training:
                aux_logits = self.aux_head(s1)
        out = self.gap(s1)
        out = out.view(out.size(0), -1)
        logits = self.linear(out)
        if aux_logits is not None:
            return logits, aux_logits
        return logits

    def drop_path_prob(self, p):
        for module in self.modules():
            if isinstance(module, ops.DropPath):
                module.p = p


class DropPath(nn.Module):

    def __init__(self, p=0.0):
        """
        Drop path with probability.

        Parameters
        ----------
        p : float
            Probability of an path to be zeroed.
        """
        super().__init__()
        self.p = p

    def forward(self, x):
        if self.training and self.p > 0.0:
            keep_prob = 1.0 - self.p
            mask = torch.zeros((x.size(0), 1, 1, 1), device=x.device
                ).bernoulli_(keep_prob)
            return x / keep_prob * mask
        return x


class PoolBN(nn.Module):
    """
    AvgPool or MaxPool with BN. `pool_type` must be `max` or `avg`.
    """

    def __init__(self, pool_type, C, kernel_size, stride, padding, affine=True
        ):
        super().__init__()
        if pool_type.lower() == 'max':
            self.pool = nn.MaxPool2d(kernel_size, stride, padding)
        elif pool_type.lower() == 'avg':
            self.pool = nn.AvgPool2d(kernel_size, stride, padding,
                count_include_pad=False)
        else:
            raise ValueError()
        self.bn = nn.BatchNorm2d(C, affine=affine)

    def forward(self, x):
        out = self.pool(x)
        out = self.bn(out)
        return out


class StdConv(nn.Module):
    """
    Standard conv: ReLU - Conv - BN
    """

    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
        super().__init__()
        self.net = nn.Sequential(nn.ReLU(), nn.Conv2d(C_in, C_out,
            kernel_size, stride, padding, bias=False), nn.BatchNorm2d(C_out,
            affine=affine))

    def forward(self, x):
        return self.net(x)


class FacConv(nn.Module):
    """
    Factorized conv: ReLU - Conv(Kx1) - Conv(1xK) - BN
    """

    def __init__(self, C_in, C_out, kernel_length, stride, padding, affine=True
        ):
        super().__init__()
        self.net = nn.Sequential(nn.ReLU(), nn.Conv2d(C_in, C_in, (
            kernel_length, 1), stride, padding, bias=False), nn.Conv2d(C_in,
            C_out, (1, kernel_length), stride, padding, bias=False), nn.
            BatchNorm2d(C_out, affine=affine))

    def forward(self, x):
        return self.net(x)


class DilConv(nn.Module):
    """
    (Dilated) depthwise separable conv.
    ReLU - (Dilated) depthwise separable - Pointwise - BN.
    If dilation == 2, 3x3 conv => 5x5 receptive field, 5x5 conv => 9x9 receptive field.
    """

    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation,
        affine=True):
        super().__init__()
        self.net = nn.Sequential(nn.ReLU(), nn.Conv2d(C_in, C_in,
            kernel_size, stride, padding, dilation=dilation, groups=C_in,
            bias=False), nn.Conv2d(C_in, C_out, 1, stride=1, padding=0,
            bias=False), nn.BatchNorm2d(C_out, affine=affine))

    def forward(self, x):
        return self.net(x)


class SepConv(nn.Module):
    """
    Depthwise separable conv.
    DilConv(dilation=1) * 2.
    """

    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
        super().__init__()
        self.net = nn.Sequential(DilConv(C_in, C_in, kernel_size, stride,
            padding, dilation=1, affine=affine), DilConv(C_in, C_out,
            kernel_size, 1, padding, dilation=1, affine=affine))

    def forward(self, x):
        return self.net(x)


class FactorizedReduce(nn.Module):
    """
    Reduce feature map size by factorized pointwise (stride=2).
    """

    def __init__(self, C_in, C_out, affine=True):
        super().__init__()
        self.relu = nn.ReLU()
        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0,
            bias=False)
        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0,
            bias=False)
        self.bn = nn.BatchNorm2d(C_out, affine=affine)

    def forward(self, x):
        x = self.relu(x)
        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)
        out = self.bn(out)
        return out


class AuxiliaryHead(nn.Module):

    def __init__(self, in_channels, num_classes):
        super().__init__()
        self.in_channels = in_channels
        self.num_classes = num_classes
        self.pooling = nn.Sequential(nn.ReLU(), nn.AvgPool2d(5, 3, 2))
        self.proj = nn.Sequential(StdConv(in_channels, 128), StdConv(128, 768))
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(768, 10, bias=False)

    def forward(self, x):
        bs = x.size(0)
        x = self.pooling(x)
        x = self.proj(x)
        x = self.avg_pool(x).view(bs, -1)
        x = self.fc(x)
        return x


class Cell(nn.Module):

    def __init__(self, cell_name, prev_labels, channels):
        super().__init__()
        self.input_choice = mutables.InputChoice(choose_from=prev_labels,
            n_chosen=1, return_mask=True, key=cell_name + '_input')
        self.op_choice = mutables.LayerChoice([SepConvBN(channels, channels,
            3, 1), SepConvBN(channels, channels, 5, 2), Pool('avg', 3, 1, 1
            ), Pool('max', 3, 1, 1), nn.Identity()], key=cell_name + '_op')

    def forward(self, prev_layers):
        chosen_input, chosen_mask = self.input_choice(prev_layers)
        cell_out = self.op_choice(chosen_input)
        return cell_out, chosen_mask


class Calibration(nn.Module):

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.process = None
        if in_channels != out_channels:
            self.process = StdConv(in_channels, out_channels)

    def forward(self, x):
        if self.process is None:
            return x
        return self.process(x)


class ReductionLayer(nn.Module):

    def __init__(self, in_channels_pp, in_channels_p, out_channels):
        super().__init__()
        self.reduce0 = FactorizedReduce(in_channels_pp, out_channels,
            affine=False)
        self.reduce1 = FactorizedReduce(in_channels_p, out_channels, affine
            =False)

    def forward(self, pprev, prev):
        return self.reduce0(pprev), self.reduce1(prev)


class ENASLayer(nn.Module):

    def __init__(self, num_nodes, in_channels_pp, in_channels_p,
        out_channels, reduction):
        super().__init__()
        self.preproc0 = Calibration(in_channels_pp, out_channels)
        self.preproc1 = Calibration(in_channels_p, out_channels)
        self.num_nodes = num_nodes
        name_prefix = 'reduce' if reduction else 'normal'
        self.nodes = nn.ModuleList()
        node_labels = [mutables.InputChoice.NO_KEY, mutables.InputChoice.NO_KEY
            ]
        for i in range(num_nodes):
            node_labels.append('{}_node_{}'.format(name_prefix, i))
            self.nodes.append(Node(node_labels[-1], node_labels[:-1],
                out_channels))
        self.final_conv_w = nn.Parameter(torch.zeros(out_channels, self.
            num_nodes + 2, out_channels, 1, 1), requires_grad=True)
        self.bn = nn.BatchNorm2d(out_channels, affine=False)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_normal_(self.final_conv_w)

    def forward(self, pprev, prev):
        pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)
        prev_nodes_out = [pprev_, prev_]
        nodes_used_mask = torch.zeros(self.num_nodes + 2, dtype=torch.bool,
            device=prev.device)
        for i in range(self.num_nodes):
            node_out, mask = self.nodes[i](prev_nodes_out)
            nodes_used_mask[:mask.size(0)] |= mask.to(node_out.device)
            prev_nodes_out.append(node_out)
        unused_nodes = torch.cat([out for used, out in zip(nodes_used_mask,
            prev_nodes_out) if not used], 1)
        unused_nodes = F.relu(unused_nodes)
        conv_weight = self.final_conv_w[:, (~nodes_used_mask), :, :, :]
        conv_weight = conv_weight.view(conv_weight.size(0), -1, 1, 1)
        out = F.conv2d(unused_nodes, conv_weight)
        return prev, self.bn(out)


class MicroNetwork(nn.Module):

    def __init__(self, num_layers=2, num_nodes=5, out_channels=24,
        in_channels=3, num_classes=10, dropout_rate=0.0, use_aux_heads=False):
        super().__init__()
        self.num_layers = num_layers
        self.use_aux_heads = use_aux_heads
        self.stem = nn.Sequential(nn.Conv2d(in_channels, out_channels * 3, 
            3, 1, 1, bias=False), nn.BatchNorm2d(out_channels * 3))
        pool_distance = self.num_layers // 3
        pool_layers = [pool_distance, 2 * pool_distance + 1]
        self.dropout = nn.Dropout(dropout_rate)
        self.layers = nn.ModuleList()
        c_pp = c_p = out_channels * 3
        c_cur = out_channels
        for layer_id in range(self.num_layers + 2):
            reduction = False
            if layer_id in pool_layers:
                c_cur, reduction = c_p * 2, True
                self.layers.append(ReductionLayer(c_pp, c_p, c_cur))
                c_pp = c_p = c_cur
            self.layers.append(ENASLayer(num_nodes, c_pp, c_p, c_cur,
                reduction))
            if self.use_aux_heads and layer_id == pool_layers[-1] + 1:
                self.layers.append(AuxiliaryHead(c_cur, num_classes))
            c_pp, c_p = c_p, c_cur
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.dense = nn.Linear(c_cur, num_classes)
        self.reset_parameters()

    def reset_parameters(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)

    def forward(self, x):
        bs = x.size(0)
        prev = cur = self.stem(x)
        aux_logits = None
        for layer in self.layers:
            if isinstance(layer, AuxiliaryHead):
                if self.training:
                    aux_logits = layer(cur)
            else:
                prev, cur = layer(prev, cur)
        cur = self.gap(F.relu(cur)).view(bs, -1)
        cur = self.dropout(cur)
        logits = self.dense(cur)
        if aux_logits is not None:
            return logits, aux_logits
        return logits


class StdConv(nn.Module):

    def __init__(self, C_in, C_out):
        super(StdConv, self).__init__()
        self.conv = nn.Sequential(nn.Conv2d(C_in, C_out, 1, stride=1,
            padding=0, bias=False), nn.BatchNorm2d(C_out, affine=False), nn
            .ReLU())

    def forward(self, x):
        return self.conv(x)


class PoolBranch(nn.Module):

    def __init__(self, pool_type, C_in, C_out, kernel_size, stride, padding,
        affine=False):
        super().__init__()
        self.preproc = StdConv(C_in, C_out)
        self.pool = Pool(pool_type, kernel_size, stride, padding)
        self.bn = nn.BatchNorm2d(C_out, affine=affine)

    def forward(self, x):
        out = self.preproc(x)
        out = self.pool(out)
        out = self.bn(out)
        return out


class SeparableConv(nn.Module):

    def __init__(self, C_in, C_out, kernel_size, stride, padding):
        super(SeparableConv, self).__init__()
        self.depthwise = nn.Conv2d(C_in, C_in, kernel_size=kernel_size,
            padding=padding, stride=stride, groups=C_in, bias=False)
        self.pointwise = nn.Conv2d(C_in, C_out, kernel_size=1, bias=False)

    def forward(self, x):
        out = self.depthwise(x)
        out = self.pointwise(out)
        return out


class ConvBranch(nn.Module):

    def __init__(self, C_in, C_out, kernel_size, stride, padding, separable):
        super(ConvBranch, self).__init__()
        self.preproc = StdConv(C_in, C_out)
        if separable:
            self.conv = SeparableConv(C_out, C_out, kernel_size, stride,
                padding)
        else:
            self.conv = nn.Conv2d(C_out, C_out, kernel_size, stride=stride,
                padding=padding)
        self.postproc = nn.Sequential(nn.BatchNorm2d(C_out, affine=False),
            nn.ReLU())

    def forward(self, x):
        out = self.preproc(x)
        out = self.conv(out)
        out = self.postproc(out)
        return out


class FactorizedReduce(nn.Module):

    def __init__(self, C_in, C_out, affine=False):
        super().__init__()
        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0,
            bias=False)
        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0,
            bias=False)
        self.bn = nn.BatchNorm2d(C_out, affine=affine)

    def forward(self, x):
        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)
        out = self.bn(out)
        return out


class Pool(nn.Module):

    def __init__(self, pool_type, kernel_size, stride, padding):
        super().__init__()
        if pool_type.lower() == 'max':
            self.pool = nn.MaxPool2d(kernel_size, stride, padding)
        elif pool_type.lower() == 'avg':
            self.pool = nn.AvgPool2d(kernel_size, stride, padding,
                count_include_pad=False)
        else:
            raise ValueError()

    def forward(self, x):
        return self.pool(x)


class SepConvBN(nn.Module):

    def __init__(self, C_in, C_out, kernel_size, padding):
        super().__init__()
        self.relu = nn.ReLU()
        self.conv = SeparableConv(C_in, C_out, kernel_size, 1, padding)
        self.bn = nn.BatchNorm2d(C_out, affine=True)

    def forward(self, x):
        x = self.relu(x)
        x = self.conv(x)
        x = self.bn(x)
        return x


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = LayerChoice([nn.Conv2d(3, 6, 3, padding=1), nn.Conv2d(
            3, 6, 5, padding=2)])
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = LayerChoice([nn.Conv2d(6, 16, 3, padding=1), nn.Conv2d
            (6, 16, 5, padding=2)])
        self.conv3 = nn.Conv2d(16, 16, 1)
        self.skipconnect = InputChoice(n_candidates=1)
        self.bn = nn.BatchNorm2d(16)
        self.gap = nn.AdaptiveAvgPool2d(4)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        bs = x.size(0)
        x = self.pool(F.relu(self.conv1(x)))
        x0 = F.relu(self.conv2(x))
        x1 = F.relu(self.conv3(x0))
        x0 = self.skipconnect([x0])
        if x0 is not None:
            x1 += x0
        x = self.pool(self.bn(x1))
        x = self.gap(x).view(bs, -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


class SearchMobileNet(nn.Module):

    def __init__(self, width_stages=[24, 40, 80, 96, 192, 320],
        n_cell_stages=[4, 4, 4, 4, 4, 1], stride_stages=[2, 2, 2, 1, 2, 1],
        width_mult=1, n_classes=1000, dropout_rate=0, bn_param=(0.1, 0.001)):
        """
        Parameters
        ----------
        width_stages: str
            width (output channels) of each cell stage in the block
        n_cell_stages: str
            number of cells in each cell stage
        stride_strages: str
            stride of each cell stage in the block
        width_mult : int
            the scale factor of width
        """
        super(SearchMobileNet, self).__init__()
        input_channel = putils.make_divisible(32 * width_mult, 8)
        first_cell_width = putils.make_divisible(16 * width_mult, 8)
        for i in range(len(width_stages)):
            width_stages[i] = putils.make_divisible(width_stages[i] *
                width_mult, 8)
        first_conv = ops.ConvLayer(3, input_channel, kernel_size=3, stride=
            2, use_bn=True, act_func='relu6', ops_order='weight_bn_act')
        first_block_conv = ops.OPS['3x3_MBConv1'](input_channel,
            first_cell_width, 1)
        first_block = first_block_conv
        input_channel = first_cell_width
        blocks = [first_block]
        stage_cnt = 0
        for width, n_cell, s in zip(width_stages, n_cell_stages, stride_stages
            ):
            for i in range(n_cell):
                if i == 0:
                    stride = s
                else:
                    stride = 1
                op_candidates = [ops.OPS['3x3_MBConv3'](input_channel,
                    width, stride), ops.OPS['3x3_MBConv6'](input_channel,
                    width, stride), ops.OPS['5x5_MBConv3'](input_channel,
                    width, stride), ops.OPS['5x5_MBConv6'](input_channel,
                    width, stride), ops.OPS['7x7_MBConv3'](input_channel,
                    width, stride), ops.OPS['7x7_MBConv6'](input_channel,
                    width, stride)]
                if stride == 1 and input_channel == width:
                    op_candidates += [ops.OPS['Zero'](input_channel, width,
                        stride)]
                    conv_op = nas.mutables.LayerChoice(op_candidates,
                        return_mask=True, key='s{}_c{}'.format(stage_cnt, i))
                else:
                    conv_op = nas.mutables.LayerChoice(op_candidates,
                        return_mask=True, key='s{}_c{}'.format(stage_cnt, i))
                if stride == 1 and input_channel == width:
                    shortcut = ops.IdentityLayer(input_channel, input_channel)
                else:
                    shortcut = None
                inverted_residual_block = ops.MobileInvertedResidualBlock(
                    conv_op, shortcut, op_candidates)
                blocks.append(inverted_residual_block)
                input_channel = width
            stage_cnt += 1
        last_channel = putils.make_devisible(1280 * width_mult, 8
            ) if width_mult > 1.0 else 1280
        feature_mix_layer = ops.ConvLayer(input_channel, last_channel,
            kernel_size=1, use_bn=True, act_func='relu6', ops_order=
            'weight_bn_act')
        classifier = ops.LinearLayer(last_channel, n_classes, dropout_rate=
            dropout_rate)
        self.first_conv = first_conv
        self.blocks = nn.ModuleList(blocks)
        self.feature_mix_layer = feature_mix_layer
        self.global_avg_pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = classifier
        self.set_bn_param(momentum=bn_param[0], eps=bn_param[1])

    def forward(self, x):
        x = self.first_conv(x)
        for block in self.blocks:
            x = block(x)
        x = self.feature_mix_layer(x)
        x = self.global_avg_pooling(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    def set_bn_param(self, momentum, eps):
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):
                m.momentum = momentum
                m.eps = eps
        return

    def init_model(self, model_init='he_fout', init_div_groups=False):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                if model_init == 'he_fout':
                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                    if init_div_groups:
                        n /= m.groups
                    m.weight.data.normal_(0, math.sqrt(2.0 / n))
                elif model_init == 'he_fin':
                    n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels
                    if init_div_groups:
                        n /= m.groups
                    m.weight.data.normal_(0, math.sqrt(2.0 / n))
                else:
                    raise NotImplementedError
            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d
                ):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                stdv = 1.0 / math.sqrt(m.weight.size(1))
                m.weight.data.uniform_(-stdv, stdv)
                if m.bias is not None:
                    m.bias.data.zero_()


class MobileInvertedResidualBlock(nn.Module):

    def __init__(self, mobile_inverted_conv, shortcut, op_candidates_list):
        super(MobileInvertedResidualBlock, self).__init__()
        self.mobile_inverted_conv = mobile_inverted_conv
        self.shortcut = shortcut
        self.op_candidates_list = op_candidates_list

    def forward(self, x):
        out, idx = self.mobile_inverted_conv(x)
        if not isinstance(idx, int):
            idx = (idx == 1).nonzero()
        if self.op_candidates_list[idx].is_zero_layer():
            res = x
        elif self.shortcut is None:
            res = out
        else:
            conv_x = out
            skip_x = self.shortcut(x)
            res = skip_x + conv_x
        return res


class ShuffleLayer(nn.Module):

    def __init__(self, groups):
        super(ShuffleLayer, self).__init__()
        self.groups = groups

    def forward(self, x):
        batchsize, num_channels, height, width = x.size()
        channels_per_group = num_channels // self.groups
        x = x.view(batchsize, self.groups, channels_per_group, height, width)
        x = torch.transpose(x, 1, 2).contiguous()
        x = x.view(batchsize, -1, height, width)
        return x


def build_activation(act_func, inplace=True):
    if act_func == 'relu':
        return nn.ReLU(inplace=inplace)
    elif act_func == 'relu6':
        return nn.ReLU6(inplace=inplace)
    elif act_func == 'tanh':
        return nn.Tanh()
    elif act_func == 'sigmoid':
        return nn.Sigmoid()
    elif act_func is None:
        return None
    else:
        raise ValueError('do not support: %s' % act_func)


class Base2DLayer(nn.Module):

    def __init__(self, in_channels, out_channels, use_bn=True, act_func=
        'relu', dropout_rate=0, ops_order='weight_bn_act'):
        super(Base2DLayer, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.use_bn = use_bn
        self.act_func = act_func
        self.dropout_rate = dropout_rate
        self.ops_order = ops_order
        """ modules """
        modules = {}
        if self.use_bn:
            if self.bn_before_weight:
                modules['bn'] = nn.BatchNorm2d(in_channels)
            else:
                modules['bn'] = nn.BatchNorm2d(out_channels)
        else:
            modules['bn'] = None
        modules['act'] = build_activation(self.act_func, self.ops_list[0] !=
            'act')
        if self.dropout_rate > 0:
            modules['dropout'] = nn.Dropout2d(self.dropout_rate, inplace=True)
        else:
            modules['dropout'] = None
        modules['weight'] = self.weight_op()
        for op in self.ops_list:
            if modules[op] is None:
                continue
            elif op == 'weight':
                if modules['dropout'] is not None:
                    self.add_module('dropout', modules['dropout'])
                for key in modules['weight']:
                    self.add_module(key, modules['weight'][key])
            else:
                self.add_module(op, modules[op])

    @property
    def ops_list(self):
        return self.ops_order.split('_')

    @property
    def bn_before_weight(self):
        for op in self.ops_list:
            if op == 'bn':
                return True
            elif op == 'weight':
                return False
        raise ValueError('Invalid ops_order: %s' % self.ops_order)

    def weight_op(self):
        raise NotImplementedError

    def forward(self, x):
        for module in self._modules.values():
            x = module(x)
        return x

    @staticmethod
    def is_zero_layer():
        return False


class LinearLayer(nn.Module):

    def __init__(self, in_features, out_features, bias=True, use_bn=False,
        act_func=None, dropout_rate=0, ops_order='weight_bn_act'):
        super(LinearLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.bias = bias
        self.use_bn = use_bn
        self.act_func = act_func
        self.dropout_rate = dropout_rate
        self.ops_order = ops_order
        """ modules """
        modules = {}
        if self.use_bn:
            if self.bn_before_weight:
                modules['bn'] = nn.BatchNorm1d(in_features)
            else:
                modules['bn'] = nn.BatchNorm1d(out_features)
        else:
            modules['bn'] = None
        modules['act'] = build_activation(self.act_func, self.ops_list[0] !=
            'act')
        if self.dropout_rate > 0:
            modules['dropout'] = nn.Dropout(self.dropout_rate, inplace=True)
        else:
            modules['dropout'] = None
        modules['weight'] = {'linear': nn.Linear(self.in_features, self.
            out_features, self.bias)}
        for op in self.ops_list:
            if modules[op] is None:
                continue
            elif op == 'weight':
                if modules['dropout'] is not None:
                    self.add_module('dropout', modules['dropout'])
                for key in modules['weight']:
                    self.add_module(key, modules['weight'][key])
            else:
                self.add_module(op, modules[op])

    @property
    def ops_list(self):
        return self.ops_order.split('_')

    @property
    def bn_before_weight(self):
        for op in self.ops_list:
            if op == 'bn':
                return True
            elif op == 'weight':
                return False
        raise ValueError('Invalid ops_order: %s' % self.ops_order)

    def forward(self, x):
        for module in self._modules.values():
            x = module(x)
        return x

    @staticmethod
    def is_zero_layer():
        return False


def get_same_padding(kernel_size):
    if isinstance(kernel_size, tuple):
        assert len(kernel_size) == 2, 'invalid kernel size: %s' % kernel_size
        p1 = get_same_padding(kernel_size[0])
        p2 = get_same_padding(kernel_size[1])
        return p1, p2
    assert isinstance(kernel_size, int
        ), 'kernel size should be either `int` or `tuple`'
    assert kernel_size % 2 > 0, 'kernel size should be odd number'
    return kernel_size // 2


class MBInvertedConvLayer(nn.Module):
    """
    This layer is introduced in section 4.2 in the paper https://arxiv.org/pdf/1812.00332.pdf
    """

    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,
        expand_ratio=6, mid_channels=None):
        super(MBInvertedConvLayer, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.expand_ratio = expand_ratio
        self.mid_channels = mid_channels
        if self.mid_channels is None:
            feature_dim = round(self.in_channels * self.expand_ratio)
        else:
            feature_dim = self.mid_channels
        if self.expand_ratio == 1:
            self.inverted_bottleneck = None
        else:
            self.inverted_bottleneck = nn.Sequential(OrderedDict([('conv',
                nn.Conv2d(self.in_channels, feature_dim, 1, 1, 0, bias=
                False)), ('bn', nn.BatchNorm2d(feature_dim)), ('act', nn.
                ReLU6(inplace=True))]))
        pad = get_same_padding(self.kernel_size)
        self.depth_conv = nn.Sequential(OrderedDict([('conv', nn.Conv2d(
            feature_dim, feature_dim, kernel_size, stride, pad, groups=
            feature_dim, bias=False)), ('bn', nn.BatchNorm2d(feature_dim)),
            ('act', nn.ReLU6(inplace=True))]))
        self.point_linear = nn.Sequential(OrderedDict([('conv', nn.Conv2d(
            feature_dim, out_channels, 1, 1, 0, bias=False)), ('bn', nn.
            BatchNorm2d(out_channels))]))

    def forward(self, x):
        if self.inverted_bottleneck:
            x = self.inverted_bottleneck(x)
        x = self.depth_conv(x)
        x = self.point_linear(x)
        return x

    @staticmethod
    def is_zero_layer():
        return False


class ZeroLayer(nn.Module):

    def __init__(self, stride):
        super(ZeroLayer, self).__init__()
        self.stride = stride

    def forward(self, x):
        """n, c, h, w = x.size()
        h //= self.stride
        w //= self.stride
        device = x.get_device() if x.is_cuda else torch.device('cpu')
        # noinspection PyUnresolvedReferences
        padding = torch.zeros(n, c, h, w, device=device, requires_grad=False)
        return padding"""
        return x * 0

    @staticmethod
    def is_zero_layer():
        return True


class ShuffleNetBlock(nn.Module):
    """
    When stride = 1, the block receives input with 2 * inp channels. Otherwise inp channels.
    """

    def __init__(self, inp, oup, mid_channels, ksize, stride, sequence='pdp'):
        super().__init__()
        assert stride in [1, 2]
        assert ksize in [3, 5, 7]
        self.channels = inp // 2 if stride == 1 else inp
        self.inp = inp
        self.oup = oup
        self.mid_channels = mid_channels
        self.ksize = ksize
        self.stride = stride
        self.pad = ksize // 2
        self.oup_main = oup - self.channels
        assert self.oup_main > 0
        self.branch_main = nn.Sequential(*self._decode_point_depth_conv(
            sequence))
        if stride == 2:
            self.branch_proj = nn.Sequential(nn.Conv2d(self.channels, self.
                channels, ksize, stride, self.pad, groups=self.channels,
                bias=False), nn.BatchNorm2d(self.channels, affine=False),
                nn.Conv2d(self.channels, self.channels, 1, 1, 0, bias=False
                ), nn.BatchNorm2d(self.channels, affine=False), nn.ReLU(
                inplace=True))

    def forward(self, x):
        if self.stride == 2:
            x_proj, x = self.branch_proj(x), x
        else:
            x_proj, x = self._channel_shuffle(x)
        return torch.cat((x_proj, self.branch_main(x)), 1)

    def _decode_point_depth_conv(self, sequence):
        result = []
        first_depth = first_point = True
        pc = c = self.channels
        for i, token in enumerate(sequence):
            if i + 1 == len(sequence):
                assert token == 'p', 'Last conv must be point-wise conv.'
                c = self.oup_main
            elif token == 'p' and first_point:
                c = self.mid_channels
            if token == 'd':
                assert pc == c, 'Depth-wise conv must not change channels.'
                result.append(nn.Conv2d(pc, c, self.ksize, self.stride if
                    first_depth else 1, self.pad, groups=c, bias=False))
                result.append(nn.BatchNorm2d(c, affine=False))
                first_depth = False
            elif token == 'p':
                result.append(nn.Conv2d(pc, c, 1, 1, 0, bias=False))
                result.append(nn.BatchNorm2d(c, affine=False))
                result.append(nn.ReLU(inplace=True))
                first_point = False
            else:
                raise ValueError('Conv sequence must be d and p.')
            pc = c
        return result

    def _channel_shuffle(self, x):
        bs, num_channels, height, width = x.data.size()
        assert num_channels % 4 == 0
        x = x.reshape(bs * num_channels // 2, 2, height * width)
        x = x.permute(1, 0, 2)
        x = x.reshape(2, -1, num_channels // 2, height, width)
        return x[0], x[1]


class ShuffleXceptionBlock(ShuffleNetBlock):

    def __init__(self, inp, oup, mid_channels, stride):
        super().__init__(inp, oup, mid_channels, 3, stride, 'dpdpdp')


class ShuffleNetV2OneShot(nn.Module):
    block_keys = ['shufflenet_3x3', 'shufflenet_5x5', 'shufflenet_7x7',
        'xception_3x3']

    def __init__(self, input_size=224, first_conv_channels=16,
        last_conv_channels=1024, n_classes=1000, op_flops_path=
        './data/op_flops_dict.pkl'):
        super().__init__()
        assert input_size % 32 == 0
        with open(os.path.join(os.path.dirname(__file__), op_flops_path), 'rb'
            ) as fp:
            self._op_flops_dict = pickle.load(fp)
        self.stage_blocks = [4, 4, 8, 4]
        self.stage_channels = [64, 160, 320, 640]
        self._parsed_flops = dict()
        self._input_size = input_size
        self._feature_map_size = input_size
        self._first_conv_channels = first_conv_channels
        self._last_conv_channels = last_conv_channels
        self._n_classes = n_classes
        self.first_conv = nn.Sequential(nn.Conv2d(3, first_conv_channels, 3,
            2, 1, bias=False), nn.BatchNorm2d(first_conv_channels, affine=
            False), nn.ReLU(inplace=True))
        self._feature_map_size //= 2
        p_channels = first_conv_channels
        features = []
        for num_blocks, channels in zip(self.stage_blocks, self.stage_channels
            ):
            features.extend(self._make_blocks(num_blocks, p_channels, channels)
                )
            p_channels = channels
        self.features = nn.Sequential(*features)
        self.conv_last = nn.Sequential(nn.Conv2d(p_channels,
            last_conv_channels, 1, 1, 0, bias=False), nn.BatchNorm2d(
            last_conv_channels, affine=False), nn.ReLU(inplace=True))
        self.globalpool = nn.AvgPool2d(self._feature_map_size)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Sequential(nn.Linear(last_conv_channels,
            n_classes, bias=False))
        self._initialize_weights()

    def _make_blocks(self, blocks, in_channels, channels):
        result = []
        for i in range(blocks):
            stride = 2 if i == 0 else 1
            inp = in_channels if i == 0 else channels
            oup = channels
            base_mid_channels = channels // 2
            mid_channels = int(base_mid_channels)
            choice_block = mutables.LayerChoice([ShuffleNetBlock(inp, oup,
                mid_channels=mid_channels, ksize=3, stride=stride),
                ShuffleNetBlock(inp, oup, mid_channels=mid_channels, ksize=
                5, stride=stride), ShuffleNetBlock(inp, oup, mid_channels=
                mid_channels, ksize=7, stride=stride), ShuffleXceptionBlock
                (inp, oup, mid_channels=mid_channels, stride=stride)])
            result.append(choice_block)
            flop_key = (inp, oup, mid_channels, self._feature_map_size,
                self._feature_map_size, stride)
            self._parsed_flops[choice_block.key] = [self._op_flops_dict[
                '{}_stride_{}'.format(k, stride)][flop_key] for k in self.
                block_keys]
            if stride == 2:
                self._feature_map_size //= 2
        return result

    def forward(self, x):
        bs = x.size(0)
        x = self.first_conv(x)
        x = self.features(x)
        x = self.conv_last(x)
        x = self.globalpool(x)
        x = self.dropout(x)
        x = x.contiguous().view(bs, -1)
        x = self.classifier(x)
        return x

    def get_candidate_flops(self, candidate):
        conv1_flops = self._op_flops_dict['conv1'][3, self.
            _first_conv_channels, self._input_size, self._input_size, 2]
        rest_flops = self._op_flops_dict['rest_operation'][self.
            stage_channels[-1], self._n_classes, self._feature_map_size,
            self._feature_map_size, 1]
        total_flops = conv1_flops + rest_flops
        for k, m in candidate.items():
            parsed_flops_dict = self._parsed_flops[k]
            if isinstance(m, dict):
                total_flops += parsed_flops_dict[m['_idx']]
            else:
                total_flops += parsed_flops_dict[torch.max(m, 0)[1]]
        return total_flops

    def _initialize_weights(self):
        for name, m in self.named_modules():
            if isinstance(m, nn.Conv2d):
                if 'first' in name:
                    nn.init.normal_(m.weight, 0, 0.01)
                else:
                    nn.init.normal_(m.weight, 0, 1.0 / m.weight.shape[1])
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0001)
                nn.init.constant_(m.running_mean, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0001)
                nn.init.constant_(m.running_mean, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)


class CrossEntropyLabelSmooth(nn.Module):

    def __init__(self, num_classes, epsilon):
        super(CrossEntropyLabelSmooth, self).__init__()
        self.num_classes = num_classes
        self.epsilon = epsilon
        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, inputs, targets):
        log_probs = self.logsoftmax(inputs)
        targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze
            (1), 1)
        targets = (1 - self.epsilon
            ) * targets + self.epsilon / self.num_classes
        loss = (-targets * log_probs).mean(0).sum()
        return loss


class Mask(nn.Module):

    def forward(self, seq, mask):
        seq_mask = torch.unsqueeze(mask, 2)
        seq_mask = torch.transpose(seq_mask.repeat(1, 1, seq.size()[1]), 1, 2)
        return seq.where(torch.eq(seq_mask, 1), torch.zeros_like(seq))


class BatchNorm(nn.Module):

    def __init__(self, num_features, pre_mask, post_mask, eps=1e-05, decay=
        0.9, affine=True):
        super(BatchNorm, self).__init__()
        self.mask_opt = Mask()
        self.pre_mask = pre_mask
        self.post_mask = post_mask
        self.bn = nn.BatchNorm1d(num_features, eps=eps, momentum=1.0 -
            decay, affine=affine)

    def forward(self, seq, mask):
        if self.pre_mask:
            seq = self.mask_opt(seq, mask)
        seq = self.bn(seq)
        if self.post_mask:
            seq = self.mask_opt(seq, mask)
        return seq


class ConvBN(nn.Module):

    def __init__(self, kernal_size, in_channels, out_channels,
        cnn_keep_prob, pre_mask, post_mask, with_bn=True, with_relu=True):
        super(ConvBN, self).__init__()
        self.mask_opt = Mask()
        self.pre_mask = pre_mask
        self.post_mask = post_mask
        self.with_bn = with_bn
        self.with_relu = with_relu
        self.conv = nn.Conv1d(in_channels, out_channels, kernal_size, 1,
            bias=True, padding=(kernal_size - 1) // 2)
        self.dropout = nn.Dropout(p=1 - cnn_keep_prob)
        if with_bn:
            self.bn = BatchNorm(out_channels, not post_mask, True)
        if with_relu:
            self.relu = nn.ReLU()

    def forward(self, seq, mask):
        if self.pre_mask:
            seq = self.mask_opt(seq, mask)
        seq = self.conv(seq)
        if self.post_mask:
            seq = self.mask_opt(seq, mask)
        if self.with_bn:
            seq = self.bn(seq, mask)
        if self.with_relu:
            seq = self.relu(seq)
        seq = self.dropout(seq)
        return seq


class AvgPool(nn.Module):

    def __init__(self, kernal_size, pre_mask, post_mask):
        super(AvgPool, self).__init__()
        self.avg_pool = nn.AvgPool1d(kernal_size, 1, padding=(kernal_size -
            1) // 2)
        self.pre_mask = pre_mask
        self.post_mask = post_mask
        self.mask_opt = Mask()

    def forward(self, seq, mask):
        if self.pre_mask:
            seq = self.mask_opt(seq, mask)
        seq = self.avg_pool(seq)
        if self.post_mask:
            seq = self.mask_opt(seq, mask)
        return seq


class MaxPool(nn.Module):

    def __init__(self, kernal_size, pre_mask, post_mask):
        super(MaxPool, self).__init__()
        self.max_pool = nn.MaxPool1d(kernal_size, 1, padding=(kernal_size -
            1) // 2)
        self.pre_mask = pre_mask
        self.post_mask = post_mask
        self.mask_opt = Mask()

    def forward(self, seq, mask):
        if self.pre_mask:
            seq = self.mask_opt(seq, mask)
        seq = self.max_pool(seq)
        if self.post_mask:
            seq = self.mask_opt(seq, mask)
        return seq


INF = 10000000000.0


class Attention(nn.Module):

    def __init__(self, num_units, num_heads, keep_prob, is_mask):
        super(Attention, self).__init__()
        self.num_heads = num_heads
        self.keep_prob = keep_prob
        self.linear_q = nn.Linear(num_units, num_units)
        self.linear_k = nn.Linear(num_units, num_units)
        self.linear_v = nn.Linear(num_units, num_units)
        self.bn = BatchNorm(num_units, True, is_mask)
        self.dropout = nn.Dropout(p=1 - self.keep_prob)

    def forward(self, seq, mask):
        in_c = seq.size()[1]
        seq = torch.transpose(seq, 1, 2)
        queries = seq
        keys = seq
        num_heads = self.num_heads
        Q = F.relu(self.linear_q(seq))
        K = F.relu(self.linear_k(seq))
        V = F.relu(self.linear_v(seq))
        Q_ = torch.cat(torch.split(Q, in_c // num_heads, dim=2), dim=0)
        K_ = torch.cat(torch.split(K, in_c // num_heads, dim=2), dim=0)
        V_ = torch.cat(torch.split(V, in_c // num_heads, dim=2), dim=0)
        outputs = torch.matmul(Q_, K_.transpose(1, 2))
        outputs = outputs / K_.size()[-1] ** 0.5
        key_masks = mask.repeat(num_heads, 1)
        key_masks = torch.unsqueeze(key_masks, 1)
        key_masks = key_masks.repeat(1, queries.size()[1], 1)
        paddings = torch.ones_like(outputs) * -INF
        outputs = torch.where(torch.eq(key_masks, 0), paddings, outputs)
        query_masks = mask.repeat(num_heads, 1)
        query_masks = torch.unsqueeze(query_masks, -1)
        query_masks = query_masks.repeat(1, 1, keys.size()[1]).float()
        att_scores = F.softmax(outputs, dim=-1) * query_masks
        att_scores = self.dropout(att_scores)
        x_outputs = torch.matmul(att_scores, V_)
        x_outputs = torch.cat(torch.split(x_outputs, x_outputs.size()[0] //
            num_heads, dim=0), dim=2)
        x = torch.transpose(x_outputs, 1, 2)
        x = self.bn(x, mask)
        return x


def get_length(mask):
    length = torch.sum(mask, 1)
    length = length.long()
    return length


class RNN(nn.Module):

    def __init__(self, hidden_size, output_keep_prob):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.bid_rnn = nn.GRU(hidden_size, hidden_size, batch_first=True,
            bidirectional=True)
        self.output_keep_prob = output_keep_prob
        self.out_dropout = nn.Dropout(p=1 - self.output_keep_prob)

    def forward(self, seq, mask):
        max_len = seq.size()[2]
        length = get_length(mask)
        seq = torch.transpose(seq, 1, 2)
        packed_seq = nn.utils.rnn.pack_padded_sequence(seq, length,
            batch_first=True, enforce_sorted=False)
        outputs, _ = self.bid_rnn(packed_seq)
        outputs = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=
            True, total_length=max_len)[0]
        outputs = outputs.view(-1, max_len, 2, self.hidden_size).sum(2)
        outputs = self.out_dropout(outputs)
        return torch.transpose(outputs, 1, 2)


class LinearCombine(nn.Module):

    def __init__(self, layers_num, trainable=True, input_aware=False,
        word_level=False):
        super(LinearCombine, self).__init__()
        self.input_aware = input_aware
        self.word_level = word_level
        if input_aware:
            raise NotImplementedError('Input aware is not supported.')
        self.w = nn.Parameter(torch.full((layers_num, 1, 1, 1), 1.0 /
            layers_num), requires_grad=trainable)

    def forward(self, seq):
        nw = F.softmax(self.w, dim=0)
        seq = torch.mul(seq, nw)
        seq = torch.sum(seq, dim=0)
        return seq


EPS = 1e-12


class GlobalAvgPool(nn.Module):

    def forward(self, x, mask):
        x = torch.sum(x, 2)
        length = torch.sum(mask, 1, keepdim=True).float()
        length += torch.eq(length, 0.0).float() * EPS
        length = length.repeat(1, x.size()[1])
        x /= length
        return x


class GlobalMaxPool(nn.Module):

    def forward(self, x, mask):
        mask = torch.eq(mask.float(), 0.0).long()
        mask = torch.unsqueeze(mask, dim=1).repeat(1, x.size()[1], 1)
        mask *= -INF
        x += mask
        x, _ = torch.max(x + mask, 2)
        return x


class Bottleneck(nn.Module):

    def __init__(self, in_planes, growth_rate):
        super(Bottleneck, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1,
            bias=False)
        self.bn2 = nn.BatchNorm2d(4 * growth_rate)
        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3,
            padding=1, bias=False)

    def forward(self, x):
        out = self.conv1(F.relu(self.bn1(x)))
        out = self.conv2(F.relu(self.bn2(out)))
        out = torch.cat([out, x], 1)
        return out


class Transition(nn.Module):

    def __init__(self, in_planes, out_planes):
        super(Transition, self).__init__()
        self.bn = nn.BatchNorm2d(in_planes)
        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)

    def forward(self, x):
        out = self.conv(F.relu(self.bn(x)))
        out = F.avg_pool2d(out, 2)
        return out


class DenseNet(nn.Module):

    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5,
        num_classes=10):
        super(DenseNet, self).__init__()
        self.growth_rate = growth_rate
        num_planes = 2 * growth_rate
        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1,
            bias=False)
        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])
        num_planes += nblocks[0] * growth_rate
        out_planes = int(math.floor(num_planes * reduction))
        self.trans1 = Transition(num_planes, out_planes)
        num_planes = out_planes
        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])
        num_planes += nblocks[1] * growth_rate
        out_planes = int(math.floor(num_planes * reduction))
        self.trans2 = Transition(num_planes, out_planes)
        num_planes = out_planes
        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])
        num_planes += nblocks[2] * growth_rate
        out_planes = int(math.floor(num_planes * reduction))
        self.trans3 = Transition(num_planes, out_planes)
        num_planes = out_planes
        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])
        num_planes += nblocks[3] * growth_rate
        self.bn = nn.BatchNorm2d(num_planes)
        self.linear = nn.Linear(num_planes, num_classes)

    def _make_dense_layers(self, block, in_planes, nblock):
        layers = []
        for i in range(nblock):
            layers.append(block(in_planes, self.growth_rate))
            in_planes += self.growth_rate
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.trans1(self.dense1(out))
        out = self.trans2(self.dense2(out))
        out = self.trans3(self.dense3(out))
        out = self.dense4(out)
        out = F.avg_pool2d(F.relu(self.bn(out)), 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class Bottleneck(nn.Module):

    def __init__(self, last_planes, in_planes, out_planes, dense_depth,
        stride, first_layer):
        super(Bottleneck, self).__init__()
        self.out_planes = out_planes
        self.dense_depth = dense_depth
        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=
            False)
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=
            stride, padding=1, groups=32, bias=False)
        self.bn2 = nn.BatchNorm2d(in_planes)
        self.conv3 = nn.Conv2d(in_planes, out_planes + dense_depth,
            kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_planes + dense_depth)
        self.shortcut = nn.Sequential()
        if first_layer:
            self.shortcut = nn.Sequential(nn.Conv2d(last_planes, out_planes +
                dense_depth, kernel_size=1, stride=stride, bias=False), nn.
                BatchNorm2d(out_planes + dense_depth))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        x = self.shortcut(x)
        d = self.out_planes
        out = torch.cat([x[:, :d, :, :] + out[:, :d, :, :], x[:, d:, :, :],
            out[:, d:, :, :]], 1)
        out = F.relu(out)
        return out


class DPN(nn.Module):

    def __init__(self, cfg):
        super(DPN, self).__init__()
        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']
        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,
            bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.last_planes = 64
        self.layer1 = self._make_layer(in_planes[0], out_planes[0],
            num_blocks[0], dense_depth[0], stride=1)
        self.layer2 = self._make_layer(in_planes[1], out_planes[1],
            num_blocks[1], dense_depth[1], stride=2)
        self.layer3 = self._make_layer(in_planes[2], out_planes[2],
            num_blocks[2], dense_depth[2], stride=2)
        self.layer4 = self._make_layer(in_planes[3], out_planes[3],
            num_blocks[3], dense_depth[3], stride=2)
        self.linear = nn.Linear(out_planes[3] + (num_blocks[3] + 1) *
            dense_depth[3], 10)

    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth,
        stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for i, stride in enumerate(strides):
            layers.append(Bottleneck(self.last_planes, in_planes,
                out_planes, dense_depth, stride, i == 0))
            self.last_planes = out_planes + (i + 2) * dense_depth
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class Inception(nn.Module):

    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5,
        pool_planes):
        super(Inception, self).__init__()
        self.b1 = nn.Sequential(nn.Conv2d(in_planes, n1x1, kernel_size=1),
            nn.BatchNorm2d(n1x1), nn.ReLU(True))
        self.b2 = nn.Sequential(nn.Conv2d(in_planes, n3x3red, kernel_size=1
            ), nn.BatchNorm2d(n3x3red), nn.ReLU(True), nn.Conv2d(n3x3red,
            n3x3, kernel_size=3, padding=1), nn.BatchNorm2d(n3x3), nn.ReLU(
            True))
        self.b3 = nn.Sequential(nn.Conv2d(in_planes, n5x5red, kernel_size=1
            ), nn.BatchNorm2d(n5x5red), nn.ReLU(True), nn.Conv2d(n5x5red,
            n5x5, kernel_size=3, padding=1), nn.BatchNorm2d(n5x5), nn.ReLU(
            True), nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1), nn.
            BatchNorm2d(n5x5), nn.ReLU(True))
        self.b4 = nn.Sequential(nn.MaxPool2d(3, stride=1, padding=1), nn.
            Conv2d(in_planes, pool_planes, kernel_size=1), nn.BatchNorm2d(
            pool_planes), nn.ReLU(True))

    def forward(self, x):
        y1 = self.b1(x)
        y2 = self.b2(x)
        y3 = self.b3(x)
        y4 = self.b4(x)
        return torch.cat([y1, y2, y3, y4], 1)


class GoogLeNet(nn.Module):

    def __init__(self):
        super(GoogLeNet, self).__init__()
        self.pre_layers = nn.Sequential(nn.Conv2d(3, 192, kernel_size=3,
            padding=1), nn.BatchNorm2d(192), nn.ReLU(True))
        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)
        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)
        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)
        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)
        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)
        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)
        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)
        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)
        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)
        self.avgpool = nn.AvgPool2d(8, stride=1)
        self.linear = nn.Linear(1024, 10)

    def forward(self, x):
        out = self.pre_layers(x)
        out = self.a3(out)
        out = self.b3(out)
        out = self.maxpool(out)
        out = self.a4(out)
        out = self.b4(out)
        out = self.c4(out)
        out = self.d4(out)
        out = self.e4(out)
        out = self.maxpool(out)
        out = self.a5(out)
        out = self.b5(out)
        out = self.avgpool(out)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class LeNet(nn.Module):

    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        out = F.relu(self.conv1(x))
        out = F.max_pool2d(out, 2)
        out = F.relu(self.conv2(out))
        out = F.max_pool2d(out, 2)
        out = out.view(out.size(0), -1)
        out = F.relu(self.fc1(out))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out


class Block(nn.Module):
    """Depthwise conv + Pointwise conv"""

    def __init__(self, in_planes, out_planes, stride=1):
        super(Block, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=
            stride, padding=1, groups=in_planes, bias=False)
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride
            =1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_planes)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        return out


class MobileNet(nn.Module):
    cfg = [64, (128, 2), 128, (256, 2), 256, (512, 2), 512, 512, 512, 512, 
        512, (1024, 2), 1024]

    def __init__(self, num_classes=10):
        super(MobileNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1,
            bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.layers = self._make_layers(in_planes=32)
        self.linear = nn.Linear(1024, num_classes)

    def _make_layers(self, in_planes):
        layers = []
        for x in self.cfg:
            out_planes = x if isinstance(x, int) else x[0]
            stride = 1 if isinstance(x, int) else x[1]
            layers.append(Block(in_planes, out_planes, stride))
            in_planes = out_planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layers(out)
        out = F.avg_pool2d(out, 2)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class Block(nn.Module):
    """expand + depthwise + pointwise"""

    def __init__(self, in_planes, out_planes, expansion, stride):
        super(Block, self).__init__()
        self.stride = stride
        planes = expansion * in_planes
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1,
            padding=0, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
            padding=1, groups=planes, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1,
            padding=0, bias=False)
        self.bn3 = nn.BatchNorm2d(out_planes)
        self.shortcut = nn.Sequential()
        if stride == 1 and in_planes != out_planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, out_planes,
                kernel_size=1, stride=1, padding=0, bias=False), nn.
                BatchNorm2d(out_planes))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out = out + self.shortcut(x) if self.stride == 1 else out
        return out


class MobileNetV2(nn.Module):
    cfg = [(1, 16, 1, 1), (6, 24, 2, 1), (6, 32, 3, 2), (6, 64, 4, 2), (6, 
        96, 3, 1), (6, 160, 3, 2), (6, 320, 1, 1)]

    def __init__(self, num_classes=10):
        super(MobileNetV2, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1,
            bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.layers = self._make_layers(in_planes=32)
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=
            0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        self.linear = nn.Linear(1280, num_classes)

    def _make_layers(self, in_planes):
        layers = []
        for expansion, out_planes, num_blocks, stride in self.cfg:
            strides = [stride] + [1] * (num_blocks - 1)
            for stride in strides:
                layers.append(Block(in_planes, out_planes, expansion, stride))
                in_planes = out_planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layers(out)
        out = F.relu(self.bn2(self.conv2(out)))
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class SepConv(nn.Module):
    """Separable Convolution."""

    def __init__(self, in_planes, out_planes, kernel_size, stride):
        super(SepConv, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size, stride,
            padding=(kernel_size - 1) // 2, bias=False, groups=in_planes)
        self.bn1 = nn.BatchNorm2d(out_planes)

    def forward(self, x):
        return self.bn1(self.conv1(x))


class CellA(nn.Module):

    def __init__(self, in_planes, out_planes, stride=1):
        super(CellA, self).__init__()
        self.stride = stride
        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7,
            stride=stride)
        if stride == 2:
            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1,
                stride=1, padding=0, bias=False)
            self.bn1 = nn.BatchNorm2d(out_planes)

    def forward(self, x):
        y1 = self.sep_conv1(x)
        y2 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)
        if self.stride == 2:
            y2 = self.bn1(self.conv1(y2))
        return F.relu(y1 + y2)


class CellB(nn.Module):

    def __init__(self, in_planes, out_planes, stride=1):
        super(CellB, self).__init__()
        self.stride = stride
        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7,
            stride=stride)
        self.sep_conv2 = SepConv(in_planes, out_planes, kernel_size=3,
            stride=stride)
        self.sep_conv3 = SepConv(in_planes, out_planes, kernel_size=5,
            stride=stride)
        if stride == 2:
            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1,
                stride=1, padding=0, bias=False)
            self.bn1 = nn.BatchNorm2d(out_planes)
        self.conv2 = nn.Conv2d(2 * out_planes, out_planes, kernel_size=1,
            stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_planes)

    def forward(self, x):
        y1 = self.sep_conv1(x)
        y2 = self.sep_conv2(x)
        y3 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)
        if self.stride == 2:
            y3 = self.bn1(self.conv1(y3))
        y4 = self.sep_conv3(x)
        b1 = F.relu(y1 + y2)
        b2 = F.relu(y3 + y4)
        y = torch.cat([b1, b2], 1)
        return F.relu(self.bn2(self.conv2(y)))


class PNASNet(nn.Module):

    def __init__(self, cell_type, num_cells, num_planes):
        super(PNASNet, self).__init__()
        self.in_planes = num_planes
        self.cell_type = cell_type
        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, stride=1,
            padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(num_planes)
        self.layer1 = self._make_layer(num_planes, num_cells=6)
        self.layer2 = self._downsample(num_planes * 2)
        self.layer3 = self._make_layer(num_planes * 2, num_cells=6)
        self.layer4 = self._downsample(num_planes * 4)
        self.layer5 = self._make_layer(num_planes * 4, num_cells=6)
        self.linear = nn.Linear(num_planes * 4, 10)

    def _make_layer(self, planes, num_cells):
        layers = []
        for _ in range(num_cells):
            layers.append(self.cell_type(self.in_planes, planes, stride=1))
            self.in_planes = planes
        return nn.Sequential(*layers)

    def _downsample(self, planes):
        layer = self.cell_type(self.in_planes, planes, stride=2)
        self.in_planes = planes
        return layer

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.layer5(out)
        out = F.avg_pool2d(out, 8)
        out = self.linear(out.view(out.size(0), -1))
        return out


class PreActBlock(nn.Module):
    """Pre-activation version of the BasicBlock."""
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(PreActBlock, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=
            stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,
            padding=1, bias=False)
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.
                expansion * planes, kernel_size=1, stride=stride, bias=False))

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out += shortcut
        return out


class PreActBottleneck(nn.Module):
    """Pre-activation version of the original Bottleneck module."""
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(PreActBottleneck, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
            padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size
            =1, bias=False)
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.
                expansion * planes, kernel_size=1, stride=stride, bias=False))

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out = self.conv3(F.relu(self.bn3(out)))
        out += shortcut
        return out


class PreActResNet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super(PreActResNet, self).__init__()
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,
            bias=False)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=
            stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,
            padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.
                expansion * planes, kernel_size=1, stride=stride, bias=
                False), nn.BatchNorm2d(self.expansion * planes))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
            padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size
            =1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * planes)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.
                expansion * planes, kernel_size=1, stride=stride, bias=
                False), nn.BatchNorm2d(self.expansion * planes))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,
            bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class Block(nn.Module):
    """Grouped convolution block."""
    expansion = 2

    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1
        ):
        super(Block, self).__init__()
        group_width = cardinality * bottleneck_width
        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=
            False)
        self.bn1 = nn.BatchNorm2d(group_width)
        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3,
            stride=stride, padding=1, groups=cardinality, bias=False)
        self.bn2 = nn.BatchNorm2d(group_width)
        self.conv3 = nn.Conv2d(group_width, self.expansion * group_width,
            kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * group_width)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * group_width:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.
                expansion * group_width, kernel_size=1, stride=stride, bias
                =False), nn.BatchNorm2d(self.expansion * group_width))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNeXt(nn.Module):

    def __init__(self, num_blocks, cardinality, bottleneck_width,
        num_classes=10):
        super(ResNeXt, self).__init__()
        self.cardinality = cardinality
        self.bottleneck_width = bottleneck_width
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(num_blocks[0], 1)
        self.layer2 = self._make_layer(num_blocks[1], 2)
        self.layer3 = self._make_layer(num_blocks[2], 2)
        self.linear = nn.Linear(cardinality * bottleneck_width * 8, num_classes
            )

    def _make_layer(self, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(Block(self.in_planes, self.cardinality, self.
                bottleneck_width, stride))
            self.in_planes = (Block.expansion * self.cardinality * self.
                bottleneck_width)
        self.bottleneck_width *= 2
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, 8)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class BasicBlock(nn.Module):

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=
            stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,
            padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, planes,
                kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(
                planes))
        self.fc1 = nn.Conv2d(planes, planes // 16, kernel_size=1)
        self.fc2 = nn.Conv2d(planes // 16, planes, kernel_size=1)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        w = F.avg_pool2d(out, out.size(2))
        w = F.relu(self.fc1(w))
        w = F.sigmoid(self.fc2(w))
        out = out * w
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class PreActBlock(nn.Module):

    def __init__(self, in_planes, planes, stride=1):
        super(PreActBlock, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=
            stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,
            padding=1, bias=False)
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, planes,
                kernel_size=1, stride=stride, bias=False))
        self.fc1 = nn.Conv2d(planes, planes // 16, kernel_size=1)
        self.fc2 = nn.Conv2d(planes // 16, planes, kernel_size=1)

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        w = F.avg_pool2d(out, out.size(2))
        w = F.relu(self.fc1(w))
        w = F.sigmoid(self.fc2(w))
        out = out * w
        out += shortcut
        return out


class SENet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super(SENet, self).__init__()
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,
            bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class ShuffleBlock(nn.Module):

    def __init__(self, groups):
        super(ShuffleBlock, self).__init__()
        self.groups = groups

    def forward(self, x):
        """Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]"""
        N, C, H, W = x.size()
        g = self.groups
        return x.view(N, g, C / g, H, W).permute(0, 2, 1, 3, 4).contiguous(
            ).view(N, C, H, W)


class Bottleneck(nn.Module):

    def __init__(self, in_planes, out_planes, stride, groups):
        super(Bottleneck, self).__init__()
        self.stride = stride
        mid_planes = out_planes / 4
        g = 1 if in_planes == 24 else groups
        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups
            =g, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_planes)
        self.shuffle1 = ShuffleBlock(groups=g)
        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3,
            stride=stride, padding=1, groups=mid_planes, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_planes)
        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1,
            groups=groups, bias=False)
        self.bn3 = nn.BatchNorm2d(out_planes)
        self.shortcut = nn.Sequential()
        if stride == 2:
            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.shuffle1(out)
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        res = self.shortcut(x)
        out = F.relu(torch.cat([out, res], 1)) if self.stride == 2 else F.relu(
            out + res)
        return out


class ShuffleNet(nn.Module):

    def __init__(self, cfg):
        super(ShuffleNet, self).__init__()
        out_planes = cfg['out_planes']
        num_blocks = cfg['num_blocks']
        groups = cfg['groups']
        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(24)
        self.in_planes = 24
        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)
        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)
        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)
        self.linear = nn.Linear(out_planes[2], 10)

    def _make_layer(self, out_planes, num_blocks, groups):
        layers = []
        for i in range(num_blocks):
            stride = 2 if i == 0 else 1
            cat_planes = self.in_planes if i == 0 else 0
            layers.append(Bottleneck(self.in_planes, out_planes -
                cat_planes, stride=stride, groups=groups))
            self.in_planes = out_planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class VGG(nn.Module):

    def __init__(self, vgg_name):
        super(VGG, self).__init__()
        self.features = self._make_layers(cfg[vgg_name])
        self.classifier = nn.Linear(512, 10)

    def forward(self, x):
        out = self.features(x)
        out = out.view(out.size(0), -1)
        out = self.classifier(out)
        return out

    def _make_layers(self, cfg):
        layers = []
        in_channels = 3
        for x in cfg:
            if x == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding
                    =1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]
                in_channels = x
        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]
        return nn.Sequential(*layers)


class FocalLoss2d(nn.Module):

    def __init__(self, gamma=2, size_average=True):
        super(FocalLoss2d, self).__init__()
        self.gamma = gamma
        self.size_average = size_average

    def forward(self, logit, target, class_weight=None, type='sigmoid'):
        target = target.view(-1, 1).long()
        if type == 'sigmoid':
            if class_weight is None:
                class_weight = [1] * 2
            prob = torch.sigmoid(logit)
            prob = prob.view(-1, 1)
            prob = torch.cat((1 - prob, prob), 1)
            select = torch.FloatTensor(len(prob), 2).zero_()
            select.scatter_(1, target, 1.0)
        elif type == 'softmax':
            B, C, H, W = logit.size()
            if class_weight is None:
                class_weight = [1] * C
            logit = logit.permute(0, 2, 3, 1).contiguous().view(-1, C)
            prob = F.softmax(logit, 1)
            select = torch.FloatTensor(len(prob), C).zero_()
            select.scatter_(1, target, 1.0)
        class_weight = torch.FloatTensor(class_weight).view(-1, 1)
        class_weight = torch.gather(class_weight, 0, target)
        prob = (prob * select).sum(1).view(-1, 1)
        prob = torch.clamp(prob, 1e-08, 1 - 1e-08)
        batch_loss = -class_weight * torch.pow(1 - prob, self.gamma
            ) * prob.log()
        if self.size_average:
            loss = batch_loss.mean()
        else:
            loss = batch_loss
        return loss


class StableBCELoss(torch.nn.modules.Module):

    def __init__(self):
        super(StableBCELoss, self).__init__()

    def forward(self, input, target):
        neg_abs = -input.abs()
        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()
        return loss.mean()


def conv3x3(in_, out):
    return nn.Conv2d(in_, out, 3, padding=1)


class ConvRelu(nn.Module):

    def __init__(self, in_, out):
        super().__init__()
        self.conv = conv3x3(in_, out)
        self.activation = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x


class ConvBn2d(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size=(3, 3),
        stride=(1, 1), padding=(1, 1)):
        super(ConvBn2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=
            kernel_size, stride=stride, padding=padding, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return x


class ChannelAttentionGate(nn.Module):

    def __init__(self, channel, reduction=16):
        super(ChannelAttentionGate, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel),
            nn.Sigmoid())

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return y


class SpatialAttentionGate(nn.Module):

    def __init__(self, channel, reduction=16):
        super(SpatialAttentionGate, self).__init__()
        self.fc1 = nn.Conv2d(channel, reduction, kernel_size=1, padding=0)
        self.fc2 = nn.Conv2d(reduction, 1, kernel_size=1, padding=0)

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x, inplace=True)
        x = self.fc2(x)
        x = torch.sigmoid(x)
        return x


class DecoderBlock(nn.Module):

    def __init__(self, in_channels, middle_channels, out_channels):
        super(DecoderBlock, self).__init__()
        self.conv1 = ConvBn2d(in_channels, middle_channels)
        self.conv2 = ConvBn2d(middle_channels, out_channels)
        self.spatial_gate = SpatialAttentionGate(out_channels)
        self.channel_gate = ChannelAttentionGate(out_channels)

    def forward(self, x, e=None):
        x = F.upsample(x, scale_factor=2, mode='bilinear', align_corners=True)
        if e is not None:
            x = torch.cat([x, e], 1)
        x = F.relu(self.conv1(x), inplace=True)
        x = F.relu(self.conv2(x), inplace=True)
        g1 = self.spatial_gate(x)
        g2 = self.channel_gate(x)
        x = x * g1 + x * g2
        return x


class EncoderBlock(nn.Module):

    def __init__(self, block, out_channels):
        super(EncoderBlock, self).__init__()
        self.block = block
        self.out_channels = out_channels
        self.spatial_gate = SpatialAttentionGate(out_channels)
        self.channel_gate = ChannelAttentionGate(out_channels)

    def forward(self, x):
        x = self.block(x)
        g1 = self.spatial_gate(x)
        g2 = self.channel_gate(x)
        return x * g1 + x * g2


def create_resnet(layers):
    if layers == 34:
        return resnet34(pretrained=True), 512
    elif layers == 50:
        return resnet50(pretrained=True), 2048
    elif layers == 101:
        return resnet101(pretrained=True), 2048
    elif layers == 152:
        return resnet152(pretrained=True), 2048
    else:
        raise NotImplementedError(
            'only 34, 50, 101, 152 version of Resnet are implemented')


class UNetResNetV4(nn.Module):

    def __init__(self, encoder_depth, num_classes=1, num_filters=32,
        dropout_2d=0.4, pretrained=True, is_deconv=True):
        super(UNetResNetV4, self).__init__()
        self.name = 'UNetResNetV4_' + str(encoder_depth)
        self.num_classes = num_classes
        self.dropout_2d = dropout_2d
        self.resnet, bottom_channel_nr = create_resnet(encoder_depth)
        self.encoder1 = EncoderBlock(nn.Sequential(self.resnet.conv1, self.
            resnet.bn1, self.resnet.relu), num_filters * 2)
        self.encoder2 = EncoderBlock(self.resnet.layer1, bottom_channel_nr // 8
            )
        self.encoder3 = EncoderBlock(self.resnet.layer2, bottom_channel_nr // 4
            )
        self.encoder4 = EncoderBlock(self.resnet.layer3, bottom_channel_nr // 2
            )
        self.encoder5 = EncoderBlock(self.resnet.layer4, bottom_channel_nr)
        center_block = nn.Sequential(ConvBn2d(bottom_channel_nr,
            bottom_channel_nr, kernel_size=3, padding=1), nn.ReLU(inplace=
            True), ConvBn2d(bottom_channel_nr, bottom_channel_nr // 2,
            kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(
            kernel_size=2, stride=2))
        self.center = EncoderBlock(center_block, bottom_channel_nr // 2)
        self.decoder5 = DecoderBlock(bottom_channel_nr + bottom_channel_nr //
            2, num_filters * 16, 64)
        self.decoder4 = DecoderBlock(64 + bottom_channel_nr // 2, 
            num_filters * 8, 64)
        self.decoder3 = DecoderBlock(64 + bottom_channel_nr // 4, 
            num_filters * 4, 64)
        self.decoder2 = DecoderBlock(64 + bottom_channel_nr // 8, 
            num_filters * 2, 64)
        self.decoder1 = DecoderBlock(64, num_filters, 64)
        self.logit = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3,
            padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 1, kernel_size
            =1, padding=0))

    def forward(self, x):
        x = self.encoder1(x)
        e2 = self.encoder2(x)
        e3 = self.encoder3(e2)
        e4 = self.encoder4(e3)
        e5 = self.encoder5(e4)
        center = self.center(e5)
        d5 = self.decoder5(center, e5)
        d4 = self.decoder4(d5, e4)
        d3 = self.decoder3(d4, e3)
        d2 = self.decoder2(d3, e2)
        d1 = self.decoder1(d2)
        f = torch.cat([d1, F.upsample(d2, scale_factor=2, mode='bilinear',
            align_corners=False), F.upsample(d3, scale_factor=4, mode=
            'bilinear', align_corners=False), F.upsample(d4, scale_factor=8,
            mode='bilinear', align_corners=False), F.upsample(d5,
            scale_factor=16, mode='bilinear', align_corners=False)], 1)
        f = F.dropout2d(f, p=self.dropout_2d)
        return self.logit(f), None

    def freeze_bn(self):
        """Freeze BatchNorm layers."""
        for layer in self.modules():
            if isinstance(layer, nn.BatchNorm2d):
                layer.eval()

    def get_params(self, base_lr):
        group1 = [self.encoder1, self.encoder2, self.encoder3, self.
            encoder4, self.encoder5]
        group2 = [self.decoder1, self.decoder2, self.decoder3, self.
            decoder4, self.decoder5, self.center, self.logit]
        params1 = []
        for x in group1:
            for p in x.parameters():
                params1.append(p)
        param_group1 = {'params': params1, 'lr': base_lr / 5}
        params2 = []
        for x in group2:
            for p in x.parameters():
                params2.append(p)
        param_group2 = {'params': params2, 'lr': base_lr}
        return [param_group1, param_group2]


class DecoderBlockV5(nn.Module):

    def __init__(self, in_channels_x, in_channels_e, middle_channels,
        out_channels):
        super(DecoderBlockV5, self).__init__()
        self.in_channels = in_channels_x + in_channels_e
        self.conv1 = ConvBn2d(self.in_channels, middle_channels)
        self.conv2 = ConvBn2d(middle_channels, out_channels)
        self.deconv = nn.ConvTranspose2d(in_channels_x, in_channels_x,
            kernel_size=4, stride=2, padding=1)
        self.bn = nn.BatchNorm2d(self.in_channels)
        self.spatial_gate = SpatialAttentionGate(out_channels)
        self.channel_gate = ChannelAttentionGate(out_channels)

    def forward(self, x, e=None):
        x = self.deconv(x)
        if e is not None:
            x = torch.cat([x, e], 1)
        x = self.bn(x)
        x = F.relu(self.conv1(x), inplace=True)
        x = F.relu(self.conv2(x), inplace=True)
        g1 = self.spatial_gate(x)
        g2 = self.channel_gate(x)
        x = x * g1 + x * g2
        return x


class UNetResNetV5(nn.Module):

    def __init__(self, encoder_depth, num_classes=1, num_filters=32,
        dropout_2d=0.5):
        super(UNetResNetV5, self).__init__()
        self.name = 'UNetResNetV5_' + str(encoder_depth)
        self.num_classes = num_classes
        self.dropout_2d = dropout_2d
        self.resnet, bottom_channel_nr = create_resnet(encoder_depth)
        self.encoder1 = EncoderBlock(nn.Sequential(self.resnet.conv1, self.
            resnet.bn1, self.resnet.relu), num_filters * 2)
        self.encoder2 = EncoderBlock(self.resnet.layer1, bottom_channel_nr // 8
            )
        self.encoder3 = EncoderBlock(self.resnet.layer2, bottom_channel_nr // 4
            )
        self.encoder4 = EncoderBlock(self.resnet.layer3, bottom_channel_nr // 2
            )
        self.encoder5 = EncoderBlock(self.resnet.layer4, bottom_channel_nr)
        center_block = nn.Sequential(ConvBn2d(bottom_channel_nr,
            bottom_channel_nr, kernel_size=3, padding=1), nn.ReLU(inplace=
            True), ConvBn2d(bottom_channel_nr, bottom_channel_nr // 2,
            kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(
            kernel_size=2, stride=2))
        self.center = EncoderBlock(center_block, bottom_channel_nr // 2)
        self.decoder5 = DecoderBlockV5(bottom_channel_nr // 2,
            bottom_channel_nr, num_filters * 16, 64)
        self.decoder4 = DecoderBlockV5(64, bottom_channel_nr // 2, 
            num_filters * 8, 64)
        self.decoder3 = DecoderBlockV5(64, bottom_channel_nr // 4, 
            num_filters * 4, 64)
        self.decoder2 = DecoderBlockV5(64, bottom_channel_nr // 8, 
            num_filters * 2, 64)
        self.decoder1 = DecoderBlockV5(64, 0, num_filters, 64)
        self.logit = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3,
            padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 1, kernel_size
            =1, padding=0))

    def forward(self, x):
        x = self.encoder1(x)
        e2 = self.encoder2(x)
        e3 = self.encoder3(e2)
        e4 = self.encoder4(e3)
        e5 = self.encoder5(e4)
        center = self.center(e5)
        d5 = self.decoder5(center, e5)
        d4 = self.decoder4(d5, e4)
        d3 = self.decoder3(d4, e3)
        d2 = self.decoder2(d3, e2)
        d1 = self.decoder1(d2)
        f = torch.cat([d1, F.interpolate(d2, scale_factor=2, mode=
            'bilinear', align_corners=False), F.interpolate(d3,
            scale_factor=4, mode='bilinear', align_corners=False), F.
            interpolate(d4, scale_factor=8, mode='bilinear', align_corners=
            False), F.interpolate(d5, scale_factor=16, mode='bilinear',
            align_corners=False)], 1)
        f = F.dropout2d(f, p=self.dropout_2d)
        return self.logit(f), None


class UNetResNetV6(nn.Module):
    """
    1. Remove first pool from UNetResNetV5, such that resolution is doubled
    2. Remove scSE from center block
    3. Increase default dropout
    """

    def __init__(self, encoder_depth, num_filters=32, dropout_2d=0.5):
        super(UNetResNetV6, self).__init__()
        assert encoder_depth == 34, 'UNetResNetV6: only 34 layers is supported!'
        self.name = 'UNetResNetV6_' + str(encoder_depth)
        self.dropout_2d = dropout_2d
        self.resnet, bottom_channel_nr = create_resnet(encoder_depth)
        self.encoder1 = EncoderBlock(nn.Sequential(self.resnet.conv1, self.
            resnet.bn1, self.resnet.relu), num_filters * 2)
        self.encoder2 = EncoderBlock(self.resnet.layer1, bottom_channel_nr // 8
            )
        self.encoder3 = EncoderBlock(self.resnet.layer2, bottom_channel_nr // 4
            )
        self.encoder4 = EncoderBlock(self.resnet.layer3, bottom_channel_nr // 2
            )
        self.encoder5 = EncoderBlock(self.resnet.layer4, bottom_channel_nr)
        self.center = nn.Sequential(ConvBn2d(bottom_channel_nr,
            bottom_channel_nr, kernel_size=3, padding=1), nn.ReLU(inplace=
            True), ConvBn2d(bottom_channel_nr, bottom_channel_nr // 2,
            kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(
            kernel_size=2, stride=2))
        self.decoder5 = DecoderBlockV5(bottom_channel_nr // 2,
            bottom_channel_nr, num_filters * 16, 64)
        self.decoder4 = DecoderBlockV5(64, bottom_channel_nr // 2, 
            num_filters * 8, 64)
        self.decoder3 = DecoderBlockV5(64, bottom_channel_nr // 4, 
            num_filters * 4, 64)
        self.decoder2 = DecoderBlockV5(64, bottom_channel_nr // 8, 
            num_filters * 2, 64)
        self.decoder1 = DecoderBlockV5(64, 0, num_filters, 64)
        self.logit = nn.Sequential(nn.Conv2d(512, 64, kernel_size=3,
            padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 1, kernel_size
            =1, padding=0))
        self.logit_image = nn.Sequential(nn.Linear(512, 128), nn.ReLU(
            inplace=True), nn.Linear(128, 1))

    def forward(self, x):
        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners
            =False)
        x = self.encoder1(x)
        e2 = self.encoder2(x)
        e3 = self.encoder3(e2)
        e4 = self.encoder4(e3)
        e5 = self.encoder5(e4)
        center = self.center(e5)
        d5 = self.decoder5(center, e5)
        d4 = self.decoder4(d5, e4)
        d3 = self.decoder3(d4, e3)
        d2 = self.decoder2(d3, e2)
        f = torch.cat([d2, F.interpolate(d3, scale_factor=2, mode=
            'bilinear', align_corners=False), F.interpolate(d4,
            scale_factor=4, mode='bilinear', align_corners=False), F.
            interpolate(d5, scale_factor=8, mode='bilinear', align_corners=
            False), F.interpolate(center, scale_factor=16, mode='bilinear',
            align_corners=False)], 1)
        f = F.dropout2d(f, p=self.dropout_2d, training=self.training)
        img_f = F.adaptive_avg_pool2d(e5, 1).view(x.size(0), -1)
        img_f = F.dropout(img_f, p=0.5, training=self.training)
        img_logit = self.logit_image(img_f).view(-1)
        return self.logit(f), img_logit


class DecoderBlockV7(nn.Module):

    def __init__(self, in_channels_x, in_channels_e, middle_channels,
        out_channels):
        super(DecoderBlockV7, self).__init__()
        self.in_channels = in_channels_x + in_channels_e
        self.conv1 = ConvBn2d(self.in_channels, middle_channels)
        self.conv2 = ConvBn2d(middle_channels, out_channels)
        self.deconv = nn.ConvTranspose2d(in_channels_x, in_channels_x,
            kernel_size=4, stride=2, padding=1)
        self.bn = nn.BatchNorm2d(self.in_channels)
        self.spatial_gate = SpatialAttentionGate(out_channels)
        self.channel_gate = ChannelAttentionGate(out_channels)

    def forward(self, x, e=None, upsample=True):
        if upsample:
            x = self.deconv(x)
        if e is not None:
            x = torch.cat([x, e], 1)
        x = self.bn(x)
        x = F.relu(self.conv1(x), inplace=True)
        x = F.relu(self.conv2(x), inplace=True)
        g1 = self.spatial_gate(x)
        g2 = self.channel_gate(x)
        x = x * g1 + x * g2
        return x


class UNet7(nn.Module):

    def __init__(self, encoder_depth, num_classes=1, num_filters=32,
        dropout_2d=0.5):
        super(UNet7, self).__init__()
        nf = num_filters
        self.name = 'UNet7_' + str(encoder_depth) + '_nf' + str(nf)
        self.num_classes = num_classes
        self.dropout_2d = dropout_2d
        self.resnet, nbtm = create_resnet(encoder_depth)
        self.encoder1 = EncoderBlock(nn.Sequential(nn.Conv2d(3, 64,
            kernel_size=7, stride=1, padding=3, bias=False), nn.BatchNorm2d
            (64), nn.ReLU(inplace=True)), 64)
        self.encoder2 = EncoderBlock(nn.Sequential(nn.MaxPool2d(kernel_size
            =2, stride=2), self.resnet.layer1), nbtm // 8)
        self.encoder3 = EncoderBlock(self.resnet.layer2, nbtm // 4)
        self.encoder4 = EncoderBlock(self.resnet.layer3, nbtm // 2)
        self.encoder5 = EncoderBlock(self.resnet.layer4, nbtm)
        center_block = nn.Sequential(ConvBn2d(nbtm, nbtm, kernel_size=3,
            padding=1), nn.ReLU(inplace=True), ConvBn2d(nbtm, nbtm // 2,
            kernel_size=3, padding=1), nn.ReLU(inplace=True))
        self.center = EncoderBlock(center_block, nbtm // 2)
        self.decoder5 = DecoderBlockV7(nbtm // 2, nbtm, nf * 16, nf * 2)
        self.decoder4 = DecoderBlockV7(nf * 2, nbtm // 2, nf * 8, nf * 2)
        self.decoder3 = DecoderBlockV7(nf * 2, nbtm // 4, nf * 4, nf * 2)
        self.decoder2 = DecoderBlockV7(nf * 2, nbtm // 8, nf * 2, nf * 2)
        self.decoder1 = DecoderBlockV7(nf * 2, 64, nf * 2, nf * 2)
        self.logit = nn.Sequential(nn.Conv2d(nf * 10, 64, kernel_size=3,
            padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 1, kernel_size
            =1, padding=0))
        self.logit_image = nn.Sequential(nn.Linear(nbtm, 128), nn.ReLU(
            inplace=True), nn.Linear(128, 1))

    def forward(self, x):
        e1 = self.encoder1(x)
        e2 = self.encoder2(e1)
        e3 = self.encoder3(e2)
        e4 = self.encoder4(e3)
        e5 = self.encoder5(e4)
        center = self.center(e5)
        d5 = self.decoder5(center, e5, upsample=False)
        d4 = self.decoder4(d5, e4)
        d3 = self.decoder3(d4, e3)
        d2 = self.decoder2(d3, e2)
        d1 = self.decoder1(d2, e1)
        f = torch.cat([d1, F.interpolate(d2, scale_factor=2, mode=
            'bilinear', align_corners=False), F.interpolate(d3,
            scale_factor=4, mode='bilinear', align_corners=False), F.
            interpolate(d4, scale_factor=8, mode='bilinear', align_corners=
            False), F.interpolate(d5, scale_factor=16, mode='bilinear',
            align_corners=False)], 1)
        f = F.dropout2d(f, p=self.dropout_2d)
        img_f = F.adaptive_avg_pool2d(e5, 1).view(x.size(0), -1)
        img_f = F.dropout(img_f, p=0.5, training=self.training)
        img_logit = self.logit_image(img_f).view(-1)
        return self.logit(f), img_logit


class UNet8(nn.Module):

    def __init__(self, encoder_depth, num_classes=1, num_filters=32,
        dropout_2d=0.5):
        super(UNet8, self).__init__()
        nf = num_filters
        self.name = 'UNet8_' + str(encoder_depth) + '_nf' + str(nf)
        self.num_classes = num_classes
        self.dropout_2d = dropout_2d
        self.resnet, nbtm = create_resnet(encoder_depth)
        self.encoder1 = EncoderBlock(nn.Sequential(self.resnet.conv1, self.
            resnet.bn1, self.resnet.relu), 64)
        self.encoder2 = EncoderBlock(self.resnet.layer1, nbtm // 8)
        self.encoder3 = EncoderBlock(self.resnet.layer2, nbtm // 4)
        self.encoder4 = EncoderBlock(self.resnet.layer3, nbtm // 2)
        self.encoder5 = EncoderBlock(self.resnet.layer4, nbtm)
        center_block = nn.Sequential(ConvBn2d(nbtm, nbtm, kernel_size=3,
            padding=1), nn.ReLU(inplace=True), ConvBn2d(nbtm, nbtm // 2,
            kernel_size=3, padding=1), nn.ReLU(inplace=True))
        self.center = EncoderBlock(center_block, nbtm // 2)
        self.decoder5 = DecoderBlockV7(nbtm // 2, nbtm, nf * 16, nf * 2)
        self.decoder4 = DecoderBlockV7(nf * 2, nbtm // 2, nf * 8, nf * 2)
        self.decoder3 = DecoderBlockV7(nf * 2, nbtm // 4, nf * 4, nf * 2)
        self.decoder2 = DecoderBlockV7(nf * 2, nbtm // 8, nf * 2, nf * 2)
        self.decoder1 = DecoderBlockV7(nf * 2 + 64, 3, nf * 2, nf * 2)
        self.logit = nn.Sequential(nn.Conv2d(nf * 10, 64, kernel_size=3,
            padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 1, kernel_size
            =1, padding=0))
        self.logit_image = nn.Sequential(nn.Linear(nbtm, 128), nn.ReLU(
            inplace=True), nn.Linear(128, 1))

    def forward(self, x):
        e1 = self.encoder1(x)
        e2 = self.encoder2(e1)
        e3 = self.encoder3(e2)
        e4 = self.encoder4(e3)
        e5 = self.encoder5(e4)
        center = self.center(e5)
        d5 = self.decoder5(center, e5, upsample=False)
        d4 = self.decoder4(d5, e4)
        d3 = self.decoder3(d4, e3)
        d2 = self.decoder2(d3, e2)
        d1 = self.decoder1(torch.cat([d2, e1], 1), x)
        f = torch.cat([d1, F.interpolate(d2, scale_factor=2, mode=
            'bilinear', align_corners=False), F.interpolate(d3,
            scale_factor=4, mode='bilinear', align_corners=False), F.
            interpolate(d4, scale_factor=8, mode='bilinear', align_corners=
            False), F.interpolate(d5, scale_factor=16, mode='bilinear',
            align_corners=False)], 1)
        f = F.dropout2d(f, p=self.dropout_2d)
        img_f = F.adaptive_avg_pool2d(e5, 1).view(x.size(0), -1)
        img_f = F.dropout(img_f, p=0.5, training=self.training)
        img_logit = self.logit_image(img_f).view(-1)
        return self.logit(f), img_logit


class Net(nn.Module):
    """ Network architecture. """

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.fc1 = nn.Linear(4 * 4 * 50, 512)
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4 * 4 * 50)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class Net(nn.Module):

    def __init__(self, hidden_size):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.fc1 = nn.Linear(4 * 4 * 50, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4 * 4 * 50)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class PrunerModuleWrapper(torch.nn.Module):

    def __init__(self, module, module_name, module_type, config, pruner):
        """
        Wrap an module to enable data parallel, forward method customization and buffer registeration.

        Parameters
        ----------
        module : pytorch module
            the module user wants to compress
        config : dict
            the configurations that users specify for compression
        module_name : str
            the name of the module to compress, wrapper module shares same name
        module_type : str
            the type of the module to compress
        pruner ： Pruner
            the pruner used to calculate mask
        """
        super().__init__()
        self.module = module
        self.name = module_name
        self.type = module_type
        self.config = config
        self.pruner = pruner
        self.register_buffer('weight_mask', torch.ones(self.module.weight.
            shape))
        if hasattr(self.module, 'bias') and self.module.bias is not None:
            self.register_buffer('bias_mask', torch.ones(self.module.bias.
                shape))
        else:
            self.register_buffer('bias_mask', None)

    def forward(self, *inputs):
        self.module.weight.data = self.module.weight.data.mul_(self.weight_mask
            )
        if hasattr(self.module, 'bias') and self.module.bias is not None:
            self.module.bias.data = self.module.bias.data.mul_(self.bias_mask)
        return self.module(*inputs)


def _check_weight(module):
    try:
        return isinstance(module.weight.data, torch.Tensor)
    except AttributeError:
        return False


class QuantType:
    """
    Enum class for quantization type.
    """
    QUANT_INPUT = 0
    QUANT_WEIGHT = 1
    QUANT_OUTPUT = 2


class QuantizerModuleWrapper(torch.nn.Module):

    def __init__(self, module, module_name, module_type, config, quantizer):
        """
        Wrap an module to enable data parallel, forward method customization and buffer registeration.

        Parameters
        ----------
        module : pytorch module
            the module user wants to compress
        config : dict
            the configurations that users specify for compression
        module_name : str
            the name of the module to compress, wrapper module shares same name
        module_type : str
            the type of the module to compress
        quantizer ：quantizer
            the quantizer used to calculate mask
        """
        super().__init__()
        self.module = module
        self.name = module_name
        self.type = module_type
        self.config = config
        self.quantizer = quantizer
        if 'weight' in config['quant_types']:
            if not _check_weight(self.module):
                _logger.warning('Module %s does not have parameter "weight"',
                    self.name)
            else:
                self.module.register_parameter('old_weight', torch.nn.
                    Parameter(self.module.weight))
                delattr(self.module, 'weight')
                self.module.register_buffer('weight', self.module.old_weight)

    def forward(self, *inputs):
        if 'input' in self.config['quant_types']:
            inputs = self.quantizer.quant_grad.apply(inputs, QuantType.
                QUANT_INPUT, self)
        if 'weight' in self.config['quant_types'] and _check_weight(self.module
            ):
            new_weight = self.quantizer.quant_grad.apply(self.module.
                old_weight, QuantType.QUANT_WEIGHT, self)
            self.module.weight = new_weight
            result = self.module(*inputs)
        else:
            result = self.module(*inputs)
        if 'output' in self.config['quant_types']:
            result = self.quantizer.quant_grad.apply(result, QuantType.
                QUANT_OUTPUT, self)
        return result


def triudl(X, l):
    Zl = torch.zeros_like(X, requires_grad=False)
    U = X * l
    Zl[1:] = X[1:] * U.cumsum(dim=0)[:-1]
    return Zl


class safesqrt(torch.autograd.Function):
    """
    Square root without dividing by 0.
    """

    @staticmethod
    def forward(ctx, input_data):
        o = input_data.sqrt()
        ctx.save_for_backward(input_data, o)
        return o

    @staticmethod
    def backward(ctx, grad_output):
        _, o = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input *= 0.5 / (o + constants.EPSILON)
        return grad_input


class ramp(torch.autograd.Function):
    """
    Ensures input is between 0 and 1
    """

    @staticmethod
    def forward(ctx, input_data):
        ctx.save_for_backward(input_data)
        return input_data.clamp(min=0, max=1)

    @staticmethod
    def backward(ctx, grad_output):
        input_data, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input_data < 0] = 0.01
        grad_input[input_data > 1] = -0.01
        return grad_input


def revcumsum(U):
    """
    Reverse cumulative sum for faster performance.
    """
    return U.flip(dims=[0]).cumsum(dim=0).flip(dims=[0])


def triudr(X, r):
    Zr = torch.zeros_like(X, requires_grad=False)
    U = X * r
    Zr[:-1] = X[:-1] * revcumsum(U)[1:]
    return Zr


class StructuredMutableTreeNode:
    """
    A structured representation of a search space.
    A search space comes with a root (with `None` stored in its `mutable`), and a bunch of children in its `children`.
    This tree can be seen as a "flattened" version of the module tree. Since nested mutable entity is not supported yet,
    the following must be true: each subtree corresponds to a ``MutableScope`` and each leaf corresponds to a
    ``Mutable`` (other than ``MutableScope``).

    Parameters
    ----------
    mutable : nni.nas.pytorch.mutables.Mutable
        The mutable that current node is linked with.
    """

    def __init__(self, mutable):
        self.mutable = mutable
        self.children = []

    def add_child(self, mutable):
        """
        Add a tree node to the children list of current node.
        """
        self.children.append(StructuredMutableTreeNode(mutable))
        return self.children[-1]

    def type(self):
        """
        Return the ``type`` of mutable content.
        """
        return type(self.mutable)

    def __iter__(self):
        return self.traverse()

    def traverse(self, order='pre', deduplicate=True, memo=None):
        """
        Return a generator that generates a list of mutables in this tree.

        Parameters
        ----------
        order : str
            pre or post. If pre, current mutable is yield before children. Otherwise after.
        deduplicate : bool
            If true, mutables with the same key will not appear after the first appearance.
        memo : dict
            An auxiliary dict that memorize keys seen before, so that deduplication is possible.

        Returns
        -------
        generator of Mutable
        """
        if memo is None:
            memo = set()
        assert order in ['pre', 'post']
        if order == 'pre':
            if self.mutable is not None:
                if not deduplicate or self.mutable.key not in memo:
                    memo.add(self.mutable.key)
                    yield self.mutable
        for child in self.children:
            for m in child.traverse(order=order, deduplicate=deduplicate,
                memo=memo):
                yield m
        if order == 'post':
            if self.mutable is not None:
                if not deduplicate or self.mutable.key not in memo:
                    memo.add(self.mutable.key)
                    yield self.mutable


class MutableScope(Mutable):
    """
    Mutable scope marks a subgraph/submodule to help mutators make better decisions.

    If not annotated with mutable scope, search space will be flattened as a list. However, some mutators might
    need to leverage the concept of a "cell". So if a module is defined as a mutable scope, everything in it will
    look like "sub-search-space" in the scope. Scopes can be nested.

    There are two ways mutators can use mutable scope. One is to traverse the search space as a tree during initialization
    and reset. The other is to implement `enter_mutable_scope` and `exit_mutable_scope`. They are called before and after
    the forward method of the class inheriting mutable scope.

    Mutable scopes are also mutables that are listed in the mutator.mutables (search space), but they are not supposed
    to appear in the dict of choices.

    Parameters
    ----------
    key : str
        Key of mutable scope.
    """

    def __init__(self, key):
        super().__init__(key=key)

    def __call__(self, *args, **kwargs):
        try:
            self._check_built()
            self.mutator.enter_mutable_scope(self)
            return super().__call__(*args, **kwargs)
        finally:
            self.mutator.exit_mutable_scope(self)


class BaseMutator(nn.Module):
    """
    A mutator is responsible for mutating a graph by obtaining the search space from the network and implementing
    callbacks that are called in ``forward`` in mutables.

    Parameters
    ----------
    model : nn.Module
        PyTorch model to apply mutator on.
    """

    def __init__(self, model):
        super().__init__()
        self.__dict__['model'] = model
        self._structured_mutables = self._parse_search_space(self.model)

    def _parse_search_space(self, module, root=None, prefix='', memo=None,
        nested_detection=None):
        if memo is None:
            memo = set()
        if root is None:
            root = StructuredMutableTreeNode(None)
        if module not in memo:
            memo.add(module)
            if isinstance(module, Mutable):
                if nested_detection is not None:
                    raise RuntimeError(
                        'Cannot have nested search space. Error at {} in {}'
                        .format(module, nested_detection))
                module.name = prefix
                module.set_mutator(self)
                root = root.add_child(module)
                if not isinstance(module, MutableScope):
                    nested_detection = module
                if isinstance(module, InputChoice):
                    for k in module.choose_from:
                        if k != InputChoice.NO_KEY and k not in [m.key for
                            m in memo if isinstance(m, Mutable)]:
                            raise RuntimeError(
                                "'{}' required by '{}' not found in keys that appeared before, and is not NO_KEY."
                                .format(k, module.key))
            for name, submodule in module._modules.items():
                if submodule is None:
                    continue
                submodule_prefix = prefix + ('.' if prefix else '') + name
                self._parse_search_space(submodule, root, submodule_prefix,
                    memo=memo, nested_detection=nested_detection)
        return root

    @property
    def mutables(self):
        """
        A generator of all modules inheriting :class:`~nni.nas.pytorch.mutables.Mutable`.
        Modules are yielded in the order that they are defined in ``__init__``.
        For mutables with their keys appearing multiple times, only the first one will appear.
        """
        return self._structured_mutables

    @property
    def undedup_mutables(self):
        return self._structured_mutables.traverse(deduplicate=False)

    def forward(self, *inputs):
        """
        Warnings
        --------
        Don't call forward of a mutator.
        """
        raise RuntimeError('Forward is undefined for mutators.')

    def __setattr__(self, name, value):
        if name == 'model':
            raise AttributeError(
                "Attribute `model` can be set at most once, and you shouldn't use `self.model = model` to include you network, as it will include all parameters in model into the mutator."
                )
        return super().__setattr__(name, value)

    def enter_mutable_scope(self, mutable_scope):
        """
        Callback when forward of a MutableScope is entered.

        Parameters
        ----------
        mutable_scope : MutableScope
            The mutable scope that is entered.
        """
        pass

    def exit_mutable_scope(self, mutable_scope):
        """
        Callback when forward of a MutableScope is exited.

        Parameters
        ----------
        mutable_scope : MutableScope
            The mutable scope that is exited.
        """
        pass

    def on_forward_layer_choice(self, mutable, *args, **kwargs):
        """
        Callbacks of forward in LayerChoice.

        Parameters
        ----------
        mutable : LayerChoice
            Module whose forward is called.
        args : list of torch.Tensor
            The arguments of its forward function.
        kwargs : dict
            The keyword arguments of its forward function.

        Returns
        -------
        tuple of torch.Tensor and torch.Tensor
            Output tensor and mask.
        """
        raise NotImplementedError

    def on_forward_input_choice(self, mutable, tensor_list):
        """
        Callbacks of forward in InputChoice.

        Parameters
        ----------
        mutable : InputChoice
            Mutable that is called.
        tensor_list : list of torch.Tensor
            The arguments mutable is called with.

        Returns
        -------
        tuple of torch.Tensor and torch.Tensor
            Output tensor and mask.
        """
        raise NotImplementedError

    def export(self):
        """
        Export the data of all decisions. This should output the decisions of all the mutables, so that the whole
        network can be fully determined with these decisions for further training from scratch.

        Returns
        -------
        dict
            Mappings from mutable keys to decisions.
        """
        raise NotImplementedError


class InteractiveKLLoss(nn.Module):

    def __init__(self, temperature):
        super().__init__()
        self.temperature = temperature
        self.kl_loss = nn.KLDivLoss()

    def forward(self, student, teacher):
        return self.kl_loss(F.log_softmax(student / self.temperature, dim=1
            ), F.softmax(teacher / self.temperature, dim=1))


class StackedLSTMCell(nn.Module):

    def __init__(self, layers, size, bias):
        super().__init__()
        self.lstm_num_layers = layers
        self.lstm_modules = nn.ModuleList([nn.LSTMCell(size, size, bias=
            bias) for _ in range(self.lstm_num_layers)])

    def forward(self, inputs, hidden):
        prev_c, prev_h = hidden
        next_c, next_h = [], []
        for i, m in enumerate(self.lstm_modules):
            curr_c, curr_h = m(inputs, (prev_c[i], prev_h[i]))
            next_c.append(curr_c)
            next_h.append(curr_h)
            inputs = curr_h[-1].view(1, -1)
        return next_c, next_h


logger = logging.getLogger(__name__)


class Mutable(nn.Module):
    """
    Mutable is designed to function as a normal layer, with all necessary operators' weights.
    States and weights of architectures should be included in mutator, instead of the layer itself.

    Mutable has a key, which marks the identity of the mutable. This key can be used by users to share
    decisions among different mutables. In mutator's implementation, mutators should use the key to
    distinguish different mutables. Mutables that share the same key should be "similar" to each other.

    Currently the default scope for keys is global. By default, the keys uses a global counter from 1 to
    produce unique ids.

    Parameters
    ----------
    key : str
        The key of mutable.

    Notes
    -----
    The counter is program level, but mutables are model level. In case multiple models are defined, and
    you want to have `counter` starting from 1 in the second model, it's recommended to assign keys manually
    instead of using automatic keys.
    """

    def __init__(self, key=None):
        super().__init__()
        if key is not None:
            if not isinstance(key, str):
                key = str(key)
                logger.warning(
                    'Warning: key "%s" is not string, converted to string.',
                    key)
            self._key = key
        else:
            self._key = self.__class__.__name__ + str(global_mutable_counting()
                )
        self.init_hook = self.forward_hook = None

    def __deepcopy__(self, memodict=None):
        raise NotImplementedError("Deep copy doesn't work for mutables.")

    def __call__(self, *args, **kwargs):
        self._check_built()
        return super().__call__(*args, **kwargs)

    def set_mutator(self, mutator):
        if 'mutator' in self.__dict__:
            raise RuntimeError(
                '`set_mutator` is called more than once. Did you parse the search space multiple times? Or did you apply multiple fixed architectures?'
                )
        self.__dict__['mutator'] = mutator

    @property
    def key(self):
        """
        Read-only property of key.
        """
        return self._key

    @property
    def name(self):
        """
        After the search space is parsed, it will be the module name of the mutable.
        """
        return self._name if hasattr(self, '_name') else '_key'

    @name.setter
    def name(self, name):
        self._name = name

    def _check_built(self):
        if not hasattr(self, 'mutator'):
            raise ValueError(
                'Mutator not set for {}. You might have forgotten to initialize and apply your mutator. Or did you initialize a mutable on the fly in forward pass? Move to `__init__` so that trainer can locate all your mutables. See NNI docs for more details.'
                .format(self))


def detach_variable(inputs):
    """
    Detach variables

    Parameters
    ----------
    inputs : pytorch tensors
        pytorch tensors
    """
    if isinstance(inputs, tuple):
        return tuple([detach_variable(x) for x in inputs])
    else:
        x = inputs.detach()
        x.requires_grad = inputs.requires_grad
        return x


class ArchGradientFunction(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, binary_gates, run_func, backward_func):
        ctx.run_func = run_func
        ctx.backward_func = backward_func
        detached_x = detach_variable(x)
        with torch.enable_grad():
            output = run_func(detached_x)
        ctx.save_for_backward(detached_x, output)
        return output.data

    @staticmethod
    def backward(ctx, grad_output):
        detached_x, output = ctx.saved_tensors
        grad_x = torch.autograd.grad(output, detached_x, grad_output,
            only_inputs=True)
        binary_grads = ctx.backward_func(detached_x.data, output.data,
            grad_output.data)
        return grad_x[0], binary_grads, None, None


class MixedOp(nn.Module):
    """
    This class is to instantiate and manage info of one LayerChoice.
    It includes architecture weights, binary weights, and member functions
    operating the weights.

    forward_mode:
        forward/backward mode for LayerChoice: None, two, full, and full_v2.
        For training architecture weights, we use full_v2 by default, and for training
        model weights, we use None.
    """
    forward_mode = None

    def __init__(self, mutable):
        """
        Parameters
        ----------
        mutable : LayerChoice
            A LayerChoice in user model
        """
        super(MixedOp, self).__init__()
        self.ap_path_alpha = nn.Parameter(torch.Tensor(len(mutable)))
        self.ap_path_wb = nn.Parameter(torch.Tensor(len(mutable)))
        self.ap_path_alpha.requires_grad = False
        self.ap_path_wb.requires_grad = False
        self.active_index = [0]
        self.inactive_index = None
        self.log_prob = None
        self.current_prob_over_ops = None
        self.n_choices = len(mutable)

    def get_ap_path_alpha(self):
        return self.ap_path_alpha

    def to_requires_grad(self):
        self.ap_path_alpha.requires_grad = True
        self.ap_path_wb.requires_grad = True

    def to_disable_grad(self):
        self.ap_path_alpha.requires_grad = False
        self.ap_path_wb.requires_grad = False

    def forward(self, mutable, x):
        """
        Define forward of LayerChoice. For 'full_v2', backward is also defined.
        The 'two' mode is explained in section 3.2.1 in the paper.
        The 'full_v2' mode is explained in Appendix D in the paper.

        Parameters
        ----------
        mutable : LayerChoice
            this layer's mutable
        x : tensor
            inputs of this layer, only support one input

        Returns
        -------
        output: tensor
            output of this layer
        """
        if MixedOp.forward_mode == 'full' or MixedOp.forward_mode == 'two':
            output = 0
            for _i in self.active_index:
                oi = self.candidate_ops[_i](x)
                output = output + self.ap_path_wb[_i] * oi
            for _i in self.inactive_index:
                oi = self.candidate_ops[_i](x)
                output = output + self.ap_path_wb[_i] * oi.detach()
        elif MixedOp.forward_mode == 'full_v2':

            def run_function(key, candidate_ops, active_id):

                def forward(_x):
                    return candidate_ops[active_id](_x)
                return forward

            def backward_function(key, candidate_ops, active_id, binary_gates):

                def backward(_x, _output, grad_output):
                    binary_grads = torch.zeros_like(binary_gates.data)
                    with torch.no_grad():
                        for k in range(len(candidate_ops)):
                            if k != active_id:
                                out_k = candidate_ops[k](_x.data)
                            else:
                                out_k = _output.data
                            grad_k = torch.sum(out_k * grad_output)
                            binary_grads[k] = grad_k
                    return binary_grads
                return backward
            output = ArchGradientFunction.apply(x, self.ap_path_wb,
                run_function(mutable.key, list(mutable), self.active_index[
                0]), backward_function(mutable.key, list(mutable), self.
                active_index[0], self.ap_path_wb))
        else:
            output = self.active_op(mutable)(x)
        return output

    @property
    def probs_over_ops(self):
        """
        Apply softmax on alpha to generate probability distribution

        Returns
        -------
        pytorch tensor
            probability distribution
        """
        probs = F.softmax(self.ap_path_alpha, dim=0)
        return probs

    @property
    def chosen_index(self):
        """
        choose the op with max prob

        Returns
        -------
        int
            index of the chosen one
        numpy.float32
            prob of the chosen one
        """
        probs = self.probs_over_ops.data.cpu().numpy()
        index = int(np.argmax(probs))
        return index, probs[index]

    def active_op(self, mutable):
        """
        assume only one path is active

        Returns
        -------
        PyTorch module
            the chosen operation
        """
        return mutable[self.active_index[0]]

    @property
    def active_op_index(self):
        """
        return active op's index, the active op is sampled

        Returns
        -------
        int
            index of the active op
        """
        return self.active_index[0]

    def set_chosen_op_active(self):
        """
        set chosen index, active and inactive indexes
        """
        chosen_idx, _ = self.chosen_index
        self.active_index = [chosen_idx]
        self.inactive_index = [_i for _i in range(0, chosen_idx)] + [_i for
            _i in range(chosen_idx + 1, self.n_choices)]

    def binarize(self, mutable):
        """
        Sample based on alpha, and set binary weights accordingly.
        ap_path_wb is set in this function, which is called binarize.

        Parameters
        ----------
        mutable : LayerChoice
            this layer's mutable
        """
        self.log_prob = None
        self.ap_path_wb.data.zero_()
        probs = self.probs_over_ops
        if MixedOp.forward_mode == 'two':
            sample_op = torch.multinomial(probs.data, 2, replacement=False)
            probs_slice = F.softmax(torch.stack([self.ap_path_alpha[idx] for
                idx in sample_op]), dim=0)
            self.current_prob_over_ops = torch.zeros_like(probs)
            for i, idx in enumerate(sample_op):
                self.current_prob_over_ops[idx] = probs_slice[i]
            c = torch.multinomial(probs_slice.data, 1)[0]
            active_op = sample_op[c].item()
            inactive_op = sample_op[1 - c].item()
            self.active_index = [active_op]
            self.inactive_index = [inactive_op]
            self.ap_path_wb.data[active_op] = 1.0
        else:
            sample = torch.multinomial(probs, 1)[0].item()
            self.active_index = [sample]
            self.inactive_index = [_i for _i in range(0, sample)] + [_i for
                _i in range(sample + 1, len(mutable))]
            self.log_prob = torch.log(probs[sample])
            self.current_prob_over_ops = probs
            self.ap_path_wb.data[sample] = 1.0
        for choice in mutable:
            for _, param in choice.named_parameters():
                param.grad = None

    @staticmethod
    def delta_ij(i, j):
        if i == j:
            return 1
        else:
            return 0

    def set_arch_param_grad(self, mutable):
        """
        Calculate alpha gradient for this LayerChoice.
        It is calculated using gradient of binary gate, probs of ops.
        """
        binary_grads = self.ap_path_wb.grad.data
        if self.active_op(mutable).is_zero_layer():
            self.ap_path_alpha.grad = None
            return
        if self.ap_path_alpha.grad is None:
            self.ap_path_alpha.grad = torch.zeros_like(self.ap_path_alpha.data)
        if MixedOp.forward_mode == 'two':
            involved_idx = self.active_index + self.inactive_index
            probs_slice = F.softmax(torch.stack([self.ap_path_alpha[idx] for
                idx in involved_idx]), dim=0).data
            for i in range(2):
                for j in range(2):
                    origin_i = involved_idx[i]
                    origin_j = involved_idx[j]
                    self.ap_path_alpha.grad.data[origin_i] += binary_grads[
                        origin_j] * probs_slice[j] * (MixedOp.delta_ij(i, j
                        ) - probs_slice[i])
            for _i, idx in enumerate(self.active_index):
                self.active_index[_i] = idx, self.ap_path_alpha.data[idx].item(
                    )
            for _i, idx in enumerate(self.inactive_index):
                self.inactive_index[_i] = idx, self.ap_path_alpha.data[idx
                    ].item()
        else:
            probs = self.probs_over_ops.data
            for i in range(self.n_choices):
                for j in range(self.n_choices):
                    self.ap_path_alpha.grad.data[i] += binary_grads[j] * probs[
                        j] * (MixedOp.delta_ij(i, j) - probs[i])
        return

    def rescale_updated_arch_param(self):
        """
        rescale architecture weights for the 'two' mode.
        """
        if not isinstance(self.active_index[0], tuple):
            assert self.active_op.is_zero_layer()
            return
        involved_idx = [idx for idx, _ in self.active_index + self.
            inactive_index]
        old_alphas = [alpha for _, alpha in self.active_index + self.
            inactive_index]
        new_alphas = [self.ap_path_alpha.data[idx] for idx in involved_idx]
        offset = math.log(sum([math.exp(alpha) for alpha in new_alphas]) /
            sum([math.exp(alpha) for alpha in old_alphas]))
        for idx in involved_idx:
            self.ap_path_alpha.data[idx] -= offset


class StubLayer:
    """
    StubLayer Module. Base Module.
    """

    def __init__(self, input_node=None, output_node=None):
        self.input = input_node
        self.output = output_node
        self.weights = None

    def build(self, shape):
        """
        build shape.
        """

    def set_weights(self, weights):
        """
        set weights.
        """
        self.weights = weights

    def import_weights(self, torch_layer):
        """
        import weights.
        """

    def import_weights_keras(self, keras_layer):
        """
        import weights from keras layer.
        """

    def export_weights(self, torch_layer):
        """
        export weights.
        """

    def export_weights_keras(self, keras_layer):
        """
        export weights to keras layer.
        """

    def get_weights(self):
        """
        get weights.
        """
        return self.weights

    def size(self):
        """
        size().
        """
        return 0

    @property
    def output_shape(self):
        """
        output shape.
        """
        return self.input.shape

    def to_real_layer(self):
        """
        to real layer.
        """

    def __str__(self):
        """
        str() function to print.
        """
        return type(self).__name__[4:]


class StubAggregateLayer(StubLayer):
    """
    StubAggregateLayer Module.
    """

    def __init__(self, input_nodes=None, output_node=None):
        if input_nodes is None:
            input_nodes = []
        super().__init__(input_nodes, output_node)


class StubAdd(StubAggregateLayer):
    """
    StubAdd Module.
    """

    @property
    def output_shape(self):
        return self.input[0].shape

    def to_real_layer(self):
        return TorchAdd()


class StubConcatenate(StubAggregateLayer):
    """StubConcatenate Module.
    """

    @property
    def output_shape(self):
        ret = 0
        for current_input in self.input:
            ret += current_input.shape[-1]
        ret = self.input[0].shape[:-1] + (ret,)
        return ret

    def to_real_layer(self):
        return TorchConcatenate()


def set_torch_weight_to_stub(torch_layer, stub_layer):
    stub_layer.import_weights(torch_layer)


def set_stub_weight_to_torch(stub_layer, torch_layer):
    stub_layer.export_weights(torch_layer)


class TorchModel(torch.nn.Module):
    """A neural network class using pytorch constructed from an instance of Graph."""

    def __init__(self, graph):
        super(TorchModel, self).__init__()
        self.graph = graph
        self.layers = []
        for layer in graph.layer_list:
            self.layers.append(layer.to_real_layer())
        if graph.weighted:
            for index, layer in enumerate(self.layers):
                set_stub_weight_to_torch(self.graph.layer_list[index], layer)
        for index, layer in enumerate(self.layers):
            self.add_module(str(index), layer)

    def forward(self, input_tensor):
        topo_node_list = self.graph.topological_order
        output_id = topo_node_list[-1]
        input_id = topo_node_list[0]
        node_list = deepcopy(self.graph.node_list)
        node_list[input_id] = input_tensor
        for v in topo_node_list:
            for u, layer_id in self.graph.reverse_adj_list[v]:
                layer = self.graph.layer_list[layer_id]
                torch_layer = self.layers[layer_id]
                if isinstance(layer, (StubAdd, StubConcatenate)):
                    edge_input_tensor = list(map(lambda x: node_list[x],
                        self.graph.layer_id_to_input_node_ids[layer_id]))
                else:
                    edge_input_tensor = node_list[u]
                temp_tensor = torch_layer(edge_input_tensor)
                node_list[v] = temp_tensor
        return node_list[output_id]

    def set_weight_to_graph(self):
        self.graph.weighted = True
        for index, layer in enumerate(self.layers):
            set_torch_weight_to_stub(layer, self.graph.layer_list[index])


class AvgPool(nn.Module):
    """
    AvgPool Module.
    """

    def __init__(self):
        super().__init__()

    @abstractmethod
    def forward(self, input_tensor):
        pass


class TorchConcatenate(nn.Module):
    """
    TorchConcatenate Module.
    """

    def forward(self, input_list):
        return torch.cat(input_list, dim=1)


class TorchAdd(nn.Module):
    """
    TorchAdd Module.
    """

    def forward(self, input_list):
        return input_list[0] + input_list[1]


class TorchFlatten(nn.Module):
    """
    TorchFlatten Module.
    """

    def forward(self, input_tensor):
        return input_tensor.view(input_tensor.size(0), -1)


class LayerChoiceOnlySearchSpace(nn.Module):

    def __init__(self, test_case):
        super().__init__()
        self.test_case = test_case
        self.conv1 = LayerChoice([nn.Conv2d(3, 6, 3, padding=1), nn.Conv2d(
            3, 6, 5, padding=2)])
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = LayerChoice([nn.Conv2d(6, 16, 3, padding=1), nn.Conv2d
            (6, 16, 5, padding=2)], return_mask=True)
        self.conv3 = nn.Conv2d(16, 16, 1)
        self.bn = nn.BatchNorm2d(16)
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(16, 10)

    def forward(self, x):
        bs = x.size(0)
        x = self.pool(F.relu(self.conv1(x)))
        x0, mask = self.conv2(x)
        self.test_case.assertEqual(mask.size(), torch.Size([2]))
        x1 = F.relu(self.conv3(x0))
        x = self.pool(self.bn(x1))
        self.test_case.assertEqual(mask.size(), torch.Size([2]))
        x = self.gap(x).view(bs, -1)
        x = self.fc(x)
        return x


class Layer(nn.Module):

    def __init__(self, num_nodes, channels):
        super().__init__()
        self.num_nodes = num_nodes
        self.nodes = nn.ModuleList()
        node_labels = [InputChoice.NO_KEY, InputChoice.NO_KEY]
        for i in range(num_nodes):
            node_labels.append('node_{}'.format(i))
            self.nodes.append(Node(node_labels[-1], node_labels[:-1], channels)
                )
        self.final_conv_w = nn.Parameter(torch.zeros(channels, self.
            num_nodes + 2, channels, 1, 1), requires_grad=True)
        self.bn = nn.BatchNorm2d(channels, affine=False)

    def forward(self, pprev, prev):
        prev_nodes_out = [pprev, prev]
        nodes_used_mask = torch.zeros(self.num_nodes + 2, dtype=torch.bool,
            device=prev.device)
        for i in range(self.num_nodes):
            node_out, mask = self.nodes[i](prev_nodes_out)
            nodes_used_mask[:mask.size(0)] |= mask.to(prev.device)
            prev_nodes_out.append(node_out)
        unused_nodes = torch.cat([out for used, out in zip(nodes_used_mask,
            prev_nodes_out) if not used], 1)
        unused_nodes = F.relu(unused_nodes)
        conv_weight = self.final_conv_w[:, (~nodes_used_mask), :, :, :]
        conv_weight = conv_weight.view(conv_weight.size(0), -1, 1, 1)
        out = F.conv2d(unused_nodes, conv_weight)
        return prev, self.bn(out)


class SpaceWithMutableScope(nn.Module):

    def __init__(self, test_case, num_layers=4, num_nodes=5, channels=16,
        in_channels=3, num_classes=10):
        super().__init__()
        self.test_case = test_case
        self.num_layers = num_layers
        self.stem = nn.Sequential(nn.Conv2d(in_channels, channels, 3, 1, 1,
            bias=False), nn.BatchNorm2d(channels))
        self.layers = nn.ModuleList()
        for _ in range(self.num_layers + 2):
            self.layers.append(Layer(num_nodes, channels))
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.dense = nn.Linear(channels, num_classes)

    def forward(self, x):
        prev = cur = self.stem(x)
        for layer in self.layers:
            prev, cur = layer(prev, cur)
        cur = self.gap(F.relu(cur)).view(x.size(0), -1)
        return self.dense(cur)


class NaiveSearchSpace(nn.Module):

    def __init__(self, test_case):
        super().__init__()
        self.test_case = test_case
        self.conv1 = LayerChoice([nn.Conv2d(3, 6, 3, padding=1), nn.Conv2d(
            3, 6, 5, padding=2)])
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = LayerChoice([nn.Conv2d(6, 16, 3, padding=1), nn.Conv2d
            (6, 16, 5, padding=2)], return_mask=True)
        self.conv3 = nn.Conv2d(16, 16, 1)
        self.skipconnect = InputChoice(n_candidates=1)
        self.skipconnect2 = InputChoice(n_candidates=2, return_mask=True)
        self.bn = nn.BatchNorm2d(16)
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(16, 10)

    def forward(self, x):
        bs = x.size(0)
        x = self.pool(F.relu(self.conv1(x)))
        x0, mask = self.conv2(x)
        self.test_case.assertEqual(mask.size(), torch.Size([2]))
        x1 = F.relu(self.conv3(x0))
        _, mask = self.skipconnect2([x0, x1])
        x0 = self.skipconnect([x0])
        if x0 is not None:
            x1 += x0
        x = self.pool(self.bn(x1))
        self.test_case.assertEqual(mask.size(), torch.Size([2]))
        x = self.gap(x).view(bs, -1)
        x = self.fc(x)
        return x


class MutableOp(nn.Module):

    def __init__(self, kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(3, 120, kernel_size, padding=kernel_size // 2)
        self.nested_mutable = InputChoice(n_candidates=10)

    def forward(self, x):
        return self.conv(x)


class NestedSpace(nn.Module):

    def __init__(self, test_case):
        super().__init__()
        self.test_case = test_case
        self.conv1 = LayerChoice([MutableOp(3), MutableOp(5)])
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(120, 10)

    def forward(self, x):
        bs = x.size(0)
        x = F.relu(self.conv1(x))
        x = self.gap(x).view(bs, -1)
        x = self.fc(x)
        return x


class TorchModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(1, 5, 5, 1)
        self.bn1 = torch.nn.BatchNorm2d(5)
        self.conv2 = torch.nn.Conv2d(5, 10, 5, 1)
        self.bn2 = torch.nn.BatchNorm2d(10)
        self.fc1 = torch.nn.Linear(4 * 4 * 10, 100)
        self.fc2 = torch.nn.Linear(100, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4 * 4 * 10)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class BackboneModel1(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 1, 1, 1)

    def forward(self, x):
        return self.conv1(x)


class BackboneModel2(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)
        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)
        self.fc1 = nn.Linear(4 * 4 * 50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


class BigModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.backbone1 = BackboneModel1()
        self.backbone2 = BackboneModel2()
        self.fc3 = nn.Linear(10, 2)

    def forward(self, x):
        x = self.backbone1(x)
        x = self.backbone2(x)
        x = self.fc3(x)
        return x


class BackboneModel1(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 1, 1, 1)

    def forward(self, x):
        return self.conv1(x)


class BackboneModel2(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)
        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)
        self.fc1 = nn.Linear(4 * 4 * 50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


class BigModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.backbone1 = BackboneModel1()
        self.backbone2 = BackboneModel2()
        self.fc3 = nn.Sequential(nn.Linear(10, 10), nn.BatchNorm1d(10), nn.
            ReLU(inplace=True), nn.Linear(10, 2))

    def forward(self, x):
        x = self.backbone1(x)
        x = self.backbone2(x)
        x = self.fc3(x)
        return x


class Model(nn.Module):

    def __init__(self, bias=True):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1, bias=bias)
        self.bn1 = nn.BatchNorm2d(8)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(8, 2, bias=bias)
        self.bias = bias

    def forward(self, x):
        return self.fc(self.pool(self.bn1(self.conv1(x))).view(x.size(0), -1))


import torch
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile

class Test_microsoft_nni(_paritybench_base):
    pass
    def test_000(self):
        self._check(AuxiliaryHead(*[], **{'in_channels': 4, 'num_classes': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_001(self):
        self._check(AvgPool(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

    def test_002(self):
        self._check(BackboneModel1(*[], **{}), [torch.rand([4, 1, 64, 64])], {})

    def test_003(self):
        self._check(BasicBlock(*[], **{'in_planes': 4, 'planes': 64}), [torch.rand([4, 4, 4, 4])], {})

    def test_004(self):
        self._check(BatchNorm(*[], **{'num_features': 4, 'pre_mask': 4, 'post_mask': 4}), [torch.rand([4, 4]), torch.rand([4, 4])], {})

    def test_005(self):
        self._check(Block(*[], **{'in_planes': 4}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_006(self):
        self._check(Calibration(*[], **{'in_channels': 4, 'out_channels': 4}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_007(self):
        self._check(CellA(*[], **{'in_planes': 4, 'out_planes': 4}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_008(self):
        self._check(CellB(*[], **{'in_planes': 4, 'out_planes': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_009(self):
        self._check(ConvBn2d(*[], **{'in_channels': 4, 'out_channels': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_010(self):
        self._check(ConvBranch(*[], **{'C_in': 4, 'C_out': 4, 'kernel_size': 4, 'stride': 1, 'padding': 4, 'separable': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_011(self):
        self._check(ConvRelu(*[], **{'in_': 4, 'out': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_012(self):
        self._check(CrossEntropyLabelSmooth(*[], **{'num_classes': 4, 'epsilon': 4}), [torch.rand([4, 4]), torch.zeros([4], dtype=torch.int64)], {})

    def test_013(self):
        self._check(DilConv(*[], **{'C_in': 4, 'C_out': 4, 'kernel_size': 4, 'stride': 1, 'padding': 4, 'dilation': 1}), [torch.rand([4, 4, 4, 4])], {})

    def test_014(self):
        self._check(DropPath(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

    def test_015(self):
        self._check(FacConv(*[], **{'C_in': 4, 'C_out': 4, 'kernel_length': 4, 'stride': 1, 'padding': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_016(self):
        self._check(FactorizedReduce(*[], **{'C_in': 4, 'C_out': 4}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_017(self):
        self._check(FocalLoss2d(*[], **{}), [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_018(self):
        self._check(GlobalAvgPool(*[], **{}), [torch.rand([4, 4, 4]), torch.rand([4, 4])], {})

    def test_019(self):
        self._check(Inception(*[], **{'in_planes': 4, 'n1x1': 4, 'n3x3red': 4, 'n3x3': 4, 'n5x5red': 4, 'n5x5': 4, 'pool_planes': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_020(self):
        self._check(InteractiveKLLoss(*[], **{'temperature': 4}), [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})

    def test_021(self):
        self._check(LinearCombine(*[], **{'layers_num': 1}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_022(self):
        self._check(LinearLayer(*[], **{'in_features': 4, 'out_features': 4}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_023(self):
        self._check(MBInvertedConvLayer(*[], **{'in_channels': 4, 'out_channels': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_024(self):
        self._check(Mask(*[], **{}), [torch.rand([4, 4]), torch.rand([4, 4])], {})

    def test_025(self):
        self._check(Model(*[], **{}), [torch.rand([4, 1, 64, 64])], {})

    def test_026(self):
        self._check(PreActBlock(*[], **{'in_planes': 4, 'planes': 64}), [torch.rand([4, 4, 4, 4])], {})

    def test_027(self):
        self._check(PreActBottleneck(*[], **{'in_planes': 4, 'planes': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_028(self):
        self._check(ReductionLayer(*[], **{'in_channels_pp': 4, 'in_channels_p': 4, 'out_channels': 4}), [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})

    def test_029(self):
        self._check(SepConv(*[], **{'in_planes': 4, 'out_planes': 4, 'kernel_size': 4, 'stride': 1}), [torch.rand([4, 4, 4, 4])], {})

    def test_030(self):
        self._check(SepConvBN(*[], **{'C_in': 4, 'C_out': 4, 'kernel_size': 4, 'padding': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_031(self):
        self._check(SeparableConv(*[], **{'C_in': 4, 'C_out': 4, 'kernel_size': 4, 'stride': 1, 'padding': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_032(self):
        self._check(ShuffleLayer(*[], **{'groups': 1}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_033(self):
        self._check(ShuffleXceptionBlock(*[], **{'inp': 4, 'oup': 4, 'mid_channels': 4, 'stride': 1}), [torch.rand([4, 4, 4, 4])], {})

    def test_034(self):
        self._check(SpatialAttentionGate(*[], **{'channel': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_035(self):
        self._check(StableBCELoss(*[], **{}), [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})

    def test_036(self):
        self._check(StdConv(*[], **{'C_in': 4, 'C_out': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_037(self):
        self._check(TorchAdd(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

    def test_038(self):
        self._check(TorchFlatten(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

    def test_039(self):
        self._check(Transition(*[], **{'in_planes': 4, 'out_planes': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_040(self):
        self._check(VGG_Cifar10(*[], **{}), [torch.rand([4, 3, 64, 64])], {})

    def test_041(self):
        self._check(ZeroLayer(*[], **{'stride': 1}), [torch.rand([4, 4, 4, 4])], {})

    def test_042(self):
        self._check(fc1(*[], **{}), [torch.rand([784, 784])], {})

