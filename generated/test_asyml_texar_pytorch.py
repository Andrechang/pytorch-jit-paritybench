import sys
_module = sys.modules[__name__]
del sys
conf = _module
bert_classifier_main = _module
bert_classifier_using_executor_main = _module
bert_with_hypertuning_main = _module
config_classifier = _module
config_data = _module
download_glue_data = _module
prepare_data = _module
utils = _module
data_utils = _module
model_utils = _module
config_train = _module
gpt2_generate_main = _module
gpt2_train_main = _module
prepare_data = _module
data_utils = _module
classifier_main = _module
config_kim = _module
sst_data_preprocessor = _module
config_iwslt14 = _module
config_model = _module
config_model_full = _module
config_toy_copy = _module
prepare_data = _module
seq2seq_attn = _module
bleu_main = _module
config_iwslt15 = _module
config_model = _module
config_wmt14 = _module
model = _module
transformer_main = _module
data_utils = _module
preprocess = _module
config_lstm_ptb = _module
config_lstm_yahoo = _module
config_trans_ptb = _module
config_trans_yahoo = _module
prepare_data = _module
vae_train = _module
config_data_imdb = _module
config_data_stsb = _module
data_utils = _module
dataset = _module
model_utils = _module
processor = _module
xlnet_classification_main = _module
xlnet_generation_ipython = _module
xlnet_generation_main = _module
setup = _module
attention_mechanism_test = _module
attention_mechanism_utils_test = _module
cell_wrappers_test = _module
layers_test = _module
optimization_test = _module
regularizers_test = _module
data_iterators_test = _module
large_file_test = _module
mono_text_data_test = _module
multi_aligned_data_test = _module
paired_text_data_test = _module
record_data_test = _module
sampler_test = _module
scalar_data_test = _module
embedding_test = _module
bert_tokenizer_test = _module
bert_tokenizer_utils_test = _module
gpt2_tokenizer_test = _module
roberta_tokenizer_test = _module
sentencepiece_tokenizer_test = _module
t5_tokenizer_test = _module
xlnet_tokenizer_test = _module
vocabulary_test = _module
bleu_moses_test = _module
bleu_test = _module
bleu_transformer_test = _module
metrics_test = _module
hyperparams_test = _module
adv_losses_test = _module
entropy_test = _module
mle_losses_test = _module
pg_losses_test = _module
rewards_test = _module
bert_classifier_test = _module
conv_classifiers_test = _module
gpt2_classifier_test = _module
rnn_classifiers_test = _module
roberta_classifier_test = _module
xlnet_classifier_test = _module
connectors_test = _module
decoder_helpers_test = _module
gpt2_decoder_test = _module
rnn_decoders_test = _module
transformer_decoders_test = _module
xlnet_decoder_test = _module
embedder_utils_test = _module
embedders_test = _module
t5_encoder_decoder_test = _module
bert_encoder_test = _module
conv_encoders_test = _module
gpt2_encoder_test = _module
rnn_encoders_test = _module
roberta_encoder_test = _module
transformer_encoder_test = _module
xlnet_encoder_test = _module
conv_networks_test = _module
networks_test = _module
bert_test = _module
gpt2_test = _module
roberta_test = _module
t5_test = _module
t5_utils_test = _module
xlnet_test = _module
xlnet_utils_test = _module
xlnet_regressor_test = _module
condition_test = _module
executor_test = _module
classification_test = _module
generation_test = _module
regression_test = _module
summary_test = _module
average_recorder_test = _module
beam_search_test = _module
rnn_test = _module
shapes_test = _module
utils_test = _module
texar = _module
core = _module
attention_mechanism = _module
attention_mechanism_utils = _module
cell_wrappers = _module
layers = _module
optimization = _module
regularizers = _module
custom = _module
activation = _module
distributions = _module
initializers = _module
data = _module
data = _module
data_base = _module
data_iterators = _module
data_iterators_utils = _module
dataset_utils = _module
mono_text_data = _module
multi_aligned_data = _module
paired_text_data = _module
record_data = _module
sampler = _module
scalar_data = _module
text_data_base = _module
data_utils = _module
embedding = _module
tokenizers = _module
bert_tokenizer = _module
bert_tokenizer_utils = _module
gpt2_tokenizer = _module
gpt2_tokenizer_utils = _module
roberta_tokenizer = _module
sentencepiece_tokenizer = _module
t5_tokenizer = _module
tokenizer_base = _module
xlnet_tokenizer = _module
vocabulary = _module
evals = _module
bleu = _module
bleu_moses = _module
bleu_transformer = _module
metrics = _module
hyperparams = _module
losses = _module
adv_losses = _module
entropy = _module
losses_utils = _module
mle_losses = _module
pg_losses = _module
rewards = _module
module_base = _module
modules = _module
classifiers = _module
bert_classifier = _module
classifier_base = _module
conv_classifiers = _module
gpt2_classifier = _module
rnn_classifiers = _module
roberta_classifier = _module
xlnet_classifier = _module
connectors = _module
connector_base = _module
connectors = _module
decoders = _module
decoder_base = _module
decoder_helpers = _module
gpt2_decoder = _module
rnn_decoder_base = _module
rnn_decoders = _module
t5_decoder = _module
transformer_decoders = _module
xlnet_decoder = _module
embedders = _module
embedder_base = _module
embedder_utils = _module
embedders = _module
position_embedders = _module
encoder_decoders = _module
encoder_decoder_base = _module
t5_encoder_decoder = _module
encoders = _module
bert_encoder = _module
conv_encoders = _module
encoder_base = _module
gpt2_encoder = _module
multihead_attention = _module
rnn_encoders = _module
roberta_encoder = _module
t5_encoder = _module
transformer_encoder = _module
xlnet_encoder = _module
networks = _module
conv_networks = _module
network_base = _module
networks = _module
pretrained = _module
bert = _module
gpt2 = _module
pretrained_base = _module
roberta = _module
t5 = _module
t5_utils = _module
xlnet = _module
xlnet_utils = _module
regressors = _module
regressor_base = _module
xlnet_regressor = _module
run = _module
action = _module
condition = _module
executor = _module
executor_utils = _module
metric = _module
base_metric = _module
classification = _module
generation = _module
regression = _module
summary = _module
utils = _module
average_recorder = _module
beam_search = _module
dtypes = _module
exceptions = _module
nest = _module
rnn = _module
shapes = _module
test = _module
transformer_attentions = _module
types = _module
utils = _module
utils_io = _module
version = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, numbers, numpy, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import functools


import logging


from typing import Any


import torch


import torch.nn.functional as F


from typing import Dict


from typing import List


from typing import Optional


from typing import Tuple


from typing import Union


from torch import nn


from torch.nn import functional as F


import random


import numpy as np


import torch.nn as nn


import re


import copy


import math


import time


from torch import Tensor


from torch.optim.lr_scheduler import ExponentialLR


from typing import TypeVar


import collections


from typing import Callable


from typing import no_type_check


from collections import namedtuple


import torch.distributions as tds


import numpy


from abc import ABC


from typing import NamedTuple


from torch.autograd import Function


from typing import Generic


from typing import Type


from typing import Iterable


from torch.nn.utils import clip_grad_norm_


from torch.optim.lr_scheduler import _LRScheduler


from torch.optim.optimizer import Optimizer


from torch.distributions import Normal


from torch.distributions import Independent


import warnings


from typing import Iterator


from typing import Sequence


from torch.utils.data import Dataset


from typing import Mapping


from torch import __version__ as _torch_version


from torch.utils.data import DataLoader


from torch.utils.data.dataloader import _BaseDataLoaderIter


from torch.utils.data.dataloader import _SingleProcessDataLoaderIter


from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter


from enum import Enum


from typing import ItemsView


from typing import KeysView


from typing import ValuesView


from torch.utils.data import sampler as torch_sampler


from typing import IO


from typing import overload


from collections import defaultdict


from typing import DefaultDict


from typing import Counter


from torch.distributions.distribution import Distribution


from abc import abstractmethod


from torch.distributions import Categorical


from torch.distributions import Gumbel


from typing import Set


import itertools


import types


from enum import auto


from time import time as time_now


from collections import OrderedDict


from torch.optim.lr_scheduler import _LRScheduler as LRScheduler


from collections import Counter


from typing import Counter as CounterType


from collections import deque


from typing import Deque


import inspect


from functools import lru_cache


from typing import Collection


from typing import MutableMapping


from typing import cast


from torch.nn.modules.conv import _ConvNd


class LabelSmoothingLoss(nn.Module):
    """With label smoothing,
    KL-divergence between q_{smoothed ground truth prob.}(w)
    and p_{prob. computed by model}(w) is minimized.

    Args:
        label_confidence: the confidence weight on the ground truth label.
        tgt_vocab_size: the size of the final classification.
        ignore_index: The index in the vocabulary to ignore weight.
    """
    one_hot: torch.Tensor

    def __init__(self, label_confidence, tgt_vocab_size, ignore_index=0):
        super().__init__()
        self.ignore_index = ignore_index
        self.tgt_vocab_size = tgt_vocab_size
        label_smoothing = 1 - label_confidence
        assert 0.0 < label_smoothing <= 1.0
        smoothing_value = label_smoothing / (tgt_vocab_size - 2)
        one_hot = torch.full((tgt_vocab_size,), smoothing_value)
        one_hot[self.ignore_index] = 0
        self.register_buffer('one_hot', one_hot.unsqueeze(0))
        self.confidence = label_confidence

    def forward(self, output: torch.Tensor, target: torch.Tensor, label_lengths: torch.LongTensor) ->torch.Tensor:
        """Compute the label smoothing loss.

        Args:
            output (FloatTensor): batch_size x seq_length * n_classes
            target (LongTensor): batch_size * seq_length, specify the label
                target
            label_lengths(torch.LongTensor): specify the length of the labels
        """
        orig_shapes = output.size(), target.size()
        output = output.view(-1, self.tgt_vocab_size)
        target = target.view(-1)
        model_prob = self.one_hot.repeat(target.size(0), 1)
        model_prob = model_prob
        model_prob.scatter_(1, target.unsqueeze(1), self.confidence)
        model_prob.masked_fill_((target == self.ignore_index).unsqueeze(1), 0)
        output = output.view(orig_shapes[0])
        model_prob = model_prob.view(orig_shapes[0])
        return tx.losses.sequence_softmax_cross_entropy(labels=model_prob, logits=output, sequence_length=label_lengths, average_across_batch=False, sum_over_timesteps=False)


class Seq2SeqAttn(nn.Module):

    def __init__(self, train_data):
        super().__init__()
        self.source_vocab_size = train_data.source_vocab.size
        self.target_vocab_size = train_data.target_vocab.size
        self.bos_token_id = train_data.target_vocab.bos_token_id
        self.eos_token_id = train_data.target_vocab.eos_token_id
        self.source_embedder = tx.modules.WordEmbedder(vocab_size=self.source_vocab_size, hparams=config_model.embedder)
        self.target_embedder = tx.modules.WordEmbedder(vocab_size=self.target_vocab_size, hparams=config_model.embedder)
        self.encoder = tx.modules.BidirectionalRNNEncoder(input_size=self.source_embedder.dim, hparams=config_model.encoder)
        self.decoder = tx.modules.AttentionRNNDecoder(token_embedder=self.target_embedder, encoder_output_size=self.encoder.cell_fw.hidden_size + self.encoder.cell_bw.hidden_size, input_size=self.target_embedder.dim, vocab_size=self.target_vocab_size, hparams=config_model.decoder)

    def forward(self, batch, mode):
        enc_outputs, _ = self.encoder(inputs=self.source_embedder(batch['source_text_ids']), sequence_length=batch['source_length'])
        memory = torch.cat(enc_outputs, dim=2)
        if mode == 'train':
            helper_train = self.decoder.create_helper(decoding_strategy='train_greedy')
            training_outputs, _, _ = self.decoder(memory=memory, memory_sequence_length=batch['source_length'], helper=helper_train, inputs=batch['target_text_ids'][:, :-1], sequence_length=batch['target_length'] - 1)
            mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(labels=batch['target_text_ids'][:, 1:], logits=training_outputs.logits, sequence_length=batch['target_length'] - 1)
            return mle_loss
        else:
            start_tokens = memory.new_full(batch['target_length'].size(), self.bos_token_id, dtype=torch.int64)
            infer_outputs = self.decoder(start_tokens=start_tokens, end_token=self.eos_token_id, memory=memory, memory_sequence_length=batch['source_length'], beam_width=config_model.beam_width)
            return infer_outputs


def MultivariateNormalDiag(loc, scale_diag):
    if loc.dim() < 1:
        raise ValueError('loc must be at least one-dimensional.')
    return Independent(Normal(loc, scale_diag), 1)


def kl_divergence(means: Tensor, logvars: Tensor) ->Tensor:
    """Compute the KL divergence between Gaussian distribution
    """
    kl_cost = -0.5 * (logvars - means ** 2 - torch.exp(logvars) + 1.0)
    kl_cost = torch.mean(kl_cost, 0)
    return torch.sum(kl_cost)


State = TypeVar('State')


class RNNCellBase(nn.Module, Generic[State]):
    """The base class for RNN cells in our framework. Major differences over
    :torch_nn:`RNNCell` are two-fold:

    1. Holds an :torch_nn:`Module` which could either be a built-in
       RNN cell or a wrapped cell instance. This design allows
       :class:`RNNCellBase` to serve as the base class for both vanilla
       cells and wrapped cells.

    2. Adds :meth:`zero_state` method for initialization of hidden states,
       which can also be used to implement batch-specific initialization
       routines.
    """

    def __init__(self, cell: Union[nn.RNNCellBase, 'RNNCellBase']):
        super().__init__()
        if not isinstance(cell, nn.Module):
            raise ValueError("Type of parameter 'cell' must be derived fromnn.Module, and has 'input_size' and 'hidden_size'attributes.")
        self._cell = cell

    @property
    def input_size(self) ->int:
        """The number of expected features in the input."""
        return self._cell.input_size

    @property
    def hidden_size(self) ->int:
        """The number of features in the hidden state."""
        return self._cell.hidden_size

    @property
    def _param(self) ->nn.Parameter:
        """Convenience method to access a parameter under the module. Useful
        when creating tensors of the same attributes using `param.new_*`.
        """
        return next(self.parameters())

    def init_batch(self):
        """Perform batch-specific initialization routines. For most cells this
        is a no-op.
        """
        pass

    def zero_state(self, batch_size: int) ->State:
        """Return zero-filled state tensor(s).

        Args:
            batch_size: int, the batch size.

        Returns:
            State tensor(s) initialized to zeros. Note that different subclasses
            might return tensors of different shapes and structures.
        """
        self.init_batch()
        if isinstance(self._cell, nn.RNNCellBase):
            state = self._param.new_zeros(batch_size, self.hidden_size, requires_grad=False)
        else:
            state = self._cell.zero_state(batch_size)
        return state

    def forward(self, input: torch.Tensor, state: Optional[State]=None) ->Tuple[torch.Tensor, State]:
        """
        Returns:
            A tuple of (output, state). For single layer RNNs, output is
            the same as state.
        """
        if state is None:
            batch_size = input.size(0)
            state = self.zero_state(batch_size)
        return self._cell(input, state)


class MaxReducePool1d(nn.Module):
    """A subclass of :torch_nn:`Module`.
    Max Pool layer for 1D inputs. The same as :torch_nn:`MaxPool1d` except that
    the pooling dimension is entirely reduced (i.e., `pool_size=input_length`).
    """

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        output, _ = torch.max(input, dim=2)
        return output


class AvgReducePool1d(nn.Module):
    """A subclass of :torch_nn:`Module`.
    Avg Pool layer for 1D inputs. The same as :torch_nn:`AvgPool1d` except that
    the pooling dimension is entirely reduced (i.e., `pool_size=input_length`).
    """

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        return torch.mean(input, dim=2)


def _type_name(value):
    return type(value).__name__


class HParams:
    """A class that maintains hyperparameters for configuring Texar modules.
    The class has several useful features:

    - **Auto-completion of missing values.** Users can specify only a subset of
      hyperparameters they care about. Other hyperparameters will automatically
      take the default values. The auto-completion performs **recursively** so
      that hyperparameters taking `dict` values will also be auto-completed
      **All Texar modules** provide a :meth:`default_hparams` containing
      allowed hyperparameters and their default values. For example:

        .. code-block:: python

            ## Recursive auto-completion
            default_hparams = {"a": 1, "b": {"c": 2, "d": 3}}
            hparams = {"b": {"c": 22}}
            hparams_ = HParams(hparams, default_hparams)
            hparams_.todict() == {"a": 1, "b": {"c": 22, "d": 3}}
                # "a" and "d" are auto-completed

            ## All Texar modules have built-in `default_hparams`
            hparams = {"dropout_rate": 0.1}
            emb = tx.modules.WordEmbedder(hparams=hparams, ...)
            emb.hparams.todict() == {
                "dropout_rate": 0.1,  # provided value
                "dim": 100            # default value
                ...
            }

    - **Automatic type-check.** For most hyperparameters, provided value must
      have the same or compatible dtype with the default value. :class:`HParams`
      does necessary type-check, and raises Error if improper dtype is provided.
      Also, hyperparameters not listed in `default_hparams` are not allowed,
      except for `"kwargs"` as detailed below.

    - **Flexible dtype for specified hyperparameters.**  Some hyperparameters
      may allow different dtypes of values.

        - Hyperparameters named `"type"` are not type-checked.
          For example, in :func:`~texar.torch.core.get_rnn_cell`, hyperparameter
          `"type"` can take value of an RNNCell class, its string name of module
          path, or an RNNCell class instance. (String name or module path is
          allowed so that users can specify the value in YAML configuration
          files.)

        - For other hyperparameters, list them in the `"@no_typecheck"` field
          in :meth:`default_hparams` to skip type-check. For example, in
          :class:`~texar.torch.modules.Conv1DNetwork`, hyperparameter
          `"kernel_size"` can be set to either a `list` of `int`\\ s or simply
          an `int`.

    - **Special flexibility of keyword argument hyperparameters.**
      Hyperparameters named ``"kwargs"`` are used as keyword arguments for a
      class constructor or a function call. Such hyperparameters take a `dict`,
      and users can add arbitrary valid keyword arguments to the dict.
      For example:

        .. code-block:: python

            default_rnn_cell_hparams = {
                "type": "LSTMCell",
                "kwargs": {"num_units": 256}
                # Other hyperparameters
                ...
            }
            my_hparams = {
                "kwargs" {
                    "num_units": 123,
                    # Other valid keyword arguments for LSTMCell constructor
                    "forget_bias": 0.0
                    "activation": "torch.nn.functional.relu"
                }
            }
            _ = HParams(my_hparams, default_rnn_cell_hparams)

    - **Rich interfaces.** An :class:`HParams` instance provides rich interfaces
      for accessing, updating, or adding hyperparameters.

        .. code-block:: python

            hparams = HParams(my_hparams, default_hparams)
            # Access
            hparams.type == hparams["type"]
            # Update
            hparams.type = "GRUCell"
            hparams.kwargs = { "num_units": 100 }
            hparams.kwargs.num_units == 100
            # Add new
            hparams.add_hparam("index", 1)
            hparams.index == 1

            # Convert to `dict` (recursively)
            type(hparams.todic()) == dict

            # I/O
            pickle.dump(hparams, "hparams.dump")
            with open("hparams.dump", 'rb') as f:
                hparams_loaded = pickle.load(f)


    Args:
        hparams: A `dict` or an :class:`HParams` instance containing
            hyperparameters. If `None`, all hyperparameters are set to default
            values.
        default_hparams (dict): Hyperparameters with default values. If `None`,
            Hyperparameters are fully defined by :attr:`hparams`.
        allow_new_hparam (bool): If `False` (default), :attr:`hparams` cannot
            contain hyperparameters that are not included in
            :attr:`default_hparams`, except for the case of :attr:`"kwargs"` as
            above.
    """

    def __init__(self, hparams: Optional[Union['HParams', Dict[str, Any]]], default_hparams: Optional[Dict[str, Any]], allow_new_hparam: bool=False):
        if isinstance(hparams, HParams):
            hparams = hparams.todict()
        if default_hparams is not None:
            parsed_hparams = self._parse(hparams, default_hparams, allow_new_hparam)
        else:
            parsed_hparams = self._parse(hparams, hparams)
        super().__setattr__('_hparams', parsed_hparams)

    @staticmethod
    def _parse(hparams: Optional[Dict[str, Any]], default_hparams: Optional[Dict[str, Any]], allow_new_hparam: bool=False):
        """Parses hyperparameters.

        Args:
            hparams (dict): Hyperparameters. If `None`, all hyperparameters are
                set to default values.
            default_hparams (dict): Hyperparameters with default values.
                If `None`,Hyperparameters are fully defined by :attr:`hparams`.
            allow_new_hparam (bool): If `False` (default), :attr:`hparams`
                cannot contain hyperparameters that are not included in
                :attr:`default_hparams`, except the case of :attr:`"kwargs"`.

        Return:
            A dictionary of parsed hyperparameters. Returns `None` if both
            :attr:`hparams` and :attr:`default_hparams` are `None`.

        Raises:
            ValueError: If :attr:`hparams` is not `None` and
                :attr:`default_hparams` is `None`.
            ValueError: If :attr:`default_hparams` contains "kwargs" not does
                not contains "type".
        """
        if hparams is None and default_hparams is None:
            return None
        if hparams is None:
            return HParams._parse(default_hparams, default_hparams)
        if default_hparams is None:
            raise ValueError('`default_hparams` cannot be `None` if `hparams` is not `None`.')
        no_typecheck_names = default_hparams.get('@no_typecheck', [])
        if 'kwargs' in default_hparams and 'type' not in default_hparams:
            raise ValueError("Ill-defined hyperparameter structure: 'kwargs' must accompany with 'type'.")
        parsed_hparams = copy.deepcopy(default_hparams)
        for name, value in default_hparams.items():
            if name not in hparams and isinstance(value, dict):
                if name == 'kwargs' and 'type' in hparams and hparams['type'] != default_hparams['type']:
                    parsed_hparams[name] = HParams({}, {})
                else:
                    parsed_hparams[name] = HParams(value, value)
        for name, value in hparams.items():
            if name not in default_hparams:
                if allow_new_hparam:
                    parsed_hparams[name] = HParams._parse_value(value, name)
                    continue
                raise ValueError("Unknown hyperparameter: %s. Only hyperparameters named 'kwargs' hyperparameters can contain new entries undefined in default hyperparameters." % name)
            if value is None:
                parsed_hparams[name] = HParams._parse_value(parsed_hparams[name])
            default_value = default_hparams[name]
            if default_value is None:
                parsed_hparams[name] = HParams._parse_value(value)
                continue
            if isinstance(value, dict):
                if name not in no_typecheck_names and not isinstance(default_value, dict):
                    raise ValueError("Hyperparameter '%s' must have type %s, got %s" % (name, _type_name(default_value), _type_name(value)))
                if name == 'kwargs':
                    if 'type' in hparams and hparams['type'] != default_hparams['type']:
                        parsed_hparams[name] = HParams(value, value)
                    else:
                        parsed_hparams[name] = HParams(value, default_value, allow_new_hparam=True)
                elif name in no_typecheck_names:
                    parsed_hparams[name] = HParams(value, value)
                else:
                    parsed_hparams[name] = HParams(value, default_value, allow_new_hparam)
                continue
            if name == 'type' and 'kwargs' in default_hparams:
                parsed_hparams[name] = value
                continue
            if name in no_typecheck_names:
                parsed_hparams[name] = value
            elif isinstance(value, type(default_value)):
                parsed_hparams[name] = value
            elif callable(value) and callable(default_value):
                parsed_hparams[name] = value
            else:
                try:
                    parsed_hparams[name] = type(default_value)(value)
                except TypeError:
                    raise ValueError("Hyperparameter '%s' must have type %s, got %s" % (name, _type_name(default_value), _type_name(value)))
        return parsed_hparams

    @staticmethod
    def _parse_value(value: Any, name: Optional[str]=None) ->Any:
        if isinstance(value, dict) and (name is None or name != 'kwargs'):
            return HParams(value, None)
        else:
            return value

    def __getattr__(self, name: str) ->Any:
        """Retrieves the value of the hyperparameter.
        """
        if name == '_hparams':
            return super().__getattribute__('_hparams')
        if name not in self._hparams:
            raise AttributeError('Unknown hyperparameter: %s' % name)
        return self._hparams[name]

    def __getitem__(self, name: str) ->Any:
        """Retrieves the value of the hyperparameter.
        """
        return self.__getattr__(name)

    def __setattr__(self, name: str, value: Any):
        """Sets the value of the hyperparameter.
        """
        if name not in self._hparams:
            raise ValueError('Unknown hyperparameter: %s. Only the `kwargs` hyperparameters can contain new entries undefined in default hyperparameters.' % name)
        self._hparams[name] = self._parse_value(value, name)

    def items(self) ->ItemsView[str, Any]:
        """Returns the list of hyperparameter `(name, value)` pairs.
        """
        return self._hparams.items()

    def keys(self) ->KeysView[str]:
        """Returns the list of hyperparameter names.
        """
        return self._hparams.keys()

    def __iter__(self) ->Iterator[Tuple[str, Any]]:
        for name, value in self._hparams.items():
            yield name, value

    def __len__(self) ->int:
        return len(self._hparams)

    def __contains__(self, name) ->bool:
        return name in self._hparams

    def __str__(self) ->str:
        """Return a string of the hyperparameters.
        """
        hparams_dict = self.todict()
        return json.dumps(hparams_dict, sort_keys=True, indent=2)

    def get(self, name: str, default: Optional[Any]=None) ->Any:
        """Returns the hyperparameter value for the given name. If name is not
        available then returns :attr:`default`.

        Args:
            name (str): the name of hyperparameter.
            default: the value to be returned in case name does not exist.
        """
        try:
            return self.__getattr__(name)
        except AttributeError:
            return default

    def add_hparam(self, name: str, value: Any):
        """Adds a new hyperparameter.
        """
        if name in self._hparams or hasattr(self, name):
            raise ValueError('Hyperparameter name already exists: %s' % name)
        self._hparams[name] = self._parse_value(value, name)

    def todict(self) ->Dict[str, Any]:
        """Returns a copy of hyperparameters as a dictionary.
        """
        dict_ = copy.deepcopy(self._hparams)
        for name, value in self._hparams.items():
            if isinstance(value, HParams):
                dict_[name] = value.todict()
        return dict_


def is_str(x):
    """Returns `True` if :attr:`x` is either a str or unicode.
    Returns `False` otherwise.
    """
    return isinstance(x, str)


def get_layer(hparams: Union[HParams, Dict[str, Any]]) ->nn.Module:
    """Makes a layer instance.

    The layer must be an instance of :torch_nn:`Module`.

    Args:
        hparams (dict or HParams): Hyperparameters of the layer, with
            structure:

            .. code-block:: python

                {
                    "type": "LayerClass",
                    "kwargs": {
                        # Keyword arguments of the layer class
                        # ...
                    }
                }

            Here:

            `"type"`: str or layer class or layer instance
                The layer type. This can be

                - The string name or full module path of a layer class. If
                  the class name is provided, the class must be in module
                  :torch_nn:`Module`, :mod:`texar.torch.core`, or
                  :mod:`texar.torch.custom`.
                - A layer class.
                - An instance of a layer class.

                For example

                .. code-block:: python

                    "type": "Conv1D"                               # class name
                    "type": "texar.torch.core.MaxReducePooling1D"  # module path
                    "type": "my_module.MyLayer"                    # module path
                    "type": torch.nn.Module.Linear                 # class
                    "type": Conv1D(filters=10, kernel_size=2)  # cell instance
                    "type": MyLayer(...)                       # cell instance

            `"kwargs"`: dict
                A dictionary of keyword arguments for constructor of the
                layer class. Ignored if :attr:`"type"` is a layer instance.

                - Arguments named "activation" can be a callable, or a `str` of
                  the name or module path to the activation function.
                - Arguments named "\\*_regularizer" and "\\*_initializer" can be a
                  class instance, or a `dict` of hyperparameters of respective
                  regularizers and initializers. See
                - Arguments named "\\*_constraint" can be a callable, or a `str`
                  of the name or full path to the constraint function.

    Returns:
        A layer instance. If ``hparams["type"]`` is a layer instance, returns it
        directly.

    Raises:
        ValueError: If :attr:`hparams` is `None`.
        ValueError: If the resulting layer is not an instance of
            :torch_nn:`Module`.
    """
    if hparams is None:
        raise ValueError('`hparams` must not be `None`.')
    layer_type = hparams['type']
    if not is_str(layer_type) and not isinstance(layer_type, type):
        layer = layer_type
    else:
        layer_modules = ['torch.nn', 'texar.torch.core', 'texar.torch.custom']
        layer_class: Type[nn.Module] = utils.check_or_get_class(layer_type, layer_modules)
        if isinstance(hparams, dict):
            if layer_class.__name__ == 'Linear' and 'in_features' not in hparams['kwargs']:
                raise ValueError('"in_features" should be specified for "torch.nn.{}"'.format(layer_class.__name__))
            elif layer_class.__name__ in ['Conv1d', 'Conv2d', 'Conv3d'] and 'in_channels' not in hparams['kwargs']:
                raise ValueError('"in_channels" should be specified for "torch.nn.{}"'.format(layer_class.__name__))
            default_kwargs: Dict[str, Any] = {}
            default_hparams = {'type': layer_type, 'kwargs': default_kwargs}
            hparams = HParams(hparams, default_hparams)
        if layer_type == 'Sequential':
            names: List[str] = []
            layer = nn.Sequential()
            sub_hparams = hparams.kwargs.layers
            for hparam in sub_hparams:
                sub_layer = get_layer(hparam)
                name = utils.uniquify_str(sub_layer.__class__.__name__, names)
                names.append(name)
                layer.add_module(name=name, module=sub_layer)
        else:
            layer = utils.get_instance(layer_type, hparams.kwargs.todict(), layer_modules)
    if not isinstance(layer, nn.Module):
        raise ValueError('layer must be an instance of `torch.nn.Module`.')
    return layer


class MergeLayer(nn.Module):
    """A subclass of :torch_nn:`Module`.
    A layer that consists of multiple layers in parallel. Input is fed to
    each of the parallel layers, and the outputs are merged with a
    specified mode.

    Args:
        layers (list, optional): A list of :torch_docs:`torch.nn.Module
            <nn.html#module>` instances, or a list of hyperparameter
            dictionaries each of which specifies `"type"` and `"kwargs"` of each
            layer (see the `hparams` argument of :func:`get_layer`).

            If `None`, this layer degenerates to a merging operator that merges
            inputs directly.
        mode (str): Mode of the merge op. This can be:

            - :attr:`'concat'`: Concatenates layer outputs along one dim.
              Tensors must have the same shape except for the dimension
              specified in `dim`, which can have different sizes.
            - :attr:`'elemwise_sum'`: Outputs element-wise sum.
            - :attr:`'elemwise_mul'`: Outputs element-wise product.
            - :attr:`'sum'`: Computes the sum of layer outputs along the
              dimension given by `dim`. For example, given `dim=1`,
              two tensors of shape `[a, b]` and `[a, c]` respectively
              will result in a merged tensor of shape `[a]`.
            - :attr:`'mean'`: Computes the mean of layer outputs along the
              dimension given in `dim`.
            - :attr:`'prod'`: Computes the product of layer outputs along the
              dimension given in `dim`.
            - :attr:`'max'`: Computes the maximum of layer outputs along the
              dimension given in `dim`.
            - :attr:`'min'`: Computes the minimum of layer outputs along the
              dimension given in `dim`.
            - :attr:`'and'`: Computes the `logical and` of layer outputs along
              the dimension given in `dim`.
            - :attr:`'or'`: Computes the `logical or` of layer outputs along
              the dimension given in `dim`.
            - :attr:`'logsumexp'`: Computes
              log(sum(exp(elements across the dimension of layer outputs)))
        dim (int): The dim to use in merging. Ignored in modes
            :attr:`'elemwise_sum'` and :attr:`'elemwise_mul'`.
    """
    _functions: Dict[str, Callable[[torch.Tensor, int], torch.Tensor]] = {'sum': torch.sum, 'mean': torch.mean, 'prod': torch.prod, 'max': lambda tensors, dim: torch.max(tensors, dim)[0], 'min': lambda tensors, dim: torch.min(tensors, dim)[0], 'and': torch.all, 'or': torch.any, 'logsumexp': torch.logsumexp}

    def __init__(self, layers: Optional[List[nn.Module]]=None, mode: str='concat', dim: Optional[int]=None):
        super().__init__()
        self._mode = mode
        self._dim = dim
        self._layers: Optional[nn.ModuleList] = None
        if layers is not None:
            if len(layers) == 0:
                raise ValueError("'layers' must be either None or a non-empty list.")
            self._layers = nn.ModuleList()
            for layer in layers:
                if isinstance(layer, nn.Module):
                    self._layers.append(layer)
                else:
                    self._layers.append(get_layer(hparams=layer))

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """Feed input to every containing layer and merge the outputs.

        Args:
            input: The input tensor.

        Returns:
            The merged tensor.
        """
        layer_outputs: List[torch.Tensor]
        if self._layers is None:
            layer_outputs = input
            if not isinstance(layer_outputs, (list, tuple)):
                layer_outputs = [layer_outputs]
        else:
            layer_outputs = []
            for layer in self._layers:
                layer_output = layer(input)
                layer_outputs.append(layer_output)
        dim = self._dim if self._dim is not None else -1
        if self._mode == 'concat':
            outputs = torch.cat(tensors=layer_outputs, dim=dim)
        elif self._mode == 'elemwise_sum':
            outputs = layer_outputs[0]
            for i in range(1, len(layer_outputs)):
                outputs = torch.add(outputs, layer_outputs[i])
        elif self._mode == 'elemwise_mul':
            outputs = layer_outputs[0]
            for i in range(1, len(layer_outputs)):
                outputs = torch.mul(outputs, layer_outputs[i])
        elif self._mode in self._functions:
            _concat = torch.cat(tensors=layer_outputs, dim=dim)
            outputs = self._functions[self._mode](_concat, dim)
        else:
            raise ValueError("Unknown merge mode: '%s'" % self._mode)
        return outputs

    @property
    def layers(self) ->Optional[nn.ModuleList]:
        """The list of parallel layers.
        """
        return self._layers


class Flatten(nn.Module):
    """Flatten layer to flatten a tensor after convolution."""

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        return input.view(input.size()[0], -1)


class Identity(nn.Module):
    """Identity activation layer."""

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        return input


class BertGELU(nn.Module):
    """Bert uses GELU as the activation function for the position-wise network.
    """

    def forward(self, x):
        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))


class GPTGELU(nn.Module):
    """For information: OpenAI GPT's GELU is slightly different (and gives
    slightly different results).
    """

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))


class ModuleBase(nn.Module, ABC):
    """Base class inherited by modules that are configurable through
    hyperparameters.

    This is a subclass of :torch_nn:`Module`.

    A Texar module inheriting :class:`~texar.torch.ModuleBase` is
    **configurable through hyperparameters**. That is, each module defines
    allowed hyperparameters and default values. Hyperparameters not
    specified by users will take default values.

    Args:
        hparams (dict, optional): Hyperparameters of the module. See
            :meth:`default_hparams` for the structure and default values.
    """

    def __init__(self, hparams: Optional[Union[HParams, Dict[str, Any]]]=None):
        super().__init__()
        if not hasattr(self, '_hparams'):
            self._hparams = HParams(hparams, self.default_hparams())
        elif hparams is not None:
            raise ValueError('`self._hparams` is already assigned, but `hparams` argument is not None.')

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a `dict` of hyperparameters of the module with default
        values. Used to replace the missing values of input `hparams`
        during module construction.

        .. code-block:: python

            {
                "name": "module"
            }
        """
        return {'name': 'module'}

    @property
    def trainable_variables(self) ->List[nn.Parameter]:
        """The list of trainable variables (parameters) of the module.
        Parameters of this module and all its submodules are included.

        .. note::
            The list returned may contain duplicate parameters (e.g. output
            layer shares parameters with embeddings). For most usages, it's not
            necessary to ensure uniqueness.
        """
        return [x for x in self.parameters() if x.requires_grad]

    @property
    def hparams(self) ->HParams:
        """An :class:`~texar.torch.HParams` instance. The hyperparameters
        of the module.
        """
        return self._hparams

    @property
    def output_size(self):
        """The feature size of :meth:`forward` output tensor(s),
        usually it is equal to the last dimension value of the output
        tensor size.
        """
        raise NotImplementedError


class ClassifierBase(ModuleBase, ABC):
    """Base class inherited by all classifier classes.
    """

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.
        """
        return {'name': 'classifier'}


def uniquify_str(str_: str, str_set: Collection[str]) ->str:
    """Uniquifies :attr:`str_` if :attr:`str_` is included in :attr:`str_set`.

    This is done by appending a number to :attr:`str_`. Returns
    :attr:`str_` directly if it is not included in :attr:`str_set`.

    Args:
        str\\_ (string): A string to uniquify.
        str_set (set, dict, or list): A collection of strings. The returned
            string is guaranteed to be different from the elements in the
            collection.

    Returns:
        The uniquified string. Returns :attr:`str_` directly if it is
        already unique.

    Example:

        .. code-block:: python

            print(uniquify_str('name', ['name', 'name_1']))
            # 'name_2'

    """
    if str_ not in str_set:
        return str_
    else:
        for i in range(1, len(str_set) + 1):
            unique_str = str_ + '_%d' % i
            if unique_str not in str_set:
                return unique_str
    raise ValueError('Failed to uniquify string: ' + str_)


class FeedForwardNetworkBase(ModuleBase):
    """Base class inherited by all feed-forward network classes.

    Args:
        hparams (dict, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    See :meth:`forward` for the inputs and outputs.
    """

    def __init__(self, hparams: Optional[Union[HParams, Dict[str, Any]]]=None):
        super().__init__(hparams)
        self._layers = nn.ModuleList()
        self._layer_names: List[str] = []
        self._layers_by_name: Dict[str, nn.Module] = {}
        self._layer_outputs: List[torch.Tensor] = []
        self._layer_outputs_by_name: Dict[str, torch.Tensor] = {}

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "name": "NN"
            }
        """
        return {'name': 'NN'}

    def __repr__(self) ->str:
        if len(list(self.modules())) == 1:
            return ModuleBase.__repr__(self._layers)
        return super().__repr__()

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """Feeds forward inputs through the network layers and returns outputs.

        Args:
            input: The inputs to the network. The requirements on inputs
                depends on the first layer and subsequent layers in the
                network.

        Returns:
            The output of the network.
        """
        outputs = input
        for layer in self._layers:
            outputs = layer(outputs)
        return outputs

    def append_layer(self, layer: Union[nn.Module, HParams, Dict[str, Any]]):
        """Appends a layer to the end of the network.

        Args:
            layer: A subclass of :torch_nn:`Module`, or a dict of layer
                hyperparameters.
        """
        layer_ = layer
        if not isinstance(layer_, nn.Module):
            layer_ = get_layer(hparams=layer_)
        self._layers.append(layer_)
        layer_name = uniquify_str(layer_.__class__.__name__, self._layer_names)
        self._layer_names.append(layer_name)
        self._layers_by_name[layer_name] = layer_

    def has_layer(self, layer_name: str) ->bool:
        """Returns `True` if the network with the name exists. Returns
        `False` otherwise.

        Args:
            layer_name (str): Name of the layer.
        """
        return layer_name in self._layers_by_name

    def layer_by_name(self, layer_name: str) ->Optional[nn.Module]:
        """Returns the layer with the name. Returns `None` if the layer name
        does not exist.

        Args:
            layer_name (str): Name of the layer.
        """
        return self._layers_by_name.get(layer_name, None)

    @property
    def layers_by_name(self) ->Dict[str, nn.Module]:
        """A dictionary mapping layer names to the layers.
        """
        return self._layers_by_name

    @property
    def layers(self) ->nn.ModuleList:
        """A list of the layers.
        """
        return self._layers

    @property
    def layer_names(self) ->List[str]:
        """A list of uniquified layer names.
        """
        return self._layer_names

    def _build_layers(self, layers: Optional[nn.ModuleList]=None, layer_hparams: Optional[List[Union[HParams, Dict[str, Any]]]]=None):
        """Builds layers.

        Either :attr:`layer_hparams` or :attr:`layers` must be
        provided. If both are given, :attr:`layers` will be used.

        Args:
            layers (optional): A list of layer instances supplied as an instance
                of :torch_nn:`ModuleList`.
            layer_hparams (optional): A list of layer hparams, each to which
                is fed to :func:`~texar.torch.core.layers.get_layer` to create
                the layer instance.
        """
        if layers is not None:
            self._layers = layers
        else:
            if layer_hparams is None:
                raise ValueError('Either `layer` or `layer_hparams` is required.')
            self._layers = nn.ModuleList()
            for _, hparams in enumerate(layer_hparams):
                self._layers.append(get_layer(hparams=hparams))
        for layer in self._layers:
            layer_name = uniquify_str(layer.__class__.__name__, self._layer_names)
            self._layer_names.append(layer_name)
            self._layers_by_name[layer_name] = layer


def _to_list(value: Union[Dict[str, Any], List, Tuple, int], name=None, list_length=None):
    """Converts `hparams` value into a list.

    If :attr:`list_length` is given, then the canonicalized :attr:`value`
    must be of length :attr:`list_length`.
    """
    if not isinstance(value, (list, tuple)):
        if list_length is not None:
            value = [value] * list_length
        else:
            value = [value]
    if list_length is not None and len(value) != list_length:
        name = '' if name is None else name
        raise ValueError("hparams '%s' must be a list of length %d" % (name, list_length))
    return value


_POOLING_TO_REDUCE = {'MaxPool1d': 'MaxReducePool1d', 'AvgPool1d': 'AvgReducePool1d', torch.nn.MaxPool1d: MaxReducePool1d, torch.nn.AvgPool1d: AvgReducePool1d}


def get_pooling_layer_hparams(hparams: Union[HParams, Dict[str, Any]]) ->Dict[str, Any]:
    """Creates pooling layer hyperparameters `dict` for :func:`get_layer`.

    If the :attr:`hparams` sets `'pool_size'` to `None`, the layer will be
    changed to the respective reduce-pooling layer. For example,
    :torch_docs:`torch.conv.MaxPool1d <nn.html#torch.nn.Conv1d>` is replaced
    with :class:`~texar.torch.core.MaxReducePool1d`.
    """
    if isinstance(hparams, HParams):
        hparams = hparams.todict()
    new_hparams = copy.copy(hparams)
    kwargs = new_hparams.get('kwargs', None)
    if kwargs and kwargs.get('kernel_size', None) is None:
        pool_type = hparams['type']
        new_hparams['type'] = _POOLING_TO_REDUCE.get(pool_type, pool_type)
        kwargs.pop('kernel_size', None)
        kwargs.pop('stride', None)
        kwargs.pop('padding', None)
    return new_hparams


def transpose_batch_time(inputs: torch.Tensor) ->torch.Tensor:
    """Transposes inputs between time-major and batch-major.

    Args:
        inputs: A Tensor of shape ``[batch_size, max_time, ...]`` (batch-major)
            or ``[max_time, batch_size, ...]`` (time-major), or a (possibly
            nested) tuple of such elements.

    Returns:
        A (possibly nested tuple of) Tensor with transposed batch and
        time dimensions of inputs.
    """
    return inputs.transpose(0, 1)


def mask_sequences(sequence: Union[torch.Tensor, List[int]], sequence_length: Union[torch.LongTensor, List[int]], dtype: Optional[torch.dtype]=None, time_major: bool=False) ->torch.Tensor:
    """Masks out sequence entries that are beyond the respective sequence
    lengths. Masks along the time dimension.

    :attr:`sequence` and :attr:`sequence_length` can either be python
    arrays or Tensors, respectively. If both are Python arrays (or None), the
    return will be a Python array as well.

    Args:
        sequence: A Tensor or Python array of sequence values.
            If ``time_major==False`` (default), this must be a Tensor of shape
            ``[batch_size, max_time, ...]``. The batch and time dimension is
            exchanged if ``time_major==True``.
        sequence_length: A Tensor or python array of shape ``[batch_size]``.
            Time steps beyond the respective sequence lengths will be
            made zero.
        dtype (dtype): Type of :attr:`sequence`. If `None`, infer from
            :attr:`sequence` automatically.
        time_major (bool): The shape format of the inputs. If `True`,
            :attr:`sequence` must have shape
            ``[max_time, batch_size, ...]``.
            If `False` (default), :attr:`sequence` must have
            shape ``[batch_size, max_time, ...]``.

    Returns:
        The masked sequence, i.e., a Tensor or python array of the same shape
        as :attr:`sequence` but with masked-out entries (set to zero).

        If both :attr:`sequence` and :attr:`sequence_length` are python
        arrays, the returned value is a python array as well.
    """
    if not torch.is_tensor(sequence):
        sequence = torch.tensor(sequence, dtype=dtype)
    sequence: torch.Tensor
    rank = sequence.dim()
    if rank < 2:
        raise ValueError('`sequence` must be 2D or higher order.')
    if time_major:
        sequence = transpose_batch_time(sequence)
    max_time = sequence.size(1)
    if dtype is None:
        dtype = sequence.dtype
    mask = utils.sequence_mask(sequence_length, max_time, dtype=dtype)
    mask = mask.view(*mask.size(), *([1] * (rank - 2)))
    sequence = sequence * mask
    if time_major:
        sequence = transpose_batch_time(sequence)
    return sequence


class Conv1DNetwork(FeedForwardNetworkBase):
    """Simple `Conv-1D` network which consists of a sequence of convolutional
    layers followed with a sequence of dense layers.

    Args:
        in_channels (int): Number of channels in the input tensor.
        in_features (int): Size of the feature dimension in the input tensor.
        hparams (dict, optional): Hyperparameters. Missing
            hyperparameter will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    See :meth:`forward` for the inputs and outputs. If :attr:`"data_format"` is
    set to ``"channels_first"`` (this is the default), inputs must be a tensor
    of shape `[batch_size, channels, length]`. If :attr:`"data_format"` is set
    to ``"channels_last"``, inputs must be a tensor of shape
    `[batch_size, length, channels]`. For example, for sequence classification,
    `length` corresponds to time steps, and `channels` corresponds to embedding
    dim.

    Example:

    .. code-block:: python

        nn = Conv1DNetwork(in_channels=20, in_features=256) # Use the default

        inputs = torch.randn([64, 20, 256])
        outputs = nn(inputs)
        # outputs == Tensor of shape [64, 256], because the final dense layer
        # has size 256.

    .. document private functions
    """

    def __init__(self, in_channels: int, in_features: Optional[int]=None, hparams=None):
        super().__init__(hparams=hparams)
        if self.hparams.num_dense_layers > 0 and in_features is None:
            raise ValueError('"in_features" cannot be None if "num_dense_layers" > 0')
        layer_hparams = self._build_non_dense_layer_hparams(in_channels=in_channels)
        self._build_layers(layers=None, layer_hparams=layer_hparams)
        if self.hparams.num_dense_layers > 0:
            if in_features is None:
                raise ValueError('"in_features" cannot be None if "num_dense_layers" > 0')
            ones = torch.ones(1, in_channels, in_features)
            input_size = self._infer_dense_layer_input_size(ones)
            layer_hparams = self._build_dense_hparams(in_features=input_size[1], layer_hparams=layer_hparams)
            self._build_layers(layers=None, layer_hparams=layer_hparams)

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Conv layers
                "num_conv_layers": 1,
                "out_channels": 128,
                "kernel_size": [3, 4, 5],
                "conv_activation": "ReLU",
                "conv_activation_kwargs": None,
                "other_conv_kwargs": {},
                "data_format": "channels_first",
                # (2) Pooling layers
                "pooling": "MaxPool1d",
                "pool_size": None,
                "pool_stride": 1,
                "other_pool_kwargs": {},
                # (3) Dense layers
                "num_dense_layers": 1,
                "out_features": 256,
                "dense_activation": None,
                "dense_activation_kwargs": None,
                "final_dense_activation": None,
                "final_dense_activation_kwargs": None,
                "other_dense_kwargs": None,
                # (4) Dropout
                "dropout_conv": [1],
                "dropout_dense": [],
                "dropout_rate": 0.75,
                # (5) Others
                "name": "conv1d_network"
            }

        Here:

        1. For **convolutional** layers:

           `"num_conv_layers"`: int
               Number of convolutional layers.

           `"out_channels"`: int or list
               The number of out_channels in the convolution, i.e., the
               dimensionality of the output space.

               - If ``"num_conv_layers"`` > 1 and ``"out_channels"`` is an int,
                 all convolution layers will have the same number of output
                 channels.

               - If ``"num_conv_layers"`` > 1 and ``"out_channels"`` is a list,
                 the length must equal ``"num_conv_layers"``. The number of
                 output channels of each convolution layer will be the
                 corresponding element from this list.

           `"kernel_size"`: int or list
               Lengths of 1D convolution windows.

               - If `"num_conv_layers"` = 1, this can also be a ``int`` list of
                 arbitrary length denoting differently sized convolution
                 windows. The number of output channels of each size is
                 specified by ``"out_channels"``.
                 For example, the default values will create 3 convolution
                 layers, each of which has kernel size of 3, 4, and 5,
                 respectively, and has output channel 128.

               - If `"num_conv_layers"` > 1, this must be a list of length
                 ``"num_conv_layers"``. Each element can be an ``int`` or a
                 ``int`` list of arbitrary length denoting the kernel size of
                 each layer.

           `"conv_activation"`: str or callable
               Activation applied to the output of the convolutional
               layers. Set to `None` to maintain a linear activation.
               See :func:`~texar.torch.core.get_layer` for more details.

           `"conv_activation_kwargs"`: dict, optional
               Keyword arguments for the activation following the convolutional
               layer. See :func:`~texar.torch.core.get_layer` for more details.

           `"other_conv_kwargs"`: list or dict, optional
               Other keyword arguments for :torch_nn:`Conv1d` constructor,
               e.g., ``padding``.

               - If a dict, the same dict is applied to all the convolution
                 layers.

               - If a list, the length must equal ``"num_conv_layers"``. This
                 list can contain nested lists. If the convolution layer at
                 index i has multiple kernel sizes, then the corresponding
                 element of this list can also be a list of length equal to
                 ``"kernel_size"`` at index i. If the element at index i is
                 instead a dict, then the same dict gets applied to all the
                 convolution layers at index i.

           `"data_format"`: str, optional
               Data format of the input tensor. Defaults to ``channels_first``
               denoting the first dimension to be the channel dimension. Set it
               to ``channels_last`` to treat last dimension as the channel
               dimension. This argument can also be passed in ``forward``
               function, in which case the value specified here will be
               ignored.

        2. For **pooling** layers:

           `"pooling"`: str or class or instance
               Pooling layer after each of the convolutional layer(s). Can be a
               pooling layer class, its name or module path, or a class
               instance.

           `"pool_size"`: int or list, optional
               Size of the pooling window. If an ``int``, all pooling layer
               will have the same pool size. If a list, the list length must
               equal ``"num_conv_layers"``. If `None` and the pooling type
               is either :torch_docs:`MaxPool1d <nn.html#maxpool1d>` or
               :torch_docs:`AvgPool1d <nn.html#avgpool1d>`, the pool size will
               be set to input size. That is, the output of the pooling layer
               is a single unit.

           `"pool_stride"`: int or list, optional
               Strides of the pooling operation. If an ``int``, all
               layers will have the same stride. If a list, the list length
               must equal ``"num_conv_layers"``.

           `"other_pool_kwargs"`: list or dict, optional
               Other keyword arguments for pooling layer class constructor.

               - If a dict, the same dict is applied to all the pooling layers.

               - If a list, the length must equal ``"num_conv_layers"``. The
                 pooling arguments for layer i will be the element at index i
                 from this list.

        3. For **dense** layers (note that here dense layers always follow
           convolutional and pooling layers):

           `"num_dense_layers"`: int
               Number of dense layers.

           `"out_features"`: int or list
               Dimension of features after the dense layers. If an
               ``int``, all dense layers will have the same feature dimension.
               If a list of ``int``, the list length must equal
               ``"num_dense_layers"``.

           `"dense_activation"`: str or callable
               Activation function applied to the output of the dense
               layers **except** the last dense layer output. Set to
               `None` to maintain a linear activation.

           `"dense_activation_kwargs"`: dict, optional
               Keyword arguments for dense layer activation functions before
               the last dense layer.

           `"final_dense_activation"`: str or callable
               Activation function applied to the output of the **last** dense
               layer. Set to `None` to maintain a linear activation.

           `"final_dense_activation_kwargs"`: dict, optional
               Keyword arguments for the activation function of last
               dense layer.

           `"other_dense_kwargs"`: dict, optional
               Other keyword arguments for dense layer class constructor.

        4. For **dropouts**:

           `"dropout_conv"`: int or list
               The indices of convolutional layers (starting from 0) whose
               **inputs** are applied with dropout.
               The index = :attr:`num_conv_layers` means dropout applies to the
               final convolutional layer output. For example,

               .. code-block:: python

                   {
                       "num_conv_layers": 2,
                       "dropout_conv": [0, 2]
                   }

               will leads to a series of layers as
               `-dropout-conv0-conv1-dropout-`.

               The dropout mode (training or not) is controlled
               by :attr:`self.training`.

           `"dropout_dense"`: int or list
               Same as ``"dropout_conv"`` but applied to dense layers (index
               starting from 0).

           `"dropout_rate"`: float
               The dropout rate, between 0 and 1. For example,
               ``"dropout_rate": 0.1`` would drop out 10% of elements.

        5. Others:

           `"name"`: str
               Name of the network.
        """
        return {'num_conv_layers': 1, 'out_channels': 128, 'kernel_size': [3, 4, 5], 'conv_activation': 'ReLU', 'conv_activation_kwargs': None, 'other_conv_kwargs': {}, 'data_format': 'channels_first', 'pooling': 'MaxPool1d', 'pool_size': None, 'pool_stride': 1, 'other_pool_kwargs': {}, 'num_dense_layers': 1, 'out_features': 256, 'dense_activation': None, 'dense_activation_kwargs': None, 'final_dense_activation': None, 'final_dense_activation_kwargs': None, 'other_dense_kwargs': None, 'dropout_conv': [1], 'dropout_dense': [], 'dropout_rate': 0.75, 'name': 'conv1d_network', '@no_typecheck': ['out_channels', 'kernel_size', 'conv_activation', 'other_conv_kwargs', 'pool_size', 'pool_stride', 'other_pool_kwargs', 'out_features', 'dense_activation', 'dropout_conv', 'dropout_dense']}

    def _build_pool_hparams(self):
        pool_type = self._hparams.pooling
        if pool_type == 'MaxPool':
            pool_type = 'MaxPool1d'
        elif pool_type == 'AvgPool':
            pool_type = 'AvgPool1d'
        npool = self._hparams.num_conv_layers
        kernel_size = _to_list(self._hparams.pool_size, 'pool_size', npool)
        stride = _to_list(self._hparams.pool_stride, 'pool_stride', npool)
        other_kwargs = self._hparams.other_pool_kwargs
        if isinstance(other_kwargs, HParams):
            other_kwargs = other_kwargs.todict()
            other_kwargs = _to_list(other_kwargs, 'other_kwargs', npool)
        elif isinstance(other_kwargs, (list, tuple)):
            if len(other_kwargs) != npool:
                raise ValueError("The length of hparams['other_pool_kwargs'] must equal 'num_conv_layers'")
        else:
            raise ValueError("hparams['other_pool_kwargs'] must be either a dict or list/tuple")
        pool_hparams = []
        for i in range(npool):
            kwargs_i = {'kernel_size': kernel_size[i], 'stride': stride[i]}
            kwargs_i.update(other_kwargs[i])
            pool_hparams_ = get_pooling_layer_hparams({'type': pool_type, 'kwargs': kwargs_i})
            pool_hparams.append(pool_hparams_)
        return pool_hparams

    def _build_conv1d_hparams(self, in_channels, pool_hparams):
        """Creates the hparams for each of the convolutional layers usable for
        :func:`texar.torch.core.layers.get_layer`.
        """
        nconv = self._hparams.num_conv_layers
        if len(pool_hparams) != nconv:
            raise ValueError('`pool_hparams` must be of length %d' % nconv)
        in_channels = [in_channels]
        out_channels = _to_list(self._hparams.out_channels, 'out_channels', nconv)
        in_channels.extend(out_channels[:-1])
        if nconv == 1:
            kernel_size = _to_list(self._hparams.kernel_size)
            if not isinstance(kernel_size[0], (list, tuple)):
                kernel_size = [kernel_size]
        elif nconv > 1:
            kernel_size = _to_list(self._hparams.kernel_size, 'kernel_size', nconv)
            kernel_size = [_to_list(ks) for ks in kernel_size]
        other_kwargs = self._hparams.other_conv_kwargs
        if isinstance(other_kwargs, HParams):
            other_kwargs = other_kwargs.todict()
            other_kwargs = _to_list(other_kwargs, 'other_conv_kwargs', nconv)
        elif isinstance(other_kwargs, (list, tuple)):
            if len(other_kwargs) != nconv:
                raise ValueError("The length of hparams['other_conv_kwargs'] must be equal to 'num_conv_layers'")
        else:
            raise ValueError("hparams['other_conv_kwargs'] must be a either a dict or a list.")

        def _activation_hparams(name, kwargs=None):
            if kwargs is not None:
                return {'type': name, 'kwargs': kwargs}
            else:
                return {'type': name, 'kwargs': {}}
        conv_pool_hparams = []
        for i in range(nconv):
            hparams_i = []
            names = []
            if isinstance(other_kwargs[i], dict):
                other_kwargs[i] = _to_list(other_kwargs[i], 'other_kwargs[i]', len(kernel_size[i]))
            elif isinstance(other_kwargs[i], (list, tuple)) and len(other_kwargs[i]) != len(kernel_size[i]):
                raise ValueError("The length of hparams['other_conv_kwargs'][i] must be equal to the length of hparams['kernel_size'][i]")
            for idx, ks_ij in enumerate(kernel_size[i]):
                name = uniquify_str('conv_%d' % (i + 1), names)
                names.append(name)
                conv_kwargs_ij = {'in_channels': in_channels[i], 'out_channels': out_channels[i], 'kernel_size': ks_ij}
                conv_kwargs_ij.update(other_kwargs[i][idx])
                hparams_i.append({'type': 'Conv1d', 'kwargs': conv_kwargs_ij})
            if len(hparams_i) == 1:
                if self._hparams.conv_activation:
                    layers = {'layers': [hparams_i[0], _activation_hparams(self._hparams.conv_activation, self._hparams.conv_activation_kwargs)]}
                    sequential_layer = {'type': 'Sequential', 'kwargs': layers}
                    conv_pool_hparams.append([sequential_layer, pool_hparams[i]])
                else:
                    conv_pool_hparams.append([hparams_i[0], pool_hparams[i]])
            else:
                mrg_kwargs_layers = []
                for hparams_ij in hparams_i:
                    if self._hparams.conv_activation:
                        seq_kwargs_j = {'layers': [hparams_ij, _activation_hparams(self._hparams.conv_activation, self._hparams.conv_activation_kwargs), pool_hparams[i]]}
                    else:
                        seq_kwargs_j = {'layers': [hparams_ij, pool_hparams[i]]}
                    mrg_kwargs_layers.append({'type': 'Sequential', 'kwargs': seq_kwargs_j})
                mrg_hparams = {'type': 'MergeLayer', 'kwargs': {'layers': mrg_kwargs_layers}}
                conv_pool_hparams.append(mrg_hparams)
        return conv_pool_hparams

    def _build_dense_hparams(self, in_features: int, layer_hparams):
        ndense = self._hparams.num_dense_layers
        in_features = [in_features]
        out_features = _to_list(self._hparams.out_features, 'out_features', ndense)
        in_features.extend(out_features[:-1])
        other_kwargs = self._hparams.other_dense_kwargs or {}
        if isinstance(other_kwargs, HParams):
            other_kwargs = other_kwargs.todict()
        if not isinstance(other_kwargs, dict):
            raise ValueError("hparams['other_dense_kwargs'] must be a dict.")

        def _activation_hparams(name, kwargs=None):
            if kwargs is not None:
                return {'type': name, 'kwargs': kwargs}
            else:
                return {'type': name, 'kwargs': {}}
        dense_hparams = []
        for i in range(ndense):
            kwargs_i = {'in_features': in_features[i], 'out_features': out_features[i]}
            kwargs_i.update(other_kwargs)
            dense_hparams_i = {'type': 'Linear', 'kwargs': kwargs_i}
            if i < ndense - 1 and self._hparams.dense_activation is not None:
                layers = {'layers': [dense_hparams_i, _activation_hparams(self._hparams.dense_activation, self._hparams.dense_activation_kwargs)]}
                sequential_layer = {'type': 'Sequential', 'kwargs': layers}
                dense_hparams.append(sequential_layer)
            elif i == ndense - 1 and self._hparams.final_dense_activation is not None:
                layers = {'layers': [dense_hparams_i, _activation_hparams(self._hparams.final_dense_activation, self._hparams.final_dense_activation_kwargs)]}
                sequential_layer = {'type': 'Sequential', 'kwargs': layers}
                dense_hparams.append(sequential_layer)
            else:
                dense_hparams.append(dense_hparams_i)

        def _dropout_hparams():
            return {'type': 'Dropout', 'kwargs': {'p': self._hparams.dropout_rate}}
        dropout_dense = _to_list(self._hparams.dropout_dense)
        ndense = self._hparams.num_dense_layers
        if ndense > 0:
            layer_hparams.append({'type': 'Flatten'})
        for dense_i in range(ndense):
            if dense_i in dropout_dense:
                layer_hparams.append(_dropout_hparams())
            layer_hparams.append(dense_hparams[dense_i])
        if ndense in dropout_dense:
            layer_hparams.append(_dropout_hparams())
        return layer_hparams

    def _build_non_dense_layer_hparams(self, in_channels):
        pool_hparams = self._build_pool_hparams()
        conv_pool_hparams = self._build_conv1d_hparams(in_channels, pool_hparams)

        def _dropout_hparams():
            return {'type': 'Dropout', 'kwargs': {'p': self._hparams.dropout_rate}}
        dropout_conv = _to_list(self._hparams.dropout_conv)
        layers_hparams = []
        nconv = self._hparams.num_conv_layers
        for conv_i in range(nconv):
            if conv_i in dropout_conv:
                layers_hparams.append(_dropout_hparams())
            if isinstance(conv_pool_hparams[conv_i], (list, tuple)):
                layers_hparams += conv_pool_hparams[conv_i]
            else:
                layers_hparams.append(conv_pool_hparams[conv_i])
        if nconv in dropout_conv:
            layers_hparams.append(_dropout_hparams())
        return layers_hparams

    def forward(self, input: torch.Tensor, sequence_length: Optional[Union[torch.LongTensor, List[int]]]=None, dtype: Optional[torch.dtype]=None, data_format: Optional[str]=None) ->torch.Tensor:
        """Feeds forward inputs through the network layers and returns outputs.

        Args:
            input: The inputs to the network, which is a 3D tensor.
            sequence_length (optional): An :tensor:`LongTensor` of shape
                ``[batch_size]`` or a python array containing the length of
                each element in :attr:`inputs`. If given, time steps beyond
                the length will first be masked out before feeding to the
                layers.
            dtype (optional): Type of the inputs. If not provided,
                infers from inputs automatically.
            data_format (optional): Data type of the input tensor. If
                ``channels_last``, the last dimension will be treated as channel
                dimension so the size of the :attr:`input` should be
                `[batch_size, X, channel]`. If ``channels_first``, first
                dimension will be treated as channel dimension so the size
                should be `[batch_size, channel, X]`. Defaults to None.
                If None, the value will be picked from hyperparameters.
        Returns:
            The output of the final layer.
        """
        if input.dim() != 3:
            raise ValueError("'input' should be a 3D tensor.")
        if data_format is None:
            data_format = self.hparams['data_format']
        if data_format == 'channels_first':
            input = input.permute(0, 2, 1)
            if sequence_length is not None:
                input = mask_sequences(input, sequence_length, dtype=dtype, time_major=False)
            input = input.permute(0, 2, 1)
            output = super().forward(input)
        elif data_format == 'channels_last':
            if sequence_length is not None:
                input = mask_sequences(input, sequence_length, dtype=dtype, time_major=False)
            input = input.permute(0, 2, 1)
            output = super().forward(input)
            if output.dim() == 3:
                output = output.permute(0, 2, 1)
        else:
            raise ValueError("Invalid 'data_format'")
        return output

    def _infer_dense_layer_input_size(self, input: torch.Tensor) ->torch.Size:
        with torch.no_grad():
            output = super().forward(input)
        return output.view(output.size()[0], -1).size()

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output.
        """
        if self.hparams.num_dense_layers <= 0:
            out_channels = self._hparams.out_channels
            if not isinstance(out_channels, (list, tuple)):
                out_channels = [out_channels]
            nconv = self._hparams.num_conv_layers
            if nconv == 1:
                kernel_size = _to_list(self._hparams.kernel_size)
                if not isinstance(kernel_size[0], (list, tuple)):
                    kernel_size = [kernel_size]
            elif nconv > 1:
                kernel_size = _to_list(self._hparams.kernel_size, 'kernel_size', nconv)
                kernel_size = [_to_list(ks) for ks in kernel_size]
            return out_channels[-1] * len(kernel_size[-1])
        else:
            out_features = self._hparams.out_features
            if isinstance(out_features, (list, tuple)):
                return out_features[-1]
            else:
                return out_features


class EncoderBase(ModuleBase, ABC):
    """Base class inherited by all encoder classes.
    """

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.
        """
        return {'name': 'encoder'}


class Conv1DEncoder(Conv1DNetwork, EncoderBase):
    """Simple `Conv-1D` encoder which consists of a sequence of convolutional
    layers followed with a sequence of dense layers.

    Wraps :class:`~texar.torch.modules.Conv1DNetwork` to be a subclass of
    :class:`~texar.torch.modules.EncoderBase`. Has exact the same functionality
    with :class:`~texar.torch.modules.Conv1DNetwork`.
    """

    def __init__(self, in_channels: int, in_features: Optional[int]=None, hparams=None):
        super().__init__(in_channels=in_channels, in_features=in_features, hparams=hparams)

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.

        The same as :meth:`~texar.torch.modules.Conv1DNetwork.default_hparams`
        of :class:`~texar.torch.modules.Conv1DNetwork`, except that the default
        name is ``"conv_encoder"``.
        """
        hparams = Conv1DNetwork.default_hparams()
        hparams['name'] = 'conv_encoder'
        return hparams


class Conv1DClassifier(ClassifierBase):
    """Simple `Conv-1D` classifier.
    This is a combination of the :class:`~texar.torch.modules.Conv1DEncoder`
    with a classification layer.

    Args:
        in_channels (int): Number of channels in the input tensor.
        in_features (int): Size of the feature dimension in the input tensor.
        hparams (dict, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    See :meth:`forward` for the inputs and outputs. If :attr:`"data_format"` is
    set to ``"channels_first"`` (this is the default), inputs must be a tensor
    of shape `[batch_size, channels, length]`. If :attr:`"data_format"` is set
    to ``"channels_last"``, inputs must be a tensor of shape
    `[batch_size, length, channels]`. For example, for sequence classification,
    `length` corresponds to time steps, and `channels` corresponds to embedding
    dim.

    Example:

    .. code-block:: python

        inputs = torch.randn([64, 20, 256])

        clas = Conv1DClassifier(in_channels=20, in_features=256,
                                hparams={'num_classes': 10})

        logits, pred = clas(inputs)
        # logits == Tensor of shape [64, 10]
        # pred   == Tensor of shape [64]

    .. document private functions
    """

    def __init__(self, in_channels: int, in_features: Optional[int]=None, hparams: Optional[Union[HParams, Dict[str, Any]]]=None):
        super().__init__(hparams=hparams)
        encoder_hparams = utils.dict_fetch(hparams, Conv1DEncoder.default_hparams())
        self._encoder = Conv1DEncoder(in_channels=in_channels, in_features=in_features, hparams=encoder_hparams)
        self._num_classes = self._hparams.num_classes
        if self._num_classes > 0:
            if self._hparams.num_dense_layers <= 0:
                if in_features is None:
                    raise ValueError("'in_features' is required for logits layer when 'num_dense_layers' <= 0")
                self._encoder.append_layer({'type': 'Flatten'})
                ones = torch.ones(1, in_channels, in_features)
                input_size = self._encoder._infer_dense_layer_input_size(ones)
                self.hparams.logit_layer_kwargs.in_features = input_size[1]
            logit_kwargs = self._hparams.logit_layer_kwargs
            if logit_kwargs is None:
                logit_kwargs = {}
            elif not isinstance(logit_kwargs, HParams):
                raise ValueError("hparams['logit_layer_kwargs'] must be a dict.")
            else:
                logit_kwargs = logit_kwargs.todict()
            logit_kwargs.update({'out_features': self._num_classes})
            self._encoder.append_layer({'type': 'Linear', 'kwargs': logit_kwargs})

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in Conv1DEncoder
                ...

                # (2) Additional hyperparameters
                "num_classes": 2,
                "logit_layer_kwargs": {
                    "use_bias": False
                },
                "name": "conv1d_classifier"
            }

        Here:

        1. Same hyperparameters as in
           :class:`~texar.torch.modules.Conv1DEncoder`.
           See the :meth:`~texar.torch.modules.Conv1DEncoder.default_hparams`.
           An instance of :class:`~texar.torch.modules.Conv1DEncoder` is created
           for feature extraction.

        2. Additional hyperparameters:

           `"num_classes"`: int
               Number of classes:

               - If `> 0`, an additional :torch_nn:`Linear`
                 layer is appended to the encoder to compute the logits over
                 classes.

               - If `<= 0`, no dense layer is appended. The number of
                 classes is assumed to be equal to ``out_features`` of the
                 final dense layer size of the encoder.

           `"logit_layer_kwargs"`: dict
               Keyword arguments for the logit :torch_nn:`Linear` layer
               constructor, except for argument ``out_features`` which is set
               to ``"num_classes"``. Ignored if no extra logit layer is
               appended.

           `"name"`: str
               Name of the classifier.
        """
        hparams = Conv1DEncoder.default_hparams()
        hparams.update({'name': 'conv1d_classifier', 'num_classes': 2, 'logit_layer_kwargs': {'in_features': hparams['out_features'], 'bias': True}})
        return hparams

    def forward(self, input: torch.Tensor, sequence_length: Optional[Union[torch.LongTensor, List[int]]]=None, dtype: Optional[torch.dtype]=None, data_format: Optional[str]=None) ->Tuple[torch.Tensor, torch.Tensor]:
        """Feeds the inputs through the network and makes classification.

        The arguments are the same as in
        :class:`~texar.torch.modules.Conv1DEncoder`.

        The predictions of binary classification (``num_classes`` =1) and
        multi-way classification (``num_classes`` >1) are different, as
        explained below.

        Args:
            input: The inputs to the network, which is a 3D tensor. See
                :class:`~texar.torch.modules.Conv1DEncoder` for more details.
            sequence_length (optional): An int tensor of shape `[batch_size]` or
                a python array containing the length of each element in
                :attr:`inputs`. If given, time steps beyond the length will
                first be masked out before feeding to the layers.
            dtype (optional): Type of the inputs. If not provided, infers
                from inputs automatically.
            data_format (optional): Data type of the input tensor. If
                ``channels_last``, the last dimension will be treated as channel
                dimension so the size of the :attr:`input` should be
                `[batch_size, X, channel]`. If ``channels_first``, first
                dimension will be treated as channel dimension so the size
                should be `[batch_size, channel, X]`. Defaults to None.
                If None, the value will be picked from hyperparameters.

        Returns:
            A tuple ``(logits, pred)``, where

            - ``logits`` is a :tensor:`Tensor` of shape
              ``[batch_size, num_classes]`` for ``num_classes`` >1, and
              ``[batch_size]`` for ``num_classes`` =1 (i.e., binary
              classification).
            - ``pred`` is the prediction, a :tensor:`LongTensor` of shape
              ``[batch_size]``. For binary classification, the standard
              sigmoid function is used for prediction, and the class labels are
              ``{0, 1}``.
        """
        logits = self._encoder(input, sequence_length=sequence_length, dtype=dtype, data_format=data_format)
        num_classes = self._hparams.num_classes
        is_binary = num_classes == 1
        is_binary = is_binary or num_classes <= 0 and logits.shape[1] == 1
        if is_binary:
            pred = logits > 0
            logits = logits.view(-1)
        else:
            pred = torch.argmax(logits, dim=1)
        pred = pred.view(-1).long()
        return logits, pred

    @property
    def num_classes(self) ->int:
        """The number of classes.
        """
        return self._num_classes

    @property
    def encoder(self) ->nn.Module:
        """The classifier neural network.
        """
        return self._encoder

    def has_layer(self, layer_name) ->bool:
        """Returns `True` if the network with the name exists. Returns
        `False` otherwise.

        Args:
            layer_name (str): Name of the layer.
        """
        return self._encoder.has_layer(layer_name)

    def layer_by_name(self, layer_name) ->Optional[nn.Module]:
        """Returns the layer with the name. Returns `None` if the layer name
        does not exist.

        Args:
            layer_name (str): Name of the layer.
        """
        return self._encoder.layer_by_name(layer_name)

    @property
    def layers_by_name(self) ->Dict[str, nn.Module]:
        """A dictionary mapping layer names to the layers.
        """
        return self._encoder.layers_by_name

    @property
    def layers(self) ->nn.ModuleList:
        """A list of the layers.
        """
        return self._encoder.layers

    @property
    def layer_names(self) ->List[str]:
        """A list of uniquified layer names.
        """
        return self._encoder.layer_names

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output :attr:`logits`.
        If :attr:`logits` size is only determined by input
        (i.e. if ``num_classes`` == 1), the feature size is equal
        to ``-1``. Otherwise, if ``num_classes`` > 1, it is equal
        to ``num_classes``.
        """
        if self._hparams.num_classes > 1:
            logit_dim = self._hparams.num_classes
        elif self._hparams.num_classes == 1:
            logit_dim = -1
        else:
            raise AttributeError("'Conv1DClassifier' object hasno attribute 'output_size'if 'self._hparams.num_classes' < 1.")
        return logit_dim


class EmbedderBase(ModuleBase):
    """The base embedder class that all embedder classes inherit.

    Args:
        num_embeds (int, optional): The number of embedding elements, e.g.,
            the vocabulary size of a word embedder.
        init_value (Tensor or numpy array, optional): Initial values of the
            embedding variable. If not given, embedding is initialized as
            specified in ``hparams["initializer"]``.
        hparams (dict or HParams, optional): Embedder hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.
    """

    def __init__(self, num_embeds: Optional[int]=None, init_value: Optional[torch.Tensor]=None, hparams=None):
        super().__init__(hparams=hparams)
        if num_embeds is not None or init_value is not None:
            self._embedding = nn.Parameter(embedder_utils.get_embedding(num_embeds, init_value, hparams), requires_grad=self.hparams['trainable'])
            self._num_embeds = self._embedding.size(0)
            self._dim_rank = self._embedding.dim() - 1
            if self._dim_rank == 1:
                self._dim = self._embedding.size(1)
            else:
                self._dim = self._embedding.size()[1:]

    def _get_noise_shape(self, dropout_strategy: str, ids_rank: Optional[int]=None, dropout_input: Optional[torch.Tensor]=None) ->Optional[Tuple[int, ...]]:
        if dropout_strategy == 'element':
            noise_shape = None
        elif dropout_strategy == 'item':
            assert dropout_input is not None
            assert ids_rank is not None
            shape_a = dropout_input.size()[:ids_rank]
            shape_b = (1,) * self._dim_rank
            noise_shape = shape_a + shape_b
        elif dropout_strategy == 'item_type':
            noise_shape = (self._num_embeds,) + (1,) * self._dim_rank
        else:
            raise ValueError(f'Unknown dropout strategy: {dropout_strategy}')
        return noise_shape

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "name": "embedder"
            }
        """
        return {'name': 'embedder'}

    @property
    def num_embeds(self) ->int:
        """The number of embedding elements.
        """
        return self._num_embeds


class EmbeddingDropout(ModuleBase):
    """The dropout layer that used for the embedding.

    Args:
        rate (float, required): The dropout rate applied to the embedding.
            For example, if rate is 0.1, 10% of the embedding will be zeroed
            out. Set to 0 to disable dropout.

        hparams (dict or HParams, optional): Embedder hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.
    """

    def __init__(self, rate: float, hparams=None):
        super().__init__(hparams=hparams)
        self._rate = rate

    def forward(self, input_tensor: torch.Tensor, noise_shape: Optional[torch.Size]=None) ->torch.Tensor:
        """Apply dropout on the tensor.

        Args:
            input_tensor: The tensor to apply dropout on.
            noise_shape (list, optional): The shape of the noise mask which
                specifies the dropout dimensions for the embedding.

        Returns:
            The tensor after applying dropout.
        """
        if not self.training or self._rate == 0.0:
            return input_tensor
        if noise_shape is None:
            noise_shape = input_tensor.size()
        keep_rate = 1 - self._rate
        mask = input_tensor.new_full(noise_shape, keep_rate)
        mask += input_tensor.new_empty(noise_shape).uniform_(0, 1)
        mask = torch.floor(mask).div_(keep_rate)
        return input_tensor * mask

    @property
    def output_size(self):
        raise ValueError("'output_size' can not be calculated because it is equal to the input size.")


class PositionEmbedder(EmbedderBase):
    """Simple position embedder that maps position indexes into embeddings
    via lookup.

    Either :attr:`init_value` or :attr:`position_size` is required. If both are
    given, there must be ``init_value.shape[0]==position_size``.

    Args:
        init_value (optional): A Tensor or numpy array that contains the
            initial value of embeddings. It is typically of shape
            ``[position_size, embedding dim]``.

            If `None`, embedding is initialized as specified in
            ``hparams["initializer"]``. Otherwise, the
            ``"initializer"`` and ``"dim"``
            hyperparameters in :attr:`hparams` are ignored.
        position_size (int, optional): The number of possible positions, e.g.,
            the maximum sequence length. Required if :attr:`init_value` is
            not given.
        hparams (dict, optional): Embedder hyperparameters. If it is not
            specified, the default hyperparameter setting is used. See
            :attr:`default_hparams` for the structure and default values.


    .. document private functions
    """

    def __init__(self, position_size: Optional[int]=None, init_value: Optional[torch.Tensor]=None, hparams=None):
        if init_value is None and position_size is None:
            raise ValueError('Either `init_value` or `position_size` is required.')
        super().__init__(position_size, init_value, hparams=hparams)
        self._position_size = position_size
        if position_size is None:
            self._position_size = self._num_embeds
        if self._position_size != self._num_embeds:
            raise ValueError(f'position_size must be equal to init_value.shape[0]. Got {self._position_size} and {self._num_embeds}')
        self._built = True
        self._dropout_layer = EmbeddingDropout(self._hparams.dropout_rate)

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "dim": 100,
                "initializer": {
                    "type": "random_uniform_initializer",
                    "kwargs": {
                        "minval": -0.1,
                        "maxval": 0.1,
                        "seed": None
                    }
                },
                "dropout_rate": 0,
                "dropout_strategy": 'element',
                "trainable": True,
                "name": "position_embedder"
            }

        The hyperparameters have the same meaning as those in
        :meth:`texar.torch.modules.WordEmbedder.default_hparams`.
        """
        hparams = embedder_utils.default_embedding_hparams()
        hparams['name'] = 'position_embedder'
        return hparams

    def extra_repr(self) ->str:
        return f'position_size={self.position_size}, embedding_dim={self.dim}'

    def forward(self, positions: Optional[torch.LongTensor]=None, sequence_length: Optional[torch.LongTensor]=None, **kwargs):
        """Embeds the positions.

        Either :attr:`positions` or :attr:`sequence_length` is required:

            - If both are given, :attr:`sequence_length` is used to mask out
              embeddings of those time steps beyond the respective sequence
              lengths.
            - If only :attr:`sequence_length` is given, then positions
              from 0 to ``sequence_length - 1`` are embedded.

        Args:
            positions (optional): A :tensor:`LongTensor` containing the position
                IDs to embed.
            sequence_length (optional): An :tensor:`LongTensor` of shape
                ``[batch_size]``. Time steps beyond the respective sequence
                lengths will have zero-valued embeddings.
            kwargs: Additional keyword arguments for
                :torch_nn:`functional.embedding` besides
                :attr:`params` and :attr:`ids`.

        Returns:
            A `Tensor` of shape `shape(inputs) + embedding dimension`.
        """
        if positions is None:
            if sequence_length is None:
                raise ValueError('Either `positions` or `sequence_length` is required.')
            max_length = torch.max(sequence_length)
            single_inputs = torch.arange(start=0, end=max_length)
            inputs = single_inputs.unsqueeze(0)
            inputs = inputs.expand(len(sequence_length), -1).contiguous()
        else:
            inputs = positions
        ids_rank = inputs.dim()
        embedding = self._embedding
        inputs = inputs
        st = self._hparams.dropout_strategy
        if st == 'item_type':
            noise_shape = self._get_noise_shape(dropout_strategy=st, dropout_input=embedding)
            embedding = self._dropout_layer(embedding, noise_shape)
        outputs = torch.nn.functional.embedding(inputs.type(torch.long), embedding, **kwargs)
        if st != 'item_type':
            noise_shape = self._get_noise_shape(dropout_strategy=st, dropout_input=outputs, ids_rank=ids_rank)
            outputs = self._dropout_layer(outputs, noise_shape)
        if sequence_length is not None:
            outputs = mask_sequences(outputs, sequence_length)
        return outputs

    @property
    def embedding(self):
        """The embedding tensor.
        """
        return self._embedding

    @property
    def dim(self):
        """The embedding dimension.
        """
        return self._dim

    @property
    def position_size(self):
        """The position size, i.e., maximum number of positions.
        """
        return self._position_size

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output. If the :attr:`dim`
        hyperparameter is a ``list`` or ``tuple``, the feature size
        equals its final dimension; otherwise, if :attr:`dim` is an
        ``int``, the feature size equals :attr:`dim`.
        """
        if isinstance(self._dim, (list, tuple)):
            return self._dim[-1]
        else:
            return self._dim


T = TypeVar('T')


MaybeList = Union[T, List[T]]


def _extract_google_drive_file_id(url: str) ->str:
    url_suffix = url[url.find('/d/') + 3:]
    if url_suffix.find('/') == -1:
        return url_suffix
    file_id = url_suffix[:url_suffix.find('/')]
    return file_id


def get_filename(url: str) ->str:
    """Extracts the filename of the downloaded checkpoint file from the URL.
    """
    if 'drive.google.com' in url:
        return _extract_google_drive_file_id(url)
    url, filename = os.path.split(url)
    return filename or os.path.basename(url)


def _download(url: str, filename: str, path: str) ->str:

    def _progress_hook(count, block_size, total_size):
        percent = float(count * block_size) / float(total_size) * 100.0
        sys.stdout.write(f'\r>> Downloading {filename} {percent:.1f}%')
        sys.stdout.flush()
    filepath = os.path.join(path, filename)
    filepath, _ = urllib.request.urlretrieve(url, filepath, _progress_hook)
    None
    statinfo = os.stat(filepath)
    None
    return filepath


def maybe_download(urls, path, filenames=None, extract=False):
    """Downloads a set of files.

    Args:
        urls: A (list of) URLs to download files.
        path (str): The destination path to save the files.
        filenames: A (list of) strings of the file names. If given,
            must have the same length with :attr:`urls`. If `None`,
            filenames are extracted from :attr:`urls`.
        extract (bool): Whether to extract compressed files.

    Returns:
        A list of paths to the downloaded files.
    """
    utils_io.maybe_create_dir(path)
    if not isinstance(urls, (list, tuple)):
        is_list = False
        urls = [urls]
    else:
        is_list = True
    if filenames is not None:
        if not isinstance(filenames, (list, tuple)):
            filenames = [filenames]
        if len(urls) != len(filenames):
            raise ValueError('`filenames` must have the same number of elements as `urls`.')
    result = []
    for i, url in enumerate(urls):
        if filenames is not None:
            filename = filenames[i]
        elif 'drive.google.com' in url:
            filename = _extract_google_drive_file_id(url)
        else:
            filename = url.split('/')[-1]
            if filename.endswith('?raw=true'):
                filename = filename[:-9]
        filepath = os.path.join(path, filename)
        result.append(filepath)
        if not os.path.exists(filepath):
            if 'drive.google.com' in url:
                filepath = _download_from_google_drive(url, filename, path)
            else:
                filepath = _download(url, filename, path)
            if extract:
                logging.info('Extract %s', filepath)
                if tarfile.is_tarfile(filepath):
                    tarfile.open(filepath, 'r').extractall(path)
                elif zipfile.is_zipfile(filepath):
                    with zipfile.ZipFile(filepath) as zfile:
                        zfile.extractall(path)
                else:
                    logging.info('Unknown compression type. Only .tar.gz.tar.bz2, .tar, and .zip are supported')
    if not is_list:
        return result[0]
    return result


class PretrainedMixin(ModuleBase, ABC):
    """A mixin class for all pre-trained classes to inherit.
    """
    _MODEL_NAME: str
    _MODEL2URL: Dict[str, MaybeList[str]]
    pretrained_model_dir: Optional[str]

    @classmethod
    def available_checkpoints(cls) ->List[str]:
        return list(cls._MODEL2URL.keys())

    def _name_to_variable(self, name: str) ->nn.Parameter:
        """Find the corresponding variable given the specified name.
        """
        pointer = self
        for m_name in name.split('.'):
            if m_name.isdigit():
                num = int(m_name)
                pointer = pointer[num]
            else:
                pointer = getattr(pointer, m_name)
        return pointer

    def load_pretrained_config(self, pretrained_model_name: Optional[str]=None, cache_dir: Optional[str]=None, hparams=None):
        """Load paths and configurations of the pre-trained model.

        Args:
            pretrained_model_name (optional): A str with the name
                of a pre-trained model to load. If `None`, will use the model
                name in :attr:`hparams`.
            cache_dir (optional): The path to a folder in which the
                pre-trained models will be cached. If `None` (default),
                a default directory will be used.
            hparams (dict or HParams, optional): Hyperparameters. Missing
                hyperparameter will be set to default values. See
                :meth:`default_hparams` for the hyperparameter structure
                and default values.
        """
        if not hasattr(self, '_hparams'):
            self._hparams = HParams(hparams, self.default_hparams())
        elif hparams is not None:
            raise ValueError('`self._hparams` is already assigned, but `hparams` argument is not None.')
        self.pretrained_model_dir = None
        self.pretrained_model_name = pretrained_model_name
        if self.pretrained_model_name is None:
            self.pretrained_model_name = self._hparams.pretrained_model_name
        if self.pretrained_model_name is not None:
            self.pretrained_model_dir = self.download_checkpoint(self.pretrained_model_name, cache_dir)
            pretrained_model_hparams = self._transform_config(self.pretrained_model_name, self.pretrained_model_dir)
            self._hparams = HParams(pretrained_model_hparams, self._hparams.todict())

    def init_pretrained_weights(self, *args, **kwargs):
        if self.pretrained_model_dir:
            self._init_from_checkpoint(self.pretrained_model_name, self.pretrained_model_dir, *args, **kwargs)
        else:
            self.reset_parameters()

    def reset_parameters(self):
        """Initialize parameters of the pre-trained model. This method is only
        called if pre-trained checkpoints are not loaded.
        """
        pass

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "pretrained_model_name": None,
                "name": "pretrained_base"
            }
        """
        return {'pretrained_model_name': None, 'name': 'pretrained_base', '@no_typecheck': ['pretrained_model_name']}

    @classmethod
    def download_checkpoint(cls, pretrained_model_name: str, cache_dir: Optional[str]=None) ->str:
        """Download the specified pre-trained checkpoint, and return the
        directory in which the checkpoint is cached.

        Args:
            pretrained_model_name (str): Name of the model checkpoint.
            cache_dir (str, optional): Path to the cache directory. If `None`,
                uses the default directory (user's home directory).

        Returns:
            Path to the cache directory.
        """
        if pretrained_model_name in cls._MODEL2URL:
            download_path = cls._MODEL2URL[pretrained_model_name]
        else:
            raise ValueError(f'Pre-trained model not found: {pretrained_model_name}')
        if cache_dir is None:
            cache_path = default_download_dir(cls._MODEL_NAME)
        else:
            cache_path = Path(cache_dir)
        cache_path = cache_path / pretrained_model_name
        if not cache_path.exists():
            if isinstance(download_path, str):
                filename = get_filename(download_path)
                maybe_download(download_path, cache_path, extract=True)
                (cache_path / filename).unlink()
                folder = None
                for file in cache_path.iterdir():
                    if file.is_dir():
                        folder = file
                if folder is not None:
                    for file in folder.iterdir():
                        file.rename(file.parents[1] / file.name)
                    folder.rmdir()
            else:
                for path in download_path:
                    maybe_download(path, cache_path)
            None
        else:
            None
        return str(cache_path)

    @classmethod
    @abstractmethod
    def _transform_config(cls, pretrained_model_name: str, cache_dir: str) ->Dict[str, Any]:
        """Load the official configuration file and transform it into
        Texar-style hyperparameters.

        Args:
            pretrained_model_name (str): Name of the pre-trained model.
            cache_dir (str): Path to the cache directory.

        Returns:
            dict: Texar module hyperparameters.
        """
        raise NotImplementedError

    @abstractmethod
    def _init_from_checkpoint(self, pretrained_model_name: str, cache_dir: str, **kwargs):
        """Initialize model parameters from weights stored in the pre-trained
        checkpoint.

        Args:
            pretrained_model_name (str): Name of the pre-trained model.
            cache_dir (str): Path to the cache directory.
            **kwargs: Additional arguments for specific models.
        """
        raise NotImplementedError


_CHECKPOINT_FILES = ['checkpoint', 'encoder.json', 'hparams.json', 'vocab.bpe', 'model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta']


_GPT2_PATH = 'https://storage.googleapis.com/gpt-2/models/'


class PretrainedGPT2Mixin(PretrainedMixin, ABC):
    """A mixin class to support loading pre-trained checkpoints for modules
    that implement the GPT2 model.

    The GPT2 model was proposed in
    `Language Models are Unsupervised Multitask Learners`_
    by `Radford et al.` from OpenAI. It is a unidirectional Transformer model
    pre-trained using the vanilla language modeling objective on a large corpus.

    The available GPT2 models are as follows:

      * ``gpt2-small``: Small version of GPT-2, 124M parameters.
      * ``gpt2-medium``: Medium version of GPT-2, 355M parameters.
      * ``gpt2-large``: Large version of GPT-2, 774M parameters.
      * ``gpt2-xl``: XL version of GPT-2, 1558M parameters.

    We provide the following GPT2 classes:

      * :class:`~texar.torch.modules.GPT2Encoder` for text encoding.
      * :class:`~texar.torch.modules.GPT2Decoder` for text generation and
        decoding.
      * :class:`~texar.torch.modules.GPT2Classifier` for text classification and
        sequence tagging.

    .. _`Language Models are Unsupervised Multitask Learners`:
        https://openai.com/blog/better-language-models/
    """
    _MODEL_NAME = 'GPT2'
    _MODEL2URL = {'gpt2-small': [(_GPT2_PATH + f'124M/{file}') for file in _CHECKPOINT_FILES], 'gpt2-medium': [(_GPT2_PATH + f'355M/{file}') for file in _CHECKPOINT_FILES], 'gpt2-large': [(_GPT2_PATH + f'774M/{file}') for file in _CHECKPOINT_FILES], 'gpt2-xl': [(_GPT2_PATH + f'1558M/{file}') for file in _CHECKPOINT_FILES]}
    _IS_DECODE = False


    class MyDict(dict):

        def __contains__(self, key):
            if key == '117M':
                warnings.warn("Pre-trained model name '117M' is deprecated, use 'gpt2-small' instead.", UserWarning)
                return True
            elif key == '345M':
                warnings.warn("Pre-trained model name '345M' is deprecated, use 'gpt2-medium' instead.", UserWarning)
                return True
            else:
                return super().__contains__(key)
    _DEPRECATED_MODEL2URL = {'117M': [(_GPT2_PATH + f'124M/{file}') for file in _CHECKPOINT_FILES], '345M': [(_GPT2_PATH + f'355M/{file}') for file in _CHECKPOINT_FILES]}
    _MODEL2URL.update(_DEPRECATED_MODEL2URL)
    _MODEL2URL = MyDict(_MODEL2URL)

    @classmethod
    def _transform_config(cls, pretrained_model_name: str, cache_dir: str) ->Dict[str, Any]:
        info = list(os.walk(cache_dir))
        root, _, files = info[0]
        config_path = None
        for file in files:
            if file.endswith('hparams.json'):
                config_path = os.path.join(root, file)
        if config_path is None:
            raise ValueError(f'Cannot find the config file in {cache_dir}')
        with open(config_path) as f:
            config_gpt = json.loads(f.read())
        hidden_dim = config_gpt['n_embd']
        configs = {'vocab_size': config_gpt['n_vocab'], 'context_size': config_gpt['n_ctx'], 'embedding_size': config_gpt['n_embd'], 'embed': {'dim': hidden_dim}, 'position_size': config_gpt['n_ctx'], 'position_embed': {'dim': hidden_dim}}
        module_name = 'decoder' if cls._IS_DECODE else 'encoder'
        eps = 1e-05 if cls._IS_DECODE else 1e-06
        configs.update({module_name: {'dim': hidden_dim, 'num_blocks': config_gpt['n_layer'], 'embedding_dropout': 0, 'residual_dropout': 0, 'multihead_attention': {'use_bias': True, 'num_units': hidden_dim, 'num_heads': config_gpt['n_head'], 'output_dim': hidden_dim}, 'initializer': {'type': 'variance_scaling_initializer', 'kwargs': {'factor': 1.0, 'mode': 'FAN_AVG', 'uniform': True}}, 'eps': eps, 'poswise_feedforward': {'layers': [{'type': 'Linear', 'kwargs': {'in_features': hidden_dim, 'out_features': hidden_dim * 4, 'bias': True}}, {'type': 'GPTGELU', 'kwargs': {}}, {'type': 'Linear', 'kwargs': {'in_features': hidden_dim * 4, 'out_features': hidden_dim, 'bias': True}}], 'name': 'ffn'}}})
        if not cls._IS_DECODE:
            configs[module_name].update({'use_bert_config': False})
        return configs

    def _init_from_checkpoint(self, pretrained_model_name: str, cache_dir: str, load_output_layer: bool=True, **kwargs):
        """Initialize model parameters from weights stored in the pre-trained
        checkpoint.

        Args:
            pretrained_model_name (str): Name of the pre-trained model.
            cache_dir (str): Path to the cache directory.
            load_output_layer (bool): If `False`, will not load weights of the
                output layer. Set this argument to `False` when loading weights
                into a GPT2 encoder. Defaults to `True`.
        """
        try:
            import numpy as np
            import tensorflow as tf
        except ImportError:
            None
            raise
        module_name = 'decoder' if self._IS_DECODE else 'encoder'
        global_tensor_map = {'model/wte': 'word_embedder.embedding', 'model/wpe': 'position_embedder.embedding', 'model/ln_f/b': module_name + '.final_layer_norm.bias', 'model/ln_f/g': module_name + '.final_layer_norm.weight'}
        layer_tensor_map = {'ln_1/b': module_name + '.self_attn_layer_norm.{}.bias', 'ln_1/g': module_name + '.self_attn_layer_norm.{}.weight', 'ln_2/b': module_name + '.poswise_layer_norm.{}.bias', 'ln_2/g': module_name + '.poswise_layer_norm.{}.weight', 'mlp/c_fc/b': module_name + '.poswise_networks.{}._layers.0.bias', 'mlp/c_proj/b': module_name + '.poswise_networks.{}._layers.2.bias', 'attn/c_proj/b': module_name + '.self_attns.{}.O_dense.bias'}
        layer_transpose_map = {'mlp/c_fc/w': module_name + '.poswise_networks.{}._layers.0.weight', 'mlp/c_proj/w': module_name + '.poswise_networks.{}._layers.2.weight', 'attn/c_proj/w': module_name + '.self_attns.{}.O_dense.weight'}
        tf_path = os.path.abspath(os.path.join(cache_dir, 'model.ckpt'))
        init_vars = tf.train.list_variables(tf_path)
        names = []
        arrays = []
        for name, _ in init_vars:
            array = tf.train.load_variable(tf_path, name)
            names.append(name)
            arrays.append(array.squeeze())
        tensor_names = []
        for name, _ in self.named_parameters():
            tensor_names.append(name)
        for name, array in zip(names, arrays):
            if name in global_tensor_map:
                v_name = global_tensor_map[name]
                if name == 'model/wte':
                    pointer = self._name_to_variable(v_name)
                    assert pointer.shape == array.shape
                    pointer.data = torch.from_numpy(array)
                    if load_output_layer:
                        output_pointer = self._name_to_variable('decoder._output_layer.weight')
                        assert output_pointer.shape == array.shape
                        output_pointer.data = torch.from_numpy(array)
                elif name == 'model/wpe':
                    pointer = self._name_to_variable(v_name)
                    assert pointer.shape == array.shape
                    pointer.data = torch.from_numpy(array)
                else:
                    pointer = self._name_to_variable(v_name)
                    assert pointer.shape == array.shape
                    pointer.data = torch.from_numpy(array)
            else:
                name_tmp = name.split('/')
                layer_no = name_tmp[1][1:]
                name = '/'.join(name_tmp[2:])
                if name in layer_tensor_map:
                    v_name = layer_tensor_map[name].format(layer_no)
                    pointer = self._name_to_variable(v_name)
                    assert pointer.shape == array.shape
                    pointer.data = torch.from_numpy(array)
                elif name in layer_transpose_map:
                    v_name = layer_transpose_map[name].format(layer_no)
                    pointer = self._name_to_variable(v_name)
                    array_t = np.transpose(array)
                    assert pointer.shape == array_t.shape
                    pointer.data = torch.from_numpy(array_t)
                elif name == 'attn/c_attn/w':
                    index_d = array.shape[-1] // 3
                    Q_w = np.transpose(array[:, :index_d])
                    K_w = np.transpose(array[:, index_d:2 * index_d])
                    V_w = np.transpose(array[:, 2 * index_d:])
                    q_weight = self._name_to_variable(f'{module_name}.self_attns.{layer_no}.Q_dense.weight')
                    k_weight = self._name_to_variable(f'{module_name}.self_attns.{layer_no}.K_dense.weight')
                    v_weight = self._name_to_variable(f'{module_name}.self_attns.{layer_no}.V_dense.weight')
                    assert q_weight.shape == Q_w.shape
                    assert k_weight.shape == K_w.shape
                    assert v_weight.shape == V_w.shape
                    q_weight.data = torch.from_numpy(Q_w)
                    k_weight.data = torch.from_numpy(K_w)
                    v_weight.data = torch.from_numpy(V_w)
                elif name == 'attn/c_attn/b':
                    d = array.shape[0]
                    Q_b = array[:d // 3]
                    K_b = array[d // 3:2 * d // 3]
                    V_b = array[2 * d // 3:]
                    q_bias = self._name_to_variable(f'{module_name}.self_attns.{layer_no}.Q_dense.bias')
                    k_bias = self._name_to_variable(f'{module_name}.self_attns.{layer_no}.K_dense.bias')
                    v_bias = self._name_to_variable(f'{module_name}.self_attns.{layer_no}.V_dense.bias')
                    assert q_bias.shape == Q_b.shape
                    assert k_bias.shape == K_b.shape
                    assert v_bias.shape == V_b.shape
                    q_bias.data = torch.from_numpy(Q_b)
                    k_bias.data = torch.from_numpy(K_b)
                    v_bias.data = torch.from_numpy(V_b)
                else:
                    None
                    raise Exception


Type_size_keeper = [nn.ELU, nn.Hardshrink, nn.Hardtanh, nn.LeakyReLU, nn.LogSigmoid, nn.PReLU, nn.ReLU, nn.RReLU, nn.SELU, nn.CELU, nn.Sigmoid, nn.Softplus, nn.Softshrink, nn.Softsign, nn.Tanh, nn.Tanhshrink, nn.Threshold, nn.Softmin, nn.Softmax, nn.LogSoftmax, nn.Dropout, nn.AlphaDropout]


Type_size_lambda_map = {nn.Linear: lambda x: x.out_features, nn.Bilinear: lambda x: x.out_features, _ConvNd: lambda x: x.out_channels * len(x.kernel_size), nn.Embedding: lambda x: x.embedding_dim, nn.EmbeddingBag: lambda x: x.embedding_dim, nn.RNNCellBase: lambda x: x.hidden_size}


def get_output_size(input_instance: nn.Module) ->Optional[int]:
    """Return the final dimension size of :attr:`input_instance` output.

    If type of :attr:`input_instance` is among the common types, the final
    dimension size will be computed.

    Args:
        input_instance: A :class:`~torch.nn.Module` instance from
            which to compute the final dimension size.

    Returns:
        int (optional): The final dimension size of the output.
            If output size is determined by input, returns ``-1``,
            otherwise if output size is not computable, return `None`.
    """
    for t, l in Type_size_lambda_map.items():
        if isinstance(input_instance, t):
            return l(input_instance)
    for t in Type_size_keeper:
        if isinstance(input_instance, t):
            return -1
    return None


class FeedForwardNetwork(FeedForwardNetworkBase):
    """Feed-forward neural network that consists of a sequence of layers.

    Args:
        layers (list, optional): A list of :torch_nn:`Linear`
            instances composing the network. If not given, layers are created
            according to :attr:`hparams`.
        hparams (dict, optional): Embedder hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    See :meth:`forward` for the inputs and outputs.

    Example:

        .. code-block:: python

            hparams = { # Builds a two-layer dense NN
                "layers": [
                    { "type": "Dense", "kwargs": { "units": 256 },
                    { "type": "Dense", "kwargs": { "units": 10 }
                ]
            }
            nn = FeedForwardNetwork(hparams=hparams)

            inputs = torch.randn([64, 100])
            outputs = nn(inputs)
            # outputs == Tensor of shape [64, 10]
    """

    def __init__(self, layers=None, hparams=None):
        super().__init__(hparams=hparams)
        self._build_layers(layers=layers, layer_hparams=self._hparams.layers)

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "layers": [],
                "name": "NN"
            }

        Here:

        `"layers"`: list
            A list of layer hyperparameters. See
            :func:`~texar.torch.core.get_layer` for details on layer
            hyperparameters.

        `"name"`: str
            Name of the network.
        """
        return {'layers': [], 'name': 'NN'}

    @property
    def output_size(self) ->int:
        """The feature size of network layers output. If output size is
        only determined by input, the feature size is equal to ``-1``.
        """
        for i, layer in enumerate(reversed(self._layers)):
            size = get_output_size(layer)
            size_ext = getattr(layer, 'output_size', None)
            if size_ext is not None:
                size = size_ext
            if size is None:
                break
            if size > 0:
                return size
            elif i == len(self._layers) - 1:
                return -1
        raise ValueError("'output_size' can not be calculated because 'FeedForwardNetwork' contains submodule whose output size cannot be determined.")


def default_transformer_poswise_net_hparams(input_dim: int, output_dim: int=512) ->Dict[str, Any]:
    """Returns default hyperparameters of a
    :class:`~texar.torch.modules.FeedForwardNetwork` as a position-wise network
    used in :class:`~texar.torch.modules.TransformerEncoder` and
    :class:`~texar.torch.modules.TransformerDecoder`.
    This is a 2-layer dense network with dropout in-between.

    .. code-block:: python

        {
            "layers": [
                {
                    "type": "Linear",
                    "kwargs": {
                        "in_features": input_dim,
                        "out_features": output_dim * 4,
                        "bias": True,
                    }
                },
                {
                    "type": "nn.ReLU",
                    "kwargs": {
                        "inplace": True
                    }
                },
                {
                    "type": "Dropout",
                    "kwargs": {
                        "p": 0.1,
                    }
                },
                {
                    "type": "Linear",
                    "kwargs": {
                        "in_features": output_dim * 4,
                        "out_features": output_dim,
                        "bias": True,
                    }
                }
            ],
            "name": "ffn"
        }

    Args:
        input_dim (int): The size of dense layer input.
        output_dim (int): The size of dense layer output.
    """
    return {'layers': [{'type': 'Linear', 'kwargs': {'in_features': input_dim, 'out_features': output_dim * 4, 'bias': True}}, {'type': 'ReLU', 'kwargs': {'inplace': True}}, {'type': 'Dropout', 'kwargs': {'p': 0.1}}, {'type': 'Linear', 'kwargs': {'in_features': output_dim * 4, 'out_features': output_dim, 'bias': True}}], 'name': 'ffn'}


def sequence_mask(lengths: Union[torch.LongTensor, List[int]], max_len: Optional[int]=None, dtype: Optional[torch.dtype]=None, device: Optional[torch.device]=None) ->torch.ByteTensor:
    """Return a mask tensor representing the first N positions of each cell.

    If ``lengths`` has shape ``[d_1, d_2, ..., d_n]`` the resulting tensor
    ``mask`` has dtype ``dtype`` and shape ``[d_1, d_2, ..., d_n, maxlen]``,
    with

    ```
    mask[i_1, i_2, ..., i_n, j] = (j < lengths[i_1, i_2, ..., i_n])
    ```

    Examples:

    ```python
    sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],
                                 #  [True,  True,  True, False, False],
                                 #  [True,  True, False, False, False]]

    sequence_mask([[1, 3],[2,0]])  # [[[ True, False, False],
                                   #   [ True,  True,  True]],
                                   #  [[ True,  True, False],
                                   #   [False, False, False]]]
    ```

    Args:
        lengths: integer tensor or list of int, all its values <= max_len.
        max_len: scalar integer tensor, size of last dimension of returned
            tensor. Default is the maximum value in ``lengths``.
        dtype: the desired data type of returned tensor. Default: if None,
            returns :torch:`ByteTensor`.
        device: the desired device of returned tensor. Default: if None, uses
            the current device for the default tensor type.
    Returns:
        A mask tensor of shape :python:`lengths.shape + (max_len,)`, cast to
        specified dtype.
    Raises:
        ValueError: if ``max_len`` is not a scalar.
    """
    if not isinstance(lengths, torch.Tensor):
        lengths = torch.tensor(lengths, device=device)
    elif device is None:
        device = lengths.device
    lengths: torch.LongTensor
    if max_len is None:
        max_len = torch.max(lengths).item()
    size = lengths.size()
    row_vector = torch.arange(max_len, device=device, dtype=lengths.dtype).view(*([1] * len(size)), -1).expand(*size, max_len)
    mask = row_vector < lengths.unsqueeze(-1)
    if dtype is not None:
        mask = mask
    return mask


class TransformerEncoder(EncoderBase):
    """Transformer encoder that applies multi-head self attention for encoding
    sequences.

    This module basically stacks
    :class:`~texar.torch.modules.MultiheadAttentionEncoder`,
    :class:`~texar.torch.modules.FeedForwardNetwork` and residual connections.
    This module supports two types of architectures, namely, the standard
    Transformer Encoder architecture first proposed in
    `(Vaswani et al.) "Attention is All You Need"`, and
    the variant first used in `(Devlin et al.)` BERT. See
    :meth:`default_hparams` for the nuance between the two types of
    architectures.

    Args:
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    .. document private functions
    """

    def __init__(self, hparams=None):
        super().__init__(hparams=hparams)
        self._input_size = self._hparams.dim
        self.self_attns = nn.ModuleList()
        if not self._hparams.use_bert_config:
            self.self_attn_layer_norm = nn.ModuleList()
        else:
            self.output_layer_norm = nn.ModuleList()
        self.poswise_networks = nn.ModuleList()
        self.poswise_layer_norm = nn.ModuleList()
        eps = self._hparams.eps
        self.initialize_blocks()
        self.embed_dropout = nn.Dropout(p=self._hparams.embedding_dropout)
        self.residual_dropout = nn.Dropout(p=self._hparams.residual_dropout)
        if self._hparams.use_bert_config:
            self.input_normalizer = nn.LayerNorm(self._input_size, eps=eps)
        else:
            self.final_layer_norm = nn.LayerNorm(self._input_size, eps=eps)
        if self._hparams.initializer:
            initialize = layers.get_initializer(self._hparams.initializer)
            assert initialize is not None
            for name, param in self.named_parameters():
                if name.split('.')[-1] == 'weight' and 'layer_norm' not in name:
                    initialize(param)

    def initialize_blocks(self):
        """Helper function which initializes blocks for encoder.

        Should be overridden by any classes where block initialization varies.
        """
        for _ in range(self._hparams.num_blocks):
            mh_attn = MultiheadAttentionEncoder(self._input_size, self._hparams.multihead_attention)
            self.self_attns.append(mh_attn)
            if not self._hparams.use_bert_config:
                self.self_attn_layer_norm.append(nn.LayerNorm(self._input_size, eps=self._hparams.eps))
            if self._hparams.dim != mh_attn.hparams.output_dim:
                raise ValueError('The "dim" in the hparams of "multihead_attention" should be equal to the "dim" of TransformerEncoder')
            pw_net = FeedForwardNetwork(hparams=self._hparams['poswise_feedforward'])
            final_dim = pw_net.hparams.layers[-1]['kwargs']['out_features']
            if self._hparams.dim != final_dim:
                raise ValueError('The output dimenstion of "poswise_feedforward" should be equal to the "dim" of TransformerEncoder.')
            self.poswise_networks.append(pw_net)
            self.poswise_layer_norm.append(nn.LayerNorm(self._input_size, eps=self._hparams.eps))
            if self._hparams.use_bert_config:
                self.output_layer_norm.append(nn.LayerNorm(self._input_size, eps=self._hparams.eps))

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "num_blocks": 6,
                "dim": 512,
                'use_bert_config': False,
                "embedding_dropout": 0.1,
                "residual_dropout": 0.1,
                "poswise_feedforward": default_transformer_poswise_net_hparams,
                'multihead_attention': {
                    'name': 'multihead_attention',
                    'num_units': 512,
                    'num_heads': 8,
                    'dropout_rate': 0.1,
                    'output_dim': 512,
                    'use_bias': False,
                },
                "eps": 1e-6,
                "initializer": None,
                "name": "transformer_encoder"
            }

        Here:

        `"num_blocks"`: int
            Number of stacked blocks.

        `"dim"`: int
            Hidden dimension of the encoders.

        `"use_bert_config"`: bool
            If `False`, apply the standard Transformer Encoder architecture from
            the original paper `(Vaswani et al.) "Attention is All You Need"`.
            If `True`, apply the Transformer Encoder architecture used in BERT
            `(Devlin et al.)` and the default setting of TensorFlow.
            The differences lie in:

            1. The standard arch restricts the word embedding of PAD token to
               all zero. The BERT arch does not.
            2. The attention bias for padding tokens: Standard architectures use
               ``-1e8`` for negative attention mask. BERT uses ``-1e4`` instead.
            3. The residual connections between internal tensors:
               In BERT, a residual layer connects the tensors *after* layer
               normalization. In standard architectures, the tensors are
               connected *before* layer normalization.

        `"embedding_dropout"`: float
            Dropout rate of the input embedding.

        `"residual_dropout"`: float
            Dropout rate of the residual connections.

        `"eps"`: float
            Epsilon values for layer norm layers.

        `"poswise_feedforward"`: dict
            Hyperparameters for a feed-forward network used in residual
            connections.
            Make sure the dimension of the output tensor is equal to ``"dim"``.
            See
            :func:`~texar.torch.modules.default_transformer_poswise_net_hparams`
            for details.

        `"multihead_attention"`: dict
            Hyperparameters for the multi-head attention strategy.
            Make sure the ``"output_dim"`` in this module is equal to ``"dim"``.
            See :class:`~texar.torch.modules.MultiheadAttentionEncoder` for
            details.

        `"initializer"`: dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.torch.core.get_initializer` for details.

        `"name"`: str
            Name of the module.
        """
        dim = 512
        return {'num_blocks': 6, 'dim': dim, 'use_bert_config': False, 'embedding_dropout': 0.1, 'residual_dropout': 0.1, 'poswise_feedforward': default_transformer_poswise_net_hparams(dim), 'multihead_attention': {'name': 'multihead_attention', 'num_units': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'output_dim': 512, 'use_bias': False}, 'initializer': None, 'eps': 1e-06, 'name': 'transformer_encoder'}

    def forward(self, inputs: torch.Tensor, sequence_length: torch.LongTensor) ->torch.Tensor:
        """Encodes the inputs.

        Args:
            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``,
                containing the embedding of input sequences. Note that
                the embedding dimension `dim` must equal "dim" in
                :attr:`hparams`. The input embedding is typically an
                aggregation of word embedding and position embedding.
            sequence_length: A 1D :tensor:`LongTensor` of shape
                ``[batch_size]``. Input tokens beyond respective sequence
                lengths are masked out automatically.

        Returns:
            A Tensor of shape ``[batch_size, max_time, dim]`` containing the
            encoded vectors.
        """
        inputs_padding = 1 - sequence_mask(sequence_length, inputs.size()[1]).float()
        if self._hparams.use_bert_config:
            ignore_padding = attn.attention_bias_ignore_padding(inputs_padding, bias_value=-10000.0)
        else:
            ignore_padding = attn.attention_bias_ignore_padding(inputs_padding)
        encoder_self_attention_bias = ignore_padding
        input_embedding = inputs
        if self._hparams.use_bert_config:
            x = self.input_normalizer(input_embedding)
            x = self.embed_dropout(x)
        else:
            x = self.embed_dropout(input_embedding)
        for i in range(self._hparams.num_blocks):
            if self._hparams.use_bert_config:
                _queries_input = x
            else:
                _queries_input = self.self_attn_layer_norm[i](x)
            attention_output = self.self_attns[i](queries=_queries_input, memory=_queries_input, memory_attention_bias=encoder_self_attention_bias)
            attention_output = self.residual_dropout(attention_output)
            x = x + attention_output
            poswise_network = self.poswise_networks[i]
            poswise_normalizer = self.poswise_layer_norm[i]
            if self._hparams.use_bert_config:
                x = poswise_normalizer(x)
                y = x
            else:
                y = poswise_normalizer(x)
            original_shape = y.size()
            y = y.view(-1, self._hparams.dim)
            layer_output = poswise_network(y)
            sub_output = self.residual_dropout(layer_output)
            sub_output = sub_output.view(original_shape)
            x = x + sub_output
            if self._hparams.use_bert_config:
                x = self.output_layer_norm[i](x)
        if not self._hparams.use_bert_config:
            x = self.final_layer_norm(x)
        return x

    @property
    def output_size(self) ->int:
        return self._hparams.dim


class WordEmbedder(EmbedderBase):
    """Simple word embedder that maps indexes into embeddings. The indexes
    can be soft (e.g., distributions over vocabulary).

    Either :attr:`init_value` or :attr:`vocab_size` is required. If both are
    given, there must be ``init_value.shape[0]==vocab_size``.

    Args:
        init_value (optional): A Tensor or numpy array that contains the
            initial value of embeddings. It is typically of shape
            ``[vocab_size] + embedding-dim``. Embeddings can have dimensionality
            > 1.

            If `None`, embedding is initialized as specified in
            ``hparams["initializer"]``. Otherwise, the
            ``"initializer"`` and ``"dim"`` hyperparameters in :attr:`hparams`
            are ignored.
        vocab_size (int, optional): The vocabulary size. Required if
            :attr:`init_value` is not given.
        hparams (dict, optional): Embedder hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    See :meth:`forward` for the inputs and outputs of the embedder.

    Example:

    .. code-block:: python

        ids = torch.empty([32, 10]).uniform_(to=10).type(torch.int64).
        soft_ids = torch.empty([32, 10, 100]).uniform_()

        embedder = WordEmbedder(vocab_size=100, hparams={'dim': 256})
        ids_emb = embedder(ids=ids) # shape: [32, 10, 256]
        soft_ids_emb = embedder(soft_ids=soft_ids) # shape: [32, 10, 256]

    .. code-block:: python

        # Use with Texar data module
        hparams={
            'dataset': {
                'embedding_init': {'file': 'word2vec.txt'}
                ...
            },
        }
        data = MonoTextData(data_params)
        iterator = DataIterator(data)
        batch = next(iter(iterator))

        # Use data vocab size
        embedder_1 = WordEmbedder(vocab_size=data.vocab.size)
        emb_1 = embedder_1(batch['text_ids'])

        # Use pre-trained embedding
        embedder_2 = WordEmbedder(init_value=data.embedding_init_value)
        emb_2 = embedder_2(batch['text_ids'])


    .. document private functions
    """

    def __init__(self, init_value: Optional[torch.Tensor]=None, vocab_size: Optional[int]=None, hparams=None):
        if init_value is None and vocab_size is None:
            raise ValueError('Either `init_value` or `vocab_size` is required.')
        super().__init__(init_value=init_value, num_embeds=vocab_size, hparams=hparams)
        if vocab_size is None:
            self._vocab_size = self._num_embeds
        else:
            self._vocab_size = vocab_size
        if self._vocab_size != self._num_embeds:
            raise ValueError(f'vocab_size must equal to init_value.shape[0]. Got {self._vocab_size} and {self._num_embeds}')
        self._dropout_layer = EmbeddingDropout(self._hparams.dropout_rate)

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "dim": 100,
                "dropout_rate": 0,
                "dropout_strategy": 'element',
                "initializer": {
                    "type": "random_uniform_initializer",
                    "kwargs": {
                        "minval": -0.1,
                        "maxval": 0.1,
                        "seed": None
                    }
                },
                "trainable": True,
                "name": "word_embedder",
            }

        Here:

        `"dim"`: int or list
            Embedding dimension. Can be a list of integers to yield embeddings
            with dimensionality > 1.

            Ignored if :attr:`init_value` is given to the embedder constructor.

        `"dropout_rate"`: float
            The dropout rate between 0 and 1. For example, ``dropout_rate=0.1``
            would zero out 10% of the embeddings. Set to 0 to disable dropout.

        `"dropout_strategy"`: str
            The dropout strategy. Can be one of the following

            - ``"element"``: The regular strategy that drops individual elements
              in the embedding vectors.
            - ``"item"``: Drops individual items (e.g., words) entirely. For
              example, for the word sequence "the simpler the better", the
              strategy can yield "_ simpler the better", where the first "the"
              is dropped.
            - ``"item_type"``: Drops item types (e.g., word types). For example,
              for the above sequence, the strategy can yield "_ simpler _
              better", where the word type "the" is dropped. The dropout will
              never yield "_ simpler the better" as in the ``"item"`` strategy.

        `"initializer"`: dict or None
            Hyperparameters of the initializer for embedding values. See
            :func:`~texar.torch.core.get_initializer` for the details. Ignored
            if :attr:`init_value` is given to the embedder constructor.

        `"trainable"`: bool
            Whether the embedding parameters are trainable. If false, freeze the
            embedding parameters.

        `"name"`: str
            Name of the embedding variable.
        """
        hparams = embedder_utils.default_embedding_hparams()
        hparams['name'] = 'word_embedder'
        return hparams

    def extra_repr(self) ->str:
        return f'vocab_size={self.vocab_size}, embedding_dim={self.dim}'

    def forward(self, ids: Optional[torch.LongTensor]=None, soft_ids: Optional[torch.Tensor]=None, **kwargs) ->torch.Tensor:
        """Embeds (soft) ids.

        Either :attr:`ids` or :attr:`soft_ids` must be given, and they
        must not be given at the same time.

        Args:
            ids (optional): An integer tensor containing the ids to embed.
            soft_ids (optional): A tensor of weights (probabilities) used to
                mix the embedding vectors.
            kwargs: Additional keyword arguments for
                :torch_nn:`functional.embedding` besides :attr:`params` and
                :attr:`ids`.

        Returns:
            If :attr:`ids` is given, returns a Tensor of shape
            ``list(ids.shape) + embedding-dim``. For example,
            if ``list(ids.shape) == [batch_size, max_time]``
            and ``list(embedding.shape) == [vocab_size, emb_dim]``, then the
            return tensor has shape ``[batch_size, max_time, emb_dim]``.

            If :attr:`soft_ids` is given, returns a Tensor of shape
            ``list(soft_ids.shape)[:-1] + embedding-dim``. For example,
            if ``list(soft_ids.shape) == [batch_size, max_time, vocab_size]``
            and ``list(embedding.shape) == [vocab_size, emb_dim]``, then the
            return tensor has shape ``[batch_size, max_time, emb_dim]``.
        """
        if ids is not None:
            if soft_ids is not None:
                raise ValueError('Must not specify `ids` and `soft_ids` at the same time.')
            ids_rank = ids.dim()
        elif soft_ids is not None:
            ids_rank = soft_ids.dim() - 1
        else:
            raise ValueError('Either `ids` or `soft_ids` must be given.')
        embedding = self._embedding
        if self._hparams.dropout_strategy == 'item_type':
            noise_shape = self._get_noise_shape(self._hparams.dropout_strategy)
            embedding = self._dropout_layer(embedding, noise_shape)
        if ids is not None:
            outputs = F.embedding(ids, embedding, **kwargs)
        else:
            outputs = embedder_utils.soft_embedding_lookup(embedding, soft_ids)
        if self._hparams.dropout_strategy != 'item_type':
            noise_shape = self._get_noise_shape(self._hparams.dropout_strategy, ids_rank=ids_rank, dropout_input=outputs)
            outputs = self._dropout_layer(outputs, noise_shape)
        return outputs

    @property
    def embedding(self) ->torch.Tensor:
        """The embedding tensor, of shape ``[vocab_size] + dim``.
        """
        return self._embedding

    @property
    def dim(self) ->int:
        """The embedding dimension.
        """
        return self._dim

    @property
    def vocab_size(self) ->int:
        """The vocabulary size.
        """
        return self._vocab_size

    @property
    def num_embeddings(self) ->int:
        """The vocabulary size. This interface matches
        :torch_nn:`Embedding`.
        """
        return self._vocab_size

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output. If the :attr:`dim`
        hyperparameter is a ``list`` or ``tuple``, the feature size
        equals its final dimension; otherwise, if :attr:`dim` is an
        ``int``, the feature size equals :attr:`dim`.
        """
        if isinstance(self._dim, (list, tuple)):
            return self._dim[-1]
        else:
            return self._dim


class GPT2Encoder(EncoderBase, PretrainedGPT2Mixin):
    """Raw GPT2 Transformer for encoding sequences. Please see
    :class:`~texar.torch.modules.PretrainedGPT2Mixin` for a brief description
    of GPT2.

    This module basically stacks
    :class:`~texar.torch.modules.WordEmbedder`,
    :class:`~texar.torch.modules.PositionEmbedder`,
    :class:`~texar.torch.modules.TransformerEncoder`.

    Args:
        pretrained_model_name (optional): a `str`, the name
            of pre-trained model (e.g., ``gpt2-small``). Please refer to
            :class:`~texar.torch.modules.PretrainedGPT2Mixin` for
            all supported models.
            If `None`, the model name in :attr:`hparams` is used.
        cache_dir (optional): the path to a folder in which the
            pre-trained models will be cached. If `None` (default),
            a default directory (``texar_data`` folder under user's home
            directory) will be used.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameter will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure
            and default values.
    """

    def __init__(self, pretrained_model_name: Optional[str]=None, cache_dir: Optional[str]=None, hparams=None):
        super().__init__(hparams=hparams)
        self.load_pretrained_config(pretrained_model_name, cache_dir)
        self.word_embedder = WordEmbedder(vocab_size=self._hparams.vocab_size, hparams=self._hparams.embed)
        self.position_embedder = PositionEmbedder(position_size=self._hparams.position_size, hparams=self._hparams.position_embed)
        self.encoder = TransformerEncoder(hparams=self._hparams.encoder)
        self.init_pretrained_weights(load_output_layer=False)

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        * The encoder arch is determined by the constructor argument
          :attr:`pretrained_model_name` if it's specified. In this case,
          `hparams` are ignored.
        * Otherwise, the encoder arch is determined by
          `hparams['pretrained_model_name']` if it's specified. All other
          configurations in `hparams` are ignored.
        * If the above two are `None`, the encoder arch is defined by the
          configurations in `hparams` and weights are randomly initialized.

        .. code-block:: python

            {
                "pretrained_model_name": "gpt2-small",
                "vocab_size": 50257,
                "context_size": 1024,
                "embedding_size": 768,
                "embed": {
                    "dim": 768,
                    "name": "word_embeddings"
                },
                "position_size": 1024,
                "position_embed": {
                    "dim": 768,
                    "name": "position_embeddings"
                },

                "encoder": {
                    "dim": 768,
                    "num_blocks": 12,
                    "use_bert_config": False,
                    "embedding_dropout": 0,
                    "residual_dropout": 0,
                    "multihead_attention": {
                        "use_bias": True,
                        "num_units": 768,
                        "num_heads": 12,
                        "output_dim": 768
                    },
                    "eps": 1e-6,
                    "initializer": {
                        "type": "variance_scaling_initializer",
                        "kwargs": {
                            "factor": 1.0,
                            "mode": "FAN_AVG",
                            "uniform": True
                        }
                    },
                    "poswise_feedforward": {
                        "layers": [
                            {
                                "type": "Linear",
                                "kwargs": {
                                    "in_features": 768,
                                    "out_features": 3072,
                                    "bias": True
                                }
                            },
                            {
                                "type": "GPTGELU",
                                "kwargs": {}
                            },
                            {
                                "type": "Linear",
                                "kwargs": {
                                    "in_features": 3072,
                                    "out_features": 768,
                                    "bias": True
                                }
                            }
                        ],
                        "name": "ffn"
                    }
                },
                "initializer": None,
                "name": "gpt2_encoder",
            }

        Here:

        The default parameters are values for 124M GPT2 model.

        `"pretrained_model_name"`: str or None
            The name of the pre-trained GPT2 model. If None, the model
            will be randomly initialized.

        `"embed"`: dict
            Hyperparameters for word embedding layer.

        `"vocab_size"`: int
            The vocabulary size of `inputs` in `GPT2Model`.

        `"position_embed"`: dict
            Hyperparameters for position embedding layer.

        `"position_size"`:  int
            The maximum sequence length that this model might ever be used with.

        `"decoder"`: dict
            Hyperparameters for the TransformerDecoder.
            See :func:`~texar.torch.modules.TransformerDecoder.default_hparams`
            for details.

        `"eps"`: float
            Epsilon values for layer norm layers.

        `"initializer"`: dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.torch.core.get_initializer` for details.

        `"name"`: str
            Name of the module.
        """
        return {'encoder': {'dim': 768, 'num_blocks': 12, 'use_bert_config': False, 'embedding_dropout': 0, 'residual_dropout': 0, 'multihead_attention': {'use_bias': True, 'num_units': 768, 'num_heads': 12, 'output_dim': 768}, 'eps': 1e-06, 'initializer': {'type': 'variance_scaling_initializer', 'kwargs': {'factor': 1.0, 'mode': 'FAN_AVG', 'uniform': True}}, 'poswise_feedforward': {'layers': [{'type': 'Linear', 'kwargs': {'in_features': 768, 'out_features': 3072, 'bias': True}}, {'type': 'GPTGELU', 'kwargs': {}}, {'type': 'Linear', 'kwargs': {'in_features': 3072, 'out_features': 768, 'bias': True}}], 'name': 'ffn'}}, 'pretrained_model_name': 'gpt2-small', 'vocab_size': 50257, 'context_size': 1024, 'embedding_size': 768, 'embed': {'dim': 768, 'name': 'word_embeddings'}, 'position_size': 1024, 'position_embed': {'dim': 768, 'name': 'position_embeddings'}, 'initializer': None, 'name': 'gpt2_encoder', '@no_typecheck': ['pretrained_model_name']}

    def forward(self, inputs: Union[torch.Tensor, torch.LongTensor], sequence_length: Optional[torch.LongTensor]=None):
        """Encodes the inputs.

        Args:
            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,
                containing the ids of tokens in input sequences, or
                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,
                containing soft token ids (i.e., weights or probabilities)
                used to mix the embedding vectors.
            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.
                Input tokens beyond respective sequence lengths are masked
                out automatically.

        Returns:
            outputs:  A Tensor of shape
            `[batch_size, max_time, dim]` containing the encoded vectors.
        """
        if inputs.dim() == 2:
            word_embeds = self.word_embedder(ids=inputs)
        elif inputs.dim() == 3:
            word_embeds = self.word_embedder(soft_ids=inputs)
        else:
            raise ValueError("'inputs' should be a 2D or 3D tensor.")
        batch_size = inputs.size(0)
        pos_length = inputs.new_full((batch_size,), inputs.size(1), dtype=torch.long)
        pos_embeds = self.position_embedder(sequence_length=pos_length)
        inputs_embeds = word_embeds + pos_embeds
        if sequence_length is None:
            sequence_length = inputs.new_full((batch_size,), inputs.size(1), dtype=torch.long)
        output = self.encoder(inputs=inputs_embeds, sequence_length=sequence_length)
        return output

    @property
    def output_size(self):
        """The feature size of :meth:`forward` output.
        """
        return self._hparams.encoder.dim


AnyDict = MutableMapping[str, Any]


ParamDict = Union[HParams, AnyDict]


def dict_fetch(src_dict: Optional[ParamDict], tgt_dict_or_keys: Union[ParamDict, List[str]]) ->Optional[AnyDict]:
    """Fetches a sub-dictionary of :attr:`src_dict` with the keys in
    :attr:`tgt_dict_or_keys`.

    Args:
        src_dict: A dictionary or instance of :class:`~texar.torch.HParams`.
            The source dictionary to fetch values from.
        tgt_dict_or_keys: A dictionary, instance of
            :class:`~texar.torch.HParams`, or a list (or a
            ``dict_keys``/``KeysView``) of keys to be included in the output
            dictionary.

    Returns:
        A new dictionary that is a sub-dictionary of :attr:`src_dict`.
    """
    if src_dict is None:
        return src_dict
    if isinstance(tgt_dict_or_keys, HParams):
        tgt_dict_or_keys = tgt_dict_or_keys.todict()
    if isinstance(tgt_dict_or_keys, MutableMapping):
        tgt_dict_or_keys = tgt_dict_or_keys.keys()
    keys = list(tgt_dict_or_keys)
    if isinstance(src_dict, HParams):
        src_dict = src_dict.todict()
    return {k: src_dict[k] for k in keys if k in src_dict}


def get_initializer(hparams=None) ->Optional[Callable[[torch.Tensor], torch.Tensor]]:
    """Returns an initializer instance.

    Args:
        hparams (dict or HParams, optional): Hyperparameters with the structure

            .. code-block:: python

                {
                    "type": "initializer_class_or_function",
                    "kwargs": {
                        # ...
                    }
                }

            The `"type"` field can be a function name or module path. If name is
            provided, it be must be from one the following modules:
            :torch_docs:`torch.nn.init <nn.html#torch-nn-init>` and
            :mod:`texar.torch.custom`.

            Besides, the `"type"` field can also be an initialization function
            called with :python:`initialization_fn(**kwargs)`. In this case
            `"type"` can be the function, or its name or module path. If no
            keyword argument is required, `"kwargs"` can be omitted.

    Returns:
        An initializer instance. `None` if :attr:`hparams` is `None`.
    """
    if hparams is None:
        return None
    kwargs = hparams.get('kwargs', {})
    if isinstance(kwargs, HParams):
        kwargs = kwargs.todict()
    modules = ['torch.nn.init', 'torch', 'texar.torch.custom']
    initializer_fn = utils.get_function(hparams['type'], modules)
    initializer = functools.partial(initializer_fn, **kwargs)
    return initializer


class GPT2Classifier(ClassifierBase, PretrainedGPT2Mixin):
    """Classifier based on GPT2 modules. Please see
    :class:`~texar.torch.modules.PretrainedGPT2Mixin` for a brief description
    of GPT2.

    This is a combination of the
    :class:`~texar.torch.modules.GPT2Encoder` with a classification
    layer. Both step-wise classification and sequence-level classification
    are supported, specified in :attr:`hparams`.

    Arguments are the same as in
    :class:`~texar.torch.modules.GPT2Encoder`.

    Args:
        pretrained_model_name (optional): a `str`, the name
            of pre-trained model (e.g., ``gpt2-small``). Please refer to
            :class:`~texar.torch.modules.PretrainedGPT2Mixin` for
            all supported models.
            If `None`, the model name in :attr:`hparams` is used.
        cache_dir (optional): the path to a folder in which the
            pre-trained models will be cached. If `None` (default),
            a default directory (``texar_data`` folder under user's home
            directory) will be used.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameter will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure
            and default values.

    .. document private functions
    """

    def __init__(self, pretrained_model_name: Optional[str]=None, cache_dir: Optional[str]=None, hparams=None):
        super().__init__(hparams=hparams)
        encoder_hparams = dict_fetch(hparams, GPT2Encoder.default_hparams())
        self._encoder = GPT2Encoder(pretrained_model_name=pretrained_model_name, cache_dir=cache_dir, hparams=encoder_hparams)
        self._dropout_layer = nn.Dropout(self._hparams.dropout)
        self.num_classes = self._hparams.num_classes
        if self.num_classes <= 0:
            self._logits_layer = None
        else:
            logit_kwargs = self._hparams.logit_layer_kwargs
            if logit_kwargs is None:
                logit_kwargs = {}
            elif not isinstance(logit_kwargs, HParams):
                raise ValueError("hparams['logit_layer_kwargs'] must be a dict.")
            else:
                logit_kwargs = logit_kwargs.todict()
            if self._hparams.clas_strategy == 'all_time':
                self._logits_layer = nn.Linear(self._encoder.output_size * self._hparams.max_seq_length, self.num_classes, **logit_kwargs)
            else:
                self._logits_layer = nn.Linear(self._encoder.output_size, self.num_classes, **logit_kwargs)
        if self._hparams.initializer:
            initialize = get_initializer(self._hparams.initializer)
            assert initialize is not None
            if self._logits_layer is not None:
                initialize(self._logits_layer.weight)
                if self._logits_layer.bias is not None:
                    initialize(self._logits_layer.bias)
        self.is_binary = self.num_classes == 1 or self.num_classes <= 0 and self._hparams.encoder.dim == 1

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in GPT2Encoder
                ...
                # (2) Additional hyperparameters
                "num_classes": 2,
                "logit_layer_kwargs": None,
                "clas_strategy": `cls_time`,
                "max_seq_length": None,
                "dropout": 0.1,
                "name": `gpt2_classifier`
            }

        Here:

        1. Same hyperparameters as in
           :class:`~texar.torch.modules.GPT2Encoder`.
           See the :meth:`~texar.torch.modules.GPT2Encoder.default_hparams`.
           An instance of GPT2Encoder is created for feature extraction.

        2. Additional hyperparameters:

            `"num_classes"`: int
                Number of classes:

                - If **> 0**, an additional `Linear`
                  layer is appended to the encoder to compute the logits over
                  classes.
                - If **<= 0**, no dense layer is appended. The number of
                  classes is assumed to be the final dense layer size of the
                  encoder.

            `"logit_layer_kwargs"`: dict
                Keyword arguments for the logit Dense layer constructor,
                except for argument "units" which is set to `num_classes`.
                Ignored if no extra logit layer is appended.

            `"clas_strategy"`: str
                The classification strategy, one of:

                - **cls_time**: Sequence-level classification based on the
                  output of the last time step. Each sequence has a class.
                - **all_time**: Sequence-level classification based on
                  the output of all time steps. Each sequence has a class.
                - **time_wise**: Step-wise classification, i.e., make
                  classification for each time step based on its output.

            `"max_seq_length"`: int, optional
                Maximum possible length of input sequences. Required if
                `clas_strategy` is `all_time`.

            `"dropout"`: float
                The dropout rate of the GPT2 encoder output.

            `"name"`: str
                Name of the classifier.
        """
        hparams = GPT2Encoder.default_hparams()
        hparams.update({'num_classes': 2, 'logit_layer_kwargs': None, 'clas_strategy': 'cls_time', 'max_seq_length': None, 'dropout': 0.1, 'name': 'gpt2_classifier'})
        return hparams

    def forward(self, inputs: Union[torch.Tensor, torch.LongTensor], sequence_length: Optional[torch.LongTensor]=None) ->Tuple[torch.Tensor, torch.LongTensor]:
        """Feeds the inputs through the network and makes classification.

        The arguments are the same as in
        :class:`~texar.torch.modules.GPT2Encoder`.

        Args:
            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,
                containing the ids of tokens in input sequences, or
                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,
                containing soft token ids (i.e., weights or probabilities)
                used to mix the embedding vectors.
            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.
                Input tokens beyond respective sequence lengths are masked
                out automatically.

        Returns:
            A tuple `(logits, preds)`, containing the logits over classes and
            the predictions, respectively.

            - If ``clas_strategy`` is ``cls_time`` or ``all_time``:

                - If ``num_classes`` == 1, ``logits`` and ``pred`` are of both
                  shape ``[batch_size]``.
                - If ``num_classes`` > 1, ``logits`` is of shape
                  ``[batch_size, num_classes]`` and ``pred`` is of shape
                  ``[batch_size]``.

            - If ``clas_strategy`` is ``time_wise``:

                - If ``num_classes`` == 1, ``logits`` and ``pred`` are of both
                  shape ``[batch_size, max_time]``.
                - If ``num_classes`` > 1, ``logits`` is of shape
                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of
                  shape ``[batch_size, max_time]``.
        """
        enc_outputs = self._encoder(inputs, sequence_length)
        strategy = self._hparams.clas_strategy
        if strategy == 'time_wise':
            logits = enc_outputs
        elif strategy == 'cls_time':
            if sequence_length is None:
                logits = torch.squeeze(enc_outputs[:, (-1), :], dim=1)
            else:
                logits = torch.stack([enc_outputs[(batch_idx), (time_idx - 1), :] for batch_idx, time_idx in enumerate(sequence_length)], dim=0)
        elif strategy == 'all_time':
            length_diff = self._hparams.max_seq_length - inputs.shape[1]
            logit_input = F.pad(enc_outputs, [0, 0, 0, length_diff, 0, 0])
            logit_input_dim = self._encoder.output_size * self._hparams.max_seq_length
            logits = logit_input.view(-1, logit_input_dim)
        else:
            raise ValueError('Unknown classification strategy: {}'.format(strategy))
        if self._logits_layer is not None:
            logits = self._dropout_layer(logits)
            logits = self._logits_layer(logits)
        if strategy == 'time_wise':
            if self.is_binary:
                logits = torch.squeeze(logits, -1)
                preds = (logits > 0).long()
            else:
                preds = torch.argmax(logits, dim=-1)
        else:
            if self.is_binary:
                preds = (logits > 0).long()
                logits = torch.flatten(logits)
            else:
                preds = torch.argmax(logits, dim=-1)
            preds = torch.flatten(preds)
        return logits, preds

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output :attr:`logits`.
        If :attr:`logits` size is only determined by input
        (i.e. if ``num_classes`` == 1), the feature size is equal to ``-1``.
        Otherwise it is equal to last dimension value of :attr:`logits` size.
        """
        if self._hparams.num_classes == 1:
            logit_dim = -1
        elif self._hparams.num_classes > 1:
            logit_dim = self._hparams.num_classes
        elif self._hparams.clas_strategy == 'all_time':
            logit_dim = self._encoder.output_size * self._hparams.max_seq_length
        else:
            logit_dim = self._encoder.output_size
        return logit_dim


class BuiltinCellWrapper(RNNCellBase[State]):
    """Base class for wrappers over built-in :torch_nn:`RNNCellBase`
    RNN cells.
    """

    def forward(self, input: torch.Tensor, state: Optional[State]=None) ->Tuple[torch.Tensor, State]:
        if state is None:
            batch_size = input.size(0)
            state = self.zero_state(batch_size)
        new_state = self._cell(input, state)
        return new_state, new_state


LSTMState = Tuple[torch.Tensor, torch.Tensor]


class LSTMCell(BuiltinCellWrapper[LSTMState]):
    """A wrapper over :torch_nn:`LSTMCell`, additionally providing the
    option to initialize the forget-gate bias to a constant value.
    """

    def __init__(self, input_size, hidden_size, bias=True, forget_bias: Optional[float]=None):
        if forget_bias is not None and not bias:
            raise ValueError("Parameter 'forget_bias' must be set to None when'bias' is set to False.")
        cell = nn.LSTMCell(input_size, hidden_size, bias=bias)
        if forget_bias is not None:
            with torch.no_grad():
                cell.bias_ih[hidden_size:2 * hidden_size].fill_(forget_bias)
                cell.bias_hh[hidden_size:2 * hidden_size].fill_(forget_bias)
        super().__init__(cell)

    def zero_state(self, batch_size: int) ->LSTMState:
        """Returns the zero state for LSTMs as (h, c)."""
        state = self._param.new_zeros(batch_size, self.hidden_size, requires_grad=False)
        return state, state

    def forward(self, input: torch.Tensor, state: Optional[LSTMState]=None) ->Tuple[torch.Tensor, LSTMState]:
        if state is None:
            batch_size = input.size(0)
            state = self.zero_state(batch_size)
        new_state = self._cell(input, state)
        return new_state[0], new_state


class RNNEncoderBase(EncoderBase, Generic[State]):
    """Base class for all RNN encoder classes to inherit.

    Args:
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.
    """

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "name": "rnn_encoder"
            }
        """
        return {'name': 'rnn_encoder'}


def _build_dense_output_layer(cell_output_size: int, hparams: HParams) ->Optional[nn.Sequential]:
    """Build the output layers.

    Args:
        cell_output_size: The output size of the rnn cell.
        hparams (dict or HParams): Hyperparameters. Missing hyperparameters
            will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    Returns:
        A :torch_nn:`Sequential` module containing the output layers.
    """
    nlayers = hparams.num_layers
    if nlayers <= 0:
        return None
    layer_size = _to_list(hparams.layer_size, 'output_layer.layer_size', nlayers)
    dropout_layer_ids = _to_list(hparams.dropout_layer_ids)
    other_kwargs = hparams.other_dense_kwargs or {}
    if isinstance(other_kwargs, HParams):
        other_kwargs = other_kwargs.todict()
    if not isinstance(other_kwargs, dict):
        raise ValueError("hparams 'output_layer.other_dense_kwargs' must be a dict.")
    output_layers: List[nn.Module] = []
    for i in range(nlayers):
        if i in dropout_layer_ids:
            output_layers.append(nn.Dropout(p=hparams.dropout_rate))
        dense_layer = nn.Linear(in_features=cell_output_size if i == 0 else layer_size[i - 1], out_features=layer_size[i], **other_kwargs)
        output_layers.append(dense_layer)
        if i == nlayers - 1:
            activation = hparams.final_layer_activation
        else:
            activation = hparams.activation
        if activation is not None:
            layer_hparams = {'type': activation, 'kwargs': {}}
            activation_layer = layers.get_layer(hparams=layer_hparams)
            output_layers.append(activation_layer)
    if nlayers in dropout_layer_ids:
        output_layers.append(nn.Dropout(p=hparams.dropout_rate))
    return nn.Sequential(*output_layers)


def _default_output_layer_hparams() ->Dict[str, Any]:
    return {'num_layers': 0, 'layer_size': 128, 'activation': 'Identity', 'final_layer_activation': None, 'other_dense_kwargs': None, 'dropout_layer_ids': [], 'dropout_rate': 0.5, 'variational_dropout': False, '@no_typecheck': ['activation', 'final_layer_activation', 'layer_size', 'dropout_layer_ids']}


def _forward_output_layers(inputs: torch.Tensor, output_layer: Optional[nn.Module], time_major: bool, sequence_length: Optional[Union[torch.LongTensor, List[int]]]=None) ->Tuple[torch.Tensor, int]:
    """Forwards inputs through the output layers.

    Args:
        inputs: A Tensor of shape ``[batch_size, max_time] + input_size`` if
            :attr:`time_major` is `False`, or shape
            ``[max_time, batch_size] + input_size`` if :attr:`time_major` is
            `True`.
        output_layer (optional): :torch_nn:`Sequential` or :torch_nn:`Module`
            of output layers.
        time_major (bool): The shape format of the :attr:`inputs` and
            :attr:`outputs` Tensors. If `True`, these tensors are of shape
            `[max_time, batch_size, input_size]`. If `False` (default),
            these tensors are of shape `[batch_size, max_time, input_size]`.
        sequence_length (optional): A 1D :tensor:`LongTensor` of shape
            ``[batch_size]``. Sequence lengths of the batch inputs. Used to
            copy-through state and zero-out outputs when past a batch element's
            sequence length.

    Returns:
        A pair :attr:`(outputs, outputs_size), where

        - :attr:`outputs`: A Tensor of shape
        `[batch_size, max_time] + outputs_size`.

        - :attr:`outputs_size`: An `int` representing the output size.
    """
    if output_layer is None:
        return inputs, inputs.shape[-1]
    output = output_layer(inputs)
    if sequence_length is not None:
        output = mask_sequences(output, sequence_length, time_major=time_major)
    output_size = output.shape[-1]
    return output, output_size


R = TypeVar('R')


@no_type_check
def map_structure(fn: Callable[[T], R], obj: Collection[T]) ->Collection[R]:
    """Map a function over all elements in a (possibly nested) collection.

    Args:
        fn (callable): The function to call on elements.
        obj: The collection to map function over.

    Returns:
        The collection in the same structure, with elements mapped.
    """
    if hasattr(obj, '--no-map--'):
        return fn(obj)
    if isinstance(obj, list):
        return [map_structure(fn, x) for x in obj]
    if isinstance(obj, tuple):
        if isinstance(obj, torch.Size):
            return fn(obj)
        if hasattr(obj, '_fields'):
            return type(obj)(*[map_structure(fn, x) for x in obj])
        else:
            return tuple(map_structure(fn, x) for x in obj)
    if isinstance(obj, dict):
        return {k: map_structure(fn, v) for k, v in obj.items()}
    if isinstance(obj, set):
        return {map_structure(fn, x) for x in obj}
    return fn(obj)


@no_type_check
def map_structure_zip(fn: Callable[..., R], objs: Sequence[Collection[T]]) ->Collection[R]:
    """Map a function over tuples formed by taking one elements from each
    (possibly nested) collection. Each collection must have identical
    structures.

    .. note::
        Although identical structures are required, it is not enforced by
        assertions. The structure of the first collection is assumed to be
        the structure for all collections.

        For rare cases where collections need to have different structures,
        refer to :meth:`no_map`.

    Args:
        fn (callable): The function to call on elements.
        objs: The list of collections to map function over.

    Returns:
        A collection with the same structure, with elements mapped.
    """
    obj = objs[0]
    if hasattr(obj, '--no-map--'):
        return fn(*objs)
    if isinstance(obj, list):
        return [map_structure_zip(fn, xs) for xs in zip(*objs)]
    if isinstance(obj, tuple):
        if isinstance(obj, torch.Size):
            return fn(obj)
        if hasattr(obj, '_fields'):
            return type(obj)(*[map_structure_zip(fn, xs) for xs in zip(*objs)])
        else:
            return tuple(map_structure_zip(fn, xs) for xs in zip(*objs))
    if isinstance(obj, dict):
        return {k: map_structure_zip(fn, [o[k] for o in objs]) for k in obj.keys()}
    if isinstance(obj, set):
        return {map_structure_zip(fn, xs) for xs in zip(*objs)}
    return fn(*objs)


@lru_cache(maxsize=None)
def _no_map_type(container_type: Type[T]) ->Type[T]:
    new_type = type('_no_map' + container_type.__name__, (container_type,), {'--no-map--': True})
    return new_type


def no_map(container_type: Type[T], *args, **kwargs) ->T:
    """Create a "`non-mappable`" container type, i.e. it will be treated as a
    singleton object in :meth:`map_structure` and :meth:`map_structure_zip`,
    its contents will not be traversed.

    This is implemented by dynamically creating a subclass of the required type,
    and overriding the :attr:`__subclasscheck__` class method to always return
    `False`.

    Args:
        container_type: The type of the container to create,
            e.g. `list`, `dict`.
        args: Arguments to the constructor.
        kwargs: Keyword arguments to the constructor

    Returns:
        The `non-mappable` container type.
    """
    return _no_map_type(container_type)(*args, **kwargs)


def _dynamic_rnn_loop(cell: RNNCellBase[State], inputs: torch.Tensor, initial_state: State, sequence_length: torch.LongTensor) ->Tuple[torch.Tensor, State]:
    """Internal implementation of Dynamic RNN.

    Args:
        cell: An instance of RNNCell.
        inputs: A ``Tensor`` of shape ``[time, batch_size, input_size]``,
            or a nested tuple of such elements.
        initial_state: A ``Tensor`` of shape ``[batch_size, state_size]``,
            or if ``cell.state_size`` is a tuple, then this should be a tuple
            of tensors having shapes ``[batch_size, s]`` for ``s`` in
            ``cell.state_size``.
        sequence_length: (optional) An ``int32`` ``Tensor``
            of shape ``[batch_size]``.

    Returns:
        Tuple ``(final_outputs, final_state)``.
        final_outputs:
            A ``Tensor`` of shape ``[time, batch_size, cell.output_size]``. If
            ``cell.output_size`` is a (possibly nested) tuple of ints or
            ``torch.Size`` objects, then this returns a
            (possibly nested) tuple of Tensors matching the corresponding
            shapes.
        final_state:
            A ``Tensor``, or possibly nested tuple of Tensors, matching
            in length and shapes to ``initial_state``.
    """
    state = initial_state
    time_steps = inputs.shape[0]
    all_outputs = []
    all_state = map_structure(lambda _: no_map(list), state)
    for i in range(time_steps):
        output, state = cell(inputs[i], state)
        all_outputs.append(output)
        map_structure_zip(lambda xs, x: xs.append(x), (all_state, state))
    final_outputs = torch.stack(all_outputs, dim=0)
    final_outputs = mask_sequences(final_outputs, sequence_length=sequence_length, time_major=True)
    final_state = map_structure(lambda _: no_map(list), state)
    for batch_idx, time_idx in enumerate(sequence_length.tolist()):
        if time_idx > 0:
            map_structure_zip(lambda xs, x: xs.append(x[time_idx - 1][batch_idx]), (final_state, all_state))
        else:
            map_structure_zip(lambda xs, x: xs.append(x[batch_idx]), (final_state, initial_state))
    final_state = map_structure(lambda x: torch.stack(x, dim=0), final_state)
    return final_outputs, final_state


def dynamic_rnn(cell: RNNCellBase[State], inputs: torch.Tensor, sequence_length: Optional[Union[torch.LongTensor, List[int]]]=None, initial_state: Optional[State]=None, time_major: bool=False) ->Tuple[torch.Tensor, State]:
    """Creates a recurrent neural network specified by RNNCell ``cell``.

    Performs fully dynamic unrolling of ``inputs``.

    Args:
        cell: An instance of RNNCell.
        inputs: The RNN inputs.
            If ``time_major == False`` (default), this must be a ``Tensor``
            of shape: ``[batch_size, max_time, ...]``, or a nested
            tuple of such elements.
            If ``time_major == True``, this must be a ``Tensor`` of shape:
            ``[max_time, batch_size, ...]``, or a nested tuple of such
            elements.
            This may also be a (possibly nested) tuple of Tensors satisfying
            this property.  The first two dimensions must match across all the
            inputs, but otherwise the ranks and other shape components
            may differ. In this case, input to ``cell`` at each time-step
            will replicate the structure of these tuples, except for the
            time dimension (from which the time is taken).
            The input to ``cell`` at each time step will be a
            ``Tensor`` or (possibly nested) tuple of Tensors each with
            dimensions ``[batch_size, ...]``.
        sequence_length: (optional) An int32/int64 tensor sized
            ``[batch_size]``. Used to copy-through state and
            zero-out outputs when past a batch element's sequence length.
            So it's more for performance than correctness.
        initial_state: (optional) An initial state for the RNN.
            If ``cell.state_size`` is an integer, this must be
            a ``Tensor`` of appropriate type and shape
            ``[batch_size, cell.state_size]``. If ``cell.state_size`` is
            a tuple, this should be a tuple of tensors having shapes
            ``[batch_size, s]`` for ``s`` in ``cell.state_size``.
        time_major: The shape format of the ``inputs`` and ``outputs`` Tensors.
            If true, these ``Tensors`` must be shaped
            ``[max_time, batch_size, depth]``. If false, these ``Tensors``
            must be shaped ``[batch_size, max_time, depth]``.
            Using ``time_major = True`` is a bit more efficient because
            it avoids transposes at the beginning and end of the
            RNN calculation. However, most TensorFlow data is batch-major,
            so by default this function accepts input and emits output in
            batch-major form.

    Returns:
        A pair (outputs, state) where:

        outputs: The RNN output ``Tensor``.

            If time_major == False (default), this will be a ``Tensor`` shaped:
            ``[batch_size, max_time, cell.output_size]``.

            If time_major == True, this will be a ``Tensor`` shaped:
            ``[max_time, batch_size, cell.output_size]``.

            Note, if ``cell.output_size`` is a (possibly nested) tuple of
            integers or ``torch.Size`` objects, then ``outputs``
            will be a tuple having the same structure as ``cell.output_size``,
            containing Tensors having shapes corresponding to the shape
            data in ``cell.output_size``.

        state: The final state.  If ``cell.state_size`` is an int, this
            will be shaped ``[batch_size, cell.state_size]``.  If it is a
            ``torch.Size``, this will be shaped
            ``[batch_size] + cell.state_size``.
            If it is a (possibly nested) tuple of ints or ``torch.Size``,
            this will be a tuple having the corresponding shapes.
            If cells are ``LSTMCells``, ``state`` will be a tuple containing
            a ``LSTMStateTuple`` for each cell.

    Raises:
        TypeError: If ``cell`` is not an instance of RNNCell.
        ValueError: If inputs is None or an empty list.
    """
    if not time_major:
        inputs = inputs.permute(1, 0, 2)
    time_steps = inputs.shape[0]
    batch_size = inputs.shape[1]
    if sequence_length is not None:
        if not isinstance(sequence_length, torch.Tensor):
            sequence_length = torch.tensor(sequence_length, dtype=torch.int32, device=inputs.device)
        if sequence_length.dim() != 1:
            raise ValueError('sequence_length must be a vector of length batch_size, but saw shape: %s' % sequence_length.shape)
        if sequence_length.shape != torch.Size([batch_size]):
            raise ValueError('Expected shape for Tensor sequence_length is %s' % batch_size, ' but saw shape: %s' % sequence_length.shape)
    else:
        sequence_length = torch.tensor([time_steps] * batch_size, dtype=torch.int32, device=inputs.device)
    if initial_state is not None:
        state = initial_state
    else:
        state = cell.zero_state(batch_size=batch_size)
    outputs, final_state = _dynamic_rnn_loop(cell, inputs, state, sequence_length=sequence_length)
    if not time_major:
        outputs = outputs.permute(1, 0, 2)
    return outputs, final_state


class UnidirectionalRNNEncoder(RNNEncoderBase[State]):
    """One directional RNN encoder.

    Args:
        input_size (int): The number of expected features in the input for the
            cell.
        cell: (RNNCell, optional) If not specified,
            a cell is created as specified in :attr:`hparams["rnn_cell"]`.
        output_layer (optional): An instance of
            :torch_nn:`Module`. Applies to the RNN cell
            output of each step. If `None` (default), the output layer is
            created as specified in :attr:`hparams["output_layer"]`.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    See :meth:`forward` for the inputs and outputs of the encoder.

    Example:

    .. code-block:: python

        # Use with embedder
        embedder = WordEmbedder(vocab_size, hparams=emb_hparams)
        encoder = UnidirectionalRNNEncoder(hparams=enc_hparams)

        outputs, final_state = encoder(
            inputs=embedder(data_batch['text_ids']),
            sequence_length=data_batch['length'])

    .. document private functions
    """
    _cell: RNNCellBase[State]

    def __init__(self, input_size: int, cell: Optional[RNNCellBase[State]]=None, output_layer: Optional[nn.Module]=None, hparams=None):
        super().__init__(hparams=hparams)
        if cell is not None:
            self._cell = cell
        else:
            self._cell = layers.get_rnn_cell(input_size, self._hparams.rnn_cell)
        self._output_layer: Optional[nn.Module]
        if output_layer is not None:
            self._output_layer = output_layer
            self._output_layer_hparams = None
        else:
            self._output_layer = _build_dense_output_layer(self._cell.hidden_size, self._hparams.output_layer)
            self._output_layer_hparams = self._hparams.output_layer

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "rnn_cell": default_rnn_cell_hparams(),
                "output_layer": {
                    "num_layers": 0,
                    "layer_size": 128,
                    "activation": "identity",
                    "final_layer_activation": None,
                    "other_dense_kwargs": None,
                    "dropout_layer_ids": [],
                    "dropout_rate": 0.5,
                    "variational_dropout": False
                },
                "name": "unidirectional_rnn_encoder"
            }

        Here:

        `"rnn_cell"`: dict
            A dictionary of RNN cell hyperparameters. Ignored if
            :attr:`cell` is given to the encoder constructor.

            The default value is defined in
            :func:`~texar.torch.core.default_rnn_cell_hparams`.

        `"output_layer"`: dict
            Output layer hyperparameters. Ignored if :attr:`output_layer`
            is given to the encoder constructor. Includes:

            `"num_layers"`: int
                The number of output (dense) layers. Set to 0 to avoid any
                output layers applied to the cell outputs.

            `"layer_size"`: int or list
                The size of each of the output (dense) layers.

                If an `int`, each output layer will have the same size. If
                a list, the length must equal to :attr:`num_layers`.

            `"activation"`: str or callable or None
                Activation function for each of the output (dense)
                layer except for the final layer. This can be
                a function, or its string name or module path.
                If function name is given, the function must be from
                :mod:`torch.nn`.
                For example:

                .. code-block:: python

                    "activation": "relu" # function name
                    "activation": "my_module.my_activation_fn" # module path
                    "activation": my_module.my_activation_fn # function

                Default is `None` which results in an identity activation.

            `"final_layer_activation"`: str or callable or None
                The activation function for the final output layer.

            `"other_dense_kwargs"`: dict or None
                Other keyword arguments to construct each of the output
                dense layers, e.g., ``bias``. See
                :torch_nn:`Linear` for the keyword arguments.

            `"dropout_layer_ids"`: int or list
                The indexes of layers (starting from 0) whose inputs
                are applied with dropout. The index = :attr:`num_layers`
                means dropout applies to the final layer output. For example,

                .. code-block:: python

                    {
                        "num_layers": 2,
                        "dropout_layer_ids": [0, 2]
                    }

                will leads to a series of layers as
                `-dropout-layer0-layer1-dropout-`.

                The dropout mode (training or not) is controlled
                by :attr:`self.training`.

            `"dropout_rate"`: float
                The dropout rate, between 0 and 1. For example,
                ``"dropout_rate": 0.1`` would zero out 10% of elements.

            `"variational_dropout"`: bool
                Whether the dropout mask is the same across all time steps.

        `"name"`: str
            Name of the encoder
        """
        hparams = RNNEncoderBase.default_hparams()
        hparams.update({'rnn_cell': layers.default_rnn_cell_hparams(), 'output_layer': _default_output_layer_hparams(), 'name': 'unidirectional_rnn_encoder'})
        return hparams

    def forward(self, inputs: torch.Tensor, sequence_length: Optional[Union[torch.LongTensor, List[int]]]=None, initial_state: Optional[State]=None, time_major: bool=False, return_cell_output: bool=False, return_output_size: bool=False):
        """Encodes the inputs.

        Args:
            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``.
                The first two dimensions
                :attr:`batch_size` and :attr:`max_time` are exchanged if
                :attr:`time_major` is `True`.
            sequence_length (optional): A 1D :tensor:`LongTensor` of shape
                ``[batch_size]``.
                Sequence lengths of the batch inputs. Used to copy-through
                state and zero-out outputs when past a batch element's sequence
                length.
            initial_state (optional): Initial state of the RNN.
            time_major (bool): The shape format of the :attr:`inputs` and
                :attr:`outputs` Tensors. If `True`, these tensors are of shape
                ``[max_time, batch_size, depth]``. If `False` (default),
                these tensors are of shape ``[batch_size, max_time, depth]``.
            return_cell_output (bool): Whether to return the output of the RNN
                cell. This is the results prior to the output layer.
            return_output_size (bool): Whether to return the size of the
                output (i.e., the results after output layers).

        Returns:
            - By default (both ``return_cell_output`` and ``return_output_size``
              are `False`), returns a pair :attr:`(outputs, final_state)`,
              where

              - :attr:`outputs`: The RNN output tensor by the output layer
                (if exists) or the RNN cell (otherwise). The tensor is of
                shape ``[batch_size, max_time, output_size]`` if
                ``time_major`` is `False`, or
                ``[max_time, batch_size, output_size]`` if
                ``time_major`` is `True`.
                If RNN cell output is a (nested) tuple of Tensors, then the
                :attr:`outputs` will be a (nested) tuple having the same
                nest structure as the cell output.

              - :attr:`final_state`: The final state of the RNN, which is a
                Tensor of shape ``[batch_size] + cell.state_size`` or
                a (nested) tuple of Tensors if ``cell.state_size`` is a
                (nested) tuple.

            - If ``return_cell_output`` is True, returns a triple
              :attr:`(outputs, final_state, cell_outputs)`

              - :attr:`cell_outputs`: The outputs by the RNN cell prior to the
                output layer, having the same structure with :attr:`outputs`
                except for the ``output_dim``.

            - If ``return_output_size`` is `True`, returns a tuple
              :attr:`(outputs, final_state, output_size)`

              - :attr:`output_size`: A (possibly nested tuple of) int
                representing the size of :attr:`outputs`. If a single int or
                an int array, then ``outputs`` has shape
                ``[batch/time, time/batch] + output_size``. If
                a (nested) tuple, then ``output_size`` has the same
                structure as with ``outputs``.

            - If both ``return_cell_output`` and ``return_output_size`` are
              `True`, returns
              :attr:`(outputs, final_state, cell_outputs, output_size)`.
        """
        cell_outputs, state = dynamic_rnn(cell=self._cell, inputs=inputs, sequence_length=sequence_length, initial_state=initial_state, time_major=time_major)
        outputs, output_size = _forward_output_layers(inputs=cell_outputs, output_layer=self._output_layer, time_major=time_major, sequence_length=sequence_length)
        rets = outputs, state
        if return_cell_output:
            rets += cell_outputs,
        if return_output_size:
            rets += output_size,
        return rets

    @property
    def cell(self) ->RNNCellBase[State]:
        """The RNN cell.
        """
        return self._cell

    @property
    def state_size(self) ->int:
        """The state size of encoder cell.
        Same as :attr:`encoder.cell.state_size`.
        """
        if isinstance(self._cell, LSTMCell):
            return 2 * self._cell.hidden_size
        else:
            return self._cell.hidden_size

    @property
    def output_layer(self) ->Optional[nn.Module]:
        """The output layer.
        """
        return self._output_layer

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output :attr:`outputs`.
        If output layer does not exist, the feature size is equal to
        :attr:`encoder.cell.hidden_size`, otherwise the feature size
        is equal to last dimension value of output layer output size.
        """
        dim = self._cell.hidden_size
        if self._output_layer is not None:
            dummy_tensor = torch.Tensor(dim)
            dim = self._output_layer(dummy_tensor).size(-1)
        return dim


class UnidirectionalRNNClassifier(ClassifierBase):
    """One directional RNN classifier. This is a combination of the
    :class:`~texar.torch.modules.UnidirectionalRNNEncoder` with a classification
    layer. Both step-wise classification and sequence-level classification
    are supported, specified in :attr:`hparams`.

    Arguments are the same as in
    :class:`~texar.torch.modules.UnidirectionalRNNEncoder`.

    Args:
        input_size (int): The number of expected features in the input for the
            cell.
        cell: (RNNCell, optional) If not specified,
            a cell is created as specified in :attr:`hparams["rnn_cell"]`.
        output_layer (optional): An instance of
            :torch_nn:`Module`. Applies to the RNN cell
            output of each step. If `None` (default), the output layer is
            created as specified in :attr:`hparams["output_layer"]`.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.
    """

    def __init__(self, input_size: int, cell: Optional[RNNCellBase[State]]=None, output_layer: Optional[nn.Module]=None, hparams=None):
        super().__init__(hparams=hparams)
        encoder_hparams = dict_fetch(hparams, UnidirectionalRNNEncoder.default_hparams())
        self._encoder = UnidirectionalRNNEncoder(input_size=input_size, cell=cell, output_layer=output_layer, hparams=encoder_hparams)
        self.num_classes = self._hparams.num_classes
        if self.num_classes <= 0:
            self._logits_layer = None
        else:
            logit_kwargs = self._hparams.logit_layer_kwargs
            if logit_kwargs is None:
                logit_kwargs = {}
            elif not isinstance(logit_kwargs, HParams):
                raise ValueError("hparams['logit_layer_kwargs'] must be a dict.")
            else:
                logit_kwargs = logit_kwargs.todict()
            if self._hparams.clas_strategy == 'all_time':
                self._logits_layer = nn.Linear(self._encoder.output_size * self._hparams.max_seq_length, self.num_classes, **logit_kwargs)
            else:
                self._logits_layer = nn.Linear(self._encoder.output_size, self.num_classes, **logit_kwargs)
        self.is_binary = self.num_classes == 1 or self.num_classes <= 0 and self._encoder.output_size == 1

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in UnidirectionalRNNEncoder
                ...

                # (2) Additional hyperparameters
                "num_classes": 2,
                "logit_layer_kwargs": None,
                "clas_strategy": "final_time",
                "max_seq_length": None,
                "name": "unidirectional_rnn_classifier"
            }

        Here:

        1. Same hyperparameters as in
           :class:`~texar.torch.modules.UnidirectionalRNNEncoder`.
           See the
           :meth:`~texar.torch.modules.UnidirectionalRNNEncoder.default_hparams`
           . An instance of UnidirectionalRNNEncoder is created for feature
           extraction.

        2. Additional hyperparameters:

            `"num_classes"`: int
                Number of classes:

                - If **> 0**, an additional `Linear`
                  layer is appended to the encoder to compute the logits over
                  classes.
                - If **<= 0**, no dense layer is appended. The number of
                  classes is assumed to be the final dense layer size of the
                  encoder.

            `"logit_layer_kwargs"`: dict
                Keyword arguments for the logit Dense layer constructor,
                except for argument "units" which is set to `num_classes`.
                Ignored if no extra logit layer is appended.

            `"clas_strategy"`: str
                The classification strategy, one of:

                - **final_time**: Sequence-level classification based on the
                  output of the final time step. Each sequence has a class.
                - **all_time**: Sequence-level classification based on
                  the output of all time steps. Each sequence has a class.
                - **time_wise**: Step-wise classification, i.e., make
                  classification for each time step based on its output.

            `"max_seq_length"`: int, optional
                Maximum possible length of input sequences. Required if
                `clas_strategy` is `all_time`.

            `"name"`: str
                Name of the classifier.
        """
        hparams = UnidirectionalRNNEncoder.default_hparams()
        hparams.update({'num_classes': 2, 'logit_layer_kwargs': None, 'clas_strategy': 'final_time', 'max_seq_length': None, 'name': 'bert_classifier'})
        return hparams

    def forward(self, inputs: torch.Tensor, sequence_length: Optional[torch.LongTensor]=None, initial_state: Optional[State]=None, time_major: bool=False) ->Tuple[torch.Tensor, torch.LongTensor]:
        """Feeds the inputs through the network and makes classification.

        The arguments are the same as in
        :class:`~texar.torch.modules.UnidirectionalRNNEncoder`.

        Args:
            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``.
                The first two dimensions
                :attr:`batch_size` and :attr:`max_time` are exchanged if
                :attr:`time_major` is `True`.
            sequence_length (optional): A 1D :tensor:`LongTensor` of shape
                ``[batch_size]``.
                Sequence lengths of the batch inputs. Used to copy-through
                state and zero-out outputs when past a batch element's sequence
                length.
            initial_state (optional): Initial state of the RNN.
            time_major (bool): The shape format of the :attr:`inputs` and
                :attr:`outputs` Tensors. If `True`, these tensors are of shape
                ``[max_time, batch_size, depth]``. If `False` (default),
                these tensors are of shape ``[batch_size, max_time, depth]``.

        Returns:
            A tuple `(logits, preds)`, containing the logits over classes and
            the predictions, respectively.

            - If ``clas_strategy`` is ``final_time`` or ``all_time``:

                - If ``num_classes`` == 1, ``logits`` and ``pred`` are both of
                  shape ``[batch_size]``.
                - If ``num_classes`` > 1, ``logits`` is of shape
                  ``[batch_size, num_classes]`` and ``pred`` is of shape
                  ``[batch_size]``.

            - If ``clas_strategy`` is ``time_wise``:

                - ``num_classes`` == 1, ``logits`` and ``pred`` are both of
                  shape ``[batch_size, max_time]``.
                - If ``num_classes`` > 1, ``logits`` is of shape
                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of
                  shape ``[batch_size, max_time]``.
                - If ``time_major`` is `True`, the batch and time dimensions
                  are exchanged.
        """
        enc_outputs, _ = self._encoder(inputs=inputs, sequence_length=sequence_length, initial_state=initial_state, time_major=time_major)
        strategy = self._hparams.clas_strategy
        if strategy == 'time_wise':
            logits = enc_outputs
        elif strategy == 'final_time':
            if time_major:
                logits = enc_outputs[(-1), :, :]
            else:
                logits = enc_outputs[:, (-1), :]
        elif strategy == 'all_time':
            if time_major:
                length_diff = self._hparams.max_seq_length - inputs.shape[0]
                logit_input = F.pad(enc_outputs, [0, length_diff, 0, 0, 0, 0])
                logit_input_dim = self._encoder.output_size * self._hparams.max_seq_length
                logits = logit_input.view(-1, logit_input_dim)
            else:
                length_diff = self._hparams.max_seq_length - inputs.shape[1]
                logit_input = F.pad(enc_outputs, [0, 0, 0, length_diff, 0, 0])
                logit_input_dim = self._encoder.output_size * self._hparams.max_seq_length
                logits = logit_input.view(-1, logit_input_dim)
        else:
            raise ValueError('Unknown classification strategy: {}'.format(strategy))
        if self._logits_layer is not None:
            logits = self._logits_layer(logits)
        if strategy == 'time_wise':
            if self.is_binary:
                logits = torch.squeeze(logits, -1)
                preds = (logits > 0).long()
            else:
                preds = torch.argmax(logits, dim=-1)
        else:
            if self.is_binary:
                preds = (logits > 0).long()
                logits = torch.flatten(logits)
            else:
                preds = torch.argmax(logits, dim=-1)
            preds = torch.flatten(preds)
        return logits, preds

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output :attr:`logits`.
        If :attr:`logits` size is only determined by input
        (i.e. if ``num_classes`` == 1), the feature size is equal to ``-1``.
        Otherwise it is equal to last dimension value of :attr:`logits` size.
        """
        if self._hparams.num_classes == 1:
            logit_dim = -1
        elif self._hparams.num_classes > 1:
            logit_dim = self._hparams.num_classes
        elif self._hparams.clas_strategy == 'all_time':
            logit_dim = self._encoder.output_size * self._hparams.max_seq_length
        elif self._hparams.clas_strategy == 'final_time':
            logit_dim = self._encoder.output_size
        elif self._hparams.clas_strategy == 'time_wise':
            logit_dim = self._hparams.encoder.dim
        return logit_dim


class PositionWiseFF(ModuleBase):

    def __init__(self, hparams=None):
        super().__init__(hparams=hparams)
        hidden_dim = self._hparams.hidden_dim
        ffn_inner_dim = self._hparams.ffn_inner_dim
        dropout = self._hparams.dropout
        activation = self._hparams.activation.capitalize()
        if activation == 'Relu':
            activation = 'ReLU'
        elif activation == 'Gelu':
            activation = 'GPTGELU'
        self.linear1 = nn.Linear(hidden_dim, ffn_inner_dim)
        self.activation_fn = get_layer({'type': activation})
        self.dropout = nn.Dropout(dropout, inplace=True)
        self.linear2 = nn.Linear(ffn_inner_dim, hidden_dim)
        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        return {'hidden_dim': 768, 'ffn_inner_dim': 3072, 'dropout': 0.1, 'activation': 'relu'}

    @property
    def output_size(self):
        """The feature size of :meth:`forward` output.
        """
        return self._hparams.hidden_dim

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        output = self.linear1(input)
        output = self.activation_fn(output)
        output = self.dropout(output)
        output = self.linear2(output)
        output = self.dropout(output)
        output = self.layer_norm(input + output)
        return output


class RelativeMultiheadAttention(ModuleBase):

    def __init__(self, r_r_bias: Optional[nn.Parameter]=None, r_w_bias: Optional[nn.Parameter]=None, r_s_bias: Optional[nn.Parameter]=None, hparams=None):
        super().__init__(hparams=hparams)
        self.num_heads = self._hparams.num_heads
        self.head_dim = self._hparams.head_dim
        hidden_dim = self._hparams.hidden_dim
        self.head_projection = nn.Linear(hidden_dim, 3 * self.num_heads * self.head_dim, bias=False)
        self.pos_projection = nn.Linear(hidden_dim, self.num_heads * self.head_dim, bias=False)
        self.dropout = nn.Dropout(self._hparams.dropout)
        self.dropout_attn = nn.Dropout(self._hparams.attention_dropout)
        self.output_projection = nn.Linear(self.num_heads * self.head_dim, hidden_dim, bias=False)
        bias_shape = self.num_heads, self.head_dim
        self.untie_r = r_r_bias is None
        self.r_r_bias = r_r_bias if r_r_bias is not None else nn.Parameter(torch.Tensor(*bias_shape))
        self.r_w_bias = r_w_bias if r_w_bias is not None else nn.Parameter(torch.Tensor(*bias_shape))
        if self._hparams.use_segments:
            self.segment_embed = nn.Parameter(torch.Tensor(2, self.num_heads, self.head_dim))
            self.r_s_bias = r_s_bias if r_s_bias is not None else nn.Parameter(torch.Tensor(*bias_shape))
        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)
        self.scale = 1 / self.head_dim ** 0.5
        self.reset_parameters()

    def reset_parameters(self):
        if self.untie_r:
            nn.init.normal_(self.r_w_bias, 0.0, 0.02)
            nn.init.normal_(self.r_r_bias, 0.0, 0.02)
        if self._hparams.use_segments:
            nn.init.normal_(self.segment_embed, 0.0, 0.02)
            if self.untie_r:
                nn.init.normal_(self.r_s_bias, 0.0, 0.02)

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        return {'num_heads': 12, 'hidden_dim': 768, 'head_dim': 64, 'dropout': 0.1, 'attention_dropout': 0.1, 'use_segments': True}

    @property
    def output_size(self):
        """The feature size of :meth:`forward` output
        :attr:`output_h`.
        """
        return self._hparams.hidden_dim

    @staticmethod
    def _rel_shift(x: torch.Tensor, klen: int) ->torch.Tensor:
        shape = x.size()
        x = x.view(shape[1], shape[0], *shape[2:])[1:]
        x = x.view(shape[0], shape[1] - 1, *shape[2:])[:, :klen]
        return x

    def _compute_attention_score(self, q_head: torch.Tensor, k_head_h: torch.Tensor, v_head_h: torch.Tensor, k_head_r: torch.Tensor, segment_mat: Optional[torch.Tensor]=None, attn_mask: Optional[torch.Tensor]=None) ->torch.Tensor:
        q_head_rw = q_head + self.r_w_bias
        attn_ac = torch.einsum('ibnd,jbnd->ijbn', [q_head_rw, k_head_h])
        q_head_rr = q_head + self.r_r_bias
        attn_bd = torch.einsum('ibnd,jbnd->ijbn', [q_head_rr, k_head_r])
        attn_bd = self._rel_shift(attn_bd, klen=attn_ac.size(1))
        if segment_mat is None:
            attn_ef = 0
        else:
            q_head_rs = q_head + self.r_s_bias
            attn_ef = torch.einsum('ibnd,snd->ibns', [q_head_rs, self.segment_embed])
            attn_ef = torch.einsum('ijbs,ibns->ijbn', [segment_mat, attn_ef])
        attn_score = attn_ac + attn_bd + attn_ef
        attn_score.mul_(self.scale)
        if attn_mask is not None:
            if attn_mask.dim() == 2:
                attn_mask = attn_mask[(None), :, :, (None)]
            elif attn_mask.dim() == 3:
                attn_mask = attn_mask[:, :, :, (None)]
            attn_score = attn_score.float().masked_fill(attn_mask, -1e+30).type_as(attn_score)
        attn_prob = F.softmax(attn_score, dim=1)
        attn_prob = self.dropout_attn(attn_prob)
        attn_vec = torch.einsum('ijbn,jbnd->ibnd', [attn_prob, v_head_h])
        return attn_vec.contiguous()

    def _post_attention(self, attn_vec: torch.Tensor) ->torch.Tensor:
        attn_vec = attn_vec.view(*attn_vec.size()[:2], -1)
        attn_out = self.output_projection(attn_vec)
        attn_out = self.dropout(attn_out)
        return attn_out

    def forward(self, states_h: torch.Tensor, pos_embed: torch.Tensor, states_g: Optional[torch.Tensor]=None, segment_mat: Optional[torch.Tensor]=None, attn_mask_h: Optional[torch.Tensor]=None, attn_mask_g: Optional[torch.Tensor]=None, target_mapping: Optional[torch.Tensor]=None, memory: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, Optional[torch.Tensor]]:
        seq_len, batch_size = states_h.size()[:2]
        pos_len = pos_embed.size(0)
        if memory is not None and memory.dim() > 1:
            concat_input = torch.cat([memory, states_h], dim=0)
        else:
            concat_input = states_h
        heads = self.head_projection(concat_input)
        q_head_h, k_head_h, v_head_h = torch.chunk(heads, 3, dim=-1)
        q_head_h = q_head_h[-seq_len:]
        tot_len = k_head_h.size(0)
        q_head_h = q_head_h.view(seq_len, batch_size, self.num_heads, self.head_dim)
        k_head_h = k_head_h.view(tot_len, batch_size, self.num_heads, self.head_dim)
        v_head_h = v_head_h.view(tot_len, batch_size, self.num_heads, self.head_dim)
        k_head_r = self.pos_projection(pos_embed)
        k_head_r = k_head_r.view(pos_len, batch_size, self.num_heads, self.head_dim)
        attn_vec_h = self._compute_attention_score(q_head_h, k_head_h, v_head_h, k_head_r, segment_mat, attn_mask_h)
        attn_out_h = self._post_attention(attn_vec_h)
        output_h = self.layer_norm(states_h + attn_out_h)
        output_g = None
        if states_g is not None:
            proj_dim = self.num_heads * self.head_dim
            proj_weight = self.head_projection.weight[:proj_dim]
            q_head_g = F.linear(states_g, proj_weight)
            q_head_g = q_head_g.view(q_head_g.size(0), batch_size, self.num_heads, self.head_dim)
            if target_mapping is not None:
                q_head_g = torch.einsum('mbnd,mlb->lbnd', [q_head_g, target_mapping])
            attn_vec_g = self._compute_attention_score(q_head_g, k_head_h, v_head_h, k_head_r, segment_mat, attn_mask_g)
            if target_mapping is not None:
                attn_vec_g = torch.einsum('lbnd,mlb->mbnd', [attn_vec_g, target_mapping])
            attn_out_g = self._post_attention(attn_vec_g)
            output_g = self.layer_norm(states_g + attn_out_g)
        return output_h, output_g


_XLNET_PATH = 'https://storage.googleapis.com/xlnet/released_models/'


def init_weights(module: nn.Module):
    if isinstance(module, nn.Linear):
        nn.init.normal_(module.weight, 0.0, 0.02)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        nn.init.normal_(module.weight, 0.0, 0.02)


class PretrainedXLNetMixin(PretrainedMixin, ABC):
    """A mixin class to support loading pre-trained checkpoints for modules
    that implement the XLNet model.

    The XLNet model was proposed in
    `XLNet: Generalized Autoregressive Pretraining for Language Understanding`_
    by `Yang et al.` It is based on the Transformer-XL model, pre-trained on a
    large corpus using a language modeling objective that considers all
    permutations of the input sentence.

    The available XLNet models are as follows:

      * ``xlnet-based-cased``: 12-layer, 768-hidden, 12-heads. This model is
        trained on full data (different from the one in the paper).
      * ``xlnet-large-cased``: 24-layer, 1024-hidden, 16-heads.

    We provide the following XLNet classes:

      * :class:`~texar.torch.modules.XLNetEncoder` for text encoding.
      * :class:`~texar.torch.modules.XLNetDecoder` for text generation and
        decoding.
      * :class:`~texar.torch.modules.XLNetClassifier` for text classification
        and sequence tagging.
      * :class:`~texar.torch.modules.XLNetRegressor` for text regression.

    .. _`XLNet: Generalized Autoregressive Pretraining for Language Understanding`:
        http://arxiv.org/abs/1906.08237
    """
    _MODEL_NAME = 'XLNet'
    _MODEL2URL = {'xlnet-base-cased': _XLNET_PATH + 'cased_L-12_H-768_A-12.zip', 'xlnet-large-cased': _XLNET_PATH + 'cased_L-24_H-1024_A-16.zip'}

    def reset_parameters(self):
        self.apply(init_weights)
        if not self._hparams.untie_r:
            nn.init.normal_(self.r_w_bias, 0.0, 0.02)
            nn.init.normal_(self.r_r_bias, 0.0, 0.02)
            if self._hparams.use_segments:
                nn.init.normal_(self.r_s_bias, 0.0, 0.02)

    @classmethod
    def _transform_config(cls, pretrained_model_name: str, cache_dir: str) ->Dict[str, Any]:
        info = list(os.walk(cache_dir))
        root, _, files = info[0]
        config_path = None
        for file in files:
            if file.endswith('config.json'):
                config_path = os.path.join(root, file)
        if config_path is None:
            raise ValueError(f'Cannot find the config file in {cache_dir}')
        with open(config_path) as f:
            config_ckpt = json.loads(f.read())
        configs = {'head_dim': config_ckpt['d_head'], 'ffn_inner_dim': config_ckpt['d_inner'], 'hidden_dim': config_ckpt['d_model'], 'activation': config_ckpt['ff_activation'], 'num_heads': config_ckpt['n_head'], 'num_layers': config_ckpt['n_layer'], 'vocab_size': config_ckpt['n_token'], 'untie_r': config_ckpt['untie_r']}
        return configs

    def _init_from_checkpoint(self, pretrained_model_name: str, cache_dir: str, **kwargs):
        try:
            import numpy as np
            import tensorflow as tf
        except ImportError:
            None
            raise
        ckpt = tf.train.load_checkpoint(os.path.join(cache_dir, 'xlnet_model.ckpt'))
        from_params: Dict[str, np.ndarray] = {key: ckpt.get_tensor(key) for key in ckpt.get_variable_to_shape_map().keys()}
        del from_params['global_step']
        to_params: Dict[str, nn.Parameter] = dict(self.named_parameters())

        def get_weight(name: str) ->torch.Tensor:
            weight = from_params['model/' + name]
            del from_params['model/' + name]
            return torch.from_numpy(weight)
        TransFn = Callable[[torch.Tensor], torch.Tensor]

        def assign(param: nn.Parameter, weight: Union[str, torch.Tensor], trans_fn: Optional[TransFn]=None, allow_fail: bool=False):
            param_key = next(k for k, v in to_params.items() if v is param)
            del to_params[param_key]
            if isinstance(weight, str):
                try:
                    weight = get_weight(weight)
                except KeyError:
                    if allow_fail:
                        None
                        return
                    else:
                        raise
            if trans_fn is not None:
                weight = trans_fn(weight).contiguous()
            if param.size() != weight.size():
                raise ValueError(f'Expected size {param.size()}, actual size {weight.size()}')
            param.data = weight

        def assign_linear(linear: nn.Linear, prefix: str):
            trans_fn = lambda p: p.view(p.size(0), -1).t()
            assign(linear.weight, prefix + 'kernel', trans_fn)
            if linear.bias is not None:
                assign(linear.bias, prefix + 'bias')

        def assign_layer_norm(layer_norm: nn.LayerNorm, prefix: str):
            assign(layer_norm.weight, prefix + 'LayerNorm/gamma')
            assign(layer_norm.bias, prefix + 'LayerNorm/beta')

        def load_xlnet_model(xlnet):
            n_layers = len(xlnet.attn_layers)
            for bias_name in ['r_r_bias', 'r_w_bias', 'r_s_bias']:
                weight = get_weight('transformer/' + bias_name)
                if xlnet.hparams.untie_r:
                    for idx in range(n_layers):
                        layer: RelativeMultiheadAttention
                        layer = xlnet.attn_layers[idx]
                        assign(getattr(layer, bias_name), weight[idx])
                else:
                    assign(getattr(xlnet, bias_name), weight)
            assign(xlnet.word_embed.weight, 'transformer/word_embedding/lookup_table')
            for idx in range(n_layers):
                layer: RelativeMultiheadAttention = xlnet.attn_layers[idx]
                prefix = f'transformer/layer_{idx}/rel_attn/'
                qkv_weights = [get_weight(prefix + f'{part}/kernel') for part in 'qkv']
                assign(layer.head_projection.weight, torch.cat([p.view(p.size(0), -1) for p in qkv_weights], dim=1).t())
                assign_linear(layer.pos_projection, prefix + 'r/')
                assign(layer.output_projection.weight, prefix + 'o/kernel', lambda p: p.view(p.size(0), -1))
                assign_layer_norm(layer.layer_norm, prefix)
            for idx in range(n_layers):
                layer: PositionWiseFF = xlnet.ff_layers[idx]
                prefix = f'transformer/layer_{idx}/ff/'
                for linear_idx in range(1, 2 + 1):
                    linear_prefix = f'{prefix}layer_{linear_idx}/'
                    linear_layer: nn.Linear = getattr(layer, f'linear{linear_idx}')
                    assign_linear(linear_layer, linear_prefix)
                assign_layer_norm(layer.layer_norm, prefix)
            seg_embeds = [p.squeeze(0) for p in torch.chunk(get_weight('transformer/seg_embed'), n_layers, dim=0)]
            for idx in range(n_layers):
                assign(xlnet.attn_layers[idx].segment_embed, seg_embeds[idx])
            if hasattr(xlnet, 'mask_emb') and hasattr(xlnet, 'lm_bias'):
                assign(xlnet.mask_emb, 'transformer/mask_emb/mask_emb')
                assign(xlnet.lm_bias, 'lm_loss/bias')
        load_xlnet_model(self)
        if len(from_params) > 0:
            None
        filtered_to_params = [k for k in to_params if k.startswith('xlnet')]
        if len(filtered_to_params) > 0:
            None


class PositionalEmbedding(nn.Module):
    inv_freq: torch.Tensor

    def __init__(self, embed_dim: int):
        super().__init__()
        freq_seq = torch.arange(0.0, embed_dim, 2.0)
        inv_freq = 1 / 10000 ** (freq_seq / embed_dim)
        self.register_buffer('inv_freq', inv_freq)

    def forward(self, pos_seq: torch.Tensor) ->torch.Tensor:
        sinusoid = torch.ger(pos_seq, self.inv_freq)
        pos_embed = torch.cat([sinusoid.sin(), sinusoid.cos()], dim=-1)
        return pos_embed


class RelativePositionalEncoding(ModuleBase):

    def __init__(self, hparams=None):
        super().__init__(hparams=hparams)
        self.sinusoid_embed = PositionalEmbedding(self._hparams.dim)

    @staticmethod
    def default_hparams():
        return {'dim': 768, 'max_seq_len': 512}

    @property
    def output_size(self):
        """The feature size of :meth:`forward` output.
        """
        return self._hparams.dim

    def _create_positional_embedding(self, start: int, end: int, step: int, batch_size: int, clamp_len: Optional[int]=None) ->torch.Tensor:
        embed_buffer = next(self.sinusoid_embed.buffers())
        pos_seq = torch.arange(start, end, step, device=embed_buffer.device, dtype=embed_buffer.dtype)
        if clamp_len is not None:
            pos_seq = torch.clamp(pos_seq, -clamp_len, clamp_len)
        pos_embed = self.sinusoid_embed(pos_seq)
        pos_embed = pos_embed.unsqueeze(1).expand(-1, batch_size, -1)
        return pos_embed

    def forward(self, batch_size: int, seq_len: int, total_len: int, clamp_len: Optional[int]=None, attn_type: str='bi', bi_data: bool=True) ->torch.Tensor:
        if attn_type == 'bi':
            start, end = total_len, -seq_len
        elif attn_type == 'uni':
            start, end = total_len, -1
        else:
            raise ValueError(f'Unknown `attn_type` {attn_type}')
        if bi_data:
            if batch_size % 2 != 0:
                raise ValueError('`batch_size` must be an even number')
            fwd_pos_embed = self._create_positional_embedding(start, end, -1, batch_size // 2, clamp_len)
            bwd_pos_embed = self._create_positional_embedding(-start, -end, 1, batch_size // 2, clamp_len)
            pos_embed = torch.cat([fwd_pos_embed, bwd_pos_embed], dim=1)
        else:
            pos_embed = self._create_positional_embedding(start, end, -1, batch_size, clamp_len)
        return pos_embed


def params_except_in(module: nn.Module, except_names: List[str]) ->Iterable[nn.Parameter]:
    return itertools.chain.from_iterable(child.parameters() for name, child in module.named_children() if name not in except_names)


def sum_tensors(xs: List[Optional[torch.Tensor]]) ->Optional[torch.Tensor]:
    """Sum a list of tensors with possible `None` values.

    Args:
        xs: A list of tensors.

    Returns:
        The summation of all the elements in the list.
    """
    idx = next((idx for idx, tensor in enumerate(xs) if tensor is not None), -1)
    if idx == -1:
        return None
    ret = xs[idx]
    for tensor in xs[idx + 1:]:
        if tensor is not None:
            ret = ret + tensor
    return ret


class XLNetEncoder(EncoderBase, PretrainedXLNetMixin):
    """Raw XLNet module for encoding sequences. Please see
    :class:`~texar.torch.modules.PretrainedXLNetMixin` for a brief description
    of XLNet.

    Args:
        pretrained_model_name (optional): a `str`, the name
            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to
            :class:`~texar.torch.modules.PretrainedXLNetMixin` for
            all supported models.
            If `None`, the model name in :attr:`hparams` is used.
        cache_dir (optional): the path to a folder in which the
            pre-trained models will be cached. If `None` (default),
            a default directory (``texar_data`` folder under user's home
            directory) will be used.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameter will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure
            and default values.
    """
    _IS_DECODE = False

    def __init__(self, pretrained_model_name: Optional[str]=None, cache_dir: Optional[str]=None, hparams=None):
        super().__init__(hparams=hparams)
        self.load_pretrained_config(pretrained_model_name, cache_dir)
        num_layers = self._hparams.num_layers
        num_heads = self._hparams.num_heads
        head_dim = self._hparams.head_dim
        self.word_embed = nn.Embedding(self._hparams.vocab_size, self._hparams.hidden_dim)
        self.pos_embed = RelativePositionalEncoding(hparams={'dim': self._hparams.hidden_dim, 'max_seq_len': self._hparams.max_seq_length})
        self.dropout = nn.Dropout(self._hparams.dropout)
        self.r_r_bias = None
        self.r_w_bias = None
        self.r_s_bias = None
        if not self._hparams.untie_r:
            self.r_r_bias = nn.Parameter(torch.Tensor(num_heads, head_dim))
            self.r_w_bias = nn.Parameter(torch.Tensor(num_heads, head_dim))
            self.r_s_bias = nn.Parameter(torch.Tensor(num_heads, head_dim)) if self._hparams.use_segments else None
        self.attn_layers = nn.ModuleList()
        self.ff_layers = nn.ModuleList()
        rel_attn_hparams = dict_fetch(self._hparams, RelativeMultiheadAttention.default_hparams())
        ff_hparams = dict_fetch(self._hparams, PositionWiseFF.default_hparams())
        for _ in range(num_layers):
            self.attn_layers.append(RelativeMultiheadAttention(self.r_r_bias, self.r_w_bias, self.r_s_bias, hparams=rel_attn_hparams))
            self.ff_layers.append(PositionWiseFF(hparams=ff_hparams))
        self.mask_emb = nn.Parameter(torch.Tensor(1, 1, self._hparams.hidden_dim))
        if self._IS_DECODE:
            self.lm_bias = nn.Parameter(torch.zeros(self._hparams.vocab_size))
        self.init_pretrained_weights()

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.

        * The encoder arch is determined by the constructor argument
          :attr:`pretrained_model_name` if it's specified. In this case,
          `hparams` are ignored.
        * Otherwise, the encoder arch is determined by
          `hparams['pretrained_model_name']` if it's specified. All other
          configurations in `hparams` are ignored.
        * If the above two are `None`, the encoder arch is defined by the
          configurations in `hparams` and weights are randomly initialized.

        .. code-block:: python

            {
                "pretrained_model_name": "xlnet-base-cased",
                "untie_r": True,
                "num_layers": 12,
                "mem_len": 0,
                "reuse_len": 0,
                "num_heads": 12,
                "hidden_dim": 768,
                "head_dim": 64,
                "dropout": 0.1,
                "attention_dropout": 0.1,
                "use_segments": True,
                "ffn_inner_dim": 3072,
                "activation": 'gelu',
                "vocab_size": 32000,
                "max_seq_length": 512,
                "initializer": None,
                "name": "xlnet_encoder",
            }

        Here:

        The default parameters are values for cased XLNet-Base model.

        `"pretrained_model_name"`: str or None
            The name of the pre-trained XLNet model. If None, the model
            will be randomly initialized.

        `"untie_r"`: bool
            Whether to untie the biases in attention.

        `"num_layers"`: int
            The number of stacked layers.

        `"mem_len"`: int
            The number of tokens to cache.

        `"reuse_len"`: int
            The number of tokens in the current batch to be cached and reused
            in the future.

        `"num_heads"`: int
            The number of attention heads.

        `"hidden_dim"`: int
            The hidden size.

        `"head_dim"`: int
            The dimension size of each attention head.

        `"dropout"`: float
            Dropout rate.

        `"attention_dropout"`: float
            Dropout rate on attention probabilities.

        `"use_segments"`: bool
            Whether to use segment embedding.

        `"ffn_inner_dim"`: int
            The hidden size in feed-forward layers.

        `"activation"`: str
            `relu` or `gelu`.

        `"vocab_size"`: int
            The vocabulary size.

        `"max_seq_length"`: int
            The maximum sequence length for `RelativePositionalEncoding`.

        `"initializer"`: dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.torch.core.get_initializer` for details.

        `"name"`: str
            Name of the module.
        """
        return {'pretrained_model_name': 'xlnet-base-cased', 'untie_r': True, 'num_layers': 12, 'mem_len': 0, 'reuse_len': 0, 'num_heads': 12, 'hidden_dim': 768, 'head_dim': 64, 'dropout': 0.1, 'attention_dropout': 0.1, 'use_segments': True, 'ffn_inner_dim': 3072, 'activation': 'gelu', 'vocab_size': 32000, 'max_seq_length': 512, 'initializer': None, 'name': 'xlnet_encoder', '@no_typecheck': ['pretrained_model_name']}

    def param_groups(self, lr: Optional[float]=None, lr_layer_scale: float=1.0, decay_base_params: bool=False):
        """Create parameter groups for optimizers. When
        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form
        separate groups with different base learning rates.

        The return value of this method can be used in the constructor of
        optimizers, for example:

        .. code-block:: python

            model = XLNetEncoder(...)
            param_groups = model.param_groups(lr=2e-5, lr_layer_scale=0.8)
            optim = torch.optim.Adam(param_groups)

        Args:
            lr (float): The learning rate. Can be omitted if
                :attr:`lr_layer_decay_rate` is 1.0.
            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer
                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.
            decay_base_params (bool): If `True`, treat non-layer parameters
                (e.g. embeddings) as if they're in layer 0. If `False`, these
                parameters are not scaled.

        Returns:
            The parameter groups, used as the first argument for optimizers.
        """
        if lr_layer_scale != 1.0:
            if lr is None:
                raise ValueError('lr must be specified when lr_layer_decay_rate is not 1.0')
            num_layers = self._hparams.num_layers
            base_group = {'params': params_except_in(self, ['attn_layers', 'ff_layers']), 'lr': lr * (lr_layer_scale ** num_layers if decay_base_params else 1.0)}
            param_groups = [base_group]
            for idx in range(num_layers):
                decay_rate = lr_layer_scale ** (num_layers - idx - 1)
                param_group = {'params': [*self.attn_layers[idx].parameters(), *self.ff_layers[idx].parameters()], 'lr': lr * decay_rate}
                param_groups.append(param_group)
            return param_groups
        return self.parameters()

    @property
    def output_size(self):
        """The feature size of :meth:`forward` output.
        """
        return self._hparams.hidden_dim

    @staticmethod
    def _cache_mem(output: torch.Tensor, prev_mem: Optional[torch.Tensor], mem_len: int, reuse_len: int=0) ->torch.Tensor:
        """Cache hidden states into memory."""
        assert mem_len > 0
        if reuse_len is not None and reuse_len > 0:
            output = output[:reuse_len]
        if prev_mem is None:
            new_mem = output[-mem_len:]
        else:
            new_mem = torch.cat([prev_mem, output], dim=0)[-mem_len:]
        return new_mem.detach()

    def _create_causal_attn_mask(self, seq_len: int, mem_len: int, same_length: bool=False) ->torch.Tensor:
        """Create causal attention mask of shape
        `(seq_len, mem_len + seq_len)`.
        """
        assert self.r_w_bias is not None
        device = self.r_w_bias.device
        attn_mask = torch.ones(seq_len, seq_len, device=device)
        mask_u = torch.triu(attn_mask, diagonal=1)
        attn_mask_pad = torch.zeros(seq_len, mem_len, device=device)
        ret = torch.cat([attn_mask_pad, mask_u], dim=1)
        if same_length:
            mask_l = torch.tril(attn_mask, diagonal=-1)
            ret = torch.cat([ret[:, :seq_len] + mask_l, ret[:, seq_len:]], 1)
        return ret

    def forward(self, inputs: Union[torch.Tensor, torch.LongTensor], segment_ids: Optional[torch.LongTensor]=None, input_mask: Optional[torch.Tensor]=None, memory: Optional[List[torch.Tensor]]=None, permute_mask: Optional[torch.Tensor]=None, target_mapping: Optional[torch.Tensor]=None, bi_data: bool=False, clamp_len: Optional[int]=None, cache_len: int=0, same_length: bool=False, attn_type: str='bi', two_stream: bool=False) ->Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:
        """Compute XLNet representations for the input.

        Args:
            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,
                containing the ids of tokens in input sequences, or
                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,
                containing soft token ids (i.e., weights or probabilities)
                used to mix the embedding vectors.
            segment_ids: Shape `[batch_size, max_time]`.
            input_mask: Float tensor of shape `[batch_size, max_time]`. Note
                that positions with value 1 are masked out.
            memory: Memory from previous batches. A list of length `num_layers`,
                each tensor of shape `[batch_size, mem_len, hidden_dim]`.
            permute_mask: The permutation mask. Float tensor of shape
                `[batch_size, max_time, max_time]`.
                A value of 0 for ``permute_mask[i, j, k]`` indicates that
                position `i` attends to position `j` in batch `k`.
            target_mapping: The target token mapping. Float tensor of shape
                `[batch_size, num_targets, max_time]`.
                A value of 1 for ``target_mapping[i, j, k]`` indicates that
                the `i`-th target token (in order of permutation) in batch `k`
                is the token at position `j`.
                Each row ``target_mapping[i, :, k]`` can have no more than one
                value of 1.
            bi_data (bool): Whether to use bidirectional data input pipeline.
            clamp_len (int): Clamp all relative distances larger than
                :attr:`clamp_len`. A value of -1 means no clamping.
            cache_len (int): Length of memory (number of tokens) to cache.
            same_length (bool): Whether to use the same attention length for
                each token.
            attn_type (str): Attention type. Supported values are `"uni"`
                and `"bi"`.
            two_stream (bool): Whether to use two-stream attention. Only set to
                `True` when pre-training or generating text. Defaults to
                `False`.

        :returns: A tuple of `(output, new_memory)`:

            - **`output`**: The final layer output representations. Shape
              `[batch_size, max_time, hidden_dim]`.
            - **`new_memory`**: The memory of the current batch.
              If `cache_len` is 0, then `new_memory` is `None`. Otherwise, it is
              a list of length `num_layers`, each tensor of shape
              `[batch_size, cache_len, hidden_dim]`.
              This can be used as the :attr:`memory` argument in the next batch.
        """
        if inputs.dim() == 2:
            word_embeds = self.word_embed(inputs)
        elif inputs.dim() == 3:
            word_embeds = torch.tensordot(inputs, self.word_embed.weight, dims=([-1], [0]))
        else:
            raise ValueError("'inputs' should be a 2D or 3D tensor.")
        return self._forward(word_embed=word_embeds, segment_ids=segment_ids, input_mask=input_mask, memory=memory, permute_mask=permute_mask, target_mapping=target_mapping, bi_data=bi_data, clamp_len=clamp_len, cache_len=cache_len, same_length=same_length, attn_type=attn_type, two_stream=two_stream)

    def _forward(self, word_embed: torch.Tensor, segment_ids: Optional[torch.LongTensor]=None, input_mask: Optional[torch.Tensor]=None, memory: Optional[List[torch.Tensor]]=None, permute_mask: Optional[torch.Tensor]=None, target_mapping: Optional[torch.Tensor]=None, bi_data: bool=False, clamp_len: Optional[int]=None, cache_len: int=0, same_length: bool=False, attn_type: str='bi', two_stream: bool=False) ->Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:
        """Compute XLNet representations for the input. This layer exists
        because :class:`XLNetDecoder` compute embeddings in the decoder helper.
        `word_embed` has shape `[batch_size, max_time, word_embed_dim]`.
        Please refer to :meth:`forward` for the detailed information of other
        arguments.
        """
        word_embed = word_embed.permute(1, 0, 2)
        if segment_ids is not None:
            segment_ids = segment_ids.permute(1, 0)
        if input_mask is not None:
            input_mask = input_mask.permute(1, 0)
        if memory is not None:
            memory = [m.permute(1, 0, 2) for m in memory]
        if permute_mask is not None:
            permute_mask = permute_mask.permute(1, 2, 0)
        if target_mapping is not None:
            target_mapping = target_mapping.permute(1, 2, 0)
        seq_len, batch_size = word_embed.size()[:2]
        mem_len = memory[0].size(0) if memory is not None else 0
        tot_len = seq_len + mem_len
        reuse_len = self._hparams.reuse_len
        masks: List[Optional[torch.Tensor]] = []
        if attn_type == 'uni':
            causal_mask = self._create_causal_attn_mask(seq_len, mem_len, same_length)
            causal_mask = causal_mask.unsqueeze(2).unsqueeze(3)
            masks.append(causal_mask)
        elif attn_type == 'bi':
            pass
        else:
            raise ValueError(f'Unsupported attention type: {attn_type}')
        if input_mask is not None:
            input_mask = input_mask.expand(seq_len, -1, -1)
        data_mask = sum_tensors([input_mask, permute_mask])
        if data_mask is not None:
            memory_mask = data_mask.new_zeros(seq_len, mem_len, batch_size)
            data_mask = torch.cat([memory_mask, data_mask], dim=1).unsqueeze(3)
            masks.append(data_mask)
        attn_mask = sum_tensors(masks)
        if attn_mask is None:
            final_mask = None
        else:
            attn_mask = attn_mask > 0
            final_mask = -torch.eye(seq_len, device=attn_mask.device)
            final_mask = torch.cat([final_mask.new_zeros(seq_len, mem_len), final_mask], dim=-1)
            final_mask = final_mask.unsqueeze(2).unsqueeze(3)
            final_mask = attn_mask.float() + final_mask > 0
        if segment_ids is not None:
            concat_segment_ids = torch.cat([segment_ids.new_zeros(mem_len, batch_size), segment_ids])
            segment_matrix = (segment_ids.unsqueeze(1) != concat_segment_ids.unsqueeze(0)).long()
            segment_matrix = F.one_hot(segment_matrix, num_classes=2).float()
        else:
            segment_matrix = None
        pos_embed = self.pos_embed(batch_size, seq_len, tot_len, clamp_len, attn_type, bi_data)
        pos_embed = self.dropout(pos_embed)
        states_h = self.dropout(word_embed)
        states_g = None
        if two_stream:
            if target_mapping is not None:
                word_embed_q = self.mask_emb.expand(target_mapping.size(0), batch_size, -1)
            else:
                word_embed_q = word_embed
            states_g = self.dropout(word_embed_q)
        new_memory = []
        for idx in range(self._hparams.num_layers):
            cur_memory = memory[idx] if memory is not None else None
            if cache_len > 0:
                new_memory.append(self._cache_mem(states_h, cur_memory, cache_len, reuse_len))
            attn_layer: RelativeMultiheadAttention
            attn_layer = self.attn_layers[idx]
            states_h, states_g = attn_layer(states_h=states_h, states_g=states_g, pos_embed=pos_embed, segment_mat=segment_matrix, attn_mask_h=final_mask, attn_mask_g=attn_mask, target_mapping=target_mapping, memory=cur_memory)
            states_h = self.ff_layers[idx](states_h)
            if states_g is not None:
                states_g = self.ff_layers[idx](states_g)
        output = self.dropout(states_h if states_g is None else states_g)
        output = output.permute(1, 0, 2)
        if new_memory is not None:
            new_memory = [m.permute(1, 0, 2) for m in new_memory]
        if cache_len == 0:
            return output, None
        return output, new_memory


class XLNetClassifier(ClassifierBase, PretrainedXLNetMixin):
    """Classifier based on XLNet modules. Please see
    :class:`~texar.torch.modules.PretrainedXLNetMixin` for a brief description
    of XLNet.

    Arguments are the same as in
    :class:`~texar.torch.modules.XLNetEncoder`.

    Args:
        pretrained_model_name (optional): a `str`, the name
            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to
            :class:`~texar.torch.modules.PretrainedXLNetMixin` for
            all supported models.
            If `None`, the model name in :attr:`hparams` is used.
        cache_dir (optional): the path to a folder in which the
            pre-trained models will be cached. If `None` (default),
            a default directory (``texar_data`` folder under user's home
            directory) will be used.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure
            and default values.
    """

    def __init__(self, pretrained_model_name: Optional[str]=None, cache_dir: Optional[str]=None, hparams=None):
        super().__init__(hparams=hparams)
        encoder_hparams = dict_fetch(hparams, XLNetEncoder.default_hparams())
        self._encoder = XLNetEncoder(pretrained_model_name=pretrained_model_name, cache_dir=cache_dir, hparams=encoder_hparams)
        if self._hparams.use_projection:
            if self._hparams.clas_strategy == 'all_time':
                self.projection = nn.Linear(self._encoder.output_size * self._hparams.max_seq_length, self._encoder.output_size * self._hparams.max_seq_length)
            else:
                self.projection = nn.Linear(self._encoder.output_size, self._encoder.output_size)
        self.dropout = nn.Dropout(self._hparams.dropout)
        self.num_classes = self._hparams.num_classes
        if self.num_classes <= 0:
            self.hidden_to_logits = None
        else:
            logit_kwargs = self._hparams.logit_layer_kwargs
            if logit_kwargs is None:
                logit_kwargs = {}
            elif not isinstance(logit_kwargs, HParams):
                raise ValueError("hparams['logit_layer_kwargs'] must be a dict.")
            else:
                logit_kwargs = logit_kwargs.todict()
            if self._hparams.clas_strategy == 'all_time':
                self.hidden_to_logits = nn.Linear(self._encoder.output_size * self._hparams.max_seq_length, self.num_classes, **logit_kwargs)
            else:
                self.hidden_to_logits = nn.Linear(self._encoder.output_size, self.num_classes, **logit_kwargs)
        if self._hparams.initializer:
            initialize = get_initializer(self._hparams.initializer)
            assert initialize is not None
            if self._hparams.use_projection:
                initialize(self.projection.weight)
                initialize(self.projection.bias)
            if self.hidden_to_logits:
                initialize(self.hidden_to_logits.weight)
                if self.hidden_to_logits.bias:
                    initialize(self.hidden_to_logits.bias)
        else:
            if self._hparams.use_projection:
                self.projection.apply(init_weights)
            if self.hidden_to_logits:
                self.hidden_to_logits.apply(init_weights)
        self.is_binary = self.num_classes == 1 or self.num_classes <= 0 and self._hparams.hidden_dim == 1

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in XLNetEncoder
                ...
                # (2) Additional hyperparameters
                "clas_strategy": "cls_time",
                "use_projection": True,
                "num_classes": 2,
                "name": "xlnet_classifier",
            }

        Here:

        1. Same hyperparameters as in
            :class:`~texar.torch.modules.XLNetEncoder`.
            See the :meth:`~texar.torch.modules.XLNetEncoder.default_hparams`.
            An instance of XLNetEncoder is created for feature extraction.

        2. Additional hyperparameters:

            `"clas_strategy"`: str
                The classification strategy, one of:

                - **cls_time**: Sequence-level classification based on the
                  output of the last time step (which is the `CLS` token).
                  Each sequence has a class.
                - **all_time**: Sequence-level classification based on
                  the output of all time steps. Each sequence has a class.
                - **time_wise**: Step-wise classification, i.e., make
                  classification for each time step based on its output.

            `"use_projection"`: bool
                If `True`, an additional `Linear` layer is added after the
                summary step.

            `"num_classes"`: int
                Number of classes:

                - If **> 0**, an additional :torch_nn:`Linear`
                  layer is appended to the encoder to compute the logits over
                  classes.
                - If **<= 0**, no dense layer is appended. The number of
                  classes is assumed to be the final dense layer size of the
                  encoder.

            `"name"`: str
                Name of the classifier.
        """
        hparams = XLNetEncoder.default_hparams()
        hparams.update({'clas_strategy': 'cls_time', 'use_projection': True, 'num_classes': 2, 'logit_layer_kwargs': None, 'name': 'xlnet_classifier'})
        return hparams

    def param_groups(self, lr: Optional[float]=None, lr_layer_scale: float=1.0, decay_base_params: bool=False):
        """Create parameter groups for optimizers. When
        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form
        separate groups with different base learning rates.

        The return value of this method can be used in the constructor of
        optimizers, for example:

        .. code-block:: python

            model = XLNetClassifier(...)
            param_groups = model.param_groups(lr=2e-5, lr_layer_scale=0.8)
            optim = torch.optim.Adam(param_groups)

        Args:
            lr (float): The learning rate. Can be omitted if
                :attr:`lr_layer_decay_rate` is 1.0.
            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer
                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.
            decay_base_params (bool): If `True`, treat non-layer parameters
                (e.g. embeddings) as if they're in layer 0. If `False`, these
                parameters are not scaled.

        Returns:
            The parameter groups, used as the first argument for optimizers.
        """
        if lr_layer_scale != 1.0:
            if lr is None:
                raise ValueError('lr must be specified when lr_layer_decay_rate is not 1.0')
            fine_tune_group = {'params': params_except_in(self, ['_encoder']), 'lr': lr}
            param_groups = [fine_tune_group]
            param_group = self._encoder.param_groups(lr, lr_layer_scale, decay_base_params)
            param_groups.extend(param_group)
            return param_groups
        return self.parameters()

    def forward(self, inputs: Union[torch.Tensor, torch.LongTensor], segment_ids: Optional[torch.LongTensor]=None, input_mask: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, torch.LongTensor]:
        """Feeds the inputs through the network and makes classification.

        Args:
            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,
                containing the ids of tokens in input sequences, or
                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,
                containing soft token ids (i.e., weights or probabilities)
                used to mix the embedding vectors.
            segment_ids: Shape `[batch_size, max_time]`.
            input_mask: Float tensor of shape `[batch_size, max_time]`. Note
                that positions with value 1 are masked out.

        Returns:
            A tuple `(logits, preds)`, containing the logits over classes and
            the predictions, respectively.

            - If ``clas_strategy`` is ``cls_time`` or ``all_time``:

                - If ``num_classes`` == 1, ``logits`` and ``pred`` are both of
                  shape ``[batch_size]``.
                - If ``num_classes`` > 1, ``logits`` is of shape
                  ``[batch_size, num_classes]`` and ``pred`` is of shape
                  ``[batch_size]``.

            - If ``clas_strategy`` is ``time_wise``:

                - ``num_classes`` == 1, ``logits`` and ``pred`` are both of
                  shape ``[batch_size, max_time]``.
                - If ``num_classes`` > 1, ``logits`` is of shape
                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of
                  shape ``[batch_size, max_time]``.
        """
        output, _ = self._encoder(inputs=inputs, segment_ids=segment_ids, input_mask=input_mask)
        strategy = self._hparams.clas_strategy
        if strategy == 'time_wise':
            summary = output
        elif strategy == 'cls_time':
            summary = output[:, (-1)]
        elif strategy == 'all_time':
            length_diff = self._hparams.max_seq_length - inputs.shape[1]
            summary_input = F.pad(output, [0, 0, 0, length_diff, 0, 0])
            summary_input_dim = self._encoder.output_size * self._hparams.max_seq_length
            summary = summary_input.contiguous().view(-1, summary_input_dim)
        else:
            raise ValueError(f'Unknown classification strategy: {strategy}.')
        if self._hparams.use_projection:
            summary = torch.tanh(self.projection(summary))
        if self.hidden_to_logits is not None:
            summary = self.dropout(summary)
            logits = self.hidden_to_logits(summary)
        else:
            logits = summary
        if strategy == 'time_wise':
            if self.is_binary:
                logits = torch.squeeze(logits, -1)
                preds = (logits > 0).long()
            else:
                preds = torch.argmax(logits, dim=-1)
        else:
            if self.is_binary:
                preds = (logits > 0).long()
                logits = torch.flatten(logits)
            else:
                preds = torch.argmax(logits, dim=-1)
            preds = torch.flatten(preds)
        return logits, preds

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output :attr:`logits`.
        If :attr:`logits` size is only determined by input
        (i.e. if ``num_classes`` == 1), the feature size is equal to ``-1``.
        Otherwise it is equal to last dimension value of :attr:`logits` size.
        """
        if self._hparams.num_classes > 1:
            logit_dim = self._hparams.num_classes
        elif self._hparams.num_classes == 1:
            logit_dim = -1
        else:
            logit_dim = self._hparams.hidden_dim
        return logit_dim


HParamsType = Optional[HParams]


MaybeTuple = Union[T, Tuple[T, ...]]


OutputSize = MaybeTuple[Union[int, torch.Size]]


TensorStruct = Union[List[torch.Tensor], Dict[Any, torch.Tensor], MaybeTuple[torch.Tensor]]


ActivationFn = Callable[[torch.Tensor], torch.Tensor]


LinearLayer = Callable[[torch.Tensor], torch.Tensor]


def _get_sizes(sizes: List[Any]) ->List[int]:
    """

    Args:
        sizes: A list of ``int`` or ``torch.Size``. If each element is of type
            ``torch.Size``, the size is computed by taking the product of the
            shape.

    Returns:
        A list of sizes with ``torch.Size`` replaced by product of its
        individual dimensions
    """
    if isinstance(sizes[0], torch.Size):
        size_list = [np.prod(shape) for shape in sizes]
    else:
        size_list = sizes
    return size_list


def _mlp_transform(inputs: TensorStruct, output_size: OutputSize, linear_layer: Optional[LinearLayer]=None, activation_fn: Optional[ActivationFn]=None) ->Any:
    """Transforms inputs through a fully-connected layer that creates
    the output with specified size.

    Args:
        inputs: A Tensor of shape `[batch_size, d1, ..., dn]`, or a (nested)
            tuple of such elements. The dimensions `d1, ..., dn` will be flatten
            and transformed by a dense layer.
        output_size: Can be an ``int``, a ``torch.Size``, or a (nested)
            tuple of ``int`` or ``torch.Size``.
        activation_fn: Activation function applied to the output.

    :returns:
        If :attr:`output_size` is an ``int`` or a ``torch.Size``,
        returns a tensor of shape ``[batch_size, *, output_size]``.
        If :attr:`output_size` is a tuple of ``int`` or ``torch.Size``,
        returns a tuple having the same structure as :attr:`output_size`,
        where each element has the same size as defined in :attr:`output_size`.
    """
    flat_input = nest.flatten(inputs)
    flat_input = [x.view(-1, x.size(-1)) for x in flat_input]
    concat_input = torch.cat(flat_input, 1)
    flat_output_size = nest.flatten(output_size)
    size_list = _get_sizes(flat_output_size)
    fc_output = concat_input
    if linear_layer is not None:
        fc_output = linear_layer(fc_output)
    if activation_fn is not None:
        fc_output = activation_fn(fc_output)
    flat_output = torch.split(fc_output, size_list, dim=1)
    flat_output = list(flat_output)
    if isinstance(flat_output_size[0], torch.Size):
        flat_output = [torch.reshape(output, (-1,) + shape) for output, shape in zip(flat_output, flat_output_size)]
    output = nest.pack_sequence_as(structure=output_size, flat_sequence=flat_output)
    return output


def _sum_output_size(output_size: OutputSize) ->int:
    """Return sum of all dim values in :attr:`output_size`

    Args:
        output_size: Can be an ``int``, a ``torch.Size``, or a (nested)
            tuple of ``int`` or ``torch.Size``.
    """
    flat_output_size = nest.flatten(output_size)
    size_list = _get_sizes(flat_output_size)
    ret = sum(size_list)
    return ret


def get_activation_fn(fn_name: Optional[Union[str, Callable[[torch.Tensor], torch.Tensor]]]=None, kwargs: Union[HParams, Dict, None]=None) ->Optional[Callable[[torch.Tensor], torch.Tensor]]:
    """Returns an activation function `fn` with the signature
    `output = fn(input)`.

    If the function specified by :attr:`fn_name` has more than one arguments
    without default values, then all these arguments except the input feature
    argument must be specified in :attr:`kwargs`. Arguments with default values
    can also be specified in :attr:`kwargs` to take values other than the
    defaults. In this case a partial function is returned with the above
    signature.

    Args:
        fn_name (str or callable): An activation function, or its name or
            module path. The function can be:

            - Built-in function defined in
              :torch_docs:`torch.nn.functional<nn.html#torch-nn-functional>`
            - User-defined activation functions in module
              :mod:`texar.torch.custom`.
            - External activation functions. Must provide the full module path,
              e.g., ``"my_module.my_activation_fn"``.

        kwargs (optional): A `dict` or instance of :class:`~texar.torch.HParams`
            containing the keyword arguments of the activation function.

    Returns:
        An activation function. `None` if :attr:`fn_name` is `None`.
    """
    if fn_name is None:
        return None
    fn_modules = ['torch', 'torch.nn.functional', 'texar.torch.custom', 'texar.torch.core.layers']
    activation_fn_ = utils.get_function(fn_name, fn_modules)
    activation_fn = activation_fn_
    if kwargs is not None:
        if isinstance(kwargs, HParams):
            kwargs = kwargs.todict()

        def _partial_fn(features):
            return activation_fn_(features, **kwargs)
        activation_fn = _partial_fn
    return activation_fn


def _assert_same_size(outputs: TensorStruct, output_size: OutputSize):
    """Check if outputs match output_size

    Args:
        outputs: A tensor or a (nested) tuple of tensors
        output_size: Can be an ``int``, a ``torch.Size``, or a (nested)
            tuple of ``int`` or ``torch.Size``.
    """
    flat_output_size = nest.flatten(output_size)
    flat_output = nest.flatten(outputs)
    for output, size in zip(flat_output, flat_output_size):
        if isinstance(size, torch.Size):
            if output[0].size() != size:
                raise ValueError('The output size does not matchthe required output_size')
        elif output[0].size()[-1] != size:
            raise ValueError('The output size does not match the required output_size')


EmbeddingFn = Callable[[torch.LongTensor, torch.LongTensor], torch.Tensor]


HelperInitTuple = Tuple[torch.ByteTensor, torch.Tensor]


IDType = TypeVar('IDType', bound=torch.Tensor)


NextInputTuple = Tuple[torch.ByteTensor, torch.Tensor]


class Helper(Generic[IDType], ABC):
    """Interface for implementing sampling in seq2seq decoders.

    Please refer to the documentation for the TensorFlow counterpart
    `tf.contrib.seq2seq.Helper
    <https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/Helper>`_.
    """

    def initialize(self, embedding_fn: EmbeddingFn, inputs: Optional[torch.Tensor], sequence_length: Optional[torch.LongTensor]) ->HelperInitTuple:
        """Initialize the current batch.

        Args:
            embedding_fn: A function taking input tokens and timestamps,
                returning embedding tensors.
            inputs: Input tensors.
            sequence_length: An int32 vector tensor.

        Returns:
            ``(initial_finished, initial_inputs)``.
        """
        raise NotImplementedError

    def sample(self, time: int, outputs: torch.Tensor) ->IDType:
        """Returns ``sample_ids``.
        """
        raise NotImplementedError

    def next_inputs(self, embedding_fn: EmbeddingFn, time: int, outputs: torch.Tensor, sample_ids: IDType) ->NextInputTuple:
        """Returns ``(finished, next_inputs, next_state)``.
        """
        raise NotImplementedError


class XLNetDecoderOutput(NamedTuple):
    """The output of :class:`XLNetDecoder`.
    """
    logits: torch.Tensor
    """A :tensor:`Tensor` of shape ``[batch_size, max_time, vocab_size]``
    containing the logits."""
    sample_id: torch.LongTensor
    """A :tensor:`LongTensor` of shape ``[batch_size, max_time]``
    (or ``[batch_size, max_time, vocab_size]``) containing the sampled token
    indices. Note that the shape of ``sample_id`` is different for different
    decoding strategy or helper. Please refer to
    :class:`~texar.torch.modules.Helper` for the detailed information."""


Output = XLNetDecoderOutput


TokenEmbedder = Union[nn.Module, Callable[[torch.LongTensor], torch.Tensor]]


TokenPosEmbedder = Union[nn.Module, Callable[[torch.LongTensor, torch.LongTensor], torch.Tensor]]


torch_bool = (torch.empty(()) < 0).dtype


class SinusoidsPositionEmbedder(EmbedderBase):
    """Sinusoid position embedder that maps position indexes into embeddings
    via sinusoid calculation. This module does not have trainable parameters.
    Used in, e.g., Transformer models
    `(Vaswani et al.) "Attention Is All You Need"`.

    Each channel of the input Tensor is incremented by a sinusoid of a
    different frequency and phase.
    This allows attention to learn to use absolute and relative positions.

    Timing signals should be added to some precursors of both the query
    and the memory inputs to attention.
    The use of relative position is possible because `sin(x+y)` and
    `cos(x+y)` can be expressed in terms of `y`, `sin(x)`, and `cos(x)`.
    In particular, we use a geometric sequence of timescales starting with
    min_timescale and ending with max_timescale.  The number of different
    timescales is equal to ``dim / 2``. For each timescale, we
    generate the two sinusoidal signals `sin(timestep/timescale)` and
    `cos(timestep/timescale)`.  All of these sinusoids are concatenated in
    the dim dimension.

    Args:
        position_size (int): The number of possible positions, e.g., the maximum
            sequence length. Set ``position_size=None`` and
            ``hparams['cache_embeddings']=False`` to use arbitrarily large or
            negative position indices.

    .. document private functions
    """
    signal: torch.Tensor
    inv_timescales: torch.Tensor

    def __init__(self, position_size: Optional[int]=None, hparams=None):
        super().__init__(hparams=hparams)
        self._num_embeds = position_size
        self._dim = self._hparams.dim
        self._cache_embeddings = self._hparams.cache_embeddings
        num_timescales = self._dim // 2
        min_timescale = self._hparams.min_timescale
        max_timescale = self._hparams.max_timescale
        log_timescale_increment = math.log(max_timescale / min_timescale) / (num_timescales - 1)
        inv_timescales = min_timescale * torch.exp(torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment)
        if self._cache_embeddings:
            if position_size is None:
                raise ValueError("'position_size' must not be None when 'cache_embeddings' is set to True")
            positions = torch.arange(position_size, dtype=torch.float)
            signal = self._compute_embeddings(positions, inv_timescales)
            self.register_buffer('signal', signal)
        else:
            self.register_buffer('inv_timescales', inv_timescales)

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values
        We use a geometric sequence of timescales starting with
        min_timescale and ending with max_timescale. The number of different
        timescales is equal to ``dim / 2``.

        .. code-block:: python

            {
                'min_timescale': 1.0,
                'max_timescale': 10000.0,
                'dim': 512,
                'cache_embeddings': True,
                'name':'sinusoid_position_embedder',
            }

        Here:

        `"cache_embeddings"`: bool
            If `True`, precompute embeddings for positions in range
            `[0, position_size - 1]`. This leads to faster lookup but requires
            lookup indices to be within this range.

            If `False`, embeddings are computed on-the-fly during lookup. Set to
            `False` if your application needs to handle sequences of arbitrary
            length, or requires embeddings at negative positions.
        """
        return {'min_timescale': 1.0, 'max_timescale': 10000.0, 'dim': 512, 'cache_embeddings': True, 'name': 'sinusoid_position_embedder'}

    def extra_repr(self) ->str:
        return f'embedding_dim={self.dim}'

    def _compute_embeddings(self, positions: torch.Tensor, inv_timescales: torch.Tensor) ->torch.Tensor:
        scaled_time = positions.type_as(inv_timescales).view(-1, 1) * inv_timescales.unsqueeze(0)
        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)
        if self._dim % 2 == 1:
            signal = torch.cat([signal, signal.new_zeros(signal.size(0), 1)], dim=1)
        signal = signal.view(*positions.size(), -1).contiguous()
        return signal

    def forward(self, positions: Optional[torch.LongTensor]=None, sequence_length: Optional[torch.LongTensor]=None, **kwargs) ->torch.Tensor:
        """Embeds.
        Either :attr:`positions` or :attr:`sequence_length` is required:

        - If both are given, :attr:`sequence_length` is used to mask out
          embeddings of those time steps beyond the respective sequence
          lengths.
        - If only :attr:`sequence_length` is given, then positions
          from `0` to `sequence_length - 1` are embedded.

        Args:
            positions (optional): An :tensor:`LongTensor` containing the
                position IDs to embed.
            sequence_length (optional): An :tensor:`LongTensor` of shape
                ``[batch_size]``. Time steps beyond
                the respective sequence lengths will have zero-valued
                embeddings.

        Returns:
            A Tensor of shape ``[batch_size, position_size, dim]``.
        """
        if positions is None:
            if sequence_length is None:
                raise ValueError('Either `positions` or `sequence_length` is required.')
            max_length = sequence_length.max()
            batch_size = sequence_length.size(0)
            inputs = torch.arange(max_length)
            inputs = inputs.expand(batch_size, max_length)
        else:
            inputs = positions
        if self._cache_embeddings:
            outputs = F.embedding(inputs, self.signal, **kwargs)
        else:
            outputs = self._compute_embeddings(inputs, self.inv_timescales)
        if sequence_length is not None:
            outputs = mask_sequences(outputs, sequence_length)
        return outputs

    @property
    def dim(self):
        """The embedding dimension.
        """
        return self._dim

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output. If the :attr:`dim`
        hyperparameter is a ``list`` or ``tuple``, the feature size
        equals its final dimension; otherwise, if :attr:`dim` is an
        ``int``, the feature size equals :attr:`dim`.
        """
        if isinstance(self._dim, (list, tuple)):
            dim = self._dim[-1]
        else:
            dim = self._dim
        return dim


class EncoderDecoderBase(ModuleBase, ABC):
    """Base class inherited by all encoderdecoder classes.
    """

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.
        """
        return {'name': 'encoderdecoder'}


_CHECKPOINT_FILES_GEN_MAP = {'small': (1000000, 16), 'base': (999900, 16), 'large': (1000700, 8), 'B': (1000000, 64)}


_T5_PATH = 'https://storage.googleapis.com/t5-data/pretrained_models/'


_T5_VOCAB_PATH = 'https://storage.googleapis.com/t5-data/vocabs/cc_all.32000/'


def _generate_t5_file_list(ckpt_tuple: tuple) ->List[str]:
    """ Helper function to generate file list given a tuple of model_id and
    partition size.

    Args:
        ckpt_tuple: A tuple of model_id and number of partitions

    """
    ckpt_id = ckpt_tuple[0]
    ckpt_parts = ckpt_tuple[1]
    return ['checkpoint', *[f'model.ckpt-{ckpt_id}.data-{idx:05d}-of-{ckpt_parts:05d}' for idx in range(ckpt_parts)], f'model.ckpt-{ckpt_id}.index', f'model.ckpt-{ckpt_id}.meta', 'operative_config.gin']


IMPORTANT_PARAMS = 'd_ff', 'd_kv', 'd_model', 'dropout', 'num_heads', 'num_layers', 'inputs_length'


def read_t5_gin_config_file(config_file_path: str) ->Dict:
    """Simple helper function to read a gin file
    and get hyperparameters for T5.

    Args:
        config_file_path: path of config.gin file as a string.

    Returns:
        A dictionary with important parameters for loading T5.

    """
    config = {}
    with open(config_file_path, 'r') as gin_file:
        for line in gin_file:
            if line.startswith(IMPORTANT_PARAMS):
                assignment = line.strip().split()
                assert len(assignment) == 3
                arg_name, _, value = assignment
                config[arg_name] = ast.literal_eval(value)
    return config


class PretrainedT5Mixin(PretrainedMixin, ABC):
    """A mixin class to support loading pre-trained checkpoints for modules
    that implement the T5 model.

    The T5 model was proposed in
    `Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer`_
    by `Raffel et al.` from Google. It treats multiple NLP tasks in a similar
    manner by encoding the different tasks as text directives in the input
    stream. This enables a single model to be trained supervised on a wide
    variety of NLP tasks. The T5 model examines factors relevant for leveraging
    transfer learning at scale from pure unsupervised pre-training to
    supervised tasks.

    The available T5 models are as follows:

      * ``T5-Small``: Small version of T5, 60 million parameters.
      * ``T5-Base``: Base-line version of T5, 220 million parameters.
      * ``T5-Large``: Large Version of T5, 770 million parameters.
      * ``T5-3B``: A version of T5 with 3 billion parameters.
      * ``T5-11B``: A version of T5 with 11 billion parameters.

    We provide the following classes:

      * :class:`~texar.torch.modules.T5Encoder` for loading weights for the
        encoder stack.
      * :class:`~texar.torch.modules.T5Decoder` for loading weights for the
        decoding stack.
      * :class:`~texar.torch.modules.T5EncoderDecoder` as a raw pre-trained
        model.

    .. _`Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer`:
        https://arxiv.org/abs/1910.10683
    """
    _MODEL_NAME = 'T5'
    _MODEL2URL = {'T5-Small': [(_T5_PATH + f'small/{file}') for file in _generate_t5_file_list(_CHECKPOINT_FILES_GEN_MAP['small'])] + [_T5_VOCAB_PATH + 'sentencepiece.model'], 'T5-Base': [(_T5_PATH + f'base/{file}') for file in _generate_t5_file_list(_CHECKPOINT_FILES_GEN_MAP['base'])] + [_T5_VOCAB_PATH + 'sentencepiece.model'], 'T5-Large': [(_T5_PATH + f'large/{file}') for file in _generate_t5_file_list(_CHECKPOINT_FILES_GEN_MAP['large'])] + [_T5_VOCAB_PATH + 'sentencepiece.model'], 'T5-3B': [(_T5_PATH + f'3B/{file}') for file in _generate_t5_file_list(_CHECKPOINT_FILES_GEN_MAP['B'])] + [_T5_VOCAB_PATH + 'sentencepiece.model'], 'T5-11B': [(_T5_PATH + f'11B/{file}') for file in _generate_t5_file_list(_CHECKPOINT_FILES_GEN_MAP['B'])] + [_T5_VOCAB_PATH + 'sentencepiece.model']}
    _MODEL2CKPT = {'T5-Small': 'model.ckpt-1000000', 'T5-Base': 'model.ckpt-999900', 'T5-Large': 'model.ckpt-1000700', 'T5-3B': 'model.ckpt-1000000', 'T5-11B': 'model.ckpt-1000000'}

    @classmethod
    def _transform_config(cls, pretrained_model_name: str, cache_dir: str) ->Dict[str, Any]:
        info = list(os.walk(cache_dir))
        root, _, files = info[0]
        config_path = None
        for file in files:
            if file.endswith('operative_config.gin'):
                config_path = os.path.join(root, file)
        if config_path is None:
            raise ValueError(f'Cannot find the config file in {cache_dir}')
        gin_config = read_t5_gin_config_file(config_path)
        hidden_dim = gin_config['d_model']
        vocab_size = 32128
        eps = 1e-06
        embedding_dropout = gin_config['dropout_rate']
        num_blocks = gin_config['num_layers']
        num_heads = gin_config['num_heads']
        num_units = gin_config['d_kv'] * num_heads
        dropout_rate = gin_config['dropout_rate']
        residual_dropout = gin_config['dropout_rate']
        intermediate_size = gin_config['d_ff']
        rel_attn_num_buckets = 32
        use_bias = False
        configs = {'hidden_size': hidden_dim, 'embed': {'name': 'word_embeddings', 'dim': hidden_dim}, 'vocab_size': vocab_size, 'encoder': {'name': 'encoder', 'embedding_dropout': embedding_dropout, 'num_blocks': num_blocks, 'multihead_attention': {'use_bias': use_bias, 'num_units': num_units, 'num_heads': num_heads, 'output_dim': hidden_dim, 'dropout_rate': dropout_rate, 'name': 'self', 'is_decoder': False, 'relative_attention_num_buckets': rel_attn_num_buckets}, 'eps': eps, 'residual_dropout': residual_dropout, 'dim': hidden_dim, 'poswise_feedforward': {'layers': [{'type': 'Linear', 'kwargs': {'in_features': hidden_dim, 'out_features': intermediate_size, 'bias': use_bias}}, {'type': 'ReLU'}, {'type': 'Linear', 'kwargs': {'in_features': intermediate_size, 'out_features': hidden_dim, 'bias': use_bias}}]}}, 'decoder': {'name': 'decoder', 'embedding_dropout': embedding_dropout, 'num_blocks': num_blocks, 'multihead_attention': {'use_bias': use_bias, 'num_units': num_units, 'num_heads': num_heads, 'output_dim': hidden_dim, 'dropout_rate': dropout_rate, 'name': 'self', 'is_decoder': True, 'relative_attention_num_buckets': rel_attn_num_buckets}, 'eps': eps, 'residual_dropout': residual_dropout, 'dim': hidden_dim, 'poswise_feedforward': {'layers': [{'type': 'Linear', 'kwargs': {'in_features': hidden_dim, 'out_features': intermediate_size, 'bias': use_bias}}, {'type': 'ReLU'}, {'type': 'Linear', 'kwargs': {'in_features': intermediate_size, 'out_features': hidden_dim, 'bias': use_bias}}]}}}
        return configs

    def assign(self, from_array, to_param, transpose=False):
        pointer = self._name_to_variable(to_param)
        if transpose:
            from_array = np.transpose(from_array)
        assert pointer.shape == from_array.shape
        pointer.data = torch.from_numpy(from_array.astype(np.float32))

    def _init_from_checkpoint(self, pretrained_model_name: str, cache_dir: str, **kwargs):
        try:
            import tensorflow as tf
        except ImportError:
            None
            raise
        tf_path = os.path.abspath(os.path.join(cache_dir, self._MODEL2CKPT[pretrained_model_name]))
        init_vars = tf.train.list_variables(tf_path)
        to_params: Set[str] = {x[0] for x in self.named_parameters()}
        to_params.remove('decoder.enc_dec_attns.0.relative_attention_bias.weight')
        tfnames, arrays = [], []
        for name, _ in init_vars:
            array = tf.train.load_variable(tf_path, name)
            tfnames.append(name)
            arrays.append(array.squeeze())
        from_params = set(copy.deepcopy(tfnames))
        global_tensor_map = {'shared/embedding': 'word_embedder._embedding'}
        self_attention_map = {'SelfAttention/k': '{}.self_attns.{}.K_dense.weight', 'SelfAttention/o': '{}.self_attns.{}.O_dense.weight', 'SelfAttention/q': '{}.self_attns.{}.Q_dense.weight', 'SelfAttention/v': '{}.self_attns.{}.V_dense.weight', 'SelfAttention/relative_attention_bias': '{}.self_attns.{}.relative_attention_bias.weight', 'layer_norm/scale': '{}.self_attn_layer_norm.{}.w'}
        enc_dec_attention_map = {'EncDecAttention/k': '{}.enc_dec_attns.{}.K_dense.weight', 'EncDecAttention/o': '{}.enc_dec_attns.{}.O_dense.weight', 'EncDecAttention/q': '{}.enc_dec_attns.{}.Q_dense.weight', 'EncDecAttention/v': '{}.enc_dec_attns.{}.V_dense.weight', 'layer_norm/scale': '{}.end_dec_attn_layer_norm.{}.w'}
        drd_map = {'DenseReluDense/wi/kernel': '{}.poswise_networks.{}._layers.0.weight', 'DenseReluDense/wo/kernel': '{}.poswise_networks.{}._layers.2.weight', 'layer_norm/scale': '{}.poswise_layer_norm.{}.w'}
        component_map = {'final_layer_norm/scale': '{}.final_layer_norm.w'}
        block_map = {'encoder0': self_attention_map, 'encoder1': drd_map, 'decoder0': self_attention_map, 'decoder1': enc_dec_attention_map, 'decoder2': drd_map}
        idx = 0
        special_param_name = 'decoder.enc_dec_attns.0.relative_attention_bias.weight'
        rab_pointer = self._name_to_variable(special_param_name)
        rab_pointer.data.normal_(mean=0.0, std=self._hparams.hidden_size ** -0.5)
        for name, array in zip(tfnames, arrays):
            if name.startswith('cls') or name == 'global_step' or name.endswith('adam_m') or name.endswith('adam_v') or '_slot_' in name:
                from_params.remove(name)
                continue
            if name in global_tensor_map:
                v_name = global_tensor_map[name]
                self.assign(array, v_name)
                idx += 1
                from_params.remove(name)
                to_params.remove(v_name)
            else:
                tmp_name = name.split('/')
                submodule = tmp_name[0]
                if len(tmp_name) > 3:
                    block_num = int(tmp_name[1][6:])
                    layer_num = str(int(tmp_name[2][6:]))
                    sublayer_name = '/'.join(tmp_name[3:])
                    map_ = block_map[submodule + layer_num]
                    if sublayer_name in map_:
                        v_name = map_[sublayer_name].format(submodule, block_num)
                        self.assign(array, v_name, True)
                        idx += 1
                        from_params.remove(name)
                        to_params.remove(v_name)
                else:
                    sublayer_name = '/'.join(tmp_name[1:])
                    if sublayer_name in component_map:
                        v_name = component_map[sublayer_name].format(submodule)
                        self.assign(array, v_name)
                        idx += 1
                        from_params.remove(name)
                        to_params.remove(v_name)
        if len(from_params) > 0:
            None
        if len(to_params) > 0:
            None


class T5LayerNorm(nn.Module):
    """ Custom LayerNorm for T5 with no mean subtraction and no bias.
    """

    def __init__(self, input_size: int, eps: float=1e-05):
        super().__init__()
        self.w = nn.Parameter(torch.ones(input_size))
        self.eps = eps

    def forward(self, x: torch.Tensor):
        x = x / torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return self.w * x


class EmbeddingHelper(Helper[IDType], ABC):
    """A generic helper for use during inference.

    Uses output logits for sampling, and passes the result through an embedding
    layer to get the next input.

    Args:
        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,
            representing the start tokens for each sequence in batch.
        end_token: Python int or scalar :tensor:`LongTensor`, denoting the
            token that marks end of decoding.

    Raises:
        ValueError: if :attr:`start_tokens` is not a 1D tensor or
            :attr:`end_token` is not a scalar.
    """
    _start_inputs: torch.Tensor

    def __init__(self, start_tokens: torch.LongTensor, end_token: Union[int, torch.LongTensor]):
        if start_tokens.dim() != 1:
            raise ValueError('start_tokens must be a vector')
        if not isinstance(end_token, int) and end_token.dim() != 0:
            raise ValueError('end_token must be a scalar')
        self._start_tokens = start_tokens
        self._batch_size = start_tokens.size(0)
        if isinstance(end_token, int):
            self._end_token = start_tokens.new_tensor(end_token)
        else:
            self._end_token = end_token

    @property
    def batch_size(self) ->int:
        return self._batch_size

    def initialize(self, embedding_fn: EmbeddingFn, inputs: Optional[torch.Tensor], sequence_length: Optional[torch.LongTensor]) ->HelperInitTuple:
        del inputs, sequence_length
        times = torch.zeros_like(self._start_tokens)
        self._start_inputs = embedding_fn(self._start_tokens, times)
        finished = torch.zeros_like(self._start_tokens, dtype=torch_bool)
        return finished, self._start_inputs


class TransformerDecoderOutput(NamedTuple):
    """The output of :class:`TransformerDecoder`.
    """
    logits: torch.Tensor
    """A :tensor:`Tensor` of shape ``[batch_size, max_time, vocab_size]``
    containing the logits."""
    sample_id: torch.LongTensor
    """A :tensor:`LongTensor` of shape ``[batch_size, max_time]``
    (or ``[batch_size, max_time, vocab_size]``) containing the sampled
    token indices. Note that the shape of ``sample_id`` is different for
    different decoding strategy or helper. Please refer to
    :class:`~texar.torch.modules.Helper` for the detailed information."""


def identity(inputs: torch.Tensor):
    """Returns a tensor with the same content as the input tensor.

    Arguments:
        inputs: The input tensor.

    Returns:
        A tensor of the same shape, type, and content.
    """
    return inputs


def _make_output_layer(layer: Optional[Union[nn.Module, torch.Tensor]], vocab_size: Optional[int], output_size: int, bias: bool) ->Tuple[nn.Module, Optional[int]]:
    """Construct the output layer for decoders. Based on the input, multiple
    types of output layers could be constructed:

    - If ``layer`` is a :torch_nn:`Module`, then the layer is returned as is.
    - If ``layer`` is `None`, then a :torch_nn:`Linear` layer is constructed
      with ``output_size`` and ``vocab_size`` as input and output dimensions.
    - If ``layer`` is a :tensor:`Tensor`, then a :torch_nn:`Linear` layer is
      constructed with the provided tensor as parameters. Note that this tensor
      should have transposed shape, i.e. shape of ``[vocab_size, output_size]``.
      Also, if the provided tensor is not an instance of :torch_nn:`Parameter`,
      it will **not** accumulate gradients.
    - If ``layer`` is :method:`texar.torch.core.identity`, identity function is
      used as the output layer.
    """
    if isinstance(layer, nn.Module):
        output_layer = layer
    elif layer is None:
        if vocab_size is None:
            raise ValueError('Either `output_layer` or `vocab_size` must be provided. Set `output_layer=tx.core.identity` if no output layer is wanted.')
        output_layer = nn.Linear(output_size, vocab_size, bias)
    elif torch.is_tensor(layer):
        vocab_size = layer.size(0)
        output_layer = nn.Linear(layer.size(1), vocab_size, bias)
        if not isinstance(layer, nn.Parameter):
            layer = nn.Parameter(layer, requires_grad=False)
        output_layer.weight = layer
    elif layer is identity:
        output_layer = Identity()
    else:
        raise ValueError(f'output_layer should be an instance of `nn.Module`, a tensor,or None. Unsupported type: {type(layer)}')
    return output_layer, vocab_size


INF = 1.0 * 10000000.0


def _expand_to_beam_size(tensor: Any, beam_size: int) ->Any:
    """Tiles a given tensor by :attr:`beam_size`.

    Args:
        tensor: tensor to tile. Shape: `[batch_size, ...]`.
        beam_size: How much to tile the tensor by.

    Returns:
        Tiled tensor of shape `[batch_size, beam_size, ...]`.
    """
    if not isinstance(tensor, torch.Tensor):
        return tensor
    tensor = torch.unsqueeze(tensor, dim=1)
    tile_dims = [1] * len(tensor.size())
    tile_dims[1] = beam_size
    return tensor.repeat(tuple(tile_dims))


def _merge_beam_dim(tensor: Any) ->Any:
    """Reshapes first two dimensions in to single dimension.

    Args:
        tensor: Tensor to reshape of shape `[A, B, ...]`.

    Returns:
        Reshaped tensor of shape `[A * B, ...]`.
    """
    if not isinstance(tensor, torch.Tensor):
        return tensor
    shape = list(tensor.size())
    shape[0] *= shape[1]
    shape.pop(1)
    return tensor.view(tuple(shape))


def _unmerge_beam_dim(tensor: Any, batch_size: int, beam_size: int) ->Any:
    """Reshapes first dimension back to `[batch_size, beam_size]`.

    Args:
        tensor: Tensor to reshape of shape `[batch_size * beam_size, ...]`.
        batch_size: int, original batch size.
        beam_size: int, original beam size.

    Returns:
        Reshaped tensor of shape `[batch_size, beam_size, ...]`.
    """
    if not isinstance(tensor, torch.Tensor):
        return tensor
    shape = list(tensor.size())
    new_shape = [batch_size] + [beam_size] + shape[1:]
    return tensor.view(tuple(new_shape))


def compute_batch_indices(batch_size: int, beam_size: int) ->torch.LongTensor:
    """Computes the i-th coordinate that contains the batch index for
    gathers.

    The batch index tensor is a tensor like `[[0,0,0,0,],[1,1,1,1],..]`.
    It says which batch the beam item is in. This will create the first
    dimension of the 2D coordinates needed for the gather.

    Args:
        batch_size: Batch size
        beam_size: Size of the beam.

    Returns:
        `[batch_size, beam_size]` tensor of ids.
    """
    batch_pos = torch.arange(batch_size)
    batch_pos = batch_pos.view(-1, 1).expand(batch_size, beam_size)
    return batch_pos


def gather_nd(params: Any, indices: torch.Tensor) ->Any:
    if not isinstance(params, torch.Tensor):
        return params
    assert len(indices.size()) == 3
    orig_size = params.size()
    index = indices[:, :, (1)].view(-1) + indices[:, :, (0)].view(-1) * orig_size[1]
    ret = torch.index_select(params.view(-1, *params.size()[2:]), dim=0, index=index)
    ret = ret.view(orig_size[0], indices.size(1), *orig_size[2:])
    return ret


def compute_topk_scores_and_seq(sequences: torch.LongTensor, scores: torch.Tensor, scores_to_gather: torch.Tensor, flags: torch.ByteTensor, beam_size: int, batch_size: int, states_to_gather: Optional[State]=None) ->Tuple[torch.LongTensor, torch.Tensor, torch.ByteTensor, Optional[State]]:
    """Given sequences and scores, will gather the top-k (`k = beam`) size
    sequences.

    This function is used to grow alive, and finished. It takes sequences,
    scores, and flags, and returns the top k from sequence
    :attr:`scores_to_gather`, and flags based on the values in scores.

    Args:
        sequences: Tensor of sequences that we need to gather from.
            Shape: `[batch_size, beam_size, seq_length]`.
        scores: Tensor of scores for each sequence in sequences. We will use
            these to compute the top-k. Shape: `[batch_size, beam_size]`.
        scores_to_gather: Tensor of scores for each sequence in sequences.
            Shape: `[batch_size, beam_size]`.
            We will return the gathered scores from here.
            Scores to gather is different from scores because for
            grow_alive, we will need to return log-probabilities, while for
            grow_finished, we will need to return the length penalized
            scores.
        flags: Tensor of booleans for sequences that say whether a sequence
            has reached `EOS`.
        beam_size: int
        batch_size: int
        states_to_gather: (possibly nested structure of) decoding states.

    :returns: Tuple of:

        - `topk_seq`: `[batch_size, beam_size, decode_length]`.
        - `topk_gathered_scores`: `[batch_size, beam_size]`.
        - `topk_finished_flags`: `[batch_size, beam_size]`.
    """
    _, topk_indexes = torch.topk(scores, k=beam_size)
    batch_pos = compute_batch_indices(batch_size, beam_size)
    batch_pos = batch_pos
    top_coordinates = torch.stack([batch_pos, topk_indexes], dim=2)
    topk_seq = gather_nd(sequences, top_coordinates)
    topk_flags = gather_nd(flags, top_coordinates)
    topk_gathered_scores = gather_nd(scores_to_gather, top_coordinates)
    if states_to_gather is not None:
        topk_gathered_states = map_structure(lambda state: gather_nd(state, top_coordinates), states_to_gather)
    else:
        topk_gathered_states = states_to_gather
    return topk_seq, topk_gathered_scores, topk_flags, topk_gathered_states


def log_prob_from_logits(logits: torch.Tensor) ->torch.Tensor:
    return logits - torch.logsumexp(logits, dim=-1, keepdim=True)


def beam_search(symbols_to_logits_fn, initial_ids, beam_size, decode_length, vocab_size, alpha, eos_id, states=None, stop_early=True):
    """Beam search with length penalties.

    Requires a function that can take the currently decoded symbols and
    return the logits for the next symbol. The implementation is inspired
    by https://arxiv.org/abs/1609.08144.

    Variables used within this function follow the naming pattern:
    `(alive|finished)_topk_(seq,scores)`.

    Variables marked `alive` represent the new beam sequences that will be
    processed in the next step.    Variables marked `finished` represent
    the completed beam sequences, which may be padded with 0 if no beams
    finished.

    Variables marked `seq` store the full beam sequence for the time step.
    Variables marked `scores` store the sequence's final log scores.

    The beam search steps will be processed sequentially in order, so when
    capturing observed from these operations, tensors, clients can make
    assumptions about which step is being recorded.

    Args:
        symbols_to_logits_fn: Interface to the model, to provide logits.
            Should take `[batch_size, decoded_ids]` and return
            `[batch_size, vocab_size]`.
        initial_ids: LongTensor of shape `[batch_size]`. IDs to start off the
            decoding, this will be the first thing handed to
            :attr:`symbols_to_logits_fn` (after expanding to beam size).
        beam_size: Size of the beam.
        decode_length: Number of steps to decode for.
        vocab_size: Size of the vocab, must equal the size of the logits
            returned by :attr:`symbols_to_logits_fn`.
        alpha: alpha for length penalty.
        eos_id: ID for end of sentence.
        states: (possibly nested structure of) decoding states.
        stop_early: a boolean - stop once best sequence is provably
            determined.

    Returns:
        Tuple of

        - decoded beams (shape: `[batch_size, beam_size, decode_length]`)
        - decoding probabilities (shape: `[batch_size, beam_size]`)
    """
    batch_size = initial_ids.size()[0]
    initial_log_probs = torch.Tensor([[0.0] + [-float('inf')] * (beam_size - 1)])
    initial_log_probs = initial_log_probs
    alive_log_probs = initial_log_probs.repeat((batch_size, 1))
    alive_seq = _expand_to_beam_size(initial_ids, beam_size)
    alive_seq = torch.unsqueeze(alive_seq, dim=2)
    if states is not None:
        states = map_structure(lambda state: _expand_to_beam_size(state, beam_size), states)
    finished_seq = torch.zeros(alive_seq.size(), dtype=torch.long)
    finished_scores = torch.full((batch_size, beam_size), -INF)
    finished_flags = torch.zeros((batch_size, beam_size), dtype=torch_bool)
    finished_seq = finished_seq
    finished_scores = finished_scores
    finished_flags = finished_flags

    def grow_finished(finished_seq: torch.LongTensor, finished_scores: torch.Tensor, finished_flags: torch.ByteTensor, curr_seq: torch.LongTensor, curr_scores: torch.Tensor, curr_finished: torch.ByteTensor) ->Tuple[torch.LongTensor, torch.Tensor, torch.ByteTensor]:
        """Given sequences and scores, will gather the top-k (`k = beam`) size
        sequences.

        Args:
            finished_seq: Finished sequences.
                Shape: `[batch_size, beam_size, current_decoded_length]`.
            finished_scores: Scores for each finished sequences.
                Shape: `[batch_size, beam_size]`.
            finished_flags: Finished flags for each of these sequences.
                Shape: `[batch_size, beam_size]`
            curr_seq: Top-k sequences that has been grown by one
                position.
                Shape: `[batch_size, beam_size, current_decoded_length]`.
            curr_scores: Scores for each of the top-k sequences.
                Shape: `[batch_size, beam_size]`.
            curr_finished: Finished flags for each of the top-k sequences.
                Shape: `[batch_size, beam_size]`.

        Returns:
            Tuple of

            - Top-k sequences based on scores.
            - Log-probabilities of these sequences.
            - Finished flags of these sequences.
        """
        _appended = torch.zeros(batch_size, beam_size, 1, dtype=torch.long)
        _appended = _appended
        finished_seq = torch.cat([finished_seq, _appended], dim=2)
        curr_scores = curr_scores + (1.0 - curr_finished.float()) * -INF
        curr_finished_seq = torch.cat([finished_seq, curr_seq], dim=1)
        curr_finished_scores = torch.cat([finished_scores, curr_scores], dim=1)
        curr_finished_flags = torch.cat([finished_flags, curr_finished], dim=1)
        next_seq, next_scores, next_flags, _ = compute_topk_scores_and_seq(curr_finished_seq, curr_finished_scores, curr_finished_scores, curr_finished_flags, beam_size, batch_size)
        return next_seq, next_scores, next_flags

    def grow_alive(curr_seq: torch.LongTensor, curr_scores: torch.Tensor, curr_log_probs: torch.Tensor, curr_finished: torch.ByteTensor, states: Optional[State]) ->Tuple[torch.LongTensor, torch.Tensor, torch.ByteTensor, Optional[State]]:
        """Given sequences and scores, will gather the top k=beam size
        sequences.

        Args:
            curr_seq: Current top-k sequences that has been grown by one
                position.
                Shape: `[batch_size, beam_size, i + 1]`.
            curr_scores: Scores for each of these sequences.
                Shape: `[batch_size, beam_size]`.
            curr_log_probs: Log-probabilities for each of these sequences.
                Shape: `[batch_size, beam_size]`.
            curr_finished: Finished flags for each of these sequences.
                Shape: `[batch_size, beam_size]`.
            states: (possibly nested structure of) decoding states.

        :returns: Tuple of:

            - Top-k sequences based on scores.
            - Log-probabilities of these sequences.
            - Finished flags of these sequences.
            - Decoding states for these sequences.
        """
        curr_scores = curr_scores + curr_finished.float() * -INF
        return compute_topk_scores_and_seq(curr_seq, curr_scores, curr_log_probs, curr_finished, beam_size, batch_size, states)

    def grow_topk(i: int, alive_seq: torch.LongTensor, alive_log_probs: torch.Tensor, states: Optional[State]) ->Tuple[torch.LongTensor, torch.Tensor, torch.Tensor, torch.ByteTensor, Optional[State]]:
        """Inner beam search loop.

        This function takes the current alive sequences, and grows them to
        top-k sequences where `k = 2 * beam`. We use `2 * beam` because we could
        have `beam_size` number of sequences that might hit `<EOS>` and there
        will be no alive sequences to continue. With `2 * beam_size`, this
        will not happen. This relies on the assumption the vocab size is >
        beam size. If this is true, we'll have at least `beam_size` non-`<EOS>`
        extensions if we extract the next top `2 * beam` words.
        Length penalty is given by :math:`(5+len(decode)/6) ^ -\\alpha`.

        Please refer to https://arxiv.org/abs/1609.08144.

        Args:
            i: loop index
            alive_seq: Top-k sequences decoded so far.
                Shape: `[batch_size, beam_size, i + 1]`.
            alive_log_probs: Log-probabilities of these sequences.
                Shape: `[batch_size, beam_size]`
            states: (possibly nested structure of) decoding states.

        :returns: Tuple of:

            - Top-k sequences extended by the next word.
            - Log-probabilities of these sequences,
            - The scores with length penalty of these sequences,
            - Flags indicating which of these sequences have finished
              decoding.
            - Transformed decoding states with same structure as :attr:`state`.
        """
        flat_ids = alive_seq.view(batch_size * beam_size, -1)
        if states is not None:
            flat_states = map_structure(_merge_beam_dim, states)
            flat_logits, flat_states = symbols_to_logits_fn(flat_ids, flat_states)
            states = map_structure(lambda t: _unmerge_beam_dim(t, batch_size, beam_size), flat_states)
        else:
            flat_logits = symbols_to_logits_fn(flat_ids)
        logits = flat_logits.view(batch_size, beam_size, -1)
        candidate_log_probs = log_prob_from_logits(logits)
        log_probs = candidate_log_probs + alive_log_probs.unsqueeze(dim=2)
        length_penalty = ((5.0 + float(i + 1)) / 6.0) ** alpha
        curr_scores = log_probs / length_penalty
        flat_curr_scores = curr_scores.view(-1, beam_size * vocab_size)
        topk_scores, topk_ids = torch.topk(flat_curr_scores, k=beam_size * 2)
        topk_log_probs = topk_scores * length_penalty
        topk_beam_index = topk_ids / vocab_size
        topk_ids %= vocab_size
        batch_pos = compute_batch_indices(batch_size, beam_size * 2)
        batch_pos = batch_pos
        topk_coordinates = torch.stack([batch_pos, topk_beam_index], dim=2)
        topk_seq = gather_nd(alive_seq, topk_coordinates)
        if states is not None:
            states = map_structure(lambda state: gather_nd(state, topk_coordinates), states)
        topk_seq = torch.cat([topk_seq, topk_ids.unsqueeze(dim=2)], dim=2)
        topk_finished = topk_ids == eos_id
        return topk_seq, topk_log_probs, topk_scores, topk_finished, states

    def inner_loop(i: int, alive_seq: torch.LongTensor, alive_log_probs: torch.Tensor, finished_seq: torch.LongTensor, finished_scores: torch.Tensor, finished_flags: torch.ByteTensor, states: Optional[State]) ->Tuple[int, torch.LongTensor, torch.Tensor, torch.LongTensor, torch.Tensor, torch.ByteTensor, Optional[State]]:
        """Inner beam search loop.

        There are three groups of tensors: `alive`, `finished`, and `top-k`.

        - The `alive` group contains information about the current alive
          sequences.
        - The `top-k` group contains information about `alive + top_k`
          current decoded words.
        - The `finished` group contains information about finished sentences,
          that is, the ones that have decoded to `<EOS>`. These are what we
          return.

        The general beam search algorithm is as follows:

            While not terminated (please refer to termination condition):

            1. Grow the current `alive` to get `beam * 2` top-k sequences.
            2. Among the `top-k`, move the top `beam_size` ones that haven't
               reached `EOS` into `alive`.
            3. Among the `top-k`, move the top `beam_size` ones have reached
               `EOS` into `finished`.

            Repeat

        To make things simple with using fixed size tensors, we will end
        up inserting unfinished sequences into finished in the beginning.
        To prevent that we add `-INF` to the score of the unfinished
        sequence so that when a true finished sequence does appear, it
        will have a higher score than all the unfinished ones.

        Args:
            i: Loop index
            alive_seq: Topk sequences decoded so far
                Shape: `[batch_size, beam_size, i + 1]`.
            alive_log_probs: Log-probabilities of the beams.
                Shape: `[batch_size, beam_size]`
            finished_seq: Current finished sequences.
                Shape: `[batch_size, beam_size, i+1]`.
            finished_scores: Scores for each of these sequences.
                Shape: `[batch_size, beam_size]`.
            finished_flags: Finished flags for each of these sequences.
                Shape: `[batch_size, beam_size]`
            states: (possibly nested structure of) decoding states.

        :returns: Tuple of:

            - Incremented loop index.
            - New `alive` sequences.
            - Log-probabilities of the `alive` sequences.
            - New `finished` sequences.
            - Scores of the `finished` sequences.
            - Flags indicating which sequences in `finished` has reached `EOS`.
            - Final decoding states with same structure as :attr:`state`.
        """
        topk_seq, topk_log_probs, topk_scores, topk_finished, states = grow_topk(i, alive_seq, alive_log_probs, states)
        alive_seq, alive_log_probs, _, states = grow_alive(topk_seq, topk_scores, topk_log_probs, topk_finished, states)
        finished_seq, finished_scores, finished_flags = grow_finished(finished_seq, finished_scores, finished_flags, topk_seq, topk_scores, topk_finished)
        return i + 1, alive_seq, alive_log_probs, finished_seq, finished_scores, finished_flags, states

    def _is_finished(i: int, alive_log_probs: torch.Tensor, finished_scores: torch.Tensor) ->bool:
        """Check termination condition.

        We terminate when we decoded up to `decode_length` or the lowest
        scoring item in finished has a greater score that the highest probable
        item in alive divided by the max length penalty.

        Args:
            i: Loop index
            alive_log_probs: Log-probabilities of the beams.
                Shape: `[batch_size, beam_size]`.
            finished_scores: Scores for each of these sequences.
                Shape: `[batch_size, beam_size]`.

        Returns:
            Bool.
        """
        max_length_penalty = ((5.0 + float(decode_length)) / 6.0) ** alpha
        lower_bound_alive_scores = alive_log_probs[:, (0)] / max_length_penalty
        if not stop_early:
            lowest_score_of_finished_in_finished = torch.min(finished_scores)
        else:
            lowest_score_of_finished_in_finished, _ = torch.max(finished_scores, dim=1)
        bound_is_met = (lowest_score_of_finished_in_finished > lower_bound_alive_scores).all().item()
        ret = (i < decode_length) & ~bound_is_met
        return ret
    step = 0
    while _is_finished(step, alive_log_probs, finished_scores):
        step, alive_seq, alive_log_probs, finished_seq, finished_scores, finished_flags, states = inner_loop(step, alive_seq, alive_log_probs, finished_seq, finished_scores, finished_flags, states)
    ret_seq, ret_scores = [], []
    for idx, flag_per_instance in enumerate(finished_flags.any(dim=1).tolist()):
        if flag_per_instance:
            ret_seq.append(finished_seq[idx])
            ret_scores.append(finished_scores[idx])
        else:
            ret_seq.append(alive_seq[idx])
            ret_scores.append(alive_log_probs[idx])
    ret_seq = torch.stack(ret_seq, dim=0)
    ret_scores = torch.stack(ret_scores, dim=0)
    return ret_seq, ret_scores


class T5Encoder(TransformerEncoder):
    """Transformer based encoder that applies multi-head self attention with
    relative positional representations for encoding sequences for T5.

    This module basically stacks
    :class:`~texar.torch.modules.pretrained.t5_utils.MultiheadRPRAttention`,
    :class:`~texar.torch.modules.FeedForwardNetwork` and residual connections.
    This module supports the standard T5 architecture proposed in
    `(Raffel et al.) "Exploring the Limits of Transfer Learning with a Unified
    Text-to-Text Transformer"`.

    Args:
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    .. document private functions
    """

    def __init__(self, hparams=None):
        super().__init__(hparams=hparams)
        self.final_layer_norm = T5LayerNorm(self._input_size, eps=self._hparams.eps)

    def initialize_blocks(self):
        """Helper function to initialize blocks.
        """
        for i in range(self._hparams.num_blocks):
            mh_attn = MultiheadRPRAttention(self._input_size, self._hparams.multihead_attention, stores_relative_position=bool(i == 0))
            self.self_attns.append(mh_attn)
            self.self_attn_layer_norm.append(T5LayerNorm(self._input_size, eps=self._hparams.eps))
            if self._hparams.dim != mh_attn.hparams.output_dim:
                raise ValueError('The "dim" in the hparams of "multihead_attention" should be equal to the "dim" of T5Encoder')
            pw_net = FeedForwardNetwork(hparams=self._hparams['poswise_feedforward'])
            final_dim = pw_net.hparams.layers[-1]['kwargs']['out_features']
            if self._hparams.dim != final_dim:
                raise ValueError('The output dimenstion of "poswise_feedforward" should be equal to the "dim" of T5Encoder.')
            self.poswise_networks.append(pw_net)
            self.poswise_layer_norm.append(T5LayerNorm(self._input_size, eps=self._hparams.eps))

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "num_blocks": 6,
                "dim": 512,
                "embedding_dropout": 0.1,
                "residual_dropout": 0.1,
                "use_bert_config: False,
                "poswise_feedforward": default_transformer_poswise_net_hparams,
                'multihead_attention': {
                    'name': 'multihead_rpr_attention',
                    'num_units': 512,
                    'num_heads': 8,
                    'dropout_rate': 0.1,
                    'output_dim': 512,
                    'use_bias': False,
                    'is_decoder': False,
                    'relative_attention_num_buckets': 32
                },
                "initializer": None,
                "eps": 1e-6,
                "name": "t5_encoder"
            }

        Here:

        `"num_blocks"`: int
            Number of stacked blocks.

        `"dim"`: int
            Hidden dimension of the encoders.

        `"embedding_dropout"`: float
            Dropout rate of the input embedding.

        `"residual_dropout"`: float
            Dropout rate of the residual connections.

        "eps"`: float
            Epsilon values for layer norm layers.

        `"poswise_feedforward"`: dict
            Hyperparameters for a feed-forward network used in residual
            connections.
            Make sure the dimension of the output tensor is equal to ``"dim"``.
            See
            :func:`~texar.torch.modules.default_transformer_poswise_net_hparams`
            for details.

        `"multihead_attention"`: dict
            Hyperparameters for the multi-head attention strategy.
            Make sure the ``"output_dim"`` in this module is equal to ``"dim"``.
            See :class:`~texar.torch.modules.MultiheadRPRAttention` for
            details.

        `"initializer"`: dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.torch.core.get_initializer` for details.

        `"name"`: str
            Name of the module.
        """
        dim = 512
        return {'num_blocks': 6, 'dim': dim, 'embedding_dropout': 0.1, 'residual_dropout': 0.1, 'use_bert_config': False, 'poswise_feedforward': default_transformer_poswise_net_hparams(dim), 'multihead_attention': {'name': 'multihead_rpr_attention', 'num_units': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'output_dim': 512, 'use_bias': False, 'is_decoder': False, 'relative_attention_num_buckets': 32}, 'initializer': None, 'eps': 1e-06, 'name': 't5_encoder'}

    def forward(self, inputs: torch.Tensor, sequence_length: torch.LongTensor) ->torch.Tensor:
        """Encodes the inputs.

        Args:
            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``,
                containing the embedding of input sequences. Note that
                the embedding dimension `dim` must equal `"dim"` in
                :attr:`hparams`. The input embedding is typically an
                aggregation of word embedding and position embedding.
            sequence_length: A 1D :tensor:`LongTensor` of shape
                ``[batch_size]``. Input tokens beyond respective sequence
                lengths are masked out automatically.

        Returns:
            A Tensor of shape ``[batch_size, max_time, dim]`` containing the
            encoded vectors.
        """
        inputs_padding = 1 - sequence_mask(sequence_length, inputs.size()[1]).float()
        ignore_padding = attn.attention_bias_ignore_padding(inputs_padding)
        encoder_self_attention_bias = ignore_padding
        x = self.embed_dropout(inputs)
        position_bias = None
        for i in range(self._hparams.num_blocks):
            _queries_input = self.self_attn_layer_norm[i](x)
            attention_output, position_bias = self.self_attns[i](queries=_queries_input, memory=_queries_input, memory_attention_bias=encoder_self_attention_bias, position_bias=position_bias)
            attention_output = self.residual_dropout(attention_output)
            x = x + attention_output
            poswise_network = self.poswise_networks[i]
            poswise_normalizer = self.poswise_layer_norm[i]
            y = poswise_normalizer(x)
            original_shape = y.size()
            y = y.view(-1, self._hparams.dim)
            layer_output = poswise_network(y)
            sub_output = self.residual_dropout(layer_output)
            sub_output = sub_output.view(original_shape)
            x = x + sub_output
        x = self.final_layer_norm(x)
        return x


class T5EncoderDecoder(EncoderDecoderBase, PretrainedT5Mixin):
    """The pre-trained T5 model. Please see
    :class:`~texar.torch.modules.PretrainedT5Mixin` for a brief description
    of T5.

    This module basically stacks
    :class:`~texar.torch.modules.WordEmbedder`,
    :class:`~texar.torch.modules.T5Encoder`, and
    :class:`~texar.torch.modules.T5Decoder`.

    Args:
        pretrained_model_name (optional): a `str`, the name
            of pre-trained model (e.g., ``T5-Small``). Please refer to
            :class:`~texar.torch.modules.PretrainedT5Mixin` for
            all supported models.
            If `None`, the model name in :attr:`hparams` is used.
        cache_dir (optional): the path to a folder in which the
            pre-trained models will be cached. If `None` (default),
            a default directory (``texar_data`` folder under user's home
            directory) will be used.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameter will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure
            and default values.
    """

    def __init__(self, pretrained_model_name: Optional[str]=None, cache_dir: Optional[str]=None, hparams=None):
        super().__init__(hparams=hparams)
        self.load_pretrained_config(pretrained_model_name, cache_dir)
        self.word_embedder = WordEmbedder(vocab_size=self._hparams.vocab_size, hparams=self._hparams.embed)
        self.encoder = T5Encoder(hparams=self._hparams.encoder)
        self.decoder = T5Decoder(token_embedder=self._embedding_fn, output_layer=Identity(), hparams=self._hparams.decoder)
        self.init_pretrained_weights()

    def _embedding_fn(self, tokens: torch.LongTensor) ->torch.Tensor:
        word_embed = self.word_embedder(tokens)
        return word_embed

    def reset_parameters(self):
        initialize = layers.get_initializer(self._hparams.initializer)
        if initialize is not None:
            for name, param in self.named_parameters():
                if name.split('.')[-1] == 'weight' and 'layer_norm' not in name:
                    initialize(param)

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        * The model arch is determined by the constructor argument
          :attr:`pretrained_model_name` if it's specified. In this case,
          `hparams` are ignored.
        * Otherwise, the model arch is determined by
          `hparams['pretrained_model_name']` if it's specified. All other
          configurations in `hparams` are ignored.
        * If the above two are `None`, the encoder arch is defined by the
          configurations in `hparams` and weights are randomly initialized.

        .. code-block:: python

            {
                "pretrained_model_name": "T5-Small",
                "embed": {
                    "dim": 768,
                    "name": "word_embeddings"
                },
                "vocab_size": 32128,

                "encoder": {
                    "dim": 768,
                    "embedding_dropout": 0.1,
                    "multihead_attention": {
                        "dropout_rate": 0.1,
                        "name": "self",
                        "num_heads": 12,
                        "num_units": 768,
                        "output_dim": 768,
                        "use_bias": False,
                        "is_decoder": False,
                        "relative_attention_num_buckets": 32,
                    },
                    "eps": 1e-6,
                    "name": "encoder",
                    "num_blocks": 12,
                    "poswise_feedforward": {
                        "layers": [
                            {
                                "kwargs": {
                                    "in_features": 768,
                                    "out_features": 3072,
                                    "bias": False
                                },
                                "type": "Linear"
                            },
                            {"type": "ReLU"},
                            {
                                "kwargs": {
                                    "in_features": 3072,
                                    "out_features": 768,
                                    "bias": False
                                },
                                "type": "Linear"
                            }
                        ]
                    },
                    "residual_dropout": 0.1,
                    },

                "decoder": {
                    "eps": 1e-6,
                    "dim": 768,
                    "embedding_dropout": 0.1,
                    "multihead_attention": {
                        "dropout_rate": 0.1,
                        "name": "self",
                        "num_heads": 12,
                        "num_units": 768,
                        "output_dim": 768,
                        "use_bias": False,
                        "is_decoder": True,
                        "relative_attention_num_buckets": 32,
                    },
                    "name": "decoder",
                    "num_blocks": 12,
                    "poswise_feedforward": {
                        "layers": [
                            {
                                "kwargs": {
                                    "in_features": 768,
                                    "out_features": 3072,
                                    "bias": False
                                },
                                "type": "Linear"
                            },
                            {"type": "ReLU"},
                            {
                                "kwargs": {
                                    "in_features": 3072,
                                    "out_features": 768,
                                    "bias": False
                                },
                                "type": "Linear"
                            }
                        ]
                    },
                    "residual_dropout": 0.1,
                    },
                "hidden_size": 768,
                "initializer": None,
                "name": "t5_encoder_decoder",
            }

        Here:

        The default parameters are values for T5-Small model.

        `"pretrained_model_name"`: str or None
            The name of the pre-trained T5 model. If None, the model
            will be randomly initialized.

        `"embed"`: dict
            Hyperparameters for word embedding layer.

        `"vocab_size"`: int
            The vocabulary size of `inputs` in T5 model.

        `"encoder"`: dict
            Hyperparameters for the `T5Encoder`.
            See :func:`~texar.torch.modules.T5Encoder.default_hparams`
            for details.

        `"decoder"`: dict
            Hyperparameters for the `T5Decoder`.
            See :func:`~texar.torch.modules.T5Decoder.default_hparams`
            for details.

        `"hidden_size"`: int
            Size of the hidden layer.

        `"initializer"`: dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.torch.core.get_initializer` for details.

        `"name"`: str
            Name of the module.
        """
        return {'pretrained_model_name': 'T5-Small', 'embed': {'dim': 768, 'name': 'word_embeddings'}, 'vocab_size': 32128, 'encoder': {'dim': 768, 'embedding_dropout': 0.1, 'multihead_attention': {'dropout_rate': 0.1, 'name': 'self', 'num_heads': 12, 'num_units': 768, 'output_dim': 768, 'use_bias': False, 'is_decoder': False, 'relative_attention_num_buckets': 32}, 'eps': 1e-06, 'name': 'encoder', 'num_blocks': 12, 'poswise_feedforward': {'layers': [{'kwargs': {'in_features': 768, 'out_features': 3072, 'bias': False}, 'type': 'Linear'}, {'type': 'ReLU'}, {'kwargs': {'in_features': 3072, 'out_features': 768, 'bias': False}, 'type': 'Linear'}]}, 'residual_dropout': 0.1}, 'decoder': {'eps': 1e-06, 'dim': 768, 'embedding_dropout': 0.1, 'multihead_attention': {'dropout_rate': 0.1, 'name': 'self', 'num_heads': 12, 'num_units': 768, 'output_dim': 768, 'use_bias': False, 'is_decoder': True, 'relative_attention_num_buckets': 32}, 'name': 'decoder', 'num_blocks': 12, 'poswise_feedforward': {'layers': [{'kwargs': {'in_features': 768, 'out_features': 3072, 'bias': False}, 'type': 'Linear'}, {'type': 'ReLU'}, {'kwargs': {'in_features': 3072, 'out_features': 768, 'bias': False}, 'type': 'Linear'}]}, 'residual_dropout': 0.1}, 'hidden_size': 768, 'initializer': None, 'name': 't5_encoder_decoder', '@no_typecheck': ['pretrained_model_name']}

    def forward(self, inputs: Union[torch.Tensor, torch.LongTensor], sequence_length: Optional[torch.LongTensor]=None):
        """Performs encoding and decoding.

        Args:
            inputs: Either a **2D Tensor** of shape ``[batch_size, max_time]``,
                containing the ids of tokens in input sequences, or
                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,
                containing soft token ids (i.e., weights or probabilities)
                used to mix the embedding vectors.
            sequence_length: A 1D :tensor:`Tensor` of shape
                ``[batch_size]``. Input tokens beyond respective sequence
                lengths are masked out automatically.

        Returns:
            A pair :attr:`(encoder_output, decoder_output)`

            - :attr:`encoder_output`: A Tensor of shape
              `[batch_size, max_time, dim]` containing the encoded vectors.

            - :attr:`decoder_output`: An instance of
              :class:`~texar.torch.modules.TransformerDecoderOutput` which
              contains `sample_id` and `logits`.
        """
        if inputs.dim() == 2:
            word_embeds = self.word_embedder(ids=inputs)
        elif inputs.dim() == 3:
            word_embeds = self.word_embedder(soft_ids=inputs)
        else:
            raise ValueError("'inputs' should be a 2D or 3D tensor.")
        batch_size = inputs.size(0)
        if sequence_length is None:
            sequence_length = inputs.new_full((batch_size,), inputs.size(1), dtype=torch.long)
        encoder_output = self.encoder(inputs=word_embeds, sequence_length=sequence_length)
        decoder_output = self.decoder(inputs=inputs, memory=encoder_output, memory_sequence_length=sequence_length)
        return encoder_output, decoder_output

    @property
    def output_size(self):
        """The feature size of :meth:`forward` output of the encoder.
        """
        return self._hparams.hidden_size


def reverse_sequence(inputs: torch.Tensor, seq_lengths: Union[torch.LongTensor, List[int]], time_major: bool) ->torch.Tensor:
    """Reverses variable length slices.

    This op first slices input along the dimension batch_axis, and for each
    slice i, reverses the first seq_lengths[i] elements along the dimension
    seq_axis.

    The elements of seq_lengths must obey seq_lengths[i] <=
    input.dims[seq_dim], and seq_lengths must be a vector of length
    input.dims[batch_dim].

    The output slice i along dimension batch_axis is then given by input slice
    i, with the first seq_lengths[i] slices along dimension seq_axis reversed.

    Args:
        inputs: A Tensor. The input to reverse.
        seq_lengths: A Tensor. Must be one of the following types: int32,
            int64. 1-D with length input.dims(batch_dim) and
            max(seq_lengths) <= input.dims(seq_dim)
        time_major: The shape format of the ``inputs`` and ``outputs`` Tensors.
            If true, these ``Tensors`` must be shaped
            ``[max_time, batch_size, depth]``. If false, these ``Tensors`` must
            be shaped ``[batch_size, max_time, depth]``.
            Using ``time_major = True`` is a bit more efficient because it
            avoids transposes at the beginning and end of the RNN calculation.
            However, most TensorFlow data is batch-major, so by
            default this functionb accepts input and emits output
            in batch-major form.

    Returns:
        A ``Tensor``. Has the same type as input.
    """
    if time_major:
        inputs = inputs.permute(1, 0, 2)
    batch_size = inputs.shape[0]
    outputs = inputs.clone()
    for i in range(batch_size):
        outputs[i][0:seq_lengths[i]] = torch.flip(inputs[i][0:seq_lengths[i]], dims=(0,))
    if time_major:
        outputs = outputs.permute(1, 0, 2)
    return outputs


def bidirectional_dynamic_rnn(cell_fw: RNNCellBase[State], cell_bw: RNNCellBase[State], inputs: torch.Tensor, sequence_length: Optional[Union[torch.LongTensor, List[int]]]=None, initial_state_fw: Optional[State]=None, initial_state_bw: Optional[State]=None, time_major: bool=False) ->Tuple[Tuple[torch.Tensor, torch.Tensor], Tuple[State, State]]:
    """Creates a dynamic version of bidirectional recurrent neural network.

    Takes input and builds independent forward and backward RNNs. The
    input_size of forward and backward cell must match. The initial state for
    both directions is zero by default (but can be set optionally) and no
    intermediate states are ever returned -- the network is fully unrolled
    for the given (passed in) length(s) of the sequence(s) or completely
    unrolled if length(s) is not given.

    Args:
        cell_fw: An instance of RNNCell, to be used for forward direction.
        cell_bw: An instance of RNNCell, to be used for backward direction.
        inputs: The RNN inputs.
            If time_major == False (default), this must be a tensor of shape:
            ``[batch_size, max_time, ...]``, or a nested tuple of such elements.
            If time_major == True, this must be a tensor of shape:
            ``[max_time, batch_size, ...]``, or a nested tuple of such elements.
        sequence_length: (optional) An int32/int64 tensor, size
            ``[batch_size]``, containing the actual lengths for each of the
            sequences in
            the batch. If not provided, all batch entries are assumed
            to be full sequences; and time reversal is applied from time
            ``0`` to ``max_time`` for each sequence.
        initial_state_fw: (optional) An initial state for the forward RNN.
            This must be a tensor of appropriate type and shape
            ``[batch_size, cell_fw.state_size]``.
            If ``cell_fw.state_size`` is a tuple, this should be a tuple of
            tensors having shapes ``[batch_size, s]``
            for ``s`` in ``cell_fw.state_size``.
        initial_state_bw: (optional) Same as for ``initial_state_fw``, but using
            the corresponding properties of ``cell_bw``.
        time_major: The shape format of the ``inputs`` and ``outputs`` Tensors.
            If true, these ``Tensors`` must be shaped
            ``[max_time, batch_size, depth]``.
            If false, these ``Tensors`` must be shaped
            ``[batch_size, max_time, depth]``.
            Using ``time_major = True`` is a bit more efficient because it
            avoids transposes at the beginning and end of the RNN calculation.
            However, most TensorFlow data is batch-major, so by
            default this function accepts input and emits output
            in batch-major form.

    Returns:
        A tuple (outputs, output_states) where:

        outputs: A tuple (output_fw, output_bw) containing the forward and
            the backward rnn output ``Tensor``.
            If time_major == False (default),
                output_fw will be a ``Tensor`` shaped:
                ``[batch_size, max_time, cell_fw.output_size]``
                and output_bw will be a ``Tensor`` shaped:
                ``[batch_size, max_time, cell_bw.output_size]``.
            If time_major == True,
                output_fw will be a ``Tensor`` shaped:
                ``[max_time, batch_size, cell_fw.output_size]``
                and output_bw will be a ``Tensor`` shaped:
                ``[max_time, batch_size, cell_bw.output_size]``.
            It returns a tuple instead of a single concatenated ``Tensor``,
            unlike in the ``bidirectional_rnn``. If the concatenated
            one is preferred, the forward and backward outputs can
            be concatenated as ``tf.concat(outputs, 2)``.
        output_states: A tuple (output_state_fw, output_state_bw) containing
            the forward and the backward final states of bidirectional rnn.
    """
    output_fw, output_state_fw = dynamic_rnn(cell=cell_fw, inputs=inputs, sequence_length=sequence_length, initial_state=initial_state_fw, time_major=time_major)
    if time_major:
        time_steps = inputs.shape[0]
        batch_size = inputs.shape[1]
    else:
        time_steps = inputs.shape[1]
        batch_size = inputs.shape[0]
    if sequence_length is None:
        sequence_length = torch.tensor([time_steps] * batch_size, dtype=torch.int32, device=inputs.device)
    inputs_reverse = reverse_sequence(inputs=inputs, seq_lengths=sequence_length, time_major=time_major)
    tmp, output_state_bw = dynamic_rnn(cell=cell_bw, inputs=inputs_reverse, sequence_length=sequence_length, initial_state=initial_state_bw, time_major=time_major)
    output_bw = reverse_sequence(inputs=tmp, seq_lengths=sequence_length, time_major=time_major)
    outputs = output_fw, output_bw
    output_states = output_state_fw, output_state_bw
    return outputs, output_states


class BidirectionalRNNEncoder(RNNEncoderBase):
    """Bidirectional forward-backward RNN encoder.

    Args:
        cell_fw (RNNCell, optional): The forward RNN cell. If not given,
            a cell is created as specified in ``hparams["rnn_cell_fw"]``.
        cell_bw (RNNCell, optional): The backward RNN cell. If not given,
            a cell is created as specified in ``hparams["rnn_cell_bw"]``.
        output_layer_fw (optional): An instance of
            :torch_nn:`Module`. Apply to the forward
            RNN cell output of each step. If `None` (default), the output
            layer is created as specified in ``hparams["output_layer_fw"]``.
        output_layer_bw (optional): An instance of
            :torch_nn:`Module`. Apply to the backward
            RNN cell output of each step. If `None` (default), the output
            layer is created as specified in ``hparams["output_layer_bw"]``.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure and
            default values.

    See :meth:`forward` for the inputs and outputs of the encoder.

    Example:

        .. code-block:: python

            # Use with embedder
            embedder = WordEmbedder(vocab_size, hparams=emb_hparams)
            encoder = BidirectionalRNNEncoder(hparams=enc_hparams)

            outputs, final_state = encoder(
                inputs=embedder(data_batch['text_ids']),
                sequence_length=data_batch['length'])
            # outputs == (outputs_fw, outputs_bw)
            # final_state == (final_state_fw, final_state_bw)

    .. document private functions
    """

    def __init__(self, input_size: int, cell_fw: Optional[RNNCellBase[State]]=None, cell_bw: Optional[RNNCellBase[State]]=None, output_layer_fw: Optional[nn.Module]=None, output_layer_bw: Optional[nn.Module]=None, hparams=None):
        super().__init__(hparams=hparams)
        if cell_fw is not None:
            self._cell_fw = cell_fw
        else:
            self._cell_fw = layers.get_rnn_cell(input_size, self._hparams.rnn_cell_fw)
        if cell_bw is not None:
            self._cell_bw = cell_bw
        elif self._hparams.rnn_cell_share_config:
            self._cell_bw = layers.get_rnn_cell(input_size, self._hparams.rnn_cell_fw)
        else:
            self._cell_bw = layers.get_rnn_cell(input_size, self._hparams.rnn_cell_bw)
        self.__output_layer_fw: Optional[nn.Module]
        if output_layer_fw is not None:
            self._output_layer_fw = output_layer_fw
            self._output_layer_hparams_fw = None
        else:
            self._output_layer_fw = _build_dense_output_layer(self._cell_fw.hidden_size, self._hparams.output_layer_fw)
            self._output_layer_hparams_fw = self._hparams.output_layer_fw
        self.__output_layer_bw: Optional[nn.Module]
        if output_layer_bw is not None:
            self._output_layer_bw = output_layer_bw
            self._output_layer_hparams_bw = None
        elif self._hparams.output_layer_share_config:
            self._output_layer_bw = _build_dense_output_layer(self._cell_bw.hidden_size, self._hparams.output_layer_fw)
            self._output_layer_hparams_bw = self._hparams.output_layer_fw
        else:
            self._output_layer_bw = _build_dense_output_layer(self._cell_bw.hidden_size, self._hparams.output_layer_bw)
            self._output_layer_hparams_bw = self._hparams.output_layer_bw

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "rnn_cell_fw": default_rnn_cell_hparams(),
                "rnn_cell_bw": default_rnn_cell_hparams(),
                "rnn_cell_share_config": True,
                "output_layer_fw": {
                    "num_layers": 0,
                    "layer_size": 128,
                    "activation": "identity",
                    "final_layer_activation": None,
                    "other_dense_kwargs": None,
                    "dropout_layer_ids": [],
                    "dropout_rate": 0.5,
                    "variational_dropout": False
                },
                "output_layer_bw": {
                    # Same hyperparams and default values as "output_layer_fw"
                    # ...
                },
                "output_layer_share_config": True,
                "name": "bidirectional_rnn_encoder"
            }

        Here:

        `"rnn_cell_fw"`: dict
            Hyperparameters of the forward RNN cell.
            Ignored if :attr:`cell_fw` is given to the encoder constructor.

            The default value is defined in
            :func:`~texar.torch.core.default_rnn_cell_hparams`.

        `"rnn_cell_bw"`: dict
            Hyperparameters of the backward RNN cell.
            Ignored if :attr:`cell_bw` is given to the encoder constructor,
            or if `"rnn_cell_share_config"` is `True`.

            The default value is defined in
            :meth:`~texar.torch.core.default_rnn_cell_hparams`.

        `"rnn_cell_share_config"`: bool
            Whether share hyperparameters of the backward cell with the
            forward cell. Note that the cell parameters (variables) are not
            shared.

        `"output_layer_fw"`: dict
            Hyperparameters of the forward output layer. Ignored if
            ``output_layer_fw`` is given to the constructor.
            See the ``"output_layer"`` field of
            :meth:`~texar.torch.modules.UnidirectionalRNNEncoder` for details.

        `"output_layer_bw"`: dict
            Hyperparameters of the backward output layer. Ignored if
            :attr:`output_layer_bw` is given to the constructor. Have the
            same structure and defaults with :attr:`"output_layer_fw"`.

            Ignored if ``output_layer_share_config`` is `True`.

        `"output_layer_share_config"`: bool
            Whether share hyperparameters of the backward output layer
            with the forward output layer. Note that the layer parameters
            (variables) are not shared.

        `"name"`: str
            Name of the encoder
        """
        hparams = RNNEncoderBase.default_hparams()
        hparams.update({'rnn_cell_fw': layers.default_rnn_cell_hparams(), 'rnn_cell_bw': layers.default_rnn_cell_hparams(), 'rnn_cell_share_config': True, 'output_layer_fw': _default_output_layer_hparams(), 'output_layer_bw': _default_output_layer_hparams(), 'output_layer_share_config': True, 'name': 'bidirectional_rnn_encoder'})
        return hparams

    def forward(self, inputs: torch.Tensor, sequence_length: Optional[Union[torch.LongTensor, List[int]]]=None, initial_state_fw: Optional[State]=None, initial_state_bw: Optional[State]=None, time_major: bool=False, return_cell_output: bool=False, return_output_size: bool=False):
        """Encodes the inputs.

        Args:
            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``.
                The first two dimensions
                ``batch_size`` and ``max_time`` may be exchanged if
                ``time_major`` is `True`.
            sequence_length (optional): A 1D :tensor:`LongTensor` of shape
                ``[batch_size]``.
                Sequence lengths of the batch inputs. Used to copy-through
                state and zero-out outputs when past a batch element's sequence
                length.
            initial_state_fw: (optional): Initial state of the forward RNN.
            initial_state_bw: (optional): Initial state of the backward RNN.
            time_major (bool): The shape format of the :attr:`inputs` and
                :attr:`outputs` Tensors. If `True`, these tensors are of shape
                ``[max_time, batch_size, depth]``. If `False` (default),
                these tensors are of shape ``[batch_size, max_time, depth]``.
            return_cell_output (bool): Whether to return the output of the RNN
                cell. This is the results prior to the output layer.
            return_output_size (bool): Whether to return the output size of the
                RNN cell. This is the results after the output layer.

        Returns:
            - By default (both ``return_cell_output`` and ``return_output_size``
              are `False`), returns a pair :attr:`(outputs, final_state)`

              - :attr:`outputs`: A tuple ``(outputs_fw, outputs_bw)``
                containing the forward and the backward RNN outputs, each of
                which is of shape ``[batch_size, max_time, output_dim]``
                if ``time_major`` is `False`, or
                ``[max_time, batch_size, output_dim]`` if ``time_major``
                is `True`.
                If RNN cell output is a (nested) tuple of Tensors, then
                ``outputs_fw`` and ``outputs_bw`` will be a (nested) tuple
                having the same structure as the cell output.

              - :attr:`final_state`: A tuple
                ``(final_state_fw, final_state_bw)`` containing the final
                states of the forward and backward RNNs, each of which is a
                Tensor of shape ``[batch_size] + cell.state_size``, or a
                (nested) tuple of Tensors if ``cell.state_size`` is a (nested)
                tuple.

            - If ``return_cell_output`` is `True`, returns a triple
              :attr:`(outputs, final_state, cell_outputs)` where

              - :attr:`cell_outputs`: A tuple
                ``(cell_outputs_fw, cell_outputs_bw)`` containing the outputs
                by the forward and backward RNN cells prior to the output
                layers, having the same structure with :attr:`outputs` except
                for the ``output_dim``.

            - If ``return_output_size`` is `True`, returns a tuple
              :attr:`(outputs, final_state, output_size)` where

              - :attr:`output_size`: A tuple
                ``(output_size_fw, output_size_bw)`` containing the size of
                ``outputs_fw`` and ``outputs_bw``, respectively.
                Take ``*_fw`` for example, ``output_size_fw`` is a (possibly
                nested tuple of) int. If a single int or an int array, then
                ``outputs_fw`` has shape
                ``[batch/time, time/batch] + output_size_fw``. If a (nested)
                tuple, then ``output_size_fw`` has the same structure as
                ``outputs_fw``. The same applies to ``output_size_bw``.

            - If both ``return_cell_output`` and ``return_output_size`` are
              `True`, returns
              :attr:`(outputs, final_state, cell_outputs, output_size)`.
        """
        cell_outputs, states = bidirectional_dynamic_rnn(cell_fw=self._cell_fw, cell_bw=self._cell_bw, inputs=inputs, sequence_length=sequence_length, initial_state_fw=initial_state_fw, initial_state_bw=initial_state_bw, time_major=time_major)
        outputs_fw, output_size_fw = _forward_output_layers(inputs=cell_outputs[0], output_layer=self._output_layer_fw, time_major=time_major, sequence_length=sequence_length)
        outputs_bw, output_size_bw = _forward_output_layers(inputs=cell_outputs[1], output_layer=self._output_layer_bw, time_major=time_major, sequence_length=sequence_length)
        outputs = outputs_fw, outputs_bw
        output_size = output_size_fw, output_size_bw
        returns = outputs, states
        if return_cell_output:
            returns += cell_outputs,
        if return_output_size:
            returns += output_size,
        return returns

    @property
    def cell_fw(self) ->RNNCellBase[State]:
        """The forward RNN cell.
        """
        return self._cell_fw

    @property
    def cell_bw(self) ->RNNCellBase[State]:
        """The backward RNN cell.
        """
        return self._cell_bw

    @property
    def state_size_fw(self) ->int:
        """The state size of the forward encoder cell.
        Same as :attr:`encoder.cell_fw.state_size`.
        """
        if isinstance(self._cell_fw, LSTMCell):
            return 2 * self._cell_fw.hidden_size
        else:
            return self._cell_fw.hidden_size

    @property
    def state_size_bw(self) ->int:
        """The state size of the backward encoder cell.
        Same as :attr:`encoder.cell_bw.state_size`.
        """
        if isinstance(self._cell_bw, LSTMCell):
            return 2 * self._cell_bw.hidden_size
        else:
            return self._cell_bw.hidden_size

    @property
    def output_layer_fw(self) ->Optional[nn.Module]:
        """The output layer of the forward RNN.
        """
        return self._output_layer_fw

    @property
    def output_layer_bw(self) ->Optional[nn.Module]:
        """The output layer of the backward RNN.
        """
        return self._output_layer_bw

    @property
    def output_size(self) ->Tuple[int, int]:
        """The feature sizes of :meth:`forward` outputs
        :attr:`output_size_fw` and :attr:`output_size_bw`.
        Each feature size is equal to last dimension
        value of corresponding result size.
        """
        dim_bw = self._cell_bw.hidden_size
        dim_fw = self._cell_fw.hidden_size
        if self._output_layer_bw is not None:
            dummy_tensor_bw = torch.Tensor(dim_bw)
            output_bw = self._output_layer_bw(dummy_tensor_bw).size()[-1]
        else:
            output_bw = dim_bw
        if self._output_layer_fw is not None:
            dummy_tensor_fw = torch.Tensor(dim_fw)
            output_fw = self._output_layer_fw(dummy_tensor_fw).size()[-1]
        else:
            output_fw = dim_fw
        return output_fw, output_bw


_ROBERTA_PATH = 'https://dl.fbaipublicfiles.com/fairseq/models/'


class PretrainedRoBERTaMixin(PretrainedMixin, ABC):
    """A mixin class to support loading pre-trained checkpoints for modules
    that implement the RoBERTa model.

    The RoBERTa model was proposed in (`Liu et al`. 2019)
    `RoBERTa: A Robustly Optimized BERT Pretraining Approach`_.
    As a variant of the standard BERT model, RoBERTa trains for more
    iterations on more data with a larger batch size as well as other tweaks
    in pre-training. Differing from the standard BERT, the RoBERTa model
    does not use segmentation embedding. Available model names include:

      * ``roberta-base``: RoBERTa using the BERT-base architecture,
        125M parameters.
      * ``roberta-large``: RoBERTa using the BERT-large architecture,
        355M parameters.

    We provide the following RoBERTa classes:

      * :class:`~texar.torch.modules.RoBERTaEncoder` for text encoding.
      * :class:`~texar.torch.modules.RoBERTaClassifier` for text
        classification and sequence tagging.

    .. _`RoBERTa: A Robustly Optimized BERT Pretraining Approach`:
        https://arxiv.org/abs/1907.11692
    """
    _MODEL_NAME = 'RoBERTa'
    _MODEL2URL = {'roberta-base': _ROBERTA_PATH + 'roberta.base.tar.gz', 'roberta-large': _ROBERTA_PATH + 'roberta.large.tar.gz'}

    @classmethod
    def _transform_config(cls, pretrained_model_name: str, cache_dir: str) ->Dict[str, Any]:
        info = list(os.walk(cache_dir))
        root, _, files = info[0]
        config_path = None
        for file in files:
            if file.endswith('model.pt'):
                config_path = os.path.join(root, file)
                args = torch.load(config_path, map_location='cpu')['args']
                hidden_dim = args.encoder_embed_dim
                vocab_size = 50265
                position_size = args.max_positions + 2
                embedding_dropout = args.dropout
                num_blocks = args.encoder_layers
                num_heads = args.encoder_attention_heads
                dropout_rate = args.attention_dropout
                residual_dropout = args.dropout
                intermediate_size = args.encoder_ffn_embed_dim
                hidden_act = args.activation_fn
        if config_path is None:
            raise ValueError(f'Cannot find the config file in {cache_dir}')
        configs = {'hidden_size': hidden_dim, 'embed': {'name': 'word_embeddings', 'dim': hidden_dim}, 'vocab_size': vocab_size, 'position_embed': {'name': 'position_embeddings', 'dim': hidden_dim}, 'position_size': position_size, 'encoder': {'name': 'encoder', 'embedding_dropout': embedding_dropout, 'num_blocks': num_blocks, 'multihead_attention': {'use_bias': True, 'num_units': hidden_dim, 'num_heads': num_heads, 'output_dim': hidden_dim, 'dropout_rate': dropout_rate, 'name': 'self'}, 'residual_dropout': residual_dropout, 'dim': hidden_dim, 'eps': 1e-12, 'use_bert_config': True, 'poswise_feedforward': {'layers': [{'type': 'Linear', 'kwargs': {'in_features': hidden_dim, 'out_features': intermediate_size, 'bias': True}}, {'type': 'Bert' + hidden_act.upper()}, {'type': 'Linear', 'kwargs': {'in_features': intermediate_size, 'out_features': hidden_dim, 'bias': True}}]}}}
        return configs

    def _init_from_checkpoint(self, pretrained_model_name: str, cache_dir: str, **kwargs):
        global_tensor_map = {'decoder.sentence_encoder.embed_tokens.weight': 'word_embedder._embedding', 'decoder.sentence_encoder.embed_positions.weight': 'position_embedder._embedding', 'decoder.sentence_encoder.emb_layer_norm.weight': 'encoder.input_normalizer.weight', 'decoder.sentence_encoder.emb_layer_norm.bias': 'encoder.input_normalizer.bias'}
        attention_tensor_map = {'final_layer_norm.weight': 'encoder.output_layer_norm.{}.weight', 'final_layer_norm.bias': 'encoder.output_layer_norm.{}.bias', 'fc1.weight': 'encoder.poswise_networks.{}._layers.0.weight', 'fc1.bias': 'encoder.poswise_networks.{}._layers.0.bias', 'fc2.weight': 'encoder.poswise_networks.{}._layers.2.weight', 'fc2.bias': 'encoder.poswise_networks.{}._layers.2.bias', 'self_attn_layer_norm.weight': 'encoder.poswise_layer_norm.{}.weight', 'self_attn_layer_norm.bias': 'encoder.poswise_layer_norm.{}.bias', 'self_attn.out_proj.weight': 'encoder.self_attns.{}.O_dense.weight', 'self_attn.out_proj.bias': 'encoder.self_attns.{}.O_dense.bias', 'self_attn.in_proj_weight': ['encoder.self_attns.{}.Q_dense.weight', 'encoder.self_attns.{}.K_dense.weight', 'encoder.self_attns.{}.V_dense.weight'], 'self_attn.in_proj_bias': ['encoder.self_attns.{}.Q_dense.bias', 'encoder.self_attns.{}.K_dense.bias', 'encoder.self_attns.{}.V_dense.bias']}
        checkpoint_path = os.path.abspath(os.path.join(cache_dir, 'model.pt'))
        device = next(self.parameters()).device
        params = torch.load(checkpoint_path, map_location=device)['model']
        for name, tensor in params.items():
            if name in global_tensor_map:
                v_name = global_tensor_map[name]
                pointer = self._name_to_variable(v_name)
                assert pointer.shape == tensor.shape
                pointer.data = tensor.data.type(pointer.dtype)
            elif name.startswith('decoder.sentence_encoder.layers.'):
                name = name.lstrip('decoder.sentence_encoder.layers.')
                layer_num, layer_name = name.split('.', 1)
                if layer_name in attention_tensor_map:
                    v_names = attention_tensor_map[layer_name]
                    if isinstance(v_names, str):
                        pointer = self._name_to_variable(v_names.format(layer_num))
                        assert pointer.shape == tensor.shape
                        pointer.data = tensor.data.type(pointer.dtype)
                    else:
                        tensors = torch.chunk(tensor, chunks=3, dim=0)
                        for i in range(3):
                            pointer = self._name_to_variable(v_names[i].format(layer_num))
                            assert pointer.shape == tensors[i].shape
                            pointer.data = tensors[i].data.type(pointer.dtype)
                else:
                    raise NameError(f"Layer name '{layer_name}' not found")


class RegressorBase(ModuleBase, ABC):
    """Base class inherited by all regressor classes.
    """

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.
        """
        return {'name': 'regressor'}


class XLNetRegressor(RegressorBase, PretrainedXLNetMixin):
    """Regressor based on XLNet modules. Please see
    :class:`~texar.torch.modules.PretrainedXLNetMixin` for a brief description
    of XLNet.

    Arguments are the same as in
    :class:`~texar.torch.modules.XLNetEncoder`.

    Args:
        pretrained_model_name (optional): a `str`, the name
            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to
            :class:`~texar.torch.modules.PretrainedXLNetMixin` for
            all supported models.
            If `None`, the model name in :attr:`hparams` is used.
        cache_dir (optional): the path to a folder in which the
            pre-trained models will be cached. If `None` (default),
            a default directory (``texar_data`` folder under user's home
            directory) will be used.
        hparams (dict or HParams, optional): Hyperparameters. Missing
            hyperparameters will be set to default values. See
            :meth:`default_hparams` for the hyperparameter structure
            and default values.
    """

    def __init__(self, pretrained_model_name: Optional[str]=None, cache_dir: Optional[str]=None, hparams=None):
        super().__init__(hparams=hparams)
        encoder_hparams = dict_fetch(hparams, XLNetEncoder.default_hparams())
        self._encoder = XLNetEncoder(pretrained_model_name=pretrained_model_name, cache_dir=cache_dir, hparams=encoder_hparams)
        if self._hparams.use_projection:
            if self._hparams.regr_strategy == 'all_time':
                self.projection = nn.Linear(self._encoder.output_size * self._hparams.max_seq_length, self._encoder.output_size * self._hparams.max_seq_length)
            else:
                self.projection = nn.Linear(self._encoder.output_size, self._encoder.output_size)
        self.dropout = nn.Dropout(self._hparams.dropout)
        logit_kwargs = self._hparams.logit_layer_kwargs
        if logit_kwargs is None:
            logit_kwargs = {}
        elif not isinstance(logit_kwargs, HParams):
            raise ValueError("hparams['logit_layer_kwargs'] must be a dict.")
        else:
            logit_kwargs = logit_kwargs.todict()
        if self._hparams.regr_strategy == 'all_time':
            self.hidden_to_logits = nn.Linear(self._encoder.output_size * self._hparams.max_seq_length, 1, **logit_kwargs)
        else:
            self.hidden_to_logits = nn.Linear(self._encoder.output_size, 1, **logit_kwargs)
        if self._hparams.initializer:
            initialize = get_initializer(self._hparams.initializer)
            assert initialize is not None
            if self._hparams.use_projection:
                initialize(self.projection.weight)
                initialize(self.projection.bias)
            initialize(self.hidden_to_logits.weight)
            if self.hidden_to_logits.bias:
                initialize(self.hidden_to_logits.bias)
        else:
            if self._hparams.use_projection:
                self.projection.apply(init_weights)
            self.hidden_to_logits.apply(init_weights)

    @staticmethod
    def default_hparams() ->Dict[str, Any]:
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in XLNetEncoder
                ...
                # (2) Additional hyperparameters
                "regr_strategy": "cls_time",
                "use_projection": True,
                "logit_layer_kwargs": None,
                "name": "xlnet_regressor",
            }

        Here:

        1. Same hyperparameters as in
           :class:`~texar.torch.modules.XLNetEncoder`.
           See the :meth:`~texar.torch.modules.XLNetEncoder.default_hparams`.
           An instance of XLNetEncoder is created for feature extraction.

        2. Additional hyperparameters:

            `"regr_strategy"`: str
                The regression strategy, one of:

                - **cls_time**: Sequence-level regression based on the
                  output of the first time step (which is the `CLS` token).
                  Each sequence has a prediction.
                - **all_time**: Sequence-level regression based on
                  the output of all time steps. Each sequence has a prediction.
                - **time_wise**: Step-wise regression, i.e., make
                  regression for each time step based on its output.

            `"logit_layer_kwargs"`: dict
                Keyword arguments for the logit :torch_nn:`Linear` layer
                constructor. Ignored if no extra logit layer is appended.

            `"use_projection"`: bool
                If `True`, an additional :torch_nn:`Linear` layer is added after
                the summary step.

            `"name"`: str
                Name of the regressor.
        """
        hparams = XLNetEncoder.default_hparams()
        hparams.update({'regr_strategy': 'cls_time', 'use_projection': True, 'logit_layer_kwargs': None, 'name': 'xlnet_regressor'})
        return hparams

    def param_groups(self, lr: Optional[float]=None, lr_layer_scale: float=1.0, decay_base_params: bool=False):
        """Create parameter groups for optimizers. When
        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form
        separate groups with different base learning rates.

        The return value of this method can be used in the constructor of
        optimizers, for example:

        .. code-block:: python

            model = XLNetRegressor(...)
            param_groups = model.param_groups(lr=2e-5, lr_layer_scale=0.8)
            optim = torch.optim.Adam(param_groups)

        Args:
            lr (float): The learning rate. Can be omitted if
                :attr:`lr_layer_decay_rate` is 1.0.
            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer
                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.
            decay_base_params (bool): If `True`, treat non-layer parameters
                (e.g. embeddings) as if they're in layer 0. If `False`, these
                parameters are not scaled.

        Returns:
            The parameter groups, used as the first argument for optimizers.
        """
        if lr_layer_scale != 1.0:
            if lr is None:
                raise ValueError('lr must be specified when lr_layer_decay_rate is not 1.0')
            fine_tune_group = {'params': params_except_in(self, ['_encoder']), 'lr': lr}
            param_groups = [fine_tune_group]
            param_group = self._encoder.param_groups(lr, lr_layer_scale, decay_base_params)
            param_groups.extend(param_group)
            return param_groups
        return self.parameters()

    def forward(self, inputs: Union[torch.Tensor, torch.LongTensor], segment_ids: Optional[torch.LongTensor]=None, input_mask: Optional[torch.Tensor]=None) ->torch.Tensor:
        """Feeds the inputs through the network and makes regression.

        Args:
            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,
                containing the ids of tokens in input sequences, or
                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,
                containing soft token ids (i.e., weights or probabilities)
                used to mix the embedding vectors.
            segment_ids: Shape `[batch_size, max_time]`.
            input_mask: Float tensor of shape `[batch_size, max_time]`. Note
                that positions with value 1 are masked out.

        Returns:
            Regression predictions.

            - If ``regr_strategy`` is ``cls_time`` or ``all_time``, predictions
              have shape `[batch_size]`.

            - If ``clas_strategy`` is ``time_wise``, predictions have shape
              `[batch_size, max_time]`.
        """
        output, _ = self._encoder(inputs=inputs, segment_ids=segment_ids, input_mask=input_mask)
        strategy = self._hparams.regr_strategy
        if strategy == 'time_wise':
            summary = output
        elif strategy == 'cls_time':
            summary = output[:, (-1)]
        elif strategy == 'all_time':
            length_diff = self._hparams.max_seq_length - inputs.shape[1]
            summary_input = F.pad(output, [0, 0, 0, length_diff, 0, 0])
            summary_input_dim = self._encoder.output_size * self._hparams.max_seq_length
            summary = summary_input.contiguous().view(-1, summary_input_dim)
        else:
            raise ValueError('Unknown regression strategy: {}'.format(strategy))
        if self._hparams.use_projection:
            summary = torch.tanh(self.projection(summary))
        summary = self.dropout(summary)
        preds = self.hidden_to_logits(summary).squeeze(-1)
        return preds

    @property
    def output_size(self) ->int:
        """The feature size of :meth:`forward` output. Since output size is
        only determined by input, the feature size is equal to ``-1``.
        """
        return -1


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AvgReducePool1d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BertGELU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (EmbeddingDropout,
     lambda: ([], {'rate': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FeedForwardNetwork,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FeedForwardNetworkBase,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Flatten,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GPTGELU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Identity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LSTMCell,
     lambda: ([], {'input_size': 4, 'hidden_size': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (MaxReducePool1d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MergeLayer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (T5LayerNorm,
     lambda: ([], {'input_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_asyml_texar_pytorch(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

