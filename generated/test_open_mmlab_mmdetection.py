import sys
_module = sys.modules[__name__]
del sys
benchmark_filter = _module
gather_models = _module
cityscapes_detection = _module
cityscapes_instance = _module
coco_detection = _module
coco_instance = _module
coco_instance_semantic = _module
voc0712 = _module
wider_face = _module
default_runtime = _module
cascade_mask_rcnn_r50_fpn = _module
cascade_rcnn_r50_fpn = _module
fast_rcnn_r50_fpn = _module
faster_rcnn_r50_caffe_c4 = _module
faster_rcnn_r50_fpn = _module
mask_rcnn_r50_caffe_c4 = _module
mask_rcnn_r50_fpn = _module
retinanet_r50_fpn = _module
rpn_r50_caffe_c4 = _module
rpn_r50_fpn = _module
ssd300 = _module
schedule_1x = _module
schedule_20e = _module
schedule_2x = _module
mask_rcnn_r50_fpn_albu_1x_coco = _module
atss_r50_fpn_1x_coco = _module
faster_rcnn_r50_fpn_carafe_1x_coco = _module
mask_rcnn_r50_fpn_carafe_1x_coco = _module
cascade_mask_rcnn_r101_caffe_fpn_1x_coco = _module
cascade_mask_rcnn_r101_fpn_1x_coco = _module
cascade_mask_rcnn_r101_fpn_20e_coco = _module
cascade_mask_rcnn_r50_caffe_fpn_1x_coco = _module
cascade_mask_rcnn_r50_fpn_1x_coco = _module
cascade_mask_rcnn_r50_fpn_20e_coco = _module
cascade_mask_rcnn_x101_32x4d_fpn_1x_coco = _module
cascade_mask_rcnn_x101_32x4d_fpn_20e_coco = _module
cascade_mask_rcnn_x101_64x4d_fpn_1x_coco = _module
cascade_mask_rcnn_x101_64x4d_fpn_20e_coco = _module
cascade_rcnn_r101_caffe_fpn_1x_coco = _module
cascade_rcnn_r101_fpn_1x_coco = _module
cascade_rcnn_r101_fpn_20e_coco = _module
cascade_rcnn_r50_caffe_fpn_1x_coco = _module
cascade_rcnn_r50_fpn_1x_coco = _module
cascade_rcnn_r50_fpn_20e_coco = _module
cascade_rcnn_x101_32x4d_fpn_1x_coco = _module
cascade_rcnn_x101_32x4d_fpn_20e_coco = _module
cascade_rcnn_x101_64x4d_fpn_1x_coco = _module
cascade_rcnn_x101_64x4d_fpn_20e_coco = _module
faster_rcnn_r50_fpn_1x_cityscapes = _module
mask_rcnn_r50_fpn_1x_cityscapes = _module
faster_rcnn_r50_fpn_dpool_1x_coco = _module
faster_rcnn_r50_fpn_mdpool_1x_coco = _module
dh_faster_rcnn_r50_fpn_1x_coco = _module
faster_rcnn_r50_fpn_attention_0010_1x_coco = _module
faster_rcnn_r50_fpn_attention_0010_dcn_1x_coco = _module
faster_rcnn_r50_fpn_attention_1111_1x_coco = _module
faster_rcnn_r50_fpn_attention_1111_dcn_1x_coco = _module
fast_rcnn_r101_caffe_fpn_1x_coco = _module
fast_rcnn_r101_fpn_1x_coco = _module
fast_rcnn_r101_fpn_2x_coco = _module
fast_rcnn_r50_caffe_fpn_1x_coco = _module
fast_rcnn_r50_fpn_1x_coco = _module
fast_rcnn_r50_fpn_2x_coco = _module
faster_rcnn_r101_caffe_fpn_1x_coco = _module
faster_rcnn_r101_fpn_1x_coco = _module
faster_rcnn_r101_fpn_2x_coco = _module
faster_rcnn_r50_caffe_c4_1x_coco = _module
faster_rcnn_r50_caffe_fpn_1x_coco = _module
faster_rcnn_r50_caffe_fpn_mstrain_1x_coco = _module
faster_rcnn_r50_caffe_fpn_mstrain_2x_coco = _module
faster_rcnn_r50_caffe_fpn_mstrain_3x_coco = _module
faster_rcnn_r50_fpn_1x_coco = _module
faster_rcnn_r50_fpn_2x_coco = _module
faster_rcnn_r50_fpn_bounded_iou_1x_coco = _module
faster_rcnn_r50_fpn_giou_1x_coco = _module
faster_rcnn_r50_fpn_iou_1x_coco = _module
faster_rcnn_r50_fpn_ohem_1x_coco = _module
faster_rcnn_r50_fpn_soft_nms_1x_coco = _module
faster_rcnn_x101_32x4d_fpn_1x_coco = _module
faster_rcnn_x101_32x4d_fpn_2x_coco = _module
faster_rcnn_x101_64x4d_fpn_1x_coco = _module
faster_rcnn_x101_64x4d_fpn_2x_coco = _module
fcos_r50_caffe_fpn_4x4_1x_coco = _module
fovea_r101_fpn_4x4_1x_coco = _module
fovea_r101_fpn_4x4_2x_coco = _module
fovea_r50_fpn_4x4_1x_coco = _module
fovea_r50_fpn_4x4_2x_coco = _module
faster_rcnn_r50_fpn_fp16_1x_coco = _module
mask_rcnn_r50_fpn_fp16_1x_coco = _module
retinanet_r50_fpn_fp16_1x_coco = _module
retinanet_free_anchor_r101_fpn_1x_coco = _module
retinanet_free_anchor_r50_fpn_1x_coco = _module
retinanet_free_anchor_x101_32x4d_fpn_1x_coco = _module
fsaf_r101_fpn_1x_coco = _module
fsaf_r50_fpn_1x_coco = _module
fsaf_x101_64x4d_fpn_1x_coco = _module
retinanet_ghm_r101_fpn_1x_coco = _module
retinanet_ghm_r50_fpn_1x_coco = _module
retinanet_ghm_x101_32x4d_fpn_1x_coco = _module
retinanet_ghm_x101_64x4d_fpn_1x_coco = _module
ga_fast_r50_caffe_fpn_1x_coco = _module
ga_faster_r101_caffe_fpn_1x_coco = _module
ga_faster_r50_caffe_fpn_1x_coco = _module
ga_faster_r50_fpn_1x_coco = _module
ga_faster_x101_32x4d_fpn_1x_coco = _module
ga_faster_x101_64x4d_fpn_1x_coco = _module
ga_retinanet_r101_caffe_fpn_1x_coco = _module
ga_retinanet_r101_caffe_fpn_mstrain_2x = _module
ga_retinanet_r50_caffe_fpn_1x_coco = _module
ga_retinanet_r50_fpn_1x_coco = _module
ga_retinanet_x101_32x4d_fpn_1x_coco = _module
ga_retinanet_x101_64x4d_fpn_1x_coco = _module
ga_rpn_r101_caffe_fpn_1x_coco = _module
ga_rpn_r50_caffe_fpn_1x_coco = _module
ga_rpn_r50_fpn_1x_coco = _module
ga_rpn_x101_32x4d_fpn_1x_coco = _module
ga_rpn_x101_64x4d_fpn_1x_coco = _module
cascade_mask_rcnn_hrnetv2p_w18_20e_coco = _module
cascade_mask_rcnn_hrnetv2p_w32_20e_coco = _module
cascade_mask_rcnn_hrnetv2p_w40_20e_coco = _module
cascade_rcnn_hrnetv2p_w18_20e_coco = _module
cascade_rcnn_hrnetv2p_w32_20e_coco = _module
cascade_rcnn_hrnetv2p_w40_20e_coco = _module
faster_rcnn_hrnetv2p_w18_1x_coco = _module
faster_rcnn_hrnetv2p_w18_2x_coco = _module
faster_rcnn_hrnetv2p_w32_1x_coco = _module
faster_rcnn_hrnetv2p_w32_2x_coco = _module
faster_rcnn_hrnetv2p_w40_1x_coco = _module
faster_rcnn_hrnetv2p_w40_2x_coco = _module
htc_hrnetv2p_w18_20e_coco = _module
htc_hrnetv2p_w32_20e_coco = _module
htc_hrnetv2p_w40_20e_coco = _module
htc_hrnetv2p_w40_28e_coco = _module
htc_x101_64x4d_fpn_16x1_28e_coco = _module
mask_rcnn_hrnetv2p_w18_1x_coco = _module
mask_rcnn_hrnetv2p_w18_2x_coco = _module
mask_rcnn_hrnetv2p_w32_1x_coco = _module
mask_rcnn_hrnetv2p_w32_2x_coco = _module
mask_rcnn_hrnetv2p_w40_1x_coco = _module
mask_rcnn_hrnetv2p_w40_2x_coco = _module
htc_r101_fpn_20e_coco = _module
htc_r50_fpn_1x_coco = _module
htc_r50_fpn_20e_coco = _module
htc_without_semantic_r50_fpn_1x_coco = _module
htc_x101_32x4d_fpn_16x1_20e_coco = _module
htc_x101_64x4d_fpn_16x1_20e_coco = _module
cascade_mask_rcnn_r101_fpn_instaboost_4x_coco = _module
cascade_mask_rcnn_r50_fpn_instaboost_4x_coco = _module
cascade_mask_rcnn_x101_64x4d_fpn_instaboost_4x_coco = _module
mask_rcnn_r101_fpn_instaboost_4x_coco = _module
mask_rcnn_r50_fpn_instaboost_4x_coco = _module
mask_rcnn_x101_64x4d_fpn_instaboost_4x_coco = _module
cascade_mask_rcnn_r50_fpn_1x_coco_v1 = _module
faster_rcnn_r50_fpn_1x_coco_v1 = _module
mask_rcnn_r50_fpn_1x_coco_v1 = _module
retinanet_r50_caffe_fpn_1x_coco_v1 = _module
retinanet_r50_fpn_1x_coco_v1 = _module
ssd300_coco_v1 = _module
libra_fast_rcnn_r50_fpn_1x_coco = _module
libra_faster_rcnn_r101_fpn_1x_coco = _module
libra_faster_rcnn_r50_fpn_1x_coco = _module
libra_faster_rcnn_x101_64x4d_fpn_1x_coco = _module
libra_retinanet_r50_fpn_1x_coco = _module
mask_rcnn_r101_caffe_fpn_1x_coco = _module
mask_rcnn_r101_fpn_1x_coco = _module
mask_rcnn_r101_fpn_2x_coco = _module
mask_rcnn_r50_caffe_c4_1x_coco = _module
mask_rcnn_r50_caffe_fpn_1x_coco = _module
mask_rcnn_r50_caffe_fpn_mstrain_1x_coco = _module
mask_rcnn_r50_caffe_fpn_poly_1x_coco_v1 = _module
mask_rcnn_r50_fpn_1x_coco = _module
mask_rcnn_r50_fpn_2x_coco = _module
mask_rcnn_r50_fpn_poly_1x_coco = _module
mask_rcnn_x101_32x4d_fpn_1x_coco = _module
mask_rcnn_x101_32x4d_fpn_2x_coco = _module
mask_rcnn_x101_64x4d_fpn_1x_coco = _module
mask_rcnn_x101_64x4d_fpn_2x_coco = _module
ms_rcnn_r101_caffe_fpn_1x_coco = _module
ms_rcnn_r101_caffe_fpn_2x_coco = _module
ms_rcnn_r50_caffe_fpn_1x_coco = _module
ms_rcnn_r50_caffe_fpn_2x_coco = _module
ms_rcnn_r50_fpn_1x_coco = _module
ms_rcnn_x101_32x4d_fpn_1x_coco = _module
ms_rcnn_x101_64x4d_fpn_1x_coco = _module
ms_rcnn_x101_64x4d_fpn_2x_coco = _module
retinanet_r50_fpn_crop640_50e_coco = _module
retinanet_r50_nasfpn_crop640_50e_coco = _module
faster_rcnn_r50_pafpn_1x_coco = _module
faster_rcnn_r50_fpn_1x_voc0712 = _module
ssd300_voc0712 = _module
ssd512_voc0712 = _module
pisa_faster_rcnn_r50_fpn_1x_coco = _module
pisa_faster_rcnn_x101_32x4d_fpn_1x_coco = _module
pisa_mask_rcnn_r50_fpn_1x_coco = _module
pisa_mask_rcnn_x101_32x4d_fpn_1x_coco = _module
pisa_retinanet_r50_fpn_1x_coco = _module
pisa_retinanet_x101_32x4d_fpn_1x_coco = _module
pisa_ssd300_coco = _module
pisa_ssd512_coco = _module
reppoints_moment_r50_fpn_1x_coco = _module
cascade_mask_rcnn_r2_101_fpn_20e_coco = _module
cascade_rcnn_r2_101_fpn_20e_coco = _module
faster_rcnn_r2_101_fpn_2x_coco = _module
htc_r2_101_fpn_20e_coco = _module
mask_rcnn_r2_101_fpn_2x_coco = _module
retinanet_r101_caffe_fpn_1x_coco = _module
retinanet_r101_fpn_1x_coco = _module
retinanet_r101_fpn_2x_coco = _module
retinanet_r50_caffe_fpn_1x_coco = _module
retinanet_r50_caffe_fpn_mstrain_1x_coco = _module
retinanet_r50_caffe_fpn_mstrain_2x_coco = _module
retinanet_r50_caffe_fpn_mstrain_3x_coco = _module
retinanet_r50_fpn_1x_coco = _module
retinanet_r50_fpn_2x_coco = _module
retinanet_x101_32x4d_fpn_1x_coco = _module
retinanet_x101_32x4d_fpn_2x_coco = _module
retinanet_x101_64x4d_fpn_1x_coco = _module
retinanet_x101_64x4d_fpn_2x_coco = _module
rpn_r101_caffe_fpn_1x_coco = _module
rpn_r101_fpn_1x_coco = _module
rpn_r101_fpn_2x_coco = _module
rpn_r50_caffe_c4_1x_coco = _module
rpn_r50_caffe_fpn_1x_coco = _module
rpn_r50_fpn_1x_coco = _module
rpn_r50_fpn_2x_coco = _module
rpn_x101_32x4d_fpn_1x_coco = _module
rpn_x101_32x4d_fpn_2x_coco = _module
rpn_x101_64x4d_fpn_1x_coco = _module
rpn_x101_64x4d_fpn_2x_coco = _module
ssd300_coco = _module
ssd512_coco = _module
ssd300_wider_face = _module
image_demo = _module
webcam_demo = _module
conf = _module
mmdet = _module
apis = _module
inference = _module
test = _module
train = _module
core = _module
anchor = _module
anchor_generator = _module
builder = _module
point_generator = _module
utils = _module
bbox = _module
assigners = _module
approx_max_iou_assigner = _module
assign_result = _module
atss_assigner = _module
base_assigner = _module
center_region_assigner = _module
max_iou_assigner = _module
point_assigner = _module
coder = _module
base_bbox_coder = _module
delta_xywh_bbox_coder = _module
legacy_delta_xywh_bbox_coder = _module
pseudo_bbox_coder = _module
tblr_bbox_coder = _module
demodata = _module
iou_calculators = _module
iou2d_calculator = _module
samplers = _module
base_sampler = _module
combined_sampler = _module
instance_balanced_pos_sampler = _module
iou_balanced_neg_sampler = _module
ohem_sampler = _module
pseudo_sampler = _module
random_sampler = _module
sampling_result = _module
score_hlr_sampler = _module
transforms = _module
evaluation = _module
bbox_overlaps = _module
class_names = _module
eval_hooks = _module
mean_ap = _module
recall = _module
fp16 = _module
decorators = _module
hooks = _module
mask = _module
mask_target = _module
structures = _module
optimizer = _module
copy_of_sgd = _module
default_constructor = _module
post_processing = _module
bbox_nms = _module
merge_augs = _module
dist_utils = _module
misc = _module
datasets = _module
cityscapes = _module
coco = _module
custom = _module
dataset_wrappers = _module
pipelines = _module
compose = _module
formating = _module
instaboost = _module
loading = _module
test_aug = _module
distributed_sampler = _module
group_sampler = _module
voc = _module
xml_style = _module
models = _module
backbones = _module
hrnet = _module
regnet = _module
res2net = _module
resnet = _module
resnext = _module
ssd_vgg = _module
dense_heads = _module
anchor_head = _module
atss_head = _module
fcos_head = _module
fovea_head = _module
free_anchor_retina_head = _module
fsaf_head = _module
ga_retina_head = _module
ga_rpn_head = _module
guided_anchor_head = _module
nasfcos_head = _module
pisa_retinanet_head = _module
pisa_ssd_head = _module
reppoints_head = _module
retina_head = _module
retina_sepbn_head = _module
rpn_head = _module
ssd_head = _module
detectors = _module
atss = _module
base = _module
cascade_rcnn = _module
fast_rcnn = _module
faster_rcnn = _module
fcos = _module
fovea = _module
fsaf = _module
grid_rcnn = _module
htc = _module
mask_rcnn = _module
mask_scoring_rcnn = _module
nasfcos = _module
reppoints_detector = _module
retinanet = _module
rpn = _module
single_stage = _module
test_mixins = _module
two_stage = _module
losses = _module
accuracy = _module
balanced_l1_loss = _module
cross_entropy_loss = _module
focal_loss = _module
ghm_loss = _module
iou_loss = _module
mse_loss = _module
pisa_loss = _module
smooth_l1_loss = _module
utils = _module
necks = _module
bfp = _module
fpn = _module
fpn_carafe = _module
hrfpn = _module
nas_fpn = _module
nasfcos_fpn = _module
pafpn = _module
roi_heads = _module
base_roi_head = _module
bbox_heads = _module
bbox_head = _module
convfc_bbox_head = _module
double_bbox_head = _module
cascade_roi_head = _module
double_roi_head = _module
grid_roi_head = _module
htc_roi_head = _module
mask_heads = _module
fcn_mask_head = _module
fused_semantic_head = _module
grid_head = _module
htc_mask_head = _module
maskiou_head = _module
mask_scoring_roi_head = _module
pisa_roi_head = _module
roi_extractors = _module
single_level = _module
shared_heads = _module
res_layer = _module
standard_roi_head = _module
res_layer = _module
ops = _module
carafe = _module
carafe = _module
grad_check = _module
setup = _module
context_block = _module
conv_ws = _module
dcn = _module
deform_conv = _module
deform_pool = _module
generalized_attention = _module
masked_conv = _module
masked_conv = _module
merge_cells = _module
nms = _module
nms_wrapper = _module
non_local = _module
plugin = _module
roi_align = _module
gradcheck = _module
roi_align = _module
roi_pool = _module
roi_pool = _module
sigmoid_focal_loss = _module
sigmoid_focal_loss = _module
wrappers = _module
collect_env = _module
contextmanagers = _module
flops_counter = _module
logger = _module
profiling = _module
util_mixins = _module
async_benchmark = _module
test_anchor = _module
test_assigner = _module
test_async = _module
test_backbone = _module
test_config = _module
test_dataset = _module
test_forward = _module
test_fp16 = _module
test_heads = _module
test_masks = _module
test_merge_cells = _module
test_necks = _module
test_nms = _module
test_optimizer = _module
test_loading = _module
test_transform = _module
test_pisa_heads = _module
test_sampler = _module
test_soft_nms = _module
test_utils = _module
test_wrappers = _module
analyze_logs = _module
benchmark = _module
browse_dataset = _module
coco_error_analysis = _module
pascal_voc = _module
detectron2pytorch = _module
fuse_conv_bn = _module
get_flops = _module
print_config = _module
publish_model = _module
pytorch2onnx = _module
regnet2mmdet = _module
robustness_eval = _module
test_robustness = _module
upgrade_model_version = _module

from _paritybench_helpers import _mock_config
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
open = mock_open()
logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import warnings


import torch


import numpy as np


import torch.distributed as dist


import random


from collections import OrderedDict


from torch.nn.modules.utils import _pair


import functools


from inspect import getfullargspec


import copy


import torch.nn as nn


from torch.nn import GroupNorm


from torch.nn import LayerNorm


from torch.nn.modules.batchnorm import _BatchNorm


from torch.nn.modules.instancenorm import _InstanceNorm


import math


from torch.utils.data import Sampler


import torch.utils.checkpoint as cp


import torch.nn.functional as F


from abc import ABCMeta


from abc import abstractmethod


from torch.utils.checkpoint import checkpoint


from torch import nn as nn


from torch.autograd import Function


from torch.nn.modules.module import Module


from torch import nn


from torch.autograd.function import once_differentiable


from torch.nn.modules.utils import _single


from torch.nn.modules.conv import _ConvNd


from torch.nn.modules.conv import _ConvTransposeMixin


from torch.nn.modules.pooling import _AdaptiveAvgPoolNd


from torch.nn.modules.pooling import _AdaptiveMaxPoolNd


from torch.nn.modules.pooling import _AvgPoolNd


from torch.nn.modules.pooling import _MaxPoolNd


from torch.nn.modules import AvgPool2d


from torch.nn.modules import GroupNorm


from itertools import product


from torch.onnx import OperatorExportTypes


class HRModule(nn.Module):
    """ High-Resolution Module for HRNet. In this module, every branch
    has 4 BasicBlocks/Bottlenecks. Fusion/Exchange is in this module.
    """

    def __init__(self, num_branches, blocks, num_blocks, in_channels,
        num_channels, multiscale_output=True, with_cp=False, conv_cfg=None,
        norm_cfg=dict(type='BN')):
        super(HRModule, self).__init__()
        self._check_branches(num_branches, num_blocks, in_channels,
            num_channels)
        self.in_channels = in_channels
        self.num_branches = num_branches
        self.multiscale_output = multiscale_output
        self.norm_cfg = norm_cfg
        self.conv_cfg = conv_cfg
        self.with_cp = with_cp
        self.branches = self._make_branches(num_branches, blocks,
            num_blocks, num_channels)
        self.fuse_layers = self._make_fuse_layers()
        self.relu = nn.ReLU(inplace=False)

    def _check_branches(self, num_branches, num_blocks, in_channels,
        num_channels):
        if num_branches != len(num_blocks):
            error_msg = (
                f'NUM_BRANCHES({num_branches}) != NUM_BLOCKS({len(num_blocks)})'
                )
            raise ValueError(error_msg)
        if num_branches != len(num_channels):
            error_msg = (
                f'NUM_BRANCHES({num_branches}) != NUM_CHANNELS({len(num_channels)})'
                )
            raise ValueError(error_msg)
        if num_branches != len(in_channels):
            error_msg = (
                f'NUM_BRANCHES({num_branches}) != NUM_INCHANNELS({len(in_channels)})'
                )
            raise ValueError(error_msg)

    def _make_one_branch(self, branch_index, block, num_blocks,
        num_channels, stride=1):
        downsample = None
        if stride != 1 or self.in_channels[branch_index] != num_channels[
            branch_index] * block.expansion:
            downsample = nn.Sequential(build_conv_layer(self.conv_cfg, self
                .in_channels[branch_index], num_channels[branch_index] *
                block.expansion, kernel_size=1, stride=stride, bias=False),
                build_norm_layer(self.norm_cfg, num_channels[branch_index] *
                block.expansion)[1])
        layers = []
        layers.append(block(self.in_channels[branch_index], num_channels[
            branch_index], stride, downsample=downsample, with_cp=self.
            with_cp, norm_cfg=self.norm_cfg, conv_cfg=self.conv_cfg))
        self.in_channels[branch_index] = num_channels[branch_index
            ] * block.expansion
        for i in range(1, num_blocks[branch_index]):
            layers.append(block(self.in_channels[branch_index],
                num_channels[branch_index], with_cp=self.with_cp, norm_cfg=
                self.norm_cfg, conv_cfg=self.conv_cfg))
        return nn.Sequential(*layers)

    def _make_branches(self, num_branches, block, num_blocks, num_channels):
        branches = []
        for i in range(num_branches):
            branches.append(self._make_one_branch(i, block, num_blocks,
                num_channels))
        return nn.ModuleList(branches)

    def _make_fuse_layers(self):
        if self.num_branches == 1:
            return None
        num_branches = self.num_branches
        in_channels = self.in_channels
        fuse_layers = []
        num_out_branches = num_branches if self.multiscale_output else 1
        for i in range(num_out_branches):
            fuse_layer = []
            for j in range(num_branches):
                if j > i:
                    fuse_layer.append(nn.Sequential(build_conv_layer(self.
                        conv_cfg, in_channels[j], in_channels[i],
                        kernel_size=1, stride=1, padding=0, bias=False),
                        build_norm_layer(self.norm_cfg, in_channels[i])[1],
                        nn.Upsample(scale_factor=2 ** (j - i), mode='nearest'))
                        )
                elif j == i:
                    fuse_layer.append(None)
                else:
                    conv_downsamples = []
                    for k in range(i - j):
                        if k == i - j - 1:
                            conv_downsamples.append(nn.Sequential(
                                build_conv_layer(self.conv_cfg, in_channels
                                [j], in_channels[i], kernel_size=3, stride=
                                2, padding=1, bias=False), build_norm_layer
                                (self.norm_cfg, in_channels[i])[1]))
                        else:
                            conv_downsamples.append(nn.Sequential(
                                build_conv_layer(self.conv_cfg, in_channels
                                [j], in_channels[j], kernel_size=3, stride=
                                2, padding=1, bias=False), build_norm_layer
                                (self.norm_cfg, in_channels[j])[1], nn.ReLU
                                (inplace=False)))
                    fuse_layer.append(nn.Sequential(*conv_downsamples))
            fuse_layers.append(nn.ModuleList(fuse_layer))
        return nn.ModuleList(fuse_layers)

    def forward(self, x):
        if self.num_branches == 1:
            return [self.branches[0](x[0])]
        for i in range(self.num_branches):
            x[i] = self.branches[i](x[i])
        x_fuse = []
        for i in range(len(self.fuse_layers)):
            y = 0
            for j in range(self.num_branches):
                if i == j:
                    y += x[j]
                else:
                    y += self.fuse_layers[i][j](x[j])
            x_fuse.append(self.relu(y))
        return x_fuse


def get_root_logger(log_file=None, log_level=logging.INFO):
    logger = get_logger(name='mmdet', log_file=log_file, log_level=log_level)
    return logger


class Res2Layer(nn.Sequential):
    """Res2Layer to build Res2Net style backbone.

    Args:
        block (nn.Module): block used to build ResLayer.
        inplanes (int): inplanes of block.
        planes (int): planes of block.
        num_blocks (int): number of blocks.
        stride (int): stride of the first block. Default: 1
        avg_down (bool): Use AvgPool instead of stride conv when
            downsampling in the bottle2neck. Default: False
        conv_cfg (dict): dictionary to construct and config conv layer.
            Default: None
        norm_cfg (dict): dictionary to construct and config norm layer.
            Default: dict(type='BN')
        scales (int): Scales used in Res2Net. Default: 4
        base_width (int): Basic width of each scale. Default: 26
    """

    def __init__(self, block, inplanes, planes, num_blocks, stride=1,
        avg_down=True, conv_cfg=None, norm_cfg=dict(type='BN'), scales=4,
        base_width=26, **kwargs):
        self.block = block
        downsample = None
        if stride != 1 or inplanes != planes * block.expansion:
            downsample = nn.Sequential(nn.AvgPool2d(kernel_size=stride,
                stride=stride, ceil_mode=True, count_include_pad=False),
                build_conv_layer(conv_cfg, inplanes, planes * block.
                expansion, kernel_size=1, stride=1, bias=False),
                build_norm_layer(norm_cfg, planes * block.expansion)[1])
        layers = []
        layers.append(block(inplanes=inplanes, planes=planes, stride=stride,
            downsample=downsample, conv_cfg=conv_cfg, norm_cfg=norm_cfg,
            scales=scales, base_width=base_width, stage_type='stage', **kwargs)
            )
        inplanes = planes * block.expansion
        for i in range(1, num_blocks):
            layers.append(block(inplanes=inplanes, planes=planes, stride=1,
                conv_cfg=conv_cfg, norm_cfg=norm_cfg, scales=scales,
                base_width=base_width, **kwargs))
        super(Res2Layer, self).__init__(*layers)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=
        None, style='pytorch', with_cp=False, conv_cfg=None, norm_cfg=dict(
        type='BN'), dcn=None, plugins=None):
        super(BasicBlock, self).__init__()
        assert dcn is None, 'Not implemented yet.'
        assert plugins is None, 'Not implemented yet.'
        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)
        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)
        self.conv1 = build_conv_layer(conv_cfg, inplanes, planes, 3, stride
            =stride, padding=dilation, dilation=dilation, bias=False)
        self.add_module(self.norm1_name, norm1)
        self.conv2 = build_conv_layer(conv_cfg, planes, planes, 3, padding=
            1, bias=False)
        self.add_module(self.norm2_name, norm2)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
        self.dilation = dilation
        assert not with_cp

    @property
    def norm1(self):
        return getattr(self, self.norm1_name)

    @property
    def norm2(self):
        return getattr(self, self.norm2_name)

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.norm1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.norm2(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out


class L2Norm(nn.Module):

    def __init__(self, n_dims, scale=20.0, eps=1e-10):
        super(L2Norm, self).__init__()
        self.n_dims = n_dims
        self.weight = nn.Parameter(torch.Tensor(self.n_dims))
        self.eps = eps
        self.scale = scale

    def forward(self, x):
        x_float = x.float()
        norm = x_float.pow(2).sum(1, keepdim=True).sqrt() + self.eps
        return (self.weight[(None), :, (None), (None)].float().expand_as(
            x_float) * x_float / norm).type_as(x)


def distance2bbox(points, distance, max_shape=None):
    """Decode distance prediction to bounding box.

    Args:
        points (Tensor): Shape (n, 2), [x, y].
        distance (Tensor): Distance from the given point to 4
            boundaries (left, top, right, bottom).
        max_shape (tuple): Shape of the image.

    Returns:
        Tensor: Decoded bboxes.
    """
    x1 = points[:, (0)] - distance[:, (0)]
    y1 = points[:, (1)] - distance[:, (1)]
    x2 = points[:, (0)] + distance[:, (2)]
    y2 = points[:, (1)] + distance[:, (3)]
    if max_shape is not None:
        x1 = x1.clamp(min=0, max=max_shape[1])
        y1 = y1.clamp(min=0, max=max_shape[0])
        x2 = x2.clamp(min=0, max=max_shape[1])
        y2 = y2.clamp(min=0, max=max_shape[0])
    return torch.stack([x1, y1, x2, y2], -1)


class FeatureAlign(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size=3,
        deformable_groups=4):
        super(FeatureAlign, self).__init__()
        offset_channels = kernel_size * kernel_size * 2
        self.conv_offset = nn.Conv2d(4, deformable_groups * offset_channels,
            1, bias=False)
        self.conv_adaption = DeformConv(in_channels, out_channels,
            kernel_size=kernel_size, padding=(kernel_size - 1) // 2,
            deformable_groups=deformable_groups)
        self.relu = nn.ReLU(inplace=True)

    def init_weights(self):
        normal_init(self.conv_offset, std=0.1)
        normal_init(self.conv_adaption, std=0.01)

    def forward(self, x, shape):
        offset = self.conv_offset(shape)
        x = self.relu(self.conv_adaption(x, offset))
        return x


def multi_apply(func, *args, **kwargs):
    pfunc = partial(func, **kwargs) if kwargs else func
    map_results = map(pfunc, *args)
    return tuple(map(list, zip(*map_results)))


def build(cfg, registry, default_args=None):
    if isinstance(cfg, list):
        modules = [build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg
            ]
        return nn.Sequential(*modules)
    else:
        return build_from_cfg(cfg, registry, default_args)


class FeatureAdaption(nn.Module):
    """Feature Adaption Module.

    Feature Adaption Module is implemented based on DCN v1.
    It uses anchor shape prediction rather than feature map to
    predict offsets of deformable conv layer.

    Args:
        in_channels (int): Number of channels in the input feature map.
        out_channels (int): Number of channels in the output feature map.
        kernel_size (int): Deformable conv kernel size.
        deformable_groups (int): Deformable conv group size.
    """

    def __init__(self, in_channels, out_channels, kernel_size=3,
        deformable_groups=4):
        super(FeatureAdaption, self).__init__()
        offset_channels = kernel_size * kernel_size * 2
        self.conv_offset = nn.Conv2d(2, deformable_groups * offset_channels,
            1, bias=False)
        self.conv_adaption = DeformConv(in_channels, out_channels,
            kernel_size=kernel_size, padding=(kernel_size - 1) // 2,
            deformable_groups=deformable_groups)
        self.relu = nn.ReLU(inplace=True)

    def init_weights(self):
        normal_init(self.conv_offset, std=0.1)
        normal_init(self.conv_adaption, std=0.01)

    def forward(self, x, shape):
        offset = self.conv_offset(shape.detach())
        x = self.relu(self.conv_adaption(x, offset))
        return x


def unmap(data, count, inds, fill=0):
    """ Unmap a subset of item (data) back to the original set of items (of
    size count) """
    if data.dim() == 1:
        ret = data.new_full((count,), fill)
        ret[inds.type(torch.bool)] = data
    else:
        new_size = (count,) + data.size()[1:]
        ret = data.new_full(new_size, fill)
        ret[(inds.type(torch.bool)), :] = data
    return ret


def cast_tensor_type(inputs, src_type, dst_type):
    if isinstance(inputs, torch.Tensor):
        return inputs.to(dst_type)
    elif isinstance(inputs, str):
        return inputs
    elif isinstance(inputs, np.ndarray):
        return inputs
    elif isinstance(inputs, abc.Mapping):
        return type(inputs)({k: cast_tensor_type(v, src_type, dst_type) for
            k, v in inputs.items()})
    elif isinstance(inputs, abc.Iterable):
        return type(inputs)(cast_tensor_type(item, src_type, dst_type) for
            item in inputs)
    else:
        return inputs


def auto_fp16(apply_to=None, out_fp32=False):
    """Decorator to enable fp16 training automatically.

    This decorator is useful when you write custom modules and want to support
    mixed precision training. If inputs arguments are fp32 tensors, they will
    be converted to fp16 automatically. Arguments other than fp32 tensors are
    ignored.

    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp32 (bool): Whether to convert the output back to fp32.

    Example:

        >>> import torch.nn as nn
        >>> class MyModule1(nn.Module):
        >>>
        >>>     # Convert x and y to fp16
        >>>     @auto_fp16()
        >>>     def forward(self, x, y):
        >>>         pass

        >>> import torch.nn as nn
        >>> class MyModule2(nn.Module):
        >>>
        >>>     # convert pred to fp16
        >>>     @auto_fp16(apply_to=('pred', ))
        >>>     def do_something(self, pred, others):
        >>>         pass
    """

    def auto_fp16_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError(
                    '@auto_fp16 can only be used to decorate the method of nn.Module'
                    )
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            args_info = getfullargspec(old_func)
            args_to_cast = args_info.args if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for i, arg_name in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.
                            float, torch.half))
                    else:
                        new_args.append(args[i])
            new_kwargs = {}
            if kwargs:
                for arg_name, arg_value in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value,
                            torch.float, torch.half)
                    else:
                        new_kwargs[arg_name] = arg_value
            output = old_func(*new_args, **new_kwargs)
            if out_fp32:
                output = cast_tensor_type(output, torch.half, torch.float)
            return output
        return new_func
    return auto_fp16_wrapper


class BaseDetector(nn.Module, metaclass=ABCMeta):
    """Base class for detectors"""

    def __init__(self):
        super(BaseDetector, self).__init__()
        self.fp16_enabled = False

    @property
    def with_neck(self):
        return hasattr(self, 'neck') and self.neck is not None

    @property
    def with_shared_head(self):
        return hasattr(self.roi_head, 'shared_head'
            ) and self.roi_head.shared_head is not None

    @property
    def with_bbox(self):
        return hasattr(self.roi_head, 'bbox_head'
            ) and self.roi_head.bbox_head is not None or hasattr(self,
            'bbox_head') and self.bbox_head is not None

    @property
    def with_mask(self):
        return hasattr(self.roi_head, 'mask_head'
            ) and self.roi_head.mask_head is not None or hasattr(self,
            'mask_head') and self.mask_head is not None

    @abstractmethod
    def extract_feat(self, imgs):
        pass

    def extract_feats(self, imgs):
        assert isinstance(imgs, list)
        for img in imgs:
            yield self.extract_feat(img)

    @abstractmethod
    def forward_train(self, imgs, img_metas, **kwargs):
        """
        Args:
            img (list[Tensor]): List of tensors of shape (1, C, H, W).
                Typically these should be mean centered and std scaled.
            img_metas (list[dict]): List of image info dict where each dict
                has: 'img_shape', 'scale_factor', 'flip', and my also contain
                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.
                For details on the values of these keys, see
                :class:`mmdet.datasets.pipelines.Collect`.
            kwargs (keyword arguments): Specific to concrete implementation.
        """
        pass

    async def async_simple_test(self, img, img_metas, **kwargs):
        raise NotImplementedError

    @abstractmethod
    def simple_test(self, img, img_metas, **kwargs):
        pass

    @abstractmethod
    def aug_test(self, imgs, img_metas, **kwargs):
        pass

    def init_weights(self, pretrained=None):
        if pretrained is not None:
            print_log(f'load model from: {pretrained}', logger='root')

    async def aforward_test(self, *, img, img_metas, **kwargs):
        for var, name in [(img, 'img'), (img_metas, 'img_metas')]:
            if not isinstance(var, list):
                raise TypeError(f'{name} must be a list, but got {type(var)}')
        num_augs = len(img)
        if num_augs != len(img_metas):
            raise ValueError(
                f'num of augmentations ({len(img)}) != num of image metas ({len(img_metas)})'
                )
        samples_per_gpu = img[0].size(0)
        assert samples_per_gpu == 1
        if num_augs == 1:
            return await self.async_simple_test(img[0], img_metas[0], **kwargs)
        else:
            raise NotImplementedError

    def forward_test(self, imgs, img_metas, **kwargs):
        """
        Args:
            imgs (List[Tensor]): the outer list indicates test-time
                augmentations and inner Tensor should have a shape NxCxHxW,
                which contains all images in the batch.
            img_metas (List[List[dict]]): the outer list indicates test-time
                augs (multiscale, flip, etc.) and the inner list indicates
                images in a batch.
        """
        for var, name in [(imgs, 'imgs'), (img_metas, 'img_metas')]:
            if not isinstance(var, list):
                raise TypeError(f'{name} must be a list, but got {type(var)}')
        num_augs = len(imgs)
        if num_augs != len(img_metas):
            raise ValueError(
                f'num of augmentations ({len(imgs)}) != num of image meta ({len(img_metas)})'
                )
        samples_per_gpu = imgs[0].size(0)
        assert samples_per_gpu == 1
        if num_augs == 1:
            """
            proposals (List[List[Tensor]]): the outer list indicates test-time
                augs (multiscale, flip, etc.) and the inner list indicates
                images in a batch. The Tensor should have a shape Px4, where
                P is the number of proposals.
            """
            if 'proposals' in kwargs:
                kwargs['proposals'] = kwargs['proposals'][0]
            return self.simple_test(imgs[0], img_metas[0], **kwargs)
        else:
            assert 'proposals' not in kwargs
            return self.aug_test(imgs, img_metas, **kwargs)

    @auto_fp16(apply_to=('img',))
    def forward(self, img, img_metas, return_loss=True, **kwargs):
        """
        Calls either forward_train or forward_test depending on whether
        return_loss=True. Note this setting will change the expected inputs.
        When `return_loss=True`, img and img_meta are single-nested (i.e.
        Tensor and List[dict]), and when `resturn_loss=False`, img and img_meta
        should be double nested (i.e.  List[Tensor], List[List[dict]]), with
        the outer list indicating test time augmentations.
        """
        if return_loss:
            return self.forward_train(img, img_metas, **kwargs)
        else:
            return self.forward_test(img, img_metas, **kwargs)

    def show_result(self, img, result, score_thr=0.3, bbox_color='green',
        text_color='green', thickness=1, font_scale=0.5, win_name='', show=
        False, wait_time=0, out_file=None):
        """Draw `result` over `img`.

        Args:
            img (str or Tensor): The image to be displayed.
            result (Tensor or tuple): The results to draw over `img`
                bbox_result or (bbox_result, segm_result).
            score_thr (float, optional): Minimum score of bboxes to be shown.
                Default: 0.3.
            bbox_color (str or tuple or :obj:`Color`): Color of bbox lines.
            text_color (str or tuple or :obj:`Color`): Color of texts.
            thickness (int): Thickness of lines.
            font_scale (float): Font scales of texts.
            win_name (str): The window name.
            wait_time (int): Value of waitKey param.
                Default: 0.
            show (bool): Whether to show the image.
                Default: False.
            out_file (str or None): The filename to write the image.
                Default: None.

        Returns:
            img (Tensor): Only if not `show` or `out_file`
        """
        img = mmcv.imread(img)
        img = img.copy()
        if isinstance(result, tuple):
            bbox_result, segm_result = result
            if isinstance(segm_result, tuple):
                segm_result = segm_result[0]
        else:
            bbox_result, segm_result = result, None
        bboxes = np.vstack(bbox_result)
        labels = [np.full(bbox.shape[0], i, dtype=np.int32) for i, bbox in
            enumerate(bbox_result)]
        labels = np.concatenate(labels)
        if segm_result is not None and len(labels) > 0:
            segms = mmcv.concat_list(segm_result)
            inds = np.where(bboxes[:, (-1)] > score_thr)[0]
            np.random.seed(42)
            color_masks = [np.random.randint(0, 256, (1, 3), dtype=np.uint8
                ) for _ in range(max(labels) + 1)]
            for i in inds:
                i = int(i)
                color_mask = color_masks[labels[i]]
                mask = maskUtils.decode(segms[i]).astype(np.bool)
                img[mask] = img[mask] * 0.5 + color_mask * 0.5
        if out_file is not None:
            show = False
        mmcv.imshow_det_bboxes(img, bboxes, labels, class_names=self.
            CLASSES, score_thr=score_thr, bbox_color=bbox_color, text_color
            =text_color, thickness=thickness, font_scale=font_scale,
            win_name=win_name, show=show, wait_time=wait_time, out_file=
            out_file)
        if not (show or out_file):
            warnings.warn(
                'show==False and out_file is not specified, only result image will be returned'
                )
            return img


def accuracy(pred, target, topk=1):
    assert isinstance(topk, (int, tuple))
    if isinstance(topk, int):
        topk = topk,
        return_single = True
    else:
        return_single = False
    maxk = max(topk)
    _, pred_label = pred.topk(maxk, dim=1)
    pred_label = pred_label.t()
    correct = pred_label.eq(target.view(1, -1).expand_as(pred_label))
    res = []
    for k in topk:
        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
        res.append(correct_k.mul_(100.0 / pred.size(0)))
    return res[0] if return_single else res


class Accuracy(nn.Module):

    def __init__(self, topk=(1,)):
        super().__init__()
        self.topk = topk

    def forward(self, pred, target):
        return accuracy(pred, target, self.topk)


def reduce_loss(loss, reduction):
    """Reduce loss as specified.

    Args:
        loss (Tensor): Elementwise loss tensor.
        reduction (str): Options are "none", "mean" and "sum".

    Return:
        Tensor: Reduced loss tensor.
    """
    reduction_enum = F._Reduction.get_enum(reduction)
    if reduction_enum == 0:
        return loss
    elif reduction_enum == 1:
        return loss.mean()
    elif reduction_enum == 2:
        return loss.sum()


def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):
    """Apply element-wise weight and reduce loss.

    Args:
        loss (Tensor): Element-wise loss.
        weight (Tensor): Element-wise weights.
        reduction (str): Same as built-in losses of PyTorch.
        avg_factor (float): Avarage factor when computing the mean of losses.

    Returns:
        Tensor: Processed loss values.
    """
    if weight is not None:
        loss = loss * weight
    if avg_factor is None:
        loss = reduce_loss(loss, reduction)
    elif reduction == 'mean':
        loss = loss.sum() / avg_factor
    elif reduction != 'none':
        raise ValueError('avg_factor can not be used with reduction="sum"')
    return loss


def weighted_loss(loss_func):
    """Create a weighted version of a given loss function.

    To use this decorator, the loss function must have the signature like
    `loss_func(pred, target, **kwargs)`. The function only needs to compute
    element-wise loss without any reduction. This decorator will add weight
    and reduction arguments to the function. The decorated function will have
    the signature like `loss_func(pred, target, weight=None, reduction='mean',
    avg_factor=None, **kwargs)`.

    :Example:

    >>> import torch
    >>> @weighted_loss
    >>> def l1_loss(pred, target):
    >>>     return (pred - target).abs()

    >>> pred = torch.Tensor([0, 2, 3])
    >>> target = torch.Tensor([1, 1, 1])
    >>> weight = torch.Tensor([1, 0, 1])

    >>> l1_loss(pred, target)
    tensor(1.3333)
    >>> l1_loss(pred, target, weight)
    tensor(1.)
    >>> l1_loss(pred, target, reduction='none')
    tensor([1., 1., 2.])
    >>> l1_loss(pred, target, weight, avg_factor=2)
    tensor(1.5000)
    """

    @functools.wraps(loss_func)
    def wrapper(pred, target, weight=None, reduction='mean', avg_factor=
        None, **kwargs):
        loss = loss_func(pred, target, **kwargs)
        loss = weight_reduce_loss(loss, weight, reduction, avg_factor)
        return loss
    return wrapper


@weighted_loss
def balanced_l1_loss(pred, target, beta=1.0, alpha=0.5, gamma=1.5,
    reduction='mean'):
    assert beta > 0
    assert pred.size() == target.size() and target.numel() > 0
    diff = torch.abs(pred - target)
    b = np.e ** (gamma / alpha) - 1
    loss = torch.where(diff < beta, alpha / b * (b * diff + 1) * torch.log(
        b * diff / beta + 1) - alpha * diff, gamma * diff + gamma / b - 
        alpha * beta)
    return loss


def cross_entropy(pred, label, weight=None, reduction='mean', avg_factor=None):
    loss = F.cross_entropy(pred, label, reduction='none')
    if weight is not None:
        weight = weight.float()
    loss = weight_reduce_loss(loss, weight=weight, reduction=reduction,
        avg_factor=avg_factor)
    return loss


def mask_cross_entropy(pred, target, label, reduction='mean', avg_factor=None):
    assert reduction == 'mean' and avg_factor is None
    num_rois = pred.size()[0]
    inds = torch.arange(0, num_rois, dtype=torch.long, device=pred.device)
    pred_slice = pred[inds, label].squeeze(1)
    return F.binary_cross_entropy_with_logits(pred_slice, target, reduction
        ='mean')[None]


def _expand_binary_labels(labels, label_weights, label_channels):
    bin_labels = labels.new_full((labels.size(0), label_channels), 0)
    inds = torch.nonzero(labels >= 1, as_tuple=False).squeeze()
    if inds.numel() > 0:
        bin_labels[inds, labels[inds] - 1] = 1
    if label_weights is None:
        bin_label_weights = None
    else:
        bin_label_weights = label_weights.view(-1, 1).expand(label_weights.
            size(0), label_channels)
    return bin_labels, bin_label_weights


def binary_cross_entropy(pred, label, weight=None, reduction='mean',
    avg_factor=None):
    if pred.dim() != label.dim():
        label, weight = _expand_binary_labels(label, weight, pred.size(-1))
    if weight is not None:
        weight = weight.float()
    loss = F.binary_cross_entropy_with_logits(pred, label.float(), weight,
        reduction='none')
    loss = weight_reduce_loss(loss, reduction=reduction, avg_factor=avg_factor)
    return loss


class SigmoidFocalLossFunction(Function):

    @staticmethod
    def forward(ctx, input, target, gamma=2.0, alpha=0.25):
        ctx.save_for_backward(input, target)
        num_classes = input.shape[1]
        ctx.num_classes = num_classes
        ctx.gamma = gamma
        ctx.alpha = alpha
        loss = sigmoid_focal_loss_ext.forward(input, target, num_classes,
            gamma, alpha)
        return loss

    @staticmethod
    @once_differentiable
    def backward(ctx, d_loss):
        input, target = ctx.saved_tensors
        num_classes = ctx.num_classes
        gamma = ctx.gamma
        alpha = ctx.alpha
        d_loss = d_loss.contiguous()
        d_input = sigmoid_focal_loss_ext.backward(input, target, d_loss,
            num_classes, gamma, alpha)
        return d_input, None, None, None, None


sigmoid_focal_loss = SigmoidFocalLossFunction.apply


def _expand_onehot_labels(labels, label_weights, label_channels):
    bin_labels = labels.new_full((labels.size(0), label_channels), 0)
    inds = torch.nonzero((labels >= 0) & (labels < label_channels),
        as_tuple=False).squeeze()
    if inds.numel() > 0:
        bin_labels[inds, labels[inds]] = 1
    bin_label_weights = label_weights.view(-1, 1).expand(label_weights.size
        (0), label_channels)
    return bin_labels, bin_label_weights


def bbox_overlaps(bboxes1, bboxes2, mode='iou', is_aligned=False):
    """Calculate overlap between two set of bboxes.

    If ``is_aligned`` is ``False``, then calculate the ious between each bbox
    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of
    bboxes1 and bboxes2.

    Args:
        bboxes1 (Tensor): shape (m, 4) in <x1, y1, x2, y2> format or empty.
        bboxes2 (Tensor): shape (n, 4) in <x1, y1, x2, y2> format or empty.
            If is_aligned is ``True``, then m and n must be equal.
        mode (str): "iou" (intersection over union) or iof (intersection over
            foreground).

    Returns:
        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)

    Example:
        >>> bboxes1 = torch.FloatTensor([
        >>>     [0, 0, 10, 10],
        >>>     [10, 10, 20, 20],
        >>>     [32, 32, 38, 42],
        >>> ])
        >>> bboxes2 = torch.FloatTensor([
        >>>     [0, 0, 10, 20],
        >>>     [0, 10, 10, 19],
        >>>     [10, 10, 20, 20],
        >>> ])
        >>> bbox_overlaps(bboxes1, bboxes2)
        tensor([[0.5000, 0.0000, 0.0000],
                [0.0000, 0.0000, 1.0000],
                [0.0000, 0.0000, 0.0000]])

    Example:
        >>> empty = torch.FloatTensor([])
        >>> nonempty = torch.FloatTensor([
        >>>     [0, 0, 10, 9],
        >>> ])
        >>> assert tuple(bbox_overlaps(empty, nonempty).shape) == (0, 1)
        >>> assert tuple(bbox_overlaps(nonempty, empty).shape) == (1, 0)
        >>> assert tuple(bbox_overlaps(empty, empty).shape) == (0, 0)
    """
    assert mode in ['iou', 'iof']
    assert bboxes1.size(-1) == 4 or bboxes1.size(0) == 0
    assert bboxes2.size(-1) == 4 or bboxes2.size(0) == 0
    rows = bboxes1.size(0)
    cols = bboxes2.size(0)
    if is_aligned:
        assert rows == cols
    if rows * cols == 0:
        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)
    if is_aligned:
        lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])
        rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])
        wh = (rb - lt).clamp(min=0)
        overlap = wh[:, (0)] * wh[:, (1)]
        area1 = (bboxes1[:, (2)] - bboxes1[:, (0)]) * (bboxes1[:, (3)] -
            bboxes1[:, (1)])
        if mode == 'iou':
            area2 = (bboxes2[:, (2)] - bboxes2[:, (0)]) * (bboxes2[:, (3)] -
                bboxes2[:, (1)])
            ious = overlap / (area1 + area2 - overlap)
        else:
            ious = overlap / area1
    else:
        lt = torch.max(bboxes1[:, (None), :2], bboxes2[:, :2])
        rb = torch.min(bboxes1[:, (None), 2:], bboxes2[:, 2:])
        wh = (rb - lt).clamp(min=0)
        overlap = wh[:, :, (0)] * wh[:, :, (1)]
        area1 = (bboxes1[:, (2)] - bboxes1[:, (0)]) * (bboxes1[:, (3)] -
            bboxes1[:, (1)])
        if mode == 'iou':
            area2 = (bboxes2[:, (2)] - bboxes2[:, (0)]) * (bboxes2[:, (3)] -
                bboxes2[:, (1)])
            ious = overlap / (area1[:, (None)] + area2 - overlap)
        else:
            ious = overlap / area1[:, (None)]
    return ious


@weighted_loss
def iou_loss(pred, target, eps=1e-06):
    """IoU loss.

    Computing the IoU loss between a set of predicted bboxes and target bboxes.
    The loss is calculated as negative log of IoU.

    Args:
        pred (Tensor): Predicted bboxes of format (x1, y1, x2, y2),
            shape (n, 4).
        target (Tensor): Corresponding gt bboxes, shape (n, 4).
        eps (float): Eps to avoid log(0).

    Return:
        Tensor: Loss tensor.
    """
    ious = bbox_overlaps(pred, target, is_aligned=True).clamp(min=eps)
    loss = -ious.log()
    return loss


@weighted_loss
def bounded_iou_loss(pred, target, beta=0.2, eps=0.001):
    """Improving Object Localization with Fitness NMS and Bounded IoU Loss,
    https://arxiv.org/abs/1711.00164.

    Args:
        pred (tensor): Predicted bboxes.
        target (tensor): Target bboxes.
        beta (float): beta parameter in smoothl1.
        eps (float): eps to avoid NaN.
    """
    pred_ctrx = (pred[:, (0)] + pred[:, (2)]) * 0.5
    pred_ctry = (pred[:, (1)] + pred[:, (3)]) * 0.5
    pred_w = pred[:, (2)] - pred[:, (0)]
    pred_h = pred[:, (3)] - pred[:, (1)]
    with torch.no_grad():
        target_ctrx = (target[:, (0)] + target[:, (2)]) * 0.5
        target_ctry = (target[:, (1)] + target[:, (3)]) * 0.5
        target_w = target[:, (2)] - target[:, (0)]
        target_h = target[:, (3)] - target[:, (1)]
    dx = target_ctrx - pred_ctrx
    dy = target_ctry - pred_ctry
    loss_dx = 1 - torch.max((target_w - 2 * dx.abs()) / (target_w + 2 * dx.
        abs() + eps), torch.zeros_like(dx))
    loss_dy = 1 - torch.max((target_h - 2 * dy.abs()) / (target_h + 2 * dy.
        abs() + eps), torch.zeros_like(dy))
    loss_dw = 1 - torch.min(target_w / (pred_w + eps), pred_w / (target_w +
        eps))
    loss_dh = 1 - torch.min(target_h / (pred_h + eps), pred_h / (target_h +
        eps))
    loss_comb = torch.stack([loss_dx, loss_dy, loss_dw, loss_dh], dim=-1).view(
        loss_dx.size(0), -1)
    loss = torch.where(loss_comb < beta, 0.5 * loss_comb * loss_comb / beta,
        loss_comb - 0.5 * beta)
    return loss


@weighted_loss
def giou_loss(pred, target, eps=1e-07):
    """
    Generalized Intersection over Union: A Metric and A Loss for
    Bounding Box Regression
    https://arxiv.org/abs/1902.09630

    code refer to:
    https://github.com/sfzhang15/ATSS/blob/master/atss_core/modeling/rpn/atss/loss.py#L36

    Args:
        pred (Tensor): Predicted bboxes of format (x1, y1, x2, y2),
            shape (n, 4).
        target (Tensor): Corresponding gt bboxes, shape (n, 4).
        eps (float): Eps to avoid log(0).

    Return:
        Tensor: Loss tensor.
    """
    lt = torch.max(pred[:, :2], target[:, :2])
    rb = torch.min(pred[:, 2:], target[:, 2:])
    wh = (rb - lt).clamp(min=0)
    overlap = wh[:, (0)] * wh[:, (1)]
    ap = (pred[:, (2)] - pred[:, (0)]) * (pred[:, (3)] - pred[:, (1)])
    ag = (target[:, (2)] - target[:, (0)]) * (target[:, (3)] - target[:, (1)])
    union = ap + ag - overlap + eps
    ious = overlap / union
    enclose_x1y1 = torch.min(pred[:, :2], target[:, :2])
    enclose_x2y2 = torch.max(pred[:, 2:], target[:, 2:])
    enclose_wh = (enclose_x2y2 - enclose_x1y1).clamp(min=0)
    enclose_area = enclose_wh[:, (0)] * enclose_wh[:, (1)] + eps
    gious = ious - (enclose_area - union) / enclose_area
    loss = 1 - gious
    return loss


@weighted_loss
def mse_loss(pred, target):
    return F.mse_loss(pred, target, reduction='none')


@weighted_loss
def smooth_l1_loss(pred, target, beta=1.0):
    assert beta > 0
    assert pred.size() == target.size() and target.numel() > 0
    diff = torch.abs(pred - target)
    loss = torch.where(diff < beta, 0.5 * diff * diff / beta, diff - 0.5 * beta
        )
    return loss


@weighted_loss
def l1_loss(pred, target):
    assert pred.size() == target.size() and target.numel() > 0
    loss = torch.abs(pred - target)
    return loss


class BasicResBlock(nn.Module):
    """Basic residual block.

    This block is a little different from the block in the ResNet backbone.
    The kernel size of conv1 is 1 in this block while 3 in ResNet BasicBlock.

    Args:
        in_channels (int): Channels of the input feature map.
        out_channels (int): Channels of the output feature map.
        conv_cfg (dict): The config dict for convolution layers.
        norm_cfg (dict): The config dict for normalization layers.
    """

    def __init__(self, in_channels, out_channels, conv_cfg=None, norm_cfg=
        dict(type='BN')):
        super(BasicResBlock, self).__init__()
        self.conv1 = ConvModule(in_channels, in_channels, kernel_size=3,
            padding=1, bias=False, conv_cfg=conv_cfg, norm_cfg=norm_cfg)
        self.conv2 = ConvModule(in_channels, out_channels, kernel_size=1,
            bias=False, conv_cfg=conv_cfg, norm_cfg=norm_cfg, act_cfg=None)
        self.conv_identity = ConvModule(in_channels, out_channels,
            kernel_size=1, conv_cfg=conv_cfg, norm_cfg=norm_cfg, act_cfg=None)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x
        x = self.conv1(x)
        x = self.conv2(x)
        identity = self.conv_identity(identity)
        out = x + identity
        out = self.relu(out)
        return out


def _do_paste_mask(masks, boxes, img_h, img_w, skip_empty=True):
    """Paste instance masks acoording to boxes.

    This implementation is modified from
    https://github.com/facebookresearch/detectron2/

    Args:
        masks (Tensor): N, 1, H, W
        boxes (Tensor): N, 4
        img_h (int): Height of the image to be pasted.
        img_w (int): Width of the image to be pasted.
        skip_empty (bool): Only paste masks within the region that
            tightly bound all boxes, and returns the results this region only.
            An important optimization for CPU.

    Returns:
        tuple: (Tensor, tuple). The first item is mask tensor, the second one
            is the slice object.
        If skip_empty == False, the whole image will be pasted. It will
            return a mask of shape (N, img_h, img_w) and an empty tuple.
        If skip_empty == True, only area around the mask will be pasted.
            A mask of shape (N, h', w') and its start and end coordinates
            in the original image will be returned.
    """
    device = masks.device
    if skip_empty:
        x0_int, y0_int = torch.clamp(boxes.min(dim=0).values.floor()[:2] - 
            1, min=0).to(dtype=torch.int32)
        x1_int = torch.clamp(boxes[:, (2)].max().ceil() + 1, max=img_w).to(
            dtype=torch.int32)
        y1_int = torch.clamp(boxes[:, (3)].max().ceil() + 1, max=img_h).to(
            dtype=torch.int32)
    else:
        x0_int, y0_int = 0, 0
        x1_int, y1_int = img_w, img_h
    x0, y0, x1, y1 = torch.split(boxes, 1, dim=1)
    N = masks.shape[0]
    img_y = torch.arange(y0_int, y1_int, device=device, dtype=torch.float32
        ) + 0.5
    img_x = torch.arange(x0_int, x1_int, device=device, dtype=torch.float32
        ) + 0.5
    img_y = (img_y - y0) / (y1 - y0) * 2 - 1
    img_x = (img_x - x0) / (x1 - x0) * 2 - 1
    if torch.isinf(img_x).any():
        inds = torch.where(torch.isinf(img_x))
        img_x[inds] = 0
    if torch.isinf(img_y).any():
        inds = torch.where(torch.isinf(img_y))
        img_y[inds] = 0
    gx = img_x[:, (None), :].expand(N, img_y.size(1), img_x.size(1))
    gy = img_y[:, :, (None)].expand(N, img_y.size(1), img_x.size(1))
    grid = torch.stack([gx, gy], dim=3)
    img_masks = F.grid_sample(masks.to(dtype=torch.float32), grid,
        align_corners=False)
    if skip_empty:
        return img_masks[:, (0)], (slice(y0_int, y1_int), slice(x0_int, x1_int)
            )
    else:
        return img_masks[:, (0)], ()


def mask_target_single(pos_proposals, pos_assigned_gt_inds, gt_masks, cfg):
    device = pos_proposals.device
    mask_size = _pair(cfg.mask_size)
    num_pos = pos_proposals.size(0)
    if num_pos > 0:
        proposals_np = pos_proposals.cpu().numpy()
        maxh, maxw = gt_masks.height, gt_masks.width
        proposals_np[:, ([0, 2])] = np.clip(proposals_np[:, ([0, 2])], 0, maxw)
        proposals_np[:, ([1, 3])] = np.clip(proposals_np[:, ([1, 3])], 0, maxh)
        pos_assigned_gt_inds = pos_assigned_gt_inds.cpu().numpy()
        mask_targets = gt_masks.crop_and_resize(proposals_np, mask_size,
            device=device, inds=pos_assigned_gt_inds).to_ndarray()
        mask_targets = torch.from_numpy(mask_targets).float().to(device)
    else:
        mask_targets = pos_proposals.new_zeros((0,) + mask_size)
    return mask_targets


def mask_target(pos_proposals_list, pos_assigned_gt_inds_list,
    gt_masks_list, cfg):
    cfg_list = [cfg for _ in range(len(pos_proposals_list))]
    mask_targets = map(mask_target_single, pos_proposals_list,
        pos_assigned_gt_inds_list, gt_masks_list, cfg_list)
    mask_targets = list(mask_targets)
    if len(mask_targets) > 0:
        mask_targets = torch.cat(mask_targets)
    return mask_targets


GPU_MEM_LIMIT = 1024 ** 3


def force_fp32(apply_to=None, out_fp16=False):
    """Decorator to convert input arguments to fp32 in force.

    This decorator is useful when you write custom modules and want to support
    mixed precision training. If there are some inputs that must be processed
    in fp32 mode, then this decorator can handle it. If inputs arguments are
    fp16 tensors, they will be converted to fp32 automatically. Arguments other
    than fp16 tensors are ignored.

    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp16 (bool): Whether to convert the output back to fp16.

    Example:

        >>> import torch.nn as nn
        >>> class MyModule1(nn.Module):
        >>>
        >>>     # Convert x and y to fp32
        >>>     @force_fp32()
        >>>     def loss(self, x, y):
        >>>         pass

        >>> import torch.nn as nn
        >>> class MyModule2(nn.Module):
        >>>
        >>>     # convert pred to fp32
        >>>     @force_fp32(apply_to=('pred', ))
        >>>     def post_process(self, pred, others):
        >>>         pass
    """

    def force_fp32_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError(
                    '@force_fp32 can only be used to decorate the method of nn.Module'
                    )
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            args_info = getfullargspec(old_func)
            args_to_cast = args_info.args if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for i, arg_name in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.
                            half, torch.float))
                    else:
                        new_args.append(args[i])
            new_kwargs = dict()
            if kwargs:
                for arg_name, arg_value in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value,
                            torch.half, torch.float)
                    else:
                        new_kwargs[arg_name] = arg_value
            output = old_func(*new_args, **new_kwargs)
            if out_fp16:
                output = cast_tensor_type(output, torch.float, torch.half)
            return output
        return new_func
    return force_fp32_wrapper


BYTES_PER_FLOAT = 4


class ResLayer(nn.Sequential):
    """ResLayer to build ResNet style backbone.

    Args:
        block (nn.Module): block used to build ResLayer.
        inplanes (int): inplanes of block.
        planes (int): planes of block.
        num_blocks (int): number of blocks.
        stride (int): stride of the first block. Default: 1
        avg_down (bool): Use AvgPool instead of stride conv when
            downsampling in the bottleneck. Default: False
        conv_cfg (dict): dictionary to construct and config conv layer.
            Default: None
        norm_cfg (dict): dictionary to construct and config norm layer.
            Default: dict(type='BN')
    """

    def __init__(self, block, inplanes, planes, num_blocks, stride=1,
        avg_down=False, conv_cfg=None, norm_cfg=dict(type='BN'), **kwargs):
        self.block = block
        downsample = None
        if stride != 1 or inplanes != planes * block.expansion:
            downsample = []
            conv_stride = stride
            if avg_down and stride != 1:
                conv_stride = 1
                downsample.append(nn.AvgPool2d(kernel_size=stride, stride=
                    stride, ceil_mode=True, count_include_pad=False))
            downsample.extend([build_conv_layer(conv_cfg, inplanes, planes *
                block.expansion, kernel_size=1, stride=conv_stride, bias=
                False), build_norm_layer(norm_cfg, planes * block.expansion
                )[1]])
            downsample = nn.Sequential(*downsample)
        layers = []
        layers.append(block(inplanes=inplanes, planes=planes, stride=stride,
            downsample=downsample, conv_cfg=conv_cfg, norm_cfg=norm_cfg, **
            kwargs))
        inplanes = planes * block.expansion
        for i in range(1, num_blocks):
            layers.append(block(inplanes=inplanes, planes=planes, stride=1,
                conv_cfg=conv_cfg, norm_cfg=norm_cfg, **kwargs))
        super(ResLayer, self).__init__(*layers)


class CARAFENaiveFunction(Function):

    @staticmethod
    def forward(ctx, features, masks, kernel_size, group_size, scale_factor):
        assert scale_factor >= 1
        assert masks.size(1) == kernel_size * kernel_size * group_size
        assert masks.size(-1) == features.size(-1) * scale_factor
        assert masks.size(-2) == features.size(-2) * scale_factor
        assert features.size(1) % group_size == 0
        assert (kernel_size - 1) % 2 == 0 and kernel_size >= 1
        ctx.kernel_size = kernel_size
        ctx.group_size = group_size
        ctx.scale_factor = scale_factor
        ctx.feature_size = features.size()
        ctx.mask_size = masks.size()
        n, c, h, w = features.size()
        output = features.new_zeros((n, c, h * scale_factor, w * scale_factor))
        if features.is_cuda:
            carafe_naive_ext.forward(features, masks, kernel_size,
                group_size, scale_factor, output)
        else:
            raise NotImplementedError
        if features.requires_grad or masks.requires_grad:
            ctx.save_for_backward(features, masks)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        assert grad_output.is_cuda
        features, masks = ctx.saved_tensors
        kernel_size = ctx.kernel_size
        group_size = ctx.group_size
        scale_factor = ctx.scale_factor
        grad_input = torch.zeros_like(features)
        grad_masks = torch.zeros_like(masks)
        carafe_naive_ext.backward(grad_output.contiguous(), features, masks,
            kernel_size, group_size, scale_factor, grad_input, grad_masks)
        return grad_input, grad_masks, None, None, None


class CARAFENaive(Module):

    def __init__(self, kernel_size, group_size, scale_factor):
        super(CARAFENaive, self).__init__()
        assert isinstance(kernel_size, int) and isinstance(group_size, int
            ) and isinstance(scale_factor, int)
        self.kernel_size = kernel_size
        self.group_size = group_size
        self.scale_factor = scale_factor

    def forward(self, features, masks):
        return CARAFENaiveFunction.apply(features, masks, self.kernel_size,
            self.group_size, self.scale_factor)


class CARAFEFunction(Function):

    @staticmethod
    def forward(ctx, features, masks, kernel_size, group_size, scale_factor):
        assert scale_factor >= 1
        assert masks.size(1) == kernel_size * kernel_size * group_size
        assert masks.size(-1) == features.size(-1) * scale_factor
        assert masks.size(-2) == features.size(-2) * scale_factor
        assert features.size(1) % group_size == 0
        assert (kernel_size - 1) % 2 == 0 and kernel_size >= 1
        ctx.kernel_size = kernel_size
        ctx.group_size = group_size
        ctx.scale_factor = scale_factor
        ctx.feature_size = features.size()
        ctx.mask_size = masks.size()
        n, c, h, w = features.size()
        output = features.new_zeros((n, c, h * scale_factor, w * scale_factor))
        routput = features.new_zeros(output.size(), requires_grad=False)
        rfeatures = features.new_zeros(features.size(), requires_grad=False)
        rmasks = masks.new_zeros(masks.size(), requires_grad=False)
        if features.is_cuda:
            carafe_ext.forward(features, rfeatures, masks, rmasks,
                kernel_size, group_size, scale_factor, routput, output)
        else:
            raise NotImplementedError
        if features.requires_grad or masks.requires_grad:
            ctx.save_for_backward(features, masks, rfeatures)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        assert grad_output.is_cuda
        features, masks, rfeatures = ctx.saved_tensors
        kernel_size = ctx.kernel_size
        group_size = ctx.group_size
        scale_factor = ctx.scale_factor
        rgrad_output = torch.zeros_like(grad_output, requires_grad=False)
        rgrad_input_hs = torch.zeros_like(grad_output, requires_grad=False)
        rgrad_input = torch.zeros_like(features, requires_grad=False)
        rgrad_masks = torch.zeros_like(masks, requires_grad=False)
        grad_input = torch.zeros_like(features, requires_grad=False)
        grad_masks = torch.zeros_like(masks, requires_grad=False)
        carafe_ext.backward(grad_output.contiguous(), rfeatures, masks,
            kernel_size, group_size, scale_factor, rgrad_output,
            rgrad_input_hs, rgrad_input, rgrad_masks, grad_input, grad_masks)
        return grad_input, grad_masks, None, None, None, None


class CARAFE(Module):
    """ CARAFE: Content-Aware ReAssembly of FEatures

    Please refer to https://arxiv.org/abs/1905.02188 for more details.

    Args:
        kernel_size (int): reassemble kernel size
        group_size (int): reassemble group size
        scale_factor (int): upsample ratio

    Returns:
        upsampled feature map
    """

    def __init__(self, kernel_size, group_size, scale_factor):
        super(CARAFE, self).__init__()
        assert isinstance(kernel_size, int) and isinstance(group_size, int
            ) and isinstance(scale_factor, int)
        self.kernel_size = kernel_size
        self.group_size = group_size
        self.scale_factor = scale_factor

    def forward(self, features, masks):
        return CARAFEFunction.apply(features, masks, self.kernel_size, self
            .group_size, self.scale_factor)


carafe = CARAFEFunction.apply


def last_zero_init(m):
    if isinstance(m, nn.Sequential):
        constant_init(m[-1], val=0)
    else:
        constant_init(m, val=0)


class ContextBlock(nn.Module):
    """ContextBlock module in GCNet.

    See 'GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond'
    (https://arxiv.org/abs/1904.11492) for details.

    Args:
        in_channels (int): Channels of the input feature map.
        ratio (float): Ratio of channels of transform bottleneck
        pooling_type (str): Pooling method for context modeling
        fusion_types (list[str]|tuple[str]): Fusion method for feature fusion,
            options: 'channels_add', 'channel_mul'
    """

    def __init__(self, in_channels, ratio, pooling_type='att', fusion_types
        =('channel_add',)):
        super(ContextBlock, self).__init__()
        assert pooling_type in ['avg', 'att']
        assert isinstance(fusion_types, (list, tuple))
        valid_fusion_types = ['channel_add', 'channel_mul']
        assert all([(f in valid_fusion_types) for f in fusion_types])
        assert len(fusion_types) > 0, 'at least one fusion should be used'
        self.in_channels = in_channels
        self.ratio = ratio
        self.planes = int(in_channels * ratio)
        self.pooling_type = pooling_type
        self.fusion_types = fusion_types
        if pooling_type == 'att':
            self.conv_mask = nn.Conv2d(in_channels, 1, kernel_size=1)
            self.softmax = nn.Softmax(dim=2)
        else:
            self.avg_pool = nn.AdaptiveAvgPool2d(1)
        if 'channel_add' in fusion_types:
            self.channel_add_conv = nn.Sequential(nn.Conv2d(self.
                in_channels, self.planes, kernel_size=1), nn.LayerNorm([
                self.planes, 1, 1]), nn.ReLU(inplace=True), nn.Conv2d(self.
                planes, self.in_channels, kernel_size=1))
        else:
            self.channel_add_conv = None
        if 'channel_mul' in fusion_types:
            self.channel_mul_conv = nn.Sequential(nn.Conv2d(self.
                in_channels, self.planes, kernel_size=1), nn.LayerNorm([
                self.planes, 1, 1]), nn.ReLU(inplace=True), nn.Conv2d(self.
                planes, self.in_channels, kernel_size=1))
        else:
            self.channel_mul_conv = None
        self.reset_parameters()

    def reset_parameters(self):
        if self.pooling_type == 'att':
            kaiming_init(self.conv_mask, mode='fan_in')
            self.conv_mask.inited = True
        if self.channel_add_conv is not None:
            last_zero_init(self.channel_add_conv)
        if self.channel_mul_conv is not None:
            last_zero_init(self.channel_mul_conv)

    def spatial_pool(self, x):
        batch, channel, height, width = x.size()
        if self.pooling_type == 'att':
            input_x = x
            input_x = input_x.view(batch, channel, height * width)
            input_x = input_x.unsqueeze(1)
            context_mask = self.conv_mask(x)
            context_mask = context_mask.view(batch, 1, height * width)
            context_mask = self.softmax(context_mask)
            context_mask = context_mask.unsqueeze(-1)
            context = torch.matmul(input_x, context_mask)
            context = context.view(batch, channel, 1, 1)
        else:
            context = self.avg_pool(x)
        return context

    def forward(self, x):
        context = self.spatial_pool(x)
        out = x
        if self.channel_mul_conv is not None:
            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))
            out = out * channel_mul_term
        if self.channel_add_conv is not None:
            channel_add_term = self.channel_add_conv(context)
            out = out + channel_add_term
        return out


def conv_ws_2d(input, weight, bias=None, stride=1, padding=0, dilation=1,
    groups=1, eps=1e-05):
    c_in = weight.size(0)
    weight_flat = weight.view(c_in, -1)
    mean = weight_flat.mean(dim=1, keepdim=True).view(c_in, 1, 1, 1)
    std = weight_flat.std(dim=1, keepdim=True).view(c_in, 1, 1, 1)
    weight = (weight - mean) / (std + eps)
    return F.conv2d(input, weight, bias, stride, padding, dilation, groups)


class DeformConvFunction(Function):

    @staticmethod
    def forward(ctx, input, offset, weight, stride=1, padding=0, dilation=1,
        groups=1, deformable_groups=1, im2col_step=64):
        if input is not None and input.dim() != 4:
            raise ValueError(
                f'Expected 4D tensor as input, got {input.dim()}D tensor instead.'
                )
        ctx.stride = _pair(stride)
        ctx.padding = _pair(padding)
        ctx.dilation = _pair(dilation)
        ctx.groups = groups
        ctx.deformable_groups = deformable_groups
        ctx.im2col_step = im2col_step
        ctx.save_for_backward(input, offset, weight)
        output = input.new_empty(DeformConvFunction._output_size(input,
            weight, ctx.padding, ctx.dilation, ctx.stride))
        ctx.bufs_ = [input.new_empty(0), input.new_empty(0)]
        if not input.is_cuda:
            raise NotImplementedError
        else:
            cur_im2col_step = min(ctx.im2col_step, input.shape[0])
            assert input.shape[0
                ] % cur_im2col_step == 0, 'im2col step must divide batchsize'
            deform_conv_ext.deform_conv_forward(input, weight, offset,
                output, ctx.bufs_[0], ctx.bufs_[1], weight.size(3), weight.
                size(2), ctx.stride[1], ctx.stride[0], ctx.padding[1], ctx.
                padding[0], ctx.dilation[1], ctx.dilation[0], ctx.groups,
                ctx.deformable_groups, cur_im2col_step)
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        input, offset, weight = ctx.saved_tensors
        grad_input = grad_offset = grad_weight = None
        if not grad_output.is_cuda:
            raise NotImplementedError
        else:
            cur_im2col_step = min(ctx.im2col_step, input.shape[0])
            assert input.shape[0
                ] % cur_im2col_step == 0, 'im2col step must divide batchsize'
            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:
                grad_input = torch.zeros_like(input)
                grad_offset = torch.zeros_like(offset)
                deform_conv_ext.deform_conv_backward_input(input, offset,
                    grad_output, grad_input, grad_offset, weight, ctx.bufs_
                    [0], weight.size(3), weight.size(2), ctx.stride[1], ctx
                    .stride[0], ctx.padding[1], ctx.padding[0], ctx.
                    dilation[1], ctx.dilation[0], ctx.groups, ctx.
                    deformable_groups, cur_im2col_step)
            if ctx.needs_input_grad[2]:
                grad_weight = torch.zeros_like(weight)
                deform_conv_ext.deform_conv_backward_parameters(input,
                    offset, grad_output, grad_weight, ctx.bufs_[0], ctx.
                    bufs_[1], weight.size(3), weight.size(2), ctx.stride[1],
                    ctx.stride[0], ctx.padding[1], ctx.padding[0], ctx.
                    dilation[1], ctx.dilation[0], ctx.groups, ctx.
                    deformable_groups, 1, cur_im2col_step)
        return (grad_input, grad_offset, grad_weight, None, None, None,
            None, None)

    @staticmethod
    def _output_size(input, weight, padding, dilation, stride):
        channels = weight.size(0)
        output_size = input.size(0), channels
        for d in range(input.dim() - 2):
            in_size = input.size(d + 2)
            pad = padding[d]
            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1
            stride_ = stride[d]
            output_size += (in_size + 2 * pad - kernel) // stride_ + 1,
        if not all(map(lambda s: s > 0, output_size)):
            raise ValueError(
                f"convolution input is too small (output would be {'x'.join(map(str, output_size))})"
                )
        return output_size


deform_conv = DeformConvFunction.apply


class DeformConv(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
        padding=0, dilation=1, groups=1, deformable_groups=1, bias=False):
        super(DeformConv, self).__init__()
        assert not bias
        assert in_channels % groups == 0, f'in_channels {in_channels} is not divisible by groups {groups}'
        assert out_channels % groups == 0, f'out_channels {out_channels} is not divisible by groups {groups}'
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = _pair(kernel_size)
        self.stride = _pair(stride)
        self.padding = _pair(padding)
        self.dilation = _pair(dilation)
        self.groups = groups
        self.deformable_groups = deformable_groups
        self.transposed = False
        self.output_padding = _single(0)
        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels //
            self.groups, *self.kernel_size))
        self.reset_parameters()

    def reset_parameters(self):
        n = self.in_channels
        for k in self.kernel_size:
            n *= k
        stdv = 1.0 / math.sqrt(n)
        self.weight.data.uniform_(-stdv, stdv)

    def forward(self, x, offset):
        input_pad = x.size(2) < self.kernel_size[0] or x.size(3
            ) < self.kernel_size[1]
        if input_pad:
            pad_h = max(self.kernel_size[0] - x.size(2), 0)
            pad_w = max(self.kernel_size[1] - x.size(3), 0)
            x = F.pad(x, (0, pad_w, 0, pad_h), 'constant', 0).contiguous()
            offset = F.pad(offset, (0, pad_w, 0, pad_h), 'constant', 0
                ).contiguous()
        out = deform_conv(x, offset, self.weight, self.stride, self.padding,
            self.dilation, self.groups, self.deformable_groups)
        if input_pad:
            out = out[:, :, :out.size(2) - pad_h, :out.size(3) - pad_w
                ].contiguous()
        return out


class ModulatedDeformConvFunction(Function):

    @staticmethod
    def forward(ctx, input, offset, mask, weight, bias=None, stride=1,
        padding=0, dilation=1, groups=1, deformable_groups=1):
        ctx.stride = stride
        ctx.padding = padding
        ctx.dilation = dilation
        ctx.groups = groups
        ctx.deformable_groups = deformable_groups
        ctx.with_bias = bias is not None
        if not ctx.with_bias:
            bias = input.new_empty(1)
        if not input.is_cuda:
            raise NotImplementedError
        if (weight.requires_grad or mask.requires_grad or offset.
            requires_grad or input.requires_grad):
            ctx.save_for_backward(input, offset, mask, weight, bias)
        output = input.new_empty(ModulatedDeformConvFunction._infer_shape(
            ctx, input, weight))
        ctx._bufs = [input.new_empty(0), input.new_empty(0)]
        deform_conv_ext.modulated_deform_conv_forward(input, weight, bias,
            ctx._bufs[0], offset, mask, output, ctx._bufs[1], weight.shape[
            2], weight.shape[3], ctx.stride, ctx.stride, ctx.padding, ctx.
            padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.
            deformable_groups, ctx.with_bias)
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        if not grad_output.is_cuda:
            raise NotImplementedError
        input, offset, mask, weight, bias = ctx.saved_tensors
        grad_input = torch.zeros_like(input)
        grad_offset = torch.zeros_like(offset)
        grad_mask = torch.zeros_like(mask)
        grad_weight = torch.zeros_like(weight)
        grad_bias = torch.zeros_like(bias)
        deform_conv_ext.modulated_deform_conv_backward(input, weight, bias,
            ctx._bufs[0], offset, mask, ctx._bufs[1], grad_input,
            grad_weight, grad_bias, grad_offset, grad_mask, grad_output,
            weight.shape[2], weight.shape[3], ctx.stride, ctx.stride, ctx.
            padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups,
            ctx.deformable_groups, ctx.with_bias)
        if not ctx.with_bias:
            grad_bias = None
        return (grad_input, grad_offset, grad_mask, grad_weight, grad_bias,
            None, None, None, None, None)

    @staticmethod
    def _infer_shape(ctx, input, weight):
        n = input.size(0)
        channels_out = weight.size(0)
        height, width = input.shape[2:4]
        kernel_h, kernel_w = weight.shape[2:4]
        height_out = (height + 2 * ctx.padding - (ctx.dilation * (kernel_h -
            1) + 1)) // ctx.stride + 1
        width_out = (width + 2 * ctx.padding - (ctx.dilation * (kernel_w - 
            1) + 1)) // ctx.stride + 1
        return n, channels_out, height_out, width_out


modulated_deform_conv = ModulatedDeformConvFunction.apply


class ModulatedDeformConv(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
        padding=0, dilation=1, groups=1, deformable_groups=1, bias=True):
        super(ModulatedDeformConv, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = _pair(kernel_size)
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups
        self.deformable_groups = deformable_groups
        self.with_bias = bias
        self.transposed = False
        self.output_padding = _single(0)
        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels //
            groups, *self.kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)
        self.init_weights()

    def init_weights(self):
        n = self.in_channels
        for k in self.kernel_size:
            n *= k
        stdv = 1.0 / math.sqrt(n)
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.zero_()

    def forward(self, x, offset, mask):
        return modulated_deform_conv(x, offset, mask, self.weight, self.
            bias, self.stride, self.padding, self.dilation, self.groups,
            self.deformable_groups)


class DeformRoIPoolingFunction(Function):

    @staticmethod
    def forward(ctx, data, rois, offset, spatial_scale, out_size,
        out_channels, no_trans, group_size=1, part_size=None,
        sample_per_part=4, trans_std=0.0):
        out_h, out_w = _pair(out_size)
        assert isinstance(out_h, int) and isinstance(out_w, int)
        assert out_h == out_w
        out_size = out_h
        ctx.spatial_scale = spatial_scale
        ctx.out_size = out_size
        ctx.out_channels = out_channels
        ctx.no_trans = no_trans
        ctx.group_size = group_size
        ctx.part_size = out_size if part_size is None else part_size
        ctx.sample_per_part = sample_per_part
        ctx.trans_std = trans_std
        assert 0.0 <= ctx.trans_std <= 1.0
        if not data.is_cuda:
            raise NotImplementedError
        n = rois.shape[0]
        output = data.new_empty(n, out_channels, out_size, out_size)
        output_count = data.new_empty(n, out_channels, out_size, out_size)
        deform_pool_ext.deform_psroi_pooling_forward(data, rois, offset,
            output, output_count, ctx.no_trans, ctx.spatial_scale, ctx.
            out_channels, ctx.group_size, ctx.out_size, ctx.part_size, ctx.
            sample_per_part, ctx.trans_std)
        if data.requires_grad or rois.requires_grad or offset.requires_grad:
            ctx.save_for_backward(data, rois, offset)
        ctx.output_count = output_count
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        if not grad_output.is_cuda:
            raise NotImplementedError
        data, rois, offset = ctx.saved_tensors
        output_count = ctx.output_count
        grad_input = torch.zeros_like(data)
        grad_rois = None
        grad_offset = torch.zeros_like(offset)
        deform_pool_ext.deform_psroi_pooling_backward(grad_output, data,
            rois, offset, output_count, grad_input, grad_offset, ctx.
            no_trans, ctx.spatial_scale, ctx.out_channels, ctx.group_size,
            ctx.out_size, ctx.part_size, ctx.sample_per_part, ctx.trans_std)
        return (grad_input, grad_rois, grad_offset, None, None, None, None,
            None, None, None, None)


deform_roi_pooling = DeformRoIPoolingFunction.apply


class DeformRoIPooling(nn.Module):

    def __init__(self, spatial_scale, out_size, out_channels, no_trans,
        group_size=1, part_size=None, sample_per_part=4, trans_std=0.0):
        super(DeformRoIPooling, self).__init__()
        self.spatial_scale = spatial_scale
        self.out_size = _pair(out_size)
        self.out_channels = out_channels
        self.no_trans = no_trans
        self.group_size = group_size
        self.part_size = out_size if part_size is None else part_size
        self.sample_per_part = sample_per_part
        self.trans_std = trans_std

    def forward(self, data, rois, offset):
        if self.no_trans:
            offset = data.new_empty(0)
        return deform_roi_pooling(data, rois, offset, self.spatial_scale,
            self.out_size, self.out_channels, self.no_trans, self.
            group_size, self.part_size, self.sample_per_part, self.trans_std)


class GeneralizedAttention(nn.Module):
    """GeneralizedAttention module.

    See 'An Empirical Study of Spatial Attention Mechanisms in Deep Networks'
    (https://arxiv.org/abs/1711.07971) for details.

    Args:
        in_channels (int): Channels of the input feature map.
        spatial_range (int): The spatial range.
            -1 indicates no spatial range constraint.
        num_heads (int): The head number of empirical_attention module.
        position_embedding_dim (int): The position embedding dimension.
        position_magnitude (int): A multiplier acting on coord difference.
        kv_stride (int): The feature stride acting on key/value feature map.
        q_stride (int): The feature stride acting on query feature map.
        attention_type (str): A binary indicator string for indicating which
            items in generalized empirical_attention module are used.
            '1000' indicates 'query and key content' (appr - appr) item,
            '0100' indicates 'query content and relative position'
              (appr - position) item,
            '0010' indicates 'key content only' (bias - appr) item,
            '0001' indicates 'relative position only' (bias - position) item.
    """

    def __init__(self, in_channels, spatial_range=-1, num_heads=9,
        position_embedding_dim=-1, position_magnitude=1, kv_stride=2,
        q_stride=1, attention_type='1111'):
        super(GeneralizedAttention, self).__init__()
        self.position_embedding_dim = (position_embedding_dim if 
            position_embedding_dim > 0 else in_channels)
        self.position_magnitude = position_magnitude
        self.num_heads = num_heads
        self.in_channels = in_channels
        self.spatial_range = spatial_range
        self.kv_stride = kv_stride
        self.q_stride = q_stride
        self.attention_type = [bool(int(_)) for _ in attention_type]
        self.qk_embed_dim = in_channels // num_heads
        out_c = self.qk_embed_dim * num_heads
        if self.attention_type[0] or self.attention_type[1]:
            self.query_conv = nn.Conv2d(in_channels=in_channels,
                out_channels=out_c, kernel_size=1, bias=False)
            self.query_conv.kaiming_init = True
        if self.attention_type[0] or self.attention_type[2]:
            self.key_conv = nn.Conv2d(in_channels=in_channels, out_channels
                =out_c, kernel_size=1, bias=False)
            self.key_conv.kaiming_init = True
        self.v_dim = in_channels // num_heads
        self.value_conv = nn.Conv2d(in_channels=in_channels, out_channels=
            self.v_dim * num_heads, kernel_size=1, bias=False)
        self.value_conv.kaiming_init = True
        if self.attention_type[1] or self.attention_type[3]:
            self.appr_geom_fc_x = nn.Linear(self.position_embedding_dim // 
                2, out_c, bias=False)
            self.appr_geom_fc_x.kaiming_init = True
            self.appr_geom_fc_y = nn.Linear(self.position_embedding_dim // 
                2, out_c, bias=False)
            self.appr_geom_fc_y.kaiming_init = True
        if self.attention_type[2]:
            stdv = 1.0 / math.sqrt(self.qk_embed_dim * 2)
            appr_bias_value = -2 * stdv * torch.rand(out_c) + stdv
            self.appr_bias = nn.Parameter(appr_bias_value)
        if self.attention_type[3]:
            stdv = 1.0 / math.sqrt(self.qk_embed_dim * 2)
            geom_bias_value = -2 * stdv * torch.rand(out_c) + stdv
            self.geom_bias = nn.Parameter(geom_bias_value)
        self.proj_conv = nn.Conv2d(in_channels=self.v_dim * num_heads,
            out_channels=in_channels, kernel_size=1, bias=True)
        self.proj_conv.kaiming_init = True
        self.gamma = nn.Parameter(torch.zeros(1))
        if self.spatial_range >= 0:
            if in_channels == 256:
                max_len = 84
            elif in_channels == 512:
                max_len = 42
            max_len_kv = int((max_len - 1.0) / self.kv_stride + 1)
            local_constraint_map = np.ones((max_len, max_len, max_len_kv,
                max_len_kv), dtype=np.int)
            for iy in range(max_len):
                for ix in range(max_len):
                    local_constraint_map[(iy), (ix), max((iy - self.
                        spatial_range) // self.kv_stride, 0):min((iy + self
                        .spatial_range + 1) // self.kv_stride + 1, max_len),
                        max((ix - self.spatial_range) // self.kv_stride, 0)
                        :min((ix + self.spatial_range + 1) // self.
                        kv_stride + 1, max_len)] = 0
            self.local_constraint_map = nn.Parameter(torch.from_numpy(
                local_constraint_map).byte(), requires_grad=False)
        if self.q_stride > 1:
            self.q_downsample = nn.AvgPool2d(kernel_size=1, stride=self.
                q_stride)
        else:
            self.q_downsample = None
        if self.kv_stride > 1:
            self.kv_downsample = nn.AvgPool2d(kernel_size=1, stride=self.
                kv_stride)
        else:
            self.kv_downsample = None
        self.init_weights()

    def get_position_embedding(self, h, w, h_kv, w_kv, q_stride, kv_stride,
        device, feat_dim, wave_length=1000):
        h_idxs = torch.linspace(0, h - 1, h)
        h_idxs = h_idxs.view((h, 1)) * q_stride
        w_idxs = torch.linspace(0, w - 1, w)
        w_idxs = w_idxs.view((w, 1)) * q_stride
        h_kv_idxs = torch.linspace(0, h_kv - 1, h_kv)
        h_kv_idxs = h_kv_idxs.view((h_kv, 1)) * kv_stride
        w_kv_idxs = torch.linspace(0, w_kv - 1, w_kv)
        w_kv_idxs = w_kv_idxs.view((w_kv, 1)) * kv_stride
        h_diff = h_idxs.unsqueeze(1) - h_kv_idxs.unsqueeze(0)
        h_diff *= self.position_magnitude
        w_diff = w_idxs.unsqueeze(1) - w_kv_idxs.unsqueeze(0)
        w_diff *= self.position_magnitude
        feat_range = torch.arange(0, feat_dim / 4)
        dim_mat = torch.Tensor([wave_length])
        dim_mat = dim_mat ** (4.0 / feat_dim * feat_range)
        dim_mat = dim_mat.view((1, 1, -1))
        embedding_x = torch.cat(((w_diff / dim_mat).sin(), (w_diff /
            dim_mat).cos()), dim=2)
        embedding_y = torch.cat(((h_diff / dim_mat).sin(), (h_diff /
            dim_mat).cos()), dim=2)
        return embedding_x, embedding_y

    def forward(self, x_input):
        num_heads = self.num_heads
        if self.q_downsample is not None:
            x_q = self.q_downsample(x_input)
        else:
            x_q = x_input
        n, _, h, w = x_q.shape
        if self.kv_downsample is not None:
            x_kv = self.kv_downsample(x_input)
        else:
            x_kv = x_input
        _, _, h_kv, w_kv = x_kv.shape
        if self.attention_type[0] or self.attention_type[1]:
            proj_query = self.query_conv(x_q).view((n, num_heads, self.
                qk_embed_dim, h * w))
            proj_query = proj_query.permute(0, 1, 3, 2)
        if self.attention_type[0] or self.attention_type[2]:
            proj_key = self.key_conv(x_kv).view((n, num_heads, self.
                qk_embed_dim, h_kv * w_kv))
        if self.attention_type[1] or self.attention_type[3]:
            position_embed_x, position_embed_y = self.get_position_embedding(h,
                w, h_kv, w_kv, self.q_stride, self.kv_stride, x_input.
                device, self.position_embedding_dim)
            position_feat_x = self.appr_geom_fc_x(position_embed_x).view(1,
                w, w_kv, num_heads, self.qk_embed_dim).permute(0, 3, 1, 2, 4
                ).repeat(n, 1, 1, 1, 1)
            position_feat_y = self.appr_geom_fc_y(position_embed_y).view(1,
                h, h_kv, num_heads, self.qk_embed_dim).permute(0, 3, 1, 2, 4
                ).repeat(n, 1, 1, 1, 1)
            position_feat_x /= math.sqrt(2)
            position_feat_y /= math.sqrt(2)
        if np.sum(self.attention_type) == 1 and self.attention_type[2]:
            appr_bias = self.appr_bias.view(1, num_heads, 1, self.qk_embed_dim
                ).repeat(n, 1, 1, 1)
            energy = torch.matmul(appr_bias, proj_key).view(n, num_heads, 1,
                h_kv * w_kv)
            h = 1
            w = 1
        else:
            if not self.attention_type[0]:
                energy = torch.zeros(n, num_heads, h, w, h_kv, w_kv, dtype=
                    x_input.dtype, device=x_input.device)
            if self.attention_type[0] or self.attention_type[2]:
                if self.attention_type[0] and self.attention_type[2]:
                    appr_bias = self.appr_bias.view(1, num_heads, 1, self.
                        qk_embed_dim)
                    energy = torch.matmul(proj_query + appr_bias, proj_key
                        ).view(n, num_heads, h, w, h_kv, w_kv)
                elif self.attention_type[0]:
                    energy = torch.matmul(proj_query, proj_key).view(n,
                        num_heads, h, w, h_kv, w_kv)
                elif self.attention_type[2]:
                    appr_bias = self.appr_bias.view(1, num_heads, 1, self.
                        qk_embed_dim).repeat(n, 1, 1, 1)
                    energy += torch.matmul(appr_bias, proj_key).view(n,
                        num_heads, 1, 1, h_kv, w_kv)
            if self.attention_type[1] or self.attention_type[3]:
                if self.attention_type[1] and self.attention_type[3]:
                    geom_bias = self.geom_bias.view(1, num_heads, 1, self.
                        qk_embed_dim)
                    proj_query_reshape = (proj_query + geom_bias).view(n,
                        num_heads, h, w, self.qk_embed_dim)
                    energy_x = torch.matmul(proj_query_reshape.permute(0, 1,
                        3, 2, 4), position_feat_x.permute(0, 1, 2, 4, 3))
                    energy_x = energy_x.permute(0, 1, 3, 2, 4).unsqueeze(4)
                    energy_y = torch.matmul(proj_query_reshape,
                        position_feat_y.permute(0, 1, 2, 4, 3))
                    energy_y = energy_y.unsqueeze(5)
                    energy += energy_x + energy_y
                elif self.attention_type[1]:
                    proj_query_reshape = proj_query.view(n, num_heads, h, w,
                        self.qk_embed_dim)
                    proj_query_reshape = proj_query_reshape.permute(0, 1, 3,
                        2, 4)
                    position_feat_x_reshape = position_feat_x.permute(0, 1,
                        2, 4, 3)
                    position_feat_y_reshape = position_feat_y.permute(0, 1,
                        2, 4, 3)
                    energy_x = torch.matmul(proj_query_reshape,
                        position_feat_x_reshape)
                    energy_x = energy_x.permute(0, 1, 3, 2, 4).unsqueeze(4)
                    energy_y = torch.matmul(proj_query_reshape,
                        position_feat_y_reshape)
                    energy_y = energy_y.unsqueeze(5)
                    energy += energy_x + energy_y
                elif self.attention_type[3]:
                    geom_bias = self.geom_bias.view(1, num_heads, self.
                        qk_embed_dim, 1).repeat(n, 1, 1, 1)
                    position_feat_x_reshape = position_feat_x.view(n,
                        num_heads, w * w_kv, self.qk_embed_dim)
                    position_feat_y_reshape = position_feat_y.view(n,
                        num_heads, h * h_kv, self.qk_embed_dim)
                    energy_x = torch.matmul(position_feat_x_reshape, geom_bias)
                    energy_x = energy_x.view(n, num_heads, 1, w, 1, w_kv)
                    energy_y = torch.matmul(position_feat_y_reshape, geom_bias)
                    energy_y = energy_y.view(n, num_heads, h, 1, h_kv, 1)
                    energy += energy_x + energy_y
            energy = energy.view(n, num_heads, h * w, h_kv * w_kv)
        if self.spatial_range >= 0:
            cur_local_constraint_map = self.local_constraint_map[:h, :w, :
                h_kv, :w_kv].contiguous().view(1, 1, h * w, h_kv * w_kv)
            energy = energy.masked_fill_(cur_local_constraint_map, float(
                '-inf'))
        attention = F.softmax(energy, 3)
        proj_value = self.value_conv(x_kv)
        proj_value_reshape = proj_value.view((n, num_heads, self.v_dim, 
            h_kv * w_kv)).permute(0, 1, 3, 2)
        out = torch.matmul(attention, proj_value_reshape).permute(0, 1, 3, 2
            ).contiguous().view(n, self.v_dim * self.num_heads, h, w)
        out = self.proj_conv(out)
        out = self.gamma * out + x_input
        return out

    def init_weights(self):
        for m in self.modules():
            if hasattr(m, 'kaiming_init') and m.kaiming_init:
                kaiming_init(m, mode='fan_in', nonlinearity='leaky_relu',
                    bias=0, distribution='uniform', a=1)


class MaskedConv2dFunction(Function):

    @staticmethod
    def forward(ctx, features, mask, weight, bias, padding=0, stride=1):
        assert mask.dim() == 3 and mask.size(0) == 1
        assert features.dim() == 4 and features.size(0) == 1
        assert features.size()[2:] == mask.size()[1:]
        pad_h, pad_w = _pair(padding)
        stride_h, stride_w = _pair(stride)
        if stride_h != 1 or stride_w != 1:
            raise ValueError(
                'Stride could not only be 1 in masked_conv2d currently.')
        if not features.is_cuda:
            raise NotImplementedError
        out_channel, in_channel, kernel_h, kernel_w = weight.size()
        batch_size = features.size(0)
        out_h = int(math.floor((features.size(2) + 2 * pad_h - (kernel_h - 
            1) - 1) / stride_h + 1))
        out_w = int(math.floor((features.size(3) + 2 * pad_w - (kernel_h - 
            1) - 1) / stride_w + 1))
        mask_inds = torch.nonzero(mask[0] > 0, as_tuple=False)
        output = features.new_zeros(batch_size, out_channel, out_h, out_w)
        if mask_inds.numel() > 0:
            mask_h_idx = mask_inds[:, (0)].contiguous()
            mask_w_idx = mask_inds[:, (1)].contiguous()
            data_col = features.new_zeros(in_channel * kernel_h * kernel_w,
                mask_inds.size(0))
            masked_conv2d_ext.masked_im2col_forward(features, mask_h_idx,
                mask_w_idx, kernel_h, kernel_w, pad_h, pad_w, data_col)
            masked_output = torch.addmm(1, bias[:, (None)], 1, weight.view(
                out_channel, -1), data_col)
            masked_conv2d_ext.masked_col2im_forward(masked_output,
                mask_h_idx, mask_w_idx, out_h, out_w, out_channel, output)
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        return (None,) * 5


masked_conv2d = MaskedConv2dFunction.apply


class MaskedConv2d(nn.Conv2d):
    """A MaskedConv2d which inherits the official Conv2d.

    The masked forward doesn't implement the backward function and only
    supports the stride parameter to be 1 currently.
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
        padding=0, dilation=1, groups=1, bias=True):
        super(MaskedConv2d, self).__init__(in_channels, out_channels,
            kernel_size, stride, padding, dilation, groups, bias)

    def forward(self, input, mask=None):
        if mask is None:
            return super(MaskedConv2d, self).forward(input)
        else:
            return masked_conv2d(input, mask, self.weight, self.bias, self.
                padding)


class BaseMergeCell(nn.Module):
    """The basic class for cells used in NAS-FPN and NAS-FCOS.

    BaseMergeCell takes 2 inputs. After applying concolution
    on them, they are resized to the target size. Then,
    they go through binary_op, which depends on the type of cell.
    If with_out_conv is True, the result of output will go through
    another convolution layer.

    Args:
        in_channels (int): number of input channels in out_conv layer.
        out_channels (int): number of output channels in out_conv layer.
        with_out_conv (bool): Whether to use out_conv layer
        out_conv_cfg (dict): Config dict for convolution layer, which should
            contain "groups", "kernel_size", "padding", "bias" to build
            out_conv layer.
        out_norm_cfg (dict): Config dict for normalization layer in out_conv.
        out_conv_order (tuple): The order of conv/norm/activation layers in
            out_conv.
        with_input1_conv (bool): Whether to use convolution on input1.
        with_input2_conv (bool): Whether to use convolution on input2.
        input_conv_cfg (dict): Config dict for building input1_conv layer and
            input2_conv layer, which is expected to contain the type of
            convolution.
            Default: None, which means using conv2d.
        input_norm_cfg (dict): Config dict for normalization layer in
            input1_conv and input2_conv layer. Default: None.
        upsample_mode (str): Interpolation method used to resize the output
            of input1_conv and input2_conv to target size. Currently, we
            support ['nearest', 'bilinear']. Default: 'nearest'.

    """

    def __init__(self, fused_channels=256, out_channels=256, with_out_conv=
        True, out_conv_cfg=dict(groups=1, kernel_size=3, padding=1, bias=
        True), out_norm_cfg=None, out_conv_order=('act', 'conv', 'norm'),
        with_input1_conv=False, with_input2_conv=False, input_conv_cfg=None,
        input_norm_cfg=None, upsample_mode='nearest'):
        super(BaseMergeCell, self).__init__()
        assert upsample_mode in ['nearest', 'bilinear']
        self.with_out_conv = with_out_conv
        self.with_input1_conv = with_input1_conv
        self.with_input2_conv = with_input2_conv
        self.upsample_mode = upsample_mode
        if self.with_out_conv:
            self.out_conv = ConvModule(fused_channels, out_channels, **
                out_conv_cfg, norm_cfg=out_norm_cfg, order=out_conv_order)
        self.input1_conv = self._build_input_conv(out_channels,
            input_conv_cfg, input_norm_cfg
            ) if with_input1_conv else nn.Sequential()
        self.input2_conv = self._build_input_conv(out_channels,
            input_conv_cfg, input_norm_cfg
            ) if with_input2_conv else nn.Sequential()

    def _build_input_conv(self, channel, conv_cfg, norm_cfg):
        return ConvModule(channel, channel, 3, padding=1, conv_cfg=conv_cfg,
            norm_cfg=norm_cfg, bias=True)

    @abstractmethod
    def _binary_op(self, x1, x2):
        pass

    def _resize(self, x, size):
        if x.shape[-2:] == size:
            return x
        elif x.shape[-2:] < size:
            return F.interpolate(x, size=size, mode=self.upsample_mode)
        else:
            assert x.shape[-2] % size[-2] == 0 and x.shape[-1] % size[-1] == 0
            kernel_size = x.shape[-1] // size[-1]
            x = F.max_pool2d(x, kernel_size=kernel_size, stride=kernel_size)
            return x

    def forward(self, x1, x2, out_size=None):
        assert x1.shape[:2] == x2.shape[:2]
        assert out_size is None or len(out_size) == 2
        if out_size is None:
            out_size = max(x1.size()[2:], x2.size()[2:])
        x1 = self.input1_conv(x1)
        x2 = self.input2_conv(x2)
        x1 = self._resize(x1, out_size)
        x2 = self._resize(x2, out_size)
        x = self._binary_op(x1, x2)
        if self.with_out_conv:
            x = self.out_conv(x)
        return x


class NonLocal2D(nn.Module):
    """Non-local module.

    See https://arxiv.org/abs/1711.07971 for details.

    Args:
        in_channels (int): Channels of the input feature map.
        reduction (int): Channel reduction ratio.
        use_scale (bool): Whether to scale pairwise_weight by 1/inter_channels.
        conv_cfg (dict): The config dict for convolution layers.
            (only applicable to conv_out)
        norm_cfg (dict): The config dict for normalization layers.
            (only applicable to conv_out)
        mode (str): Options are `embedded_gaussian` and `dot_product`.
    """

    def __init__(self, in_channels, reduction=2, use_scale=True, conv_cfg=
        None, norm_cfg=None, mode='embedded_gaussian'):
        super(NonLocal2D, self).__init__()
        self.in_channels = in_channels
        self.reduction = reduction
        self.use_scale = use_scale
        self.inter_channels = in_channels // reduction
        self.mode = mode
        assert mode in ['embedded_gaussian', 'dot_product']
        self.g = ConvModule(self.in_channels, self.inter_channels,
            kernel_size=1, act_cfg=None)
        self.theta = ConvModule(self.in_channels, self.inter_channels,
            kernel_size=1, act_cfg=None)
        self.phi = ConvModule(self.in_channels, self.inter_channels,
            kernel_size=1, act_cfg=None)
        self.conv_out = ConvModule(self.inter_channels, self.in_channels,
            kernel_size=1, conv_cfg=conv_cfg, norm_cfg=norm_cfg, act_cfg=None)
        self.init_weights()

    def init_weights(self, std=0.01, zeros_init=True):
        for m in [self.g, self.theta, self.phi]:
            normal_init(m.conv, std=std)
        if zeros_init:
            constant_init(self.conv_out.conv, 0)
        else:
            normal_init(self.conv_out.conv, std=std)

    def embedded_gaussian(self, theta_x, phi_x):
        pairwise_weight = torch.matmul(theta_x, phi_x)
        if self.use_scale:
            pairwise_weight /= theta_x.shape[-1] ** 0.5
        pairwise_weight = pairwise_weight.softmax(dim=-1)
        return pairwise_weight

    def dot_product(self, theta_x, phi_x):
        pairwise_weight = torch.matmul(theta_x, phi_x)
        pairwise_weight /= pairwise_weight.shape[-1]
        return pairwise_weight

    def forward(self, x):
        n, _, h, w = x.shape
        g_x = self.g(x).view(n, self.inter_channels, -1)
        g_x = g_x.permute(0, 2, 1)
        theta_x = self.theta(x).view(n, self.inter_channels, -1)
        theta_x = theta_x.permute(0, 2, 1)
        phi_x = self.phi(x).view(n, self.inter_channels, -1)
        pairwise_func = getattr(self, self.mode)
        pairwise_weight = pairwise_func(theta_x, phi_x)
        y = torch.matmul(pairwise_weight, g_x)
        y = y.permute(0, 2, 1).reshape(n, self.inter_channels, h, w)
        output = x + self.conv_out(y)
        return output


class RoIAlignFunction(Function):

    @staticmethod
    def forward(ctx, features, rois, out_size, spatial_scale, sample_num=0,
        aligned=True):
        out_h, out_w = _pair(out_size)
        assert isinstance(out_h, int) and isinstance(out_w, int)
        ctx.spatial_scale = spatial_scale
        ctx.sample_num = sample_num
        ctx.save_for_backward(rois)
        ctx.feature_size = features.size()
        ctx.aligned = aligned
        if aligned:
            output = roi_align_ext.forward_v2(features, rois, spatial_scale,
                out_h, out_w, sample_num, aligned)
        elif features.is_cuda:
            batch_size, num_channels, data_height, data_width = features.size()
            num_rois = rois.size(0)
            output = features.new_zeros(num_rois, num_channels, out_h, out_w)
            roi_align_ext.forward_v1(features, rois, out_h, out_w,
                spatial_scale, sample_num, output)
        else:
            raise NotImplementedError
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        feature_size = ctx.feature_size
        spatial_scale = ctx.spatial_scale
        sample_num = ctx.sample_num
        rois = ctx.saved_tensors[0]
        aligned = ctx.aligned
        assert feature_size is not None
        batch_size, num_channels, data_height, data_width = feature_size
        out_w = grad_output.size(3)
        out_h = grad_output.size(2)
        grad_input = grad_rois = None
        if not aligned:
            if ctx.needs_input_grad[0]:
                grad_input = rois.new_zeros(batch_size, num_channels,
                    data_height, data_width)
                roi_align_ext.backward_v1(grad_output.contiguous(), rois,
                    out_h, out_w, spatial_scale, sample_num, grad_input)
        else:
            grad_input = roi_align_ext.backward_v2(grad_output, rois,
                spatial_scale, out_h, out_w, batch_size, num_channels,
                data_height, data_width, sample_num, aligned)
        return grad_input, grad_rois, None, None, None, None


roi_align = RoIAlignFunction.apply


class RoIPoolFunction(Function):

    @staticmethod
    def forward(ctx, features, rois, out_size, spatial_scale):
        assert features.is_cuda
        out_h, out_w = _pair(out_size)
        assert isinstance(out_h, int) and isinstance(out_w, int)
        ctx.save_for_backward(rois)
        num_channels = features.size(1)
        num_rois = rois.size(0)
        out_size = num_rois, num_channels, out_h, out_w
        output = features.new_zeros(out_size)
        argmax = features.new_zeros(out_size, dtype=torch.int)
        roi_pool_ext.forward(features, rois, out_h, out_w, spatial_scale,
            output, argmax)
        ctx.spatial_scale = spatial_scale
        ctx.feature_size = features.size()
        ctx.argmax = argmax
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        assert grad_output.is_cuda
        spatial_scale = ctx.spatial_scale
        feature_size = ctx.feature_size
        argmax = ctx.argmax
        rois = ctx.saved_tensors[0]
        assert feature_size is not None
        grad_input = grad_rois = None
        if ctx.needs_input_grad[0]:
            grad_input = grad_output.new_zeros(feature_size)
            roi_pool_ext.backward(grad_output.contiguous(), rois, argmax,
                spatial_scale, grad_input)
        return grad_input, grad_rois, None, None


roi_pool = RoIPoolFunction.apply


class SigmoidFocalLoss(nn.Module):

    def __init__(self, gamma, alpha):
        super(SigmoidFocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, logits, targets):
        assert logits.is_cuda
        loss = sigmoid_focal_loss(logits, targets, self.gamma, self.alpha)
        return loss.sum()

    def __repr__(self):
        tmpstr = self.__class__.__name__
        tmpstr += f'(gamma={self.gamma}, alpha={self.alpha})'
        return tmpstr


class NewEmptyTensorOp(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, new_shape):
        ctx.shape = x.shape
        return x.new_empty(new_shape)

    @staticmethod
    def backward(ctx, grad):
        shape = ctx.shape
        return NewEmptyTensorOp.apply(grad, shape), None


class ConvTranspose2d(nn.ConvTranspose2d):

    def forward(self, x):
        if x.numel() == 0 and torch.__version__ <= '1.4.0':
            out_shape = [x.shape[0], self.out_channels]
            for i, k, p, s, d, op in zip(x.shape[-2:], self.kernel_size,
                self.padding, self.stride, self.dilation, self.output_padding):
                out_shape.append((i - 1) * s - 2 * p + (d * (k - 1) + 1) + op)
            empty = NewEmptyTensorOp.apply(x, out_shape)
            if self.training:
                dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0
                return empty + dummy
            else:
                return empty
        return super(ConvTranspose2d, self).forward(x)


class MaxPool2d(nn.MaxPool2d):

    def forward(self, x):
        if x.numel() == 0 and torch.__version__ <= '1.4':
            out_shape = list(x.shape[:2])
            for i, k, p, s, d in zip(x.shape[-2:], _pair(self.kernel_size),
                _pair(self.padding), _pair(self.stride), _pair(self.dilation)):
                o = (i + 2 * p - (d * (k - 1) + 1)) / s + 1
                o = math.ceil(o) if self.ceil_mode else math.floor(o)
                out_shape.append(o)
            empty = NewEmptyTensorOp.apply(x, out_shape)
            return empty
        return super().forward(x)


class Linear(torch.nn.Linear):

    def forward(self, x):
        if x.numel() == 0:
            out_shape = [x.shape[0], self.out_features]
            empty = NewEmptyTensorOp.apply(x, out_shape)
            if self.training:
                dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0
                return empty + dummy
            else:
                return empty
        return super().forward(x)


class SubModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(2, 2, kernel_size=1, groups=2)
        self.gn = nn.GroupNorm(2, 2)
        self.param1 = nn.Parameter(torch.ones(1))

    def forward(self, x):
        return x


class ExampleModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.param1 = nn.Parameter(torch.ones(1))
        self.conv1 = nn.Conv2d(3, 4, kernel_size=1, bias=False)
        self.conv2 = nn.Conv2d(4, 2, kernel_size=1)
        self.bn = nn.BatchNorm2d(2)
        self.sub = SubModel()

    def forward(self, x):
        return x


class ExampleDuplicateModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.param1 = nn.Parameter(torch.ones(1))
        self.conv1 = nn.Sequential(nn.Conv2d(3, 4, kernel_size=1, bias=False))
        self.conv2 = nn.Sequential(nn.Conv2d(4, 2, kernel_size=1))
        self.bn = nn.BatchNorm2d(2)
        self.sub = SubModel()
        self.conv3 = nn.Sequential(nn.Conv2d(3, 4, kernel_size=1, bias=False))
        self.conv3[0] = self.conv1[0]

    def forward(self, x):
        return x


class PseudoDataParallel(nn.Module):

    def __init__(self):
        super().__init__()
        self.module = ExampleModel()

    def forward(self, x):
        return x


import torch
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile

class Test_open_mmlab_mmdetection(_paritybench_base):
    pass
    @_fails_compile()
    def test_000(self):
        self._check(ConvTranspose2d(*[], **{'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_001(self):
        self._check(ExampleDuplicateModel(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

    def test_002(self):
        self._check(ExampleModel(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

    def test_003(self):
        self._check(L2Norm(*[], **{'n_dims': 4}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_004(self):
        self._check(Linear(*[], **{'in_features': 4, 'out_features': 4}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_005(self):
        self._check(MaskedConv2d(*[], **{'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_006(self):
        self._check(MaxPool2d(*[], **{'kernel_size': 4}), [torch.rand([4, 4, 4, 4])], {})

    def test_007(self):
        self._check(PseudoDataParallel(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

    def test_008(self):
        self._check(SubModel(*[], **{}), [torch.rand([4, 4, 4, 4])], {})

