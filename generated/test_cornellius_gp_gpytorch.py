import sys
_module = sys.modules[__name__]
del sys
conf = _module
densenet = _module
LBFGS = _module
gpytorch = _module
beta_features = _module
constraints = _module
constraints = _module
distributions = _module
delta = _module
distribution = _module
multitask_multivariate_normal = _module
multivariate_normal = _module
functions = _module
_dsmm = _module
_inv_matmul = _module
_inv_quad = _module
_inv_quad_log_det = _module
_log_normal_cdf = _module
_matmul = _module
_root_decomposition = _module
matern_covariance = _module
rbf_covariance = _module
kernels = _module
additive_structure_kernel = _module
arc_kernel = _module
cosine_kernel = _module
cylindrical_kernel = _module
grid_interpolation_kernel = _module
grid_kernel = _module
index_kernel = _module
inducing_point_kernel = _module
keops = _module
keops_kernel = _module
matern_kernel = _module
rbf_kernel = _module
kernel = _module
lcm_kernel = _module
linear_kernel = _module
multi_device_kernel = _module
multitask_kernel = _module
newton_girard_additive_kernel = _module
periodic_kernel = _module
polynomial_kernel = _module
polynomial_kernel_grad = _module
product_structure_kernel = _module
rbf_kernel_grad = _module
rff_kernel = _module
rq_kernel = _module
scale_kernel = _module
spectral_mixture_kernel = _module
lazy = _module
added_diag_lazy_tensor = _module
batch_repeat_lazy_tensor = _module
block_diag_lazy_tensor = _module
block_interleaved_lazy_tensor = _module
block_lazy_tensor = _module
cached_cg_lazy_tensor = _module
cat_lazy_tensor = _module
chol_lazy_tensor = _module
constant_mul_lazy_tensor = _module
diag_lazy_tensor = _module
interpolated_lazy_tensor = _module
keops_lazy_tensor = _module
kronecker_product_lazy_tensor = _module
lazy_evaluated_kernel_tensor = _module
lazy_tensor = _module
lazy_tensor_representation_tree = _module
matmul_lazy_tensor = _module
mul_lazy_tensor = _module
non_lazy_tensor = _module
psd_sum_lazy_tensor = _module
root_lazy_tensor = _module
sum_batch_lazy_tensor = _module
sum_lazy_tensor = _module
toeplitz_lazy_tensor = _module
zero_lazy_tensor = _module
likelihoods = _module
bernoulli_likelihood = _module
gaussian_likelihood = _module
likelihood = _module
likelihood_list = _module
multitask_gaussian_likelihood = _module
noise_models = _module
softmax_likelihood = _module
means = _module
constant_mean = _module
constant_mean_grad = _module
linear_mean = _module
mean = _module
multitask_mean = _module
zero_mean = _module
mlls = _module
_approximate_mll = _module
added_loss_term = _module
deep_approximate_mll = _module
exact_marginal_log_likelihood = _module
gamma_robust_variational_elbo = _module
inducing_point_kernel_added_loss_term = _module
marginal_log_likelihood = _module
noise_model_added_loss_term = _module
predictive_log_likelihood = _module
sum_marginal_log_likelihood = _module
variational_elbo = _module
models = _module
approximate_gp = _module
deep_gps = _module
deep_gp = _module
exact_gp = _module
exact_prediction_strategies = _module
gp = _module
model_list = _module
pyro = _module
_pyro_mixin = _module
pyro_gp = _module
module = _module
priors = _module
horseshoe_prior = _module
lkj_prior = _module
prior = _module
smoothed_box_prior = _module
torch_priors = _module
utils = _module
wishart_prior = _module
settings = _module
test = _module
base_kernel_test_case = _module
base_likelihood_test_case = _module
base_mean_test_case = _module
base_test_case = _module
lazy_tensor_test_case = _module
model_test_case = _module
variational_test_case = _module
broadcasting = _module
cholesky = _module
deprecation = _module
errors = _module
fft = _module
getitem = _module
grid = _module
interpolation = _module
lanczos = _module
linear_cg = _module
memoize = _module
pivoted_cholesky = _module
quadrature = _module
sparse = _module
stochastic_lq = _module
toeplitz = _module
transforms = _module
warnings = _module
variational = _module
_variational_distribution = _module
_variational_strategy = _module
additive_grid_interpolation_variational_strategy = _module
cholesky_variational_distribution = _module
delta_variational_distribution = _module
grid_interpolation_variational_strategy = _module
mean_field_variational_distribution = _module
multitask_variational_strategy = _module
orthogonally_decoupled_variational_strategy = _module
unwhitened_variational_strategy = _module
variational_strategy = _module
whitened_variational_strategy = _module
setup = _module
test_constraints = _module
test_delta = _module
test_multitask_multivariate_normal = _module
test_multivariate_normal = _module
examples = _module
test_batch_gp_regression = _module
test_batch_multitask_gp_regression = _module
test_batch_svgp_gp_regression = _module
test_decoupled_svgp_regression = _module
test_fixed_noise_fanatasy_updates = _module
test_grid_gp_regression = _module
test_hadamard_multitask_gp_regression = _module
test_independent_multitask_gp_regression = _module
test_kissgp_additive_classification = _module
test_kissgp_additive_regression = _module
test_kissgp_dkl_regression = _module
test_kissgp_gp_classification = _module
test_kissgp_gp_regression = _module
test_kissgp_kronecker_product_classification = _module
test_kissgp_kronecker_product_regression = _module
test_kissgp_multiplicative_regression = _module
test_kissgp_variational_regression = _module
test_kissgp_white_noise_regression = _module
test_kronecker_multitask_gp_regression = _module
test_kronecker_multitask_ski_gp_regression = _module
test_lcm_kernel_regression = _module
test_model_list_gp_regression = _module
test_pyro_integration = _module
test_sgpr_regression = _module
test_simple_gp_classification = _module
test_simple_gp_regression = _module
test_spectral_mixture_gp_regression = _module
test_svgp_gp_classification = _module
test_svgp_gp_regression = _module
test_unwhitened_svgp_regression = _module
test_white_noise_regression = _module
test_dsmm = _module
test_inv_matmul = _module
test_inv_quad = _module
test_inv_quad_log_det = _module
test_log_normal_cdf = _module
test_matern_covariance = _module
test_matmul = _module
test_rbf_covariance = _module
test_root_decomposition = _module
test_matern_kernel = _module
test_rbf_kernel = _module
test_additive_kernel = _module
test_arc_kernel = _module
test_cosine_kernel = _module
test_cylindrical_kernel = _module
test_grid_interpolation_kernel = _module
test_grid_kernel = _module
test_linear_kernel = _module
test_newton_girard_additive_kernel = _module
test_periodic_kernel = _module
test_polynomial_kernel = _module
test_polynomial_kernel_grad = _module
test_rbf_kernel = _module
test_rbf_kernel_grad = _module
test_rff_kernel = _module
test_rq_kernel = _module
test_scale_kernel = _module
test_spectral_mixture_kernel = _module
test_added_diag_lazy_tensor = _module
test_batch_repeat_lazy_tensor = _module
test_block_diag_lazy_tensor = _module
test_block_interleaved_lazy_tensor = _module
test_cached_cg_lazy_tensor = _module
test_cat_lazy_tensor = _module
test_chol_lazy_tensor = _module
test_constant_mul_lazy_tensor = _module
test_diag_lazy_tensor = _module
test_interpolated_lazy_tensor = _module
test_kronecker_product_lazy_tensor = _module
test_lazy_evaluated_kernel_tensor = _module
test_matmul_lazy_tensor = _module
test_mul_lazy_tensor = _module
test_non_lazy_tensor = _module
test_psd_sum_lazy_tensor = _module
test_root_lazy_tensor = _module
test_sum_batch_lazy_tensor = _module
test_sum_lazy_tensor = _module
test_toeplitz_lazy_tensor = _module
test_zero_lazy_tensor = _module
test_bernoulli_likelihood = _module
test_gaussian_likelihood = _module
test_general_multitask_gaussian_likelihood = _module
test_multitask_gaussian_likelihood = _module
test_softmax_likelihood = _module
test_constant_mean = _module
test_constant_mean_grad = _module
test_linear_mean = _module
test_multitask_mean = _module
test_zero_mean = _module
test_exact_gp = _module
test_model_list = _module
test_variational_gp = _module
test_gamma_prior = _module
test_horseshoe_prior = _module
test_lkj_prior = _module
test_multivariate_normal_prior = _module
test_normal_prior = _module
test_smoothed_box_prior = _module
test_cholesky = _module
test_fft = _module
test_getitem = _module
test_grid = _module
test_interpolation = _module
test_lanczos = _module
test_linear_cg = _module
test_pivoted_cholesky = _module
test_quadrature = _module
test_sparse = _module
test_toeplitz = _module
test_grid_interpolation_variational_strategy = _module
test_multitask_variational_strategy = _module
test_orthogonally_decoupled_variational_strategy = _module
test_unwhitened_variational_strategy = _module
test_variational_strategy = _module
test_whitened_variational_strategy = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import re, math, string, numpy, torch, torchtext, torchaudio, logging, itertools, numbers, inspect, functools, copy, scipy, types, time, torchvision, enum, random, typing, warnings, abc, collections, uuid
import numpy as np
patch_functional()
open = mock_open()
logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import re


import torch


import torch.nn as nn


import torch.nn.functional as F


from collections import OrderedDict


import math


from torch import sigmoid


from torch.nn import Module


from torch.nn.functional import softplus


from math import pi


from typing import Optional


import copy


import warnings


from abc import abstractmethod


from copy import deepcopy


from torch.nn import ModuleList


from torch.nn.parallel import DataParallel


import logging


from abc import ABC


from typing import Any


from torch import Tensor


from torch.nn import Parameter


from abc import abstractproperty


import itertools


from torch import nn


from torch.distributions import Distribution


from numbers import Number


from torch.distributions import HalfCauchy


from torch.distributions import Normal


from torch.distributions import constraints


from torch.nn import Module as TModule


from torch.distributions.utils import broadcast_all


from torch.distributions import Gamma


from torch.distributions import LogNormal


from torch.distributions import MultivariateNormal


from torch.distributions import Uniform


import numpy as np


import random


from torch import optim


import time


class _DenseLayer(nn.Sequential):

    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super(_DenseLayer, self).__init__()
        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),
        self.add_module('relu1', nn.ReLU(inplace=True)),
        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *
            growth_rate, kernel_size=1, stride=1, bias=False)),
        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),
        self.add_module('relu2', nn.ReLU(inplace=True)),
        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate,
            growth_rate, kernel_size=3, stride=1, padding=1, bias=False)),
        self.drop_rate = drop_rate

    def forward(self, x):
        new_features = super(_DenseLayer, self).forward(x)
        if self.drop_rate > 0:
            new_features = F.dropout(new_features, p=self.drop_rate,
                training=self.training)
        return torch.cat([x, new_features], 1)


class _Transition(nn.Sequential):

    def __init__(self, num_input_features, num_output_features):
        super(_Transition, self).__init__()
        self.add_module('norm', nn.BatchNorm2d(num_input_features))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv', nn.Conv2d(num_input_features,
            num_output_features, kernel_size=1, stride=1, bias=False))
        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))


class _DenseBlock(nn.Sequential):

    def __init__(self, num_layers, num_input_features, bn_size, growth_rate,
        drop_rate):
        super(_DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = _DenseLayer(num_input_features + i * growth_rate,
                growth_rate, bn_size, drop_rate)
            self.add_module('denselayer%d' % (i + 1), layer)


class DenseNet(nn.Module):
    """Densenet-BC model class, based on
    `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
    Args:
        growth_rate (int) - how many filters to add each layer (`k` in paper)
        block_config (list of 3 or 4 ints) - how many layers in each pooling block
        num_init_features (int) - the number of filters to learn in the first convolution layer
        bn_size (int) - multiplicative factor for number of bottle neck layers
            (i.e. bn_size * k features in the bottleneck layer)
        drop_rate (float) - dropout rate after each dense layer
        num_classes (int) - number of classification classes
    """

    def __init__(self, growth_rate=12, block_config=(16, 16, 16),
        compression=0.5, num_init_features=24, bn_size=4, drop_rate=0,
        avgpool_size=8, num_classes=10):
        super(DenseNet, self).__init__()
        assert 0 < compression <= 1, 'compression of densenet should be between 0 and 1'
        self.avgpool_size = avgpool_size
        self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(3,
            num_init_features, kernel_size=3, stride=1, padding=1, bias=
            False))]))
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers=num_layers, num_input_features=
                num_features, bn_size=bn_size, growth_rate=growth_rate,
                drop_rate=drop_rate)
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = _Transition(num_input_features=num_features,
                    num_output_features=int(num_features * compression))
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = int(num_features * compression)
        self.features.add_module('norm_final', nn.BatchNorm2d(num_features))
        self.classifier = nn.Linear(num_features, num_classes)

    def forward(self, x):
        features = self.features(x)
        out = F.relu(features, inplace=True)
        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(features
            .size(0), -1)
        out = self.classifier(out)
        return out


def inv_sigmoid(x):
    return torch.log(x) - torch.log(1 - x)


def inv_softplus(x):
    return x + torch.log(-torch.expm1(-x))


TRANSFORM_REGISTRY = {torch.exp: torch.log, torch.nn.functional.softplus:
    inv_softplus, torch.sigmoid: inv_sigmoid}


def _get_inv_param_transform(param_transform, inv_param_transform=None):
    reg_inv_tf = TRANSFORM_REGISTRY.get(param_transform, None)
    if reg_inv_tf is None:
        if inv_param_transform is None:
            raise RuntimeError(
                'Must specify inv_param_transform for custom param_transforms')
        return inv_param_transform
    elif inv_param_transform is not None and reg_inv_tf != inv_param_transform:
        raise RuntimeError('TODO')
    return reg_inv_tf


class Interval(Module):

    def __init__(self, lower_bound, upper_bound, transform=sigmoid,
        inv_transform=inv_sigmoid, initial_value=None):
        """
        Defines an interval constraint for GP model parameters, specified by a lower bound and upper bound. For usage
        details, see the documentation for :meth:`~gpytorch.module.Module.register_constraint`.

        Args:
            lower_bound (float or torch.Tensor): The lower bound on the parameter.
            upper_bound (float or torch.Tensor): The upper bound on the parameter.
        """
        lower_bound = torch.as_tensor(lower_bound)
        upper_bound = torch.as_tensor(upper_bound)
        if torch.any(torch.ge(lower_bound, upper_bound)):
            raise RuntimeError('Got parameter bounds with empty intervals.')
        super().__init__()
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self._transform = transform
        self._inv_transform = inv_transform
        self._initial_value = initial_value
        if transform is not None and inv_transform is None:
            self._inv_transform = _get_inv_param_transform(transform)

    def _apply(self, fn):
        self.lower_bound = fn(self.lower_bound)
        self.upper_bound = fn(self.upper_bound)
        return super()._apply(fn)

    @property
    def enforced(self):
        return self._transform is not None

    def check(self, tensor):
        return bool(torch.all(tensor <= self.upper_bound) and torch.all(
            tensor >= self.lower_bound))

    def check_raw(self, tensor):
        return bool(torch.all(self.transform(tensor) <= self.upper_bound) and
            torch.all(self.transform(tensor) >= self.lower_bound))

    def intersect(self, other):
        """
        Returns a new Interval constraint that is the intersection of this one and another specified one.

        Args:
            other (Interval): Interval constraint to intersect with

        Returns:
            Interval: intersection if this interval with the other one.
        """
        if self.transform != other.transform:
            raise RuntimeError(
                'Cant intersect Interval constraints with conflicting transforms!'
                )
        lower_bound = torch.max(self.lower_bound, other.lower_bound)
        upper_bound = torch.min(self.upper_bound, other.upper_bound)
        return Interval(lower_bound, upper_bound)

    def transform(self, tensor):
        """
        Transforms a tensor to satisfy the specified bounds.

        If upper_bound is finite, we assume that `self.transform` saturates at 1 as tensor -> infinity. Similarly,
        if lower_bound is finite, we assume that `self.transform` saturates at 0 as tensor -> -infinity.

        Example transforms for one of the bounds being finite include torch.exp and torch.nn.functional.softplus.
        An example transform for the case where both are finite is torch.nn.functional.sigmoid.
        """
        if not self.enforced:
            return tensor
        if settings.debug.on():
            max_bound = torch.max(self.upper_bound)
            min_bound = torch.min(self.lower_bound)
            if max_bound == math.inf or min_bound == -math.inf:
                raise RuntimeError(
                    'Cannot make an Interval directly with non-finite bounds. Use a derived class like GreaterThan or LessThan instead.'
                    )
        transformed_tensor = self._transform(tensor) * (self.upper_bound -
            self.lower_bound) + self.lower_bound
        return transformed_tensor

    def inverse_transform(self, transformed_tensor):
        """
        Applies the inverse transformation.
        """
        if not self.enforced:
            return transformed_tensor
        if settings.debug.on():
            max_bound = torch.max(self.upper_bound)
            min_bound = torch.min(self.lower_bound)
            if max_bound == math.inf or min_bound == -math.inf:
                raise RuntimeError(
                    'Cannot make an Interval directly with non-finite bounds. Use a derived class like GreaterThan or LessThan instead.'
                    )
        tensor = self._inv_transform((transformed_tensor - self.lower_bound
            ) / (self.upper_bound - self.lower_bound))
        return tensor

    @property
    def initial_value(self):
        """
        The initial parameter value (if specified, None otherwise)
        """
        return self._initial_value

    def __repr__(self):
        if self.lower_bound.numel() == 1 and self.upper_bound.numel() == 1:
            return self._get_name(
                ) + f'({self.lower_bound:.3E}, {self.upper_bound:.3E})'
        else:
            return super().__repr__()

    def __iter__(self):
        yield self.lower_bound
        yield self.upper_bound


def default_postprocess_script(x):
    return x


class Distance(torch.nn.Module):

    def __init__(self, postprocess_script=default_postprocess_script):
        super().__init__()
        self._postprocess = postprocess_script

    def _sq_dist(self, x1, x2, postprocess, x1_eq_x2=False):
        adjustment = x1.mean(-2, keepdim=True)
        x1 = x1 - adjustment
        x2 = x2 - adjustment
        x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)
        x1_pad = torch.ones_like(x1_norm)
        if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:
            x2_norm, x2_pad = x1_norm, x1_pad
        else:
            x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)
            x2_pad = torch.ones_like(x2_norm)
        x1_ = torch.cat([-2.0 * x1, x1_norm, x1_pad], dim=-1)
        x2_ = torch.cat([x2, x2_pad, x2_norm], dim=-1)
        res = x1_.matmul(x2_.transpose(-2, -1))
        if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:
            res.diagonal(dim1=-2, dim2=-1).fill_(0)
        res.clamp_min_(0)
        return self._postprocess(res) if postprocess else res

    def _dist(self, x1, x2, postprocess, x1_eq_x2=False):
        res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)
        res = res.clamp_min_(1e-30).sqrt_()
        return self._postprocess(res) if postprocess else res


def _solve(lazy_tsr, rhs):
    if settings.fast_computations.solves.off() or lazy_tsr.size(-1
        ) <= settings.max_cholesky_size.value():
        return lazy_tsr._cholesky()._cholesky_solve(rhs)
    else:
        with torch.no_grad():
            preconditioner = lazy_tsr.detach()._inv_matmul_preconditioner()
        return lazy_tsr._solve(rhs, preconditioner)


class InvMatmul(Function):

    @staticmethod
    def forward(ctx, representation_tree, has_left, *args):
        left_tensor = None
        right_tensor = None
        matrix_args = None
        ctx.representation_tree = representation_tree
        ctx.has_left = has_left
        if ctx.has_left:
            left_tensor, right_tensor, *matrix_args = args
        else:
            right_tensor, *matrix_args = args
        orig_right_tensor = right_tensor
        lazy_tsr = ctx.representation_tree(*matrix_args)
        ctx.is_vector = False
        if right_tensor.ndimension() == 1:
            right_tensor = right_tensor.unsqueeze(-1)
            ctx.is_vector = True
        if ctx.has_left:
            rhs = torch.cat([left_tensor.transpose(-1, -2), right_tensor], -1)
            solves = _solve(lazy_tsr, rhs)
            res = solves[(...), left_tensor.size(-2):]
            res = left_tensor @ res
        else:
            solves = _solve(lazy_tsr, right_tensor)
            res = solves
        if ctx.is_vector:
            res = res.squeeze(-1)
        if ctx.has_left:
            args = [solves, left_tensor, orig_right_tensor] + list(matrix_args)
        else:
            args = [solves, orig_right_tensor] + list(matrix_args)
        ctx.save_for_backward(*args)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return res

    @staticmethod
    def backward(ctx, grad_output):
        if ctx.has_left:
            solves, left_tensor, right_tensor, *matrix_args = ctx.saved_tensors
            left_solves = solves[(...), :left_tensor.size(-2)]
            right_solves = solves[(...), left_tensor.size(-2):]
        else:
            right_solves, right_tensor, *matrix_args = ctx.saved_tensors
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        arg_grads = [None] * len(matrix_args)
        left_grad = None
        right_grad = None
        if any(ctx.needs_input_grad):
            if ctx.is_vector:
                right_tensor = right_tensor.unsqueeze(-1)
                grad_output = grad_output.unsqueeze(-1)
            if not ctx.has_left:
                left_solves = InvMatmul.apply(ctx.representation_tree, 
                    False, grad_output, *matrix_args)
                if any(ctx.needs_input_grad[3:]):
                    arg_grads = lazy_tsr._quad_form_derivative(torch.cat([
                        left_solves, right_solves], -1), torch.cat([
                        right_solves, left_solves], -1).mul(-0.5))
                if ctx.needs_input_grad[2]:
                    right_grad = left_solves
                    if ctx.is_vector:
                        right_grad.squeeze_(-1)
                return tuple([None, None] + [right_grad] + list(arg_grads))
            else:
                left_solves = left_solves @ grad_output
                if ctx.needs_input_grad[3]:
                    left_grad = grad_output @ right_solves.transpose(-1, -2)
                if any(ctx.needs_input_grad[4:]):
                    arg_grads = lazy_tsr._quad_form_derivative(torch.cat([
                        left_solves, right_solves], -1), torch.cat([
                        right_solves, left_solves], -1).mul(-0.5))
                if ctx.needs_input_grad[2]:
                    right_grad = left_solves
                    if ctx.is_vector:
                        right_grad.squeeze_(-1)
                return tuple([None, None] + [left_grad, right_grad] + list(
                    arg_grads))


class InvQuad(Function):
    """
    Given a PSD matrix A (or a batch of PSD matrices A), this function computes b A^{-1} b
    where b is a vector or batch of vectors
    """

    @staticmethod
    def forward(ctx, representation_tree, *args):
        """
        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)
        If inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)
        - the RHS of the matrix solves.

        Returns:
        - (Scalar) The inverse quadratic form (or None, if inv_quad is False)
        - (Scalar) The log determinant (or None, if logdet is False)
        """
        inv_quad_rhs, *matrix_args = args
        ctx.representation_tree = representation_tree
        lazy_tsr = ctx.representation_tree(*matrix_args)
        ctx.is_vector = False
        if inv_quad_rhs.ndimension() == 1:
            inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)
            ctx.is_vector = True
        inv_quad_solves = _solve(lazy_tsr, inv_quad_rhs)
        inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)
        to_save = matrix_args + [inv_quad_solves]
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return inv_quad_term

    @staticmethod
    def backward(ctx, inv_quad_grad_output):
        *matrix_args, inv_quad_solves = ctx.saved_tensors
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)
        neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(
            inv_quad_grad_output).mul(-1)
        matrix_arg_grads = [None] * len(matrix_args)
        if any(ctx.needs_input_grad[2:]):
            left_factors = neg_inv_quad_solves_times_grad_out
            right_factors = inv_quad_solves
            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors,
                right_factors)
        if ctx.needs_input_grad[1]:
            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul(-2)
        else:
            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)
        if ctx.is_vector:
            inv_quad_rhs_grad.squeeze_(-1)
        res = tuple([None] + [inv_quad_rhs_grad] + list(matrix_arg_grads))
        return tuple(res)


def lanczos_tridiag(matmul_closure, max_iter, dtype, device, matrix_shape,
    batch_shape=torch.Size(), init_vecs=None, num_init_vecs=1, tol=1e-05):
    """
    """
    multiple_init_vecs = False
    if not callable(matmul_closure):
        raise RuntimeError(
            'matmul_closure should be a function callable object that multiples a (Lazy)Tensor by a vector. Got a {} instead.'
            .format(matmul_closure.__class__.__name__))
    if init_vecs is None:
        init_vecs = torch.randn(matrix_shape[-1], num_init_vecs, dtype=
            dtype, device=device)
        init_vecs = init_vecs.expand(*batch_shape, matrix_shape[-1],
            num_init_vecs)
    else:
        if settings.debug.on():
            if dtype != init_vecs.dtype:
                raise RuntimeError(
                    'Supplied dtype {} and init_vecs.dtype {} do not agree!'
                    .format(dtype, init_vecs.dtype))
            if device != init_vecs.device:
                raise RuntimeError(
                    'Supplied device {} and init_vecs.device {} do not agree!'
                    .format(device, init_vecs.device))
            if batch_shape != init_vecs.shape[:-2]:
                raise RuntimeError(
                    'batch_shape {} and init_vecs.shape {} do not agree!'.
                    format(batch_shape, init_vecs.shape))
            if matrix_shape[-1] != init_vecs.size(-2):
                raise RuntimeError(
                    'matrix_shape {} and init_vecs.shape {} do not agree!'.
                    format(matrix_shape, init_vecs.shape))
        num_init_vecs = init_vecs.size(-1)
    num_iter = min(max_iter, matrix_shape[-1])
    dim_dimension = -2
    q_mat = torch.zeros(num_iter, *batch_shape, matrix_shape[-1],
        num_init_vecs, dtype=dtype, device=device)
    t_mat = torch.zeros(num_iter, num_iter, *batch_shape, num_init_vecs,
        dtype=dtype, device=device)
    q_0_vec = init_vecs / torch.norm(init_vecs, 2, dim=dim_dimension
        ).unsqueeze(dim_dimension)
    q_mat[0].copy_(q_0_vec)
    r_vec = matmul_closure(q_0_vec)
    alpha_0 = q_0_vec.mul(r_vec).sum(dim_dimension)
    r_vec.sub_(alpha_0.unsqueeze(dim_dimension).mul(q_0_vec))
    beta_0 = torch.norm(r_vec, 2, dim=dim_dimension)
    t_mat[0, 0].copy_(alpha_0)
    t_mat[0, 1].copy_(beta_0)
    t_mat[1, 0].copy_(beta_0)
    q_mat[1].copy_(r_vec.div_(beta_0.unsqueeze(dim_dimension)))
    for k in range(1, num_iter):
        q_prev_vec = q_mat[k - 1]
        q_curr_vec = q_mat[k]
        beta_prev = t_mat[k, k - 1].unsqueeze(dim_dimension)
        r_vec = matmul_closure(q_curr_vec) - q_prev_vec.mul(beta_prev)
        alpha_curr = q_curr_vec.mul(r_vec).sum(dim_dimension, keepdim=True)
        t_mat[k, k].copy_(alpha_curr.squeeze(dim_dimension))
        if k + 1 < num_iter:
            r_vec.sub_(alpha_curr.mul(q_curr_vec))
            correction = r_vec.unsqueeze(0).mul(q_mat[:k + 1]).sum(
                dim_dimension, keepdim=True)
            correction = q_mat[:k + 1].mul(correction).sum(0)
            r_vec.sub_(correction)
            r_vec_norm = torch.norm(r_vec, 2, dim=dim_dimension, keepdim=True)
            r_vec.div_(r_vec_norm)
            beta_curr = r_vec_norm.squeeze_(dim_dimension)
            t_mat[k, k + 1].copy_(beta_curr)
            t_mat[k + 1, k].copy_(beta_curr)
            inner_products = q_mat[:k + 1].mul(r_vec.unsqueeze(0)).sum(
                dim_dimension)
            could_reorthogonalize = False
            for _ in range(10):
                if not torch.sum(inner_products > tol):
                    could_reorthogonalize = True
                    break
                correction = r_vec.unsqueeze(0).mul(q_mat[:k + 1]).sum(
                    dim_dimension, keepdim=True)
                correction = q_mat[:k + 1].mul(correction).sum(0)
                r_vec.sub_(correction)
                r_vec_norm = torch.norm(r_vec, 2, dim=dim_dimension,
                    keepdim=True)
                r_vec.div_(r_vec_norm)
                inner_products = q_mat[:k + 1].mul(r_vec.unsqueeze(0)).sum(
                    dim_dimension)
            q_mat[k + 1].copy_(r_vec)
            if torch.sum(beta_curr.abs() > 1e-06
                ) == 0 or not could_reorthogonalize:
                break
    num_iter = k + 1
    q_mat = q_mat[:num_iter + 1].permute(-1, *range(1, 1 + len(batch_shape)
        ), -2, 0).contiguous()
    t_mat = t_mat[:num_iter + 1, :num_iter + 1].permute(-1, *range(2, 2 +
        len(batch_shape)), 0, 1).contiguous()
    if not multiple_init_vecs:
        q_mat.squeeze_(0)
        t_mat.squeeze_(0)
    return q_mat, t_mat


class StochasticLQ(object):
    """
    Implements an approximate log determinant calculation for symmetric positive definite matrices
    using stochastic Lanczos quadrature. For efficient calculation of derivatives, We additionally
    compute the trace of the inverse using the same probe vector the log determinant was computed
    with. For more details, see Dong et al. 2017 (in submission).
    """

    def __init__(self, max_iter=15, num_random_probes=10):
        """
        The nature of stochastic Lanczos quadrature is that the calculation of tr(f(A)) is both inaccurate and
        stochastic. An instance of StochasticLQ has two parameters that control these tradeoffs. Increasing either
        parameter increases the running time of the algorithm.

        Args:
            - cls - Tensor constructor - to ensure correct type (default - default tensor)
            - max_iter (scalar) - The number of Lanczos iterations to perform. Increasing this makes the estimate of
                tr(f(A)) more accurate in expectation -- that is, the average value returned has lower error.
            - num_random_probes (scalar) - The number of random probes to use in the stochastic trace estimation.
                Increasing this makes the estimate of tr(f(A)) lower variance -- that is, the value
                returned is more consistent.
        """
        self.max_iter = max_iter
        self.num_random_probes = num_random_probes

    def lanczos_batch(self, matmul_closure, rhs_vectors):
        return lanczos_tridiag(matmul_closure, self.max_iter, init_vecs=
            rhs_vectors, dtype=rhs_vectors.dtype, device=rhs_vectors.device,
            batch_shape=rhs_vectors.shape[-2:], matrix_shape=torch.Size((
            rhs_vectors.size(-2), rhs_vectors.size(-2))))

    def evaluate(self, matrix_shape, eigenvalues, eigenvectors, funcs):
        """
        Computes tr(f(A)) for an arbitrary list of functions, where f(A) is equivalent to applying the function
        elementwise to the eigenvalues of A, i.e., if A = V\\LambdaV^{T}, then f(A) = Vf(\\Lambda)V^{T}, where
        f(\\Lambda) is applied elementwise.
        Note that calling this function with a list of functions to apply is significantly more efficient than
        calling it multiple times with one function -- each additional function after the first requires negligible
        additional computation.

        Args:
            - matrix_shape (torch.Size()) - size of underlying matrix (not including batch dimensions)
            - eigenvalues (Tensor n_probes x ...batch_shape x k) - batches of eigenvalues from Lanczos tridiag mats
            - eigenvectors (Tensor n_probes x ...batch_shape x k x k) - batches of eigenvectors from " " "
            - funcs (list of closures) - A list of functions [f_1,...,f_k]. tr(f_i(A)) is computed for each function.
                Each function in the closure should expect to take a torch vector of eigenvalues as input and apply
                the function elementwise. For example, to compute logdet(A) = tr(log(A)), [lambda x: x.log()] would
                be a reasonable value of funcs.

        Returns:
            - results (list of scalars) - The trace of each supplied function applied to the matrix, e.g.,
                [tr(f_1(A)),tr(f_2(A)),...,tr(f_k(A))].
        """
        batch_shape = torch.Size(eigenvalues.shape[1:-1])
        results = [torch.zeros(batch_shape, dtype=eigenvalues.dtype, device
            =eigenvalues.device) for _ in funcs]
        num_random_probes = eigenvalues.size(0)
        for j in range(num_random_probes):
            eigenvalues_for_probe = eigenvalues[j]
            eigenvectors_for_probe = eigenvectors[j]
            for i, func in enumerate(funcs):
                eigenvecs_first_component = eigenvectors_for_probe[(...), (
                    0), :]
                func_eigenvalues = func(eigenvalues_for_probe)
                dot_products = (eigenvecs_first_component.pow(2) *
                    func_eigenvalues).sum(-1)
                results[i] = results[i] + matrix_shape[-1] / float(
                    num_random_probes) * dot_products
        return results


def lanczos_tridiag_to_diag(t_mat):
    """
    Given a num_init_vecs x num_batch x k x k tridiagonal matrix t_mat,
    returns a num_init_vecs x num_batch x k set of eigenvalues
    and a num_init_vecs x num_batch x k x k set of eigenvectors.

    TODO: make the eigenvalue computations done in batch mode.
    """
    orig_device = t_mat.device
    if t_mat.size(-1) < 32:
        retr = torch.symeig(t_mat.cpu(), eigenvectors=True)
    else:
        retr = torch.symeig(t_mat, eigenvectors=True)
    evals, evecs = retr
    mask = evals.ge(0)
    evecs = evecs * mask.type_as(evecs).unsqueeze(-2)
    evals = evals.masked_fill_(~mask, 1)
    return evals.to(orig_device), evecs.to(orig_device)


class InvQuadLogDet(Function):
    """
    Given a PSD matrix A (or a batch of PSD matrices A), this function computes one or both
    of the following
    - The matrix solves A^{-1} b
    - logdet(A)
    """

    @staticmethod
    def forward(ctx, representation_tree, dtype, device, matrix_shape,
        batch_shape=torch.Size(), inv_quad=False, logdet=False,
        probe_vectors=None, probe_vector_norms=None, *args):
        """
        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)
        If self.inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)
        - the RHS of the matrix solves.

        Returns:
        - (Scalar) The inverse quadratic form (or None, if self.inv_quad is False)
        - (Scalar) The log determinant (or None, self.if logdet is False)
        """
        if not (inv_quad or logdet):
            raise RuntimeError(
                'Either inv_quad or logdet must be true (or both)')
        ctx.representation_tree = representation_tree
        ctx.dtype = dtype
        ctx.device = device
        ctx.matrix_shape = matrix_shape
        ctx.batch_shape = batch_shape
        ctx.inv_quad = inv_quad
        ctx.logdet = logdet
        matrix_args = None
        inv_quad_rhs = None
        if ctx.inv_quad:
            matrix_args = args[1:]
            inv_quad_rhs = args[0]
        else:
            matrix_args = args
        lazy_tsr = ctx.representation_tree(*matrix_args)
        with torch.no_grad():
            preconditioner, precond_lt, logdet_correction = (lazy_tsr.
                _preconditioner())
        ctx.preconditioner = preconditioner
        if (probe_vectors is None or probe_vector_norms is None) and logdet:
            num_random_probes = settings.num_trace_samples.value()
            if preconditioner is None:
                if settings.deterministic_probes.on():
                    warnings.warn(
                        "Deterministic probes will currently work only if you aren't training multiple independent models simultaneously."
                        , UserWarning)
                    if settings.deterministic_probes.probe_vectors is None:
                        probe_vectors = torch.empty(matrix_shape[-1],
                            num_random_probes, dtype=dtype, device=device)
                        probe_vectors.bernoulli_().mul_(2).add_(-1)
                        settings.deterministic_probes.probe_vectors = (
                            probe_vectors)
                    else:
                        probe_vectors = (settings.deterministic_probes.
                            probe_vectors)
                else:
                    probe_vectors = torch.empty(matrix_shape[-1],
                        num_random_probes, dtype=dtype, device=device)
                    probe_vectors.bernoulli_().mul_(2).add_(-1)
                probe_vector_norms = torch.norm(probe_vectors, 2, dim=-2,
                    keepdim=True)
                if batch_shape is not None:
                    probe_vectors = probe_vectors.expand(*batch_shape,
                        matrix_shape[-1], num_random_probes)
                    probe_vector_norms = probe_vector_norms.expand(*
                        batch_shape, 1, num_random_probes)
            else:
                if precond_lt.size()[-2:] == torch.Size([1, 1]):
                    covar_root = precond_lt.evaluate().sqrt()
                else:
                    covar_root = precond_lt.root_decomposition().root
                if settings.deterministic_probes.on():
                    warnings.warn(
                        "Deterministic probes will currently work only if you aren't training multiple independent models simultaneously."
                        , UserWarning)
                    base_samples = settings.deterministic_probes.probe_vectors
                    if base_samples is None or covar_root.size(-1
                        ) != base_samples.size(-2):
                        base_samples = torch.randn(*precond_lt.batch_shape,
                            covar_root.size(-1), num_random_probes, dtype=
                            precond_lt.dtype, device=precond_lt.device)
                        settings.deterministic_probes.probe_vectors = (
                            base_samples)
                    probe_vectors = covar_root.matmul(base_samples).permute(
                        -1, *range(precond_lt.dim() - 1))
                else:
                    base_samples = torch.randn(*precond_lt.batch_shape,
                        covar_root.size(-1), num_random_probes, dtype=
                        precond_lt.dtype, device=precond_lt.device)
                    probe_vectors = precond_lt.zero_mean_mvn_samples(
                        num_random_probes)
                probe_vectors = probe_vectors.unsqueeze(-2).transpose(0, -2
                    ).squeeze(0).transpose(-2, -1).contiguous()
                probe_vector_norms = torch.norm(probe_vectors, p=2, dim=-2,
                    keepdim=True)
            probe_vectors = probe_vectors.div(probe_vector_norms)
        ctx.probe_vectors = probe_vectors
        ctx.probe_vector_norms = probe_vector_norms
        if ctx.logdet and not ctx.probe_vectors.numel():
            raise RuntimeError(
                'Probe vectors were not supplied for logdet computation')
        rhs_list = []
        num_random_probes = 0
        num_inv_quad_solves = 0
        if ctx.logdet:
            rhs_list.append(ctx.probe_vectors)
            num_random_probes = ctx.probe_vectors.size(-1)
        ctx.is_vector = False
        if ctx.inv_quad:
            if inv_quad_rhs.ndimension() == 1:
                inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)
                ctx.is_vector = True
            rhs_list.append(inv_quad_rhs)
            num_inv_quad_solves = inv_quad_rhs.size(-1)
        rhs = torch.cat(rhs_list, -1)
        t_mat = None
        if ctx.logdet and settings.skip_logdet_forward.off():
            solves, t_mat = lazy_tsr._solve(rhs, preconditioner,
                num_tridiag=num_random_probes)
        else:
            solves = lazy_tsr._solve(rhs, preconditioner, num_tridiag=0)
        logdet_term = torch.zeros(lazy_tsr.batch_shape, dtype=ctx.dtype,
            device=ctx.device)
        inv_quad_term = torch.zeros(lazy_tsr.batch_shape, dtype=ctx.dtype,
            device=ctx.device)
        if ctx.logdet and settings.skip_logdet_forward.off():
            if torch.any(torch.isnan(t_mat)).item():
                logdet_term = torch.tensor(float('nan'), dtype=ctx.dtype,
                    device=ctx.device)
            else:
                if ctx.batch_shape is None:
                    t_mat = t_mat.unsqueeze(1)
                eigenvalues, eigenvectors = lanczos_tridiag_to_diag(t_mat)
                slq = StochasticLQ()
                logdet_term, = slq.evaluate(ctx.matrix_shape, eigenvalues,
                    eigenvectors, [lambda x: x.log()])
                if logdet_correction is not None:
                    logdet_term = logdet_term + logdet_correction
        if ctx.inv_quad:
            inv_quad_solves = solves.narrow(-1, num_random_probes,
                num_inv_quad_solves)
            inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)
        ctx.num_random_probes = num_random_probes
        ctx.num_inv_quad_solves = num_inv_quad_solves
        to_save = list(matrix_args) + [solves]
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return inv_quad_term, logdet_term

    @staticmethod
    def backward(ctx, inv_quad_grad_output, logdet_grad_output):
        matrix_arg_grads = None
        inv_quad_rhs_grad = None
        compute_inv_quad_grad = inv_quad_grad_output.abs().sum(
            ) and ctx.inv_quad
        compute_logdet_grad = logdet_grad_output.abs().sum() and ctx.logdet
        matrix_args = ctx.saved_tensors[:-1]
        solves = ctx.saved_tensors[-1]
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        if ctx.inv_quad:
            inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)
        if compute_logdet_grad:
            logdet_grad_output = logdet_grad_output.unsqueeze(-1)
            logdet_grad_output.unsqueeze_(-1)
        probe_vector_solves = None
        inv_quad_solves = None
        neg_inv_quad_solves_times_grad_out = None
        if compute_logdet_grad:
            coef = 1.0 / ctx.probe_vectors.size(-1)
            probe_vector_solves = solves.narrow(-1, 0, ctx.num_random_probes
                ).mul(coef)
            probe_vector_solves.mul_(ctx.probe_vector_norms).mul_(
                logdet_grad_output)
            probe_vectors = ctx.probe_vectors.mul(ctx.probe_vector_norms)
        if ctx.inv_quad:
            inv_quad_solves = solves.narrow(-1, ctx.num_random_probes, ctx.
                num_inv_quad_solves)
            neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(
                inv_quad_grad_output).mul_(-1)
        if any(ctx.needs_input_grad):
            left_factors_list = []
            right_factors_list = []
            if compute_logdet_grad:
                left_factors_list.append(probe_vector_solves)
                if ctx.preconditioner is not None:
                    probe_vectors = ctx.preconditioner(probe_vectors)
                right_factors_list.append(probe_vectors)
            if compute_inv_quad_grad:
                left_factors_list.append(neg_inv_quad_solves_times_grad_out)
                right_factors_list.append(inv_quad_solves)
            left_factors = torch.cat(left_factors_list, -1)
            right_factors = torch.cat(right_factors_list, -1)
            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors,
                right_factors)
        if compute_inv_quad_grad and ctx.needs_input_grad[9]:
            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul_(-2)
        elif ctx.inv_quad:
            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)
        if ctx.is_vector:
            inv_quad_rhs_grad.squeeze_(-1)
        if ctx.inv_quad:
            res = [inv_quad_rhs_grad] + list(matrix_arg_grads)
        else:
            res = list(matrix_arg_grads)
        return tuple([None] * 9 + res)


class LazyTensorRepresentationTree(object):

    def __init__(self, lazy_tsr):
        self._cls = lazy_tsr.__class__
        self._kwargs = lazy_tsr._kwargs
        counter = 0
        self.children = []
        for arg in lazy_tsr._args:
            if hasattr(arg, 'representation') and callable(arg.representation):
                representation_size = len(arg.representation())
                self.children.append((slice(counter, counter +
                    representation_size, None), arg.representation_tree()))
                counter += representation_size
            else:
                self.children.append((counter, None))
                counter += 1

    def __call__(self, *flattened_representation):
        unflattened_representation = []
        for index, subtree in self.children:
            if subtree is None:
                unflattened_representation.append(flattened_representation[
                    index])
            else:
                sub_representation = flattened_representation[index]
                unflattened_representation.append(subtree(*sub_representation))
        return self._cls(*unflattened_representation, **self._kwargs)


class Matmul(Function):

    @staticmethod
    def forward(ctx, representation_tree, rhs, *matrix_args):
        ctx.representation_tree = representation_tree
        orig_rhs = rhs
        if rhs.ndimension() == 1:
            is_vector = True
            rhs = rhs.unsqueeze(-1)
        else:
            is_vector = False
        lazy_tsr = ctx.representation_tree(*matrix_args)
        res = lazy_tsr._matmul(rhs)
        to_save = [orig_rhs] + list(matrix_args)
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        if is_vector:
            res = res.squeeze(-1)
        return res

    @staticmethod
    def backward(ctx, grad_output):
        rhs = ctx.saved_tensors[0]
        matrix_args = ctx.saved_tensors[1:]
        rhs_shape = rhs.shape
        rhs_grad = None
        arg_grads = [None] * len(matrix_args)
        if any(ctx.needs_input_grad[2:]):
            rhs = rhs.unsqueeze(-1) if rhs.ndimension() == 1 else rhs
            grad_output_matrix = grad_output.unsqueeze(-1
                ) if grad_output.ndimension() == 1 else grad_output
            arg_grads = ctx.representation_tree(*matrix_args
                )._quad_form_derivative(grad_output_matrix, rhs)
        if ctx.needs_input_grad[1]:
            if hasattr(ctx, '_lazy_tsr'):
                lazy_tsr = ctx._lazy_tsr
            else:
                lazy_tsr = ctx.representation_tree(*matrix_args)
            if grad_output.dim() == 1:
                rhs_grad = lazy_tsr._t_matmul(grad_output.unsqueeze(-1)
                    ).squeeze(-1)
            else:
                rhs_grad = lazy_tsr._t_matmul(grad_output)
            if rhs_grad.dim() > len(rhs_shape):
                rhs_grad = rhs_grad.reshape(-1, *rhs_shape).sum(0)
        return tuple([None] + [rhs_grad] + list(arg_grads))


class NumericalWarning(RuntimeWarning):
    """
    Warning thrown when convergence criteria are not met, or when comptuations require extra stability.
    """
    pass


class RootDecomposition(Function):

    @staticmethod
    def forward(ctx, representation_tree, max_iter, dtype, device,
        batch_shape, matrix_shape, root, inverse, initial_vectors, *matrix_args
        ):
        """
        :param list matrix_args: The arguments representing the symmetric matrix A (or batch of PSD matrices A)

        :rtype: (torch.Tensor, torch.Tensor)
        :return: :attr:`R`, such that :math:`R R^T \\approx A`, and :attr:`R_inv`, such that
            :math:`R_{inv} R_{inv}^T \\approx A^{-1}` (will only be populated if self.inverse = True)
        """
        from ..lazy import lazify
        ctx.representation_tree = representation_tree
        ctx.device = device
        ctx.dtype = dtype
        ctx.matrix_shape = matrix_shape
        ctx.max_iter = max_iter
        ctx.batch_shape = batch_shape
        ctx.root = root
        ctx.inverse = inverse
        ctx.initial_vectors = initial_vectors
        lazy_tsr = ctx.representation_tree(*matrix_args)
        matmul_closure = lazy_tsr._matmul
        q_mat, t_mat = lanczos.lanczos_tridiag(matmul_closure, ctx.max_iter,
            dtype=ctx.dtype, device=ctx.device, matrix_shape=ctx.
            matrix_shape, batch_shape=ctx.batch_shape, init_vecs=ctx.
            initial_vectors)
        if ctx.batch_shape is None:
            q_mat = q_mat.unsqueeze(-3)
            t_mat = t_mat.unsqueeze(-3)
        if t_mat.ndimension() == 3:
            q_mat = q_mat.unsqueeze(0)
            t_mat = t_mat.unsqueeze(0)
        n_probes = t_mat.size(0)
        mins = lazify(t_mat).diag().min(dim=-1, keepdim=True)[0].unsqueeze(-1)
        jitter_mat = settings.tridiagonal_jitter.value() * mins * torch.eye(
            t_mat.size(-1), device=t_mat.device, dtype=t_mat.dtype).expand_as(
            t_mat)
        eigenvalues, eigenvectors = lanczos.lanczos_tridiag_to_diag(t_mat +
            jitter_mat)
        q_mat = q_mat.matmul(eigenvectors)
        root_evals = eigenvalues.sqrt()
        root = torch.empty(0, dtype=q_mat.dtype, device=q_mat.device)
        inverse = torch.empty(0, dtype=q_mat.dtype, device=q_mat.device)
        if ctx.inverse:
            inverse = q_mat / root_evals.unsqueeze(-2)
        if ctx.root:
            root = q_mat * root_evals.unsqueeze(-2)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        if ctx.batch_shape is None:
            root = root.squeeze(1) if root.numel() else root
            q_mat = q_mat.squeeze(1)
            t_mat = t_mat.squeeze(1)
            root_evals = root_evals.squeeze(1)
            inverse = inverse.squeeze(1) if inverse.numel() else inverse
        if n_probes == 1:
            root = root.squeeze(0) if root.numel() else root
            q_mat = q_mat.squeeze(0)
            t_mat = t_mat.squeeze(0)
            root_evals = root_evals.squeeze(0)
            inverse = inverse.squeeze(0) if inverse.numel() else inverse
        to_save = list(matrix_args) + [q_mat, root_evals, inverse]
        ctx.save_for_backward(*to_save)
        return root, inverse

    @staticmethod
    def backward(ctx, root_grad_output, inverse_grad_output):
        if any(ctx.needs_input_grad):

            def is_empty(tensor):
                return tensor.numel() == 0 or tensor.numel() == 1 and tensor[0
                    ] == 0
            if is_empty(root_grad_output):
                root_grad_output = None
            if is_empty(inverse_grad_output):
                inverse_grad_output = None
            matrix_args = ctx.saved_tensors[:-3]
            q_mat = ctx.saved_tensors[-3]
            root_evals = ctx.saved_tensors[-2]
            inverse = ctx.saved_tensors[-1]
            is_batch = False
            if root_grad_output is not None:
                if root_grad_output.ndimension() == 2 and q_mat.ndimension(
                    ) > 2:
                    root_grad_output = root_grad_output.unsqueeze(0)
                    is_batch = True
                if root_grad_output.ndimension() == 3 and q_mat.ndimension(
                    ) > 3:
                    root_grad_output = root_grad_output.unsqueeze(0)
                    is_batch = True
            if inverse_grad_output is not None:
                if inverse_grad_output.ndimension() == 2 and q_mat.ndimension(
                    ) > 2:
                    inverse_grad_output = inverse_grad_output.unsqueeze(0)
                    is_batch = True
                if inverse_grad_output.ndimension() == 3 and q_mat.ndimension(
                    ) > 3:
                    inverse_grad_output = inverse_grad_output.unsqueeze(0)
                    is_batch = True
            if hasattr(ctx, '_lazy_tsr'):
                lazy_tsr = ctx._lazy_tsr
            else:
                lazy_tsr = ctx.representation_tree(*matrix_args)
            if not ctx.inverse:
                inverse = q_mat / root_evals.unsqueeze(-2)
            left_factor = torch.zeros_like(inverse)
            if root_grad_output is not None:
                left_factor.add_(root_grad_output)
            if inverse_grad_output is not None:
                left_factor.sub_(torch.matmul(inverse, inverse_grad_output.
                    transpose(-1, -2)).matmul(inverse))
            right_factor = inverse.div(2.0)
            if is_batch:
                left_factor = left_factor.permute(1, 0, 2, 3).contiguous()
                left_factor = left_factor.view(inverse.size(1), -1,
                    left_factor.size(-1))
                right_factor = right_factor.permute(1, 0, 2, 3).contiguous()
                right_factor = right_factor.view(inverse.size(1), -1,
                    right_factor.size(-1))
            else:
                left_factor = left_factor.contiguous()
                right_factor = right_factor.contiguous()
            res = lazy_tsr._quad_form_derivative(left_factor, right_factor)
            return tuple([None] * 9 + list(res))
        else:
            pass


def _mul_broadcast_shape(*shapes, error_msg=None):
    """Compute dimension suggested by multiple tensor indices (supports broadcasting)"""
    num_dims = max(len(shape) for shape in shapes)
    shapes = tuple([1] * (num_dims - len(shape)) + list(shape) for shape in
        shapes)
    final_size = []
    for size_by_dim in zip(*shapes):
        non_singleton_sizes = tuple(size for size in size_by_dim if size != 1)
        if len(non_singleton_sizes):
            if any(size != non_singleton_sizes[0] for size in
                non_singleton_sizes):
                if error_msg is None:
                    raise RuntimeError(
                        'Shapes are not broadcastable for mul operation')
                else:
                    raise RuntimeError(error_msg)
            final_size.append(non_singleton_sizes[0])
        else:
            final_size.append(1)
    return torch.Size(final_size)


_noop_index = slice(None, None, None)


def _compute_getitem_size(obj, indices):
    """
    Given an object and a tuple of indices, computes the final size of the
    Indices is a tuple containing ints, slices, and tensors

    .. note::
        The length of indices must match the dimensionality of obj

    Args:
        obj - tensor or LazyTensor
        indices - tuple of ints, slices, tensors

    Returns:
        :class:`torch.Size`
    """
    if obj.dim() != len(indices):
        raise RuntimeError(
            '_compute_getitem_size assumes that obj (size: {}) and indices (len: {}) have the same dimensionality.'
            .format(obj.shape, len(indices)))
    final_shape = []
    tensor_idx = None
    tensor_idx_shape = None
    slice_after_tensor_idx = False
    for i, (size, idx) in enumerate(zip(obj.shape, indices)):
        if isinstance(idx, slice):
            if idx == _noop_index:
                final_shape.append(size)
            else:
                final_shape.append(len(range(*idx.indices(size))))
            if tensor_idx is not None:
                slice_after_tensor_idx = True
        elif isinstance(idx, int):
            if settings.debug.on():
                try:
                    range(size)[idx]
                except IndexError:
                    raise IndexError(
                        'index element {} ({}) is invalid: out of range for obj of size {}.'
                        .format(i, idx, obj.shape))
        elif torch.is_tensor(idx):
            if tensor_idx_shape is None:
                tensor_idx_shape = idx.shape
                tensor_idx = len(final_shape)
            else:
                try:
                    tensor_idx_shape = _mul_broadcast_shape(tensor_idx_shape,
                        idx.shape)
                except RuntimeError:
                    raise IndexError(
                        'Incompatible tensor indices in index - got shapes of {} .'
                        .format([idx.shape for idx in indices if torch.
                        is_tensor(idx)]))
                if slice_after_tensor_idx:
                    tensor_idx = 0
    if tensor_idx is not None:
        final_shape = final_shape[:tensor_idx] + list(tensor_idx_shape
            ) + final_shape[tensor_idx:]
    return torch.Size(final_shape)


def _is_tensor_index_moved_to_start(indices):
    """
    Given an index, determine if the indexed part of the getitem is moved to the zero'th dimension
    """
    has_tensor_index = False
    continuous_tensor_index = True
    if torch.is_tensor(indices[0]):
        return True
    for index in indices[1:]:
        if torch.is_tensor(index):
            if not has_tensor_index:
                has_tensor_index = True
            elif not continuous_tensor_index:
                return True
        elif isinstance(index, slice):
            if has_tensor_index:
                continuous_tensor_index = False
    return False


def _pad_with_singletons(obj, num_singletons_before=0, num_singletons_after=0):
    """
    Pad obj with singleton dimensions on the left and right

    Example:
        >>> x = torch.randn(10, 5)
        >>> _pad_width_singletons(x, 2, 3).shape
        >>> # [1, 1, 10, 5, 1, 1, 1]
    """
    new_shape = [1] * num_singletons_before + list(obj.shape) + [1
        ] * num_singletons_after
    return obj.view(*new_shape)


def _convert_indices_to_tensors(obj, indices):
    """
    Given an index made up of tensors/slices/ints, returns a tensor-only index that has the
    same outcome as the original index (when applied to the obj)

    .. note::
        The length of indices must match the dimensionality of obj

    Args:
        obj - tensor or LazyTensor
        indices - tuple of slices, tensors, ints

    Returns:
        tuple of tensor indices (shapes of tensors will involve broadcasting)

    Example:
        >>> x = torch.randn(3, 6, 4)
        >>> _convert_indices_to_tensors(x, (torch.tensor([0, 1]), 2, slice(None, None, None)))
        >>> # (torch.tensor([[[0]], [[1]]]), torch.tensor([[[2]]]), torch.tensor([[[0, 1, 2, 3]]]))
    """
    slice_indices = tuple(index for index in indices if isinstance(index,
        slice))
    tensor_indices = tuple(index for index in indices if torch.is_tensor(index)
        )
    tensor_index_shape = _mul_broadcast_shape(*[tensor_index.shape for
        tensor_index in tensor_indices])
    num_final_dims = len(slice_indices) + len(tensor_index_shape)
    tensor_index_moved_to_start = _is_tensor_index_moved_to_start(indices)
    num_singletons_before = len(tensor_index_shape
        ) if tensor_index_moved_to_start else 0
    num_singletons_after = num_final_dims - len(tensor_index_shape
        ) if tensor_index_moved_to_start else num_final_dims
    num_singletons_before_tensor = 0 if tensor_index_moved_to_start else None
    num_singletons_after_tensor = num_final_dims - len(tensor_index_shape
        ) if tensor_index_moved_to_start else None
    new_indices = []
    for dim, index in enumerate(indices):
        if isinstance(index, slice):
            num_singletons_after -= 1
            new_index = torch.arange(0, obj.size(dim), device=obj.device)[index
                ]
            new_index = _pad_with_singletons(new_index,
                num_singletons_before, num_singletons_after)
            num_singletons_before += 1
        elif isinstance(index, int):
            new_index = torch.tensor(index, dtype=torch.long, device=obj.device
                )
            new_index = _pad_with_singletons(new_index,
                num_singletons_before, num_singletons_after)
        elif torch.is_tensor(index):
            if num_singletons_before_tensor is None:
                num_singletons_after -= len(tensor_index_shape)
                num_singletons_before_tensor = num_singletons_before
                num_singletons_after_tensor = num_singletons_after
                num_singletons_before += len(tensor_index_shape)
            new_index = _pad_with_singletons(index,
                num_singletons_before_tensor, num_singletons_after_tensor)
        new_indices.append(new_index)
    return tuple(new_indices)


def _is_noop_index(index):
    """
    Determine if a given index is a noop (e.g. ":")
    """
    return isinstance(index, slice) and index == _noop_index


def _matmul_broadcast_shape(shape_a, shape_b, error_msg=None):
    """Compute dimension of matmul operation on shapes (supports broadcasting)"""
    m, n, p = shape_a[-2], shape_a[-1], shape_b[-1]
    if len(shape_b) == 1:
        if n != p:
            if error_msg is None:
                raise RuntimeError(
                    f'Incompatible dimensions for matmul: {shape_a} and {shape_b}'
                    )
            else:
                raise RuntimeError(error_msg)
        return shape_a[:-1]
    if n != shape_b[-2]:
        if error_msg is None:
            raise RuntimeError(
                f'Incompatible dimensions for matmul: {shape_a} and {shape_b}')
        else:
            raise RuntimeError(error_msg)
    tail_shape = torch.Size([m, p])
    batch_shape_a = shape_a[:-2]
    batch_shape_b = shape_b[:-2]
    if batch_shape_a == batch_shape_b:
        bc_shape = batch_shape_a
    else:
        bc_shape = _mul_broadcast_shape(batch_shape_a, batch_shape_b)
    return bc_shape + tail_shape


def add_to_cache(obj, name, val):
    """Add a result to the cache of an object."""
    if not hasattr(obj, '_memoize_cache'):
        obj._memoize_cache = dict()
    obj._memoize_cache[name] = val
    return obj


def is_in_cache(obj, name):
    return hasattr(obj, '_memoize_cache') and name in obj._memoize_cache


def get_from_cache(obj, name):
    """Get an item from the cache."""
    if not is_in_cache(obj, name):
        raise RuntimeError('Object does not have item {} stored in cache.'.
            format(name))
    return obj._memoize_cache[name]


def cached(method=None, name=None):
    """A decorator allowing for specifying the name of a cache, allowing it to be modified elsewhere."""
    if method is None:
        return functools.partial(cached, name=name)

    @functools.wraps(method)
    def g(self, *args, **kwargs):
        cache_name = name if name is not None else method
        if not is_in_cache(self, cache_name):
            add_to_cache(self, cache_name, method(self, *args, **kwargs))
        return get_from_cache(self, cache_name)
    return g


def delazify(obj):
    """
    A function which ensures that `obj` is a (normal) Tensor.

    If `obj` is a Tensor, this function does nothing.
    If `obj` is a LazyTensor, this function evaluates it.
    """
    if torch.is_tensor(obj):
        return obj
    elif isinstance(obj, LazyTensor):
        return obj.evaluate()
    else:
        raise TypeError('object of class {} cannot be made into a Tensor'.
            format(obj.__class__.__name__))


class NanError(RuntimeError):
    pass


def psd_safe_cholesky(A, upper=False, out=None, jitter=None):
    """Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.
    Args:
        :attr:`A` (Tensor):
            The tensor to compute the Cholesky decomposition of
        :attr:`upper` (bool, optional):
            See torch.cholesky
        :attr:`out` (Tensor, optional):
            See torch.cholesky
        :attr:`jitter` (float, optional):
            The jitter to add to the diagonal of A in case A is only p.s.d. If omitted, chosen
            as 1e-6 (float) or 1e-8 (double)
    """
    try:
        L = torch.cholesky(A, upper=upper, out=out)
        return L
    except RuntimeError as e:
        isnan = torch.isnan(A)
        if isnan.any():
            raise NanError(
                f'cholesky_cpu: {isnan.sum().item()} of {A.numel()} elements of the {A.shape} tensor are NaN.'
                )
        if jitter is None:
            jitter = 1e-06 if A.dtype == torch.float32 else 1e-08
        Aprime = A.clone()
        jitter_prev = 0
        for i in range(3):
            jitter_new = jitter * 10 ** i
            Aprime.diagonal(dim1=-2, dim2=-1).add_(jitter_new - jitter_prev)
            jitter_prev = jitter_new
            try:
                L = torch.cholesky(Aprime, upper=upper, out=out)
                warnings.warn(
                    f'A not p.d., added jitter of {jitter_new} to the diagonal'
                    , NumericalWarning)
                return L
            except RuntimeError:
                continue
        raise e


class LazyTensor(ABC):
    """
    Base class for LazyTensors in GPyTorch.

    In GPyTorch, nearly all covariance matrices for Gaussian processes are handled internally as some variety of
    LazyTensor. A LazyTensor is an object that represents a tensor object, similar to :class:`torch.tensor`, but
    typically differs in two ways:

    #. A tensor represented by a LazyTensor can typically be represented more efficiently than storing a full matrix.
       For example, a LazyTensor representing :math:`K=XX^{\\top}` where :math:`K` is :math:`n \\times n` but
       :math:`X` is :math:`n \\times d` might store :math:`X` instead of :math:`K` directly.
    #. A LazyTensor typically defines a matmul routine that performs :math:`KM` that is more efficient than storing
       the full matrix. Using the above example, performing :math:`KM=X(X^{\\top}M)` requires only :math:`O(nd)` time,
       rather than the :math:`O(n^2)` time required if we were storing :math:`K` directly.

    In order to define a new LazyTensor class that can be used as a covariance matrix in GPyTorch, a user must define
    at a minimum the following methods (in each example, :math:`K` denotes the matrix that the LazyTensor represents)

    * :func:`~gpytorch.lazy.LazyTensor._matmul`, which performs a matrix multiplication :math:`KM`
    * :func:`~gpytorch.lazy.LazyTensor._size`, which returns a :class:`torch.Size` containing the dimensions of
      :math:`K`.
    * :func:`~gpytorch.lazy.LazyTensor._transpose_nonbatch`, which returns a transposed version of the LazyTensor

    In addition to these, the following methods should be implemented for maximum efficiency

    * :func:`~gpytorch.lazy.LazyTensor._quad_form_derivative`, which computes the derivative of a quadratic form
      with the LazyTensor (e.g. :math:`d (a^T X b) / dX`).
    * :func:`~gpytorch.lazy.LazyTensor._get_indices`, which returns a :class:`torch.Tensor` containing elements that
      are given by various tensor indices.
    * :func:`~gpytorch.lazy.LazyTensor._expand_batch`, which expands the batch dimensions of LazyTensors.
    * :func:`~gpytorch.lazy.LazyTensor._check_args`, which performs error checking on the arguments supplied to the
      LazyTensor constructor.

    In addition to these, a LazyTensor *may* need to define the following functions if it does anything interesting
    with the batch dimensions (e.g. sums along them, adds additional ones, etc):
    :func:`~gpytorch.lazy.LazyTensor._unsqueeze_batch`, :func:`~gpytorch.lazy.LazyTensor._getitem`, and
    :func:`~gpytorch.lazy.LazyTensor._permute_batch`.
    See the documentation for these methods for details.

    .. note::
        The base LazyTensor class provides default implementations of many other operations in order to mimic the
        behavior of a standard tensor as closely as possible. For example, we provide default implementations of
        :func:`~gpytorch.lazy.LazyTensor.__getitem__`, :func:`~gpytorch.lazy.LazyTensor.__add__`, etc that either
        make use of other lazy tensors or exploit the functions that **must** be defined above.

        Rather than overriding the public methods, we recommend that you override the private versions associated
        with these methods (e.g. - write a custom `_getitem` verses a custom `__getitem__`). This is because the
        public methods do quite a bit of error checking and casing that doesn't need to be repeated.

    .. note::
        LazyTensors are designed by default to optionally represent batches of matrices. Thus, the size of a
        LazyTensor may be (for example) :math:`b \\times n \\times n`. Many of the methods are designed to efficiently
        operate on these batches if present.
    """

    def _check_args(self, *args, **kwargs):
        """
        (Optional) run checks to see that input arguments and kwargs are valid

        Return:
            None (if all checks pass) or str (error message to raise)
        """
        return None

    def __init__(self, *args, **kwargs):
        if settings.debug.on():
            err = self._check_args(*args, **kwargs)
            if err is not None:
                raise ValueError(err)
        self._args = args
        self._kwargs = kwargs

    @abstractmethod
    def _matmul(self, rhs):
        """
        Performs a matrix multiplication :math:`KM` with the matrix :math:`K` that this LazyTensor represents. Should
        behave as :func:`torch.matmul`. If the LazyTensor represents a batch of matrices, this method should therefore
        operate in batch mode as well.

        ..note::
            This method is intended to be used only internally by various Functions that support backpropagation
            (e.g., :class:`gpytorch.functions.Matmul`). Once this method is defined, it is strongly recommended that
            one use :func:`~gpytorch.lazy.LazyTensor.matmul` instead, which makes use of this method properly.

        Args:
            rhs (:obj:`torch.tensor`): the matrix :math:`M` to multiply with.

        Returns:
            :obj:`torch.tensor`: matrix * rhs
        """
        raise NotImplementedError('The class {} requires a _matmul function!'
            .format(self.__class__.__name__))

    @abstractmethod
    def _size(self):
        """
        Returns the size of the resulting Tensor that the lazy tensor represents.

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.size`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`torch.Size`: The size of the matrix :math:`K` represented by this LazyTensor
        """
        raise NotImplementedError('The class {} requires a _size function!'
            .format(self.__class__.__name__))

    @abstractmethod
    def _transpose_nonbatch(self):
        """
        Transposes non-batch dimensions (e.g. last two)
        Implement this method, rather than transpose() or t().

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.transpose`, which
            does some additional work. Calling this method directly is discouraged.
        """
        raise NotImplementedError(
            'The class {} requires a _transpose_nonbatch function!'.format(
            self.__class__.__name__))

    def _permute_batch(self, *dims):
        """
        Permute the batch dimensions.
        This probably won't have to be overwritten by LazyTensors, unless they use batch dimensions
        in a special way (e.g. BlockDiagLazyTensor, SumBatchLazyTensor)

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.unsqueeze`,
            which does some additional work. Calling this method directly is discouraged.

        Args:
            dims (tuple of ints):
                The new order for the `self.dim() - 2` dimensions.
                It WILL contain each of the positive batch dimensions exactly once.
        """
        components = []
        for component in self._args:
            if torch.is_tensor(component):
                extra_dims = range(len(dims), component.dim())
                components.append(component.permute(*dims, *extra_dims))
            elif isinstance(component, LazyTensor):
                components.append(component._permute_batch(*dims))
            else:
                components.append(component)
        res = self.__class__(*components, **self._kwargs)
        return res

    def _getitem(self, row_index, col_index, *batch_indices):
        """
        Supports subindexing of the matrix this LazyTensor represents.

        The indices passed into this method will either be:
            Tensor indices
            Slices

        ..note::
            LazyTensor.__getitem__ uses this as a helper method. If you are writing your own custom LazyTensor,
            override this method rather than __getitem__ (so that you don't have to repeat the extra work)

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.__getitem__`,
            which does some additional work. Calling this method directly is discouraged.

        This method has a number of restrictions on the type of arguments that are passed in to reduce
        the complexity of __getitem__ calls in PyTorch. In particular:
            - This method only accepts slices and tensors for the row/column indices (no ints)
            - The row and column dimensions don't dissapear (e.g. from Tensor indexing). These cases are
              handled by the `_getindices` method

        Args:
            :attr:`row_index` (slice, Tensor):
                Index for the row of the LazyTensor
            :attr:`col_index` (slice, Tensor):
                Index for the col of the LazyTensor
            :attr:`batch_indices` (tuple of slice, int, Tensor):
                Indices for the batch dimensions

        Returns:
            `LazyTensor`
        """
        if _is_noop_index(row_index) and _is_noop_index(col_index):
            if len(batch_indices):
                components = [component[batch_indices] for component in
                    self._args]
                res = self.__class__(*components, **self._kwargs)
                return res
            else:
                return self
        row_interp_indices = torch.arange(0, self.size(-2), dtype=torch.
            long, device=self.device).view(-1, 1)
        row_interp_indices = row_interp_indices.expand(*self.batch_shape, -1, 1
            )
        row_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self
            .device).expand_as(row_interp_indices)
        col_interp_indices = torch.arange(0, self.size(-1), dtype=torch.
            long, device=self.device).view(-1, 1)
        col_interp_indices = col_interp_indices.expand(*self.batch_shape, -1, 1
            )
        col_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self
            .device).expand_as(col_interp_indices)
        from . import InterpolatedLazyTensor
        res = InterpolatedLazyTensor(self, row_interp_indices,
            row_interp_values, col_interp_indices, col_interp_values)
        return res._getitem(row_index, col_index, *batch_indices)

    def _unsqueeze_batch(self, dim):
        """
        Unsqueezes a batch dimension (positive-indexed only)
        This probably won't have to be overwritten by LazyTensors, unless they use batch dimensions
        in a special way (e.g. BlockDiagLazyTensor, SumBatchLazyTensor)

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.unsqueeze`,
            which does some additional work. Calling this method directly is discouraged.
        """
        components = [component.unsqueeze(dim) for component in self._args]
        res = self.__class__(*components, **self._kwargs)
        return res

    def _expand_batch(self, batch_shape):
        """
        Expands along batch dimensions.

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.expand`,
            which does some additional work. Calling this method directly is discouraged.
        """
        current_shape = torch.Size([(1) for _ in range(len(batch_shape) -
            self.dim() - 2)] + list(self.batch_shape))
        batch_repeat = torch.Size([(expand_size // current_size) for 
            expand_size, current_size in zip(batch_shape, current_shape)])
        return self.repeat(*batch_repeat, 1, 1)

    def _get_indices(self, row_index, col_index, *batch_indices):
        """
        This method selects elements from the LazyTensor based on tensor indices for each dimension.
        All indices are tensor indices that are broadcastable.
        There will be exactly one index per dimension of the LazyTensor

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.__getitem__`,
            which does some additional work. Calling this method directly is discouraged.

        Args:
            row_index (LongTensor): indices to select from row of LazyTensor
            row_index (LongTensor): indices to select from col of LazyTensor
            batch_indices (tuple LongTensor): indices to select from batch dimensions.

        Returns:
            Tensor (size determined by broadcasted shape of indices) of selected values
        """
        final_shape = _mul_broadcast_shape(*(index.shape for index in
            batch_indices), row_index.shape, col_index.shape)
        row_index = row_index.expand(final_shape)
        col_index = col_index.expand(final_shape)
        batch_indices = tuple(index.expand(final_shape) for index in
            batch_indices)
        base_lazy_tensor = self._getitem(_noop_index, _noop_index, *
            batch_indices)._expand_batch(final_shape)
        row_interp_indices = torch.arange(0, self.size(-2), dtype=torch.
            long, device=self.device)
        row_interp_indices = row_interp_indices[row_index].unsqueeze_(-1
            ).unsqueeze_(-1)
        row_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self
            .device).expand_as(row_interp_indices)
        col_interp_indices = torch.arange(0, self.size(-1), dtype=torch.
            long, device=self.device)
        col_interp_indices = col_interp_indices[col_index].unsqueeze_(-1
            ).unsqueeze_(-1)
        col_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self
            .device).expand_as(col_interp_indices)
        from . import InterpolatedLazyTensor
        res = InterpolatedLazyTensor(base_lazy_tensor, row_interp_indices,
            row_interp_values, col_interp_indices, col_interp_values).evaluate(
            ).squeeze(-2).squeeze(-1)
        return res

    def _quad_form_derivative(self, left_vecs, right_vecs):
        """
        Given u (left_vecs) and v (right_vecs),
        Computes the derivatives of (u^t K v) w.r.t. K

        ..note::
            This method is intended to be used only internally by various Functions that support backpropagation.
            For example, this method is used internally by :func:`~gpytorch.lazy.LazyTensor.inv_quad_logdet`. It is
            not likely that users will need to call this method directly.

        Returns:
            :obj:`torch.tensor`: derivative with respect to the arguments that are actually used to represent this
                                   this LazyTensor.
        """
        from collections import deque
        args = tuple(self.representation())
        args_with_grads = tuple(arg for arg in args if arg.requires_grad)
        if not len(args_with_grads):
            return tuple(None for _ in args)
        with torch.autograd.enable_grad():
            loss = (left_vecs * self._matmul(right_vecs)).sum()
            loss.requires_grad_(True)
            actual_grads = deque(torch.autograd.grad(loss, args_with_grads,
                allow_unused=True))
        grads = []
        for arg in args:
            if arg.requires_grad:
                grads.append(actual_grads.popleft())
            else:
                grads.append(None)
        return tuple(grads)
    _check_size = True

    @property
    def _args(self):
        return self._args_memo

    @_args.setter
    def _args(self, args):
        self._args_memo = args

    def _approx_diag(self):
        """
        (Optional) returns an (approximate) diagonal of the matrix

        Sometimes computing an exact diagonal is a bit computationally slow
        When we don't need an exact diagonal (e.g. for the pivoted cholesky
        decomposition, this function is called

        Defaults to calling the exact diagonal function

        Returns:
            tensor: - the diagonal (or batch of diagonals)
        """
        return self.diag()

    @cached(name='cholesky')
    def _cholesky(self):
        """
        (Optional) Cholesky-factorizes the LazyTensor

        ..note::
            This method is used as an internal helper. Calling this method directly is discouraged.

        Returns:
            (LazyTensor) Cholesky factor
        """
        from .non_lazy_tensor import NonLazyTensor
        from .keops_lazy_tensor import KeOpsLazyTensor
        evaluated_kern_mat = self.evaluate_kernel()
        if any(isinstance(sub_mat, KeOpsLazyTensor) for sub_mat in
            evaluated_kern_mat._args):
            raise RuntimeError(
                'Cannot run Cholesky with KeOps: it will either be really slow or not work.'
                )
        evaluated_mat = evaluated_kern_mat.evaluate()
        if evaluated_mat.size(-1) == 1:
            return NonLazyTensor(evaluated_mat.clamp_min(0.0).sqrt())
        cholesky = psd_safe_cholesky(evaluated_mat, jitter=settings.
            cholesky_jitter.value()).contiguous()
        return NonLazyTensor(cholesky)

    def _cholesky_solve(self, rhs):
        """
        (Optional) Assuming that `self` is a Cholesky factor, computes the cholesky solve

        ..note::
            This method is used as an internal helper. Calling this method directly is discouraged.

        Returns:
            (LazyTensor) Cholesky factor
        """
        return torch.cholesky_solve(rhs, self.evaluate())

    def _inv_matmul_preconditioner(self):
        """
        (Optional) define a preconditioner that can be used for linear systems, but not necessarily
        for log determinants. By default, this can call :meth:`~gpytorch.lazy.LazyTensor._preconditioner`.

        Returns:
            function: a function on x which performs P^{-1}(x)
        """
        base_precond, _, _ = self._preconditioner()
        if base_precond is not None:
            return base_precond
        elif gpytorch.beta_features.default_preconditioner.on():
            if hasattr(self, '_default_preconditioner_cache'):
                U, S, V = self._default_preconditioner_cache
            else:
                precond_basis_size = min(gpytorch.settings.
                    max_preconditioner_size.value(), self.size(-1))
                random_basis = torch.randn(self.batch_shape + torch.Size((
                    self.size(-2), precond_basis_size)), device=self.device,
                    dtype=self.dtype)
                projected_mat = self._matmul(random_basis)
                proj_q = torch.qr(projected_mat)
                orthog_projected_mat = self._matmul(proj_q).transpose(-2, -1)
                U, S, V = torch.svd(orthog_projected_mat)
                U = proj_q.matmul(U)
                self._default_preconditioner_cache = U, S, V

            def preconditioner(v):
                res = V.transpose(-2, -1).matmul(v)
                res = (1 / S).unsqueeze(-1) * res
                res = U.matmul(res)
                return res
            return preconditioner
        else:
            return None

    def _mul_constant(self, other):
        """
        Multiplies the LazyTensor by a costant.

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.mul`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`
        """
        from .constant_mul_lazy_tensor import ConstantMulLazyTensor
        return ConstantMulLazyTensor(self, other)

    def _mul_matrix(self, other):
        """
        Multiplies the LazyTensor by a (batch of) matrices.

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.mul`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`
        """
        from .non_lazy_tensor import NonLazyTensor
        from .mul_lazy_tensor import MulLazyTensor
        self = self.evaluate_kernel()
        other = other.evaluate_kernel()
        if isinstance(self, NonLazyTensor) or isinstance(other, NonLazyTensor):
            return NonLazyTensor(self.evaluate() * other.evaluate())
        else:
            left_lazy_tensor = self if self._root_decomposition_size(
                ) < other._root_decomposition_size() else other
            right_lazy_tensor = other if left_lazy_tensor is self else self
            return MulLazyTensor(left_lazy_tensor.root_decomposition(),
                right_lazy_tensor.root_decomposition())

    def _preconditioner(self):
        """
        (Optional) define a preconditioner (P) for linear conjugate gradients

        Returns:
            function: a function on x which performs P^{-1}(x)
            scalar: the log determinant of P
        """
        return None, None, None

    def _probe_vectors_and_norms(self):
        return None, None

    def _prod_batch(self, dim):
        """
        Multiply the LazyTensor across a batch dimension (supplied as a positive number).

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.prod`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`
        """
        from .mul_lazy_tensor import MulLazyTensor
        from .root_lazy_tensor import RootLazyTensor
        if self.size(dim) == 1:
            return self.squeeze(dim)
        roots = self.root_decomposition().root.evaluate()
        num_batch = roots.size(dim)
        while True:
            if num_batch % 2:
                shape = list(roots.shape)
                shape[dim] = 1
                extra_root = torch.full(shape, dtype=self.dtype, device=
                    self.device, fill_value=1.0 / math.sqrt(self.size(-2)))
                roots = torch.cat([roots, extra_root], dim)
                num_batch += 1
            part1_index = [_noop_index] * roots.dim()
            part1_index[dim] = slice(None, num_batch // 2, None)
            part1 = roots[tuple(part1_index)].contiguous()
            part2_index = [_noop_index] * roots.dim()
            part2_index[dim] = slice(num_batch // 2, None, None)
            part2 = roots[tuple(part2_index)].contiguous()
            if num_batch // 2 == 1:
                part1 = part1.squeeze(dim)
                part2 = part2.squeeze(dim)
                res = MulLazyTensor(RootLazyTensor(part1), RootLazyTensor(
                    part2))
                break
            else:
                res = MulLazyTensor(RootLazyTensor(part1), RootLazyTensor(
                    part2))
                roots = res.root_decomposition().root.evaluate()
                num_batch = num_batch // 2
        return res

    def _root_decomposition(self):
        """
        Returns the (usually low-rank) root of a lazy tensor of a PSD matrix.

        ..note::
            This method is used internally by the related function
            :func:`~gpytorch.lazy.LazyTensor.root_decomposition`, which does some additional work.
            Calling this method directly is discouraged.

        Returns:
            (Tensor or LazyTensor): The root of the root decomposition
        """
        func = RootDecomposition()
        res, _ = func.apply(self.representation_tree(), self.
            _root_decomposition_size(), self.dtype, self.device, self.
            batch_shape, self.matrix_shape, True, False, None, *self.
            representation())
        return res

    def _root_decomposition_size(self):
        """
        This is the inner size of the root decomposition.
        This is primarily used to determine if it will be cheaper to compute a
        different root or not
        """
        return settings.max_root_decomposition_size.value()

    def _root_inv_decomposition(self, initial_vectors=None):
        """
        Returns the (usually low-rank) inverse root of a lazy tensor of a PSD matrix.

        ..note::
            This method is used internally by the related function
            :func:`~gpytorch.lazy.LazyTensor.root_inv_decomposition`, which does some additional work.
            Calling this method directly is discouraged.

        Returns:
            (Tensor or LazyTensor): The root of the inverse root decomposition
        """
        from .root_lazy_tensor import RootLazyTensor
        func = RootDecomposition()
        roots, inv_roots = func.apply(self.representation_tree(), self.
            _root_decomposition_size(), self.dtype, self.device, self.
            batch_shape, self.matrix_shape, True, True, initial_vectors, *
            self.representation())
        if initial_vectors is not None and initial_vectors.size(-1) > 1:
            add_to_cache(self, 'root_decomposition', RootLazyTensor(roots[0]))
        else:
            add_to_cache(self, 'root_decomposition', RootLazyTensor(roots))
        return inv_roots

    def _solve(self, rhs, preconditioner, num_tridiag=0):
        return utils.linear_cg(self._matmul, rhs, n_tridiag=num_tridiag,
            max_iter=settings.max_cg_iterations.value(), max_tridiag_iter=
            settings.max_lanczos_quadrature_iterations.value(),
            preconditioner=preconditioner)

    def _sum_batch(self, dim):
        """
        Sum the LazyTensor across a batch dimension (supplied as a positive number).

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.sum`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`
        """
        from .sum_batch_lazy_tensor import SumBatchLazyTensor
        return SumBatchLazyTensor(self, block_dim=dim)

    def _t_matmul(self, rhs):
        """
        Performs a transpose matrix multiplication :math:`K^{\\top}M` with the matrix :math:`K` that this
        LazyTensor represents.

        Args:
            rhs (:obj:`torch.tensor`): the matrix :math:`M` to multiply with.

        Returns:
            :obj:`torch.tensor`: matrix * rhs
        """
        return self.transpose(-1, -2)._matmul(rhs)

    def add_diag(self, diag):
        """
        Adds an element to the diagonal of the matrix.

        Args:
            - diag (Scalar Tensor)
        """
        from .diag_lazy_tensor import DiagLazyTensor
        from .added_diag_lazy_tensor import AddedDiagLazyTensor
        if not self.is_square:
            raise RuntimeError('add_diag only defined for square matrices')
        try:
            expanded_diag = diag.expand(self.shape[:-1])
        except RuntimeError:
            raise RuntimeError(
                'add_diag for LazyTensor of size {} received invalid diagonal of size {}.'
                .format(self.shape, diag.shape))
        return AddedDiagLazyTensor(self, DiagLazyTensor(expanded_diag))

    def add_jitter(self, jitter_val=0.001):
        """
        Adds jitter (i.e., a small diagonal component) to the matrix this
        LazyTensor represents. This could potentially be implemented as a no-op,
        however this could lead to numerical instabilities, so this should only
        be done at the user's risk.
        """
        diag = torch.tensor(jitter_val, dtype=self.dtype, device=self.device)
        return self.add_diag(diag)

    @property
    def batch_dim(self):
        """
        Returns the dimension of the shape over which the tensor is batched.
        """
        return len(self.batch_shape)

    @property
    def batch_shape(self):
        """
        Returns the shape over which the tensor is batched.
        """
        return self.shape[:-2]

    def cholesky(self, upper=False):
        """
        Cholesky-factorizes the LazyTensor

        Parameters:
            upper (bool) - upper triangular or lower triangular factor (default: False)

        Returns:
            (LazyTensor) Cholesky factor (lower triangular)
        """
        res = self._cholesky()
        if upper:
            res = res.transpose(-1, -2)
        return res

    def clone(self):
        """
        Clones the LazyTensor (creates clones of all underlying tensors)
        """
        args = [(arg.clone() if hasattr(arg, 'clone') else arg) for arg in
            self._args]
        kwargs = {key: (val.clone() if hasattr(val, 'clone') else val) for 
            key, val in self._kwargs.items()}
        return self.__class__(*args, **kwargs)

    def cpu(self):
        """
        Returns:
            :obj:`~gpytorch.lazy.LazyTensor`: a new LazyTensor identical to ``self``, but on the CPU.
        """
        new_args = []
        new_kwargs = {}
        for arg in self._args:
            if hasattr(arg, 'cpu'):
                new_args.append(arg.cpu())
            else:
                new_args.append(arg)
        for name, val in self._kwargs.items():
            if hasattr(val, 'cpu'):
                new_kwargs[name] = val.cpu()
            else:
                new_kwargs[name] = val
        return self.__class__(*new_args, **new_kwargs)

    def cuda(self, device_id=None):
        """
        This method operates identically to :func:`torch.nn.Module.cuda`.

        Args:
            device_id (:obj:`str`, optional):
                Device ID of GPU to use.
        Returns:
            :obj:`~gpytorch.lazy.LazyTensor`:
                a new LazyTensor identical to ``self``, but on the GPU.
        """
        new_args = []
        new_kwargs = {}
        for arg in self._args:
            if hasattr(arg, 'cuda'):
                new_args.append(arg.cuda(device_id))
            else:
                new_args.append(arg)
        for name, val in self._kwargs.items():
            if hasattr(val, 'cuda'):
                new_kwargs[name] = val.cuda(device_id)
            else:
                new_kwargs[name] = val
        return self.__class__(*new_args, **new_kwargs)

    @property
    def device(self):
        return self._args[0].device

    def detach(self):
        """
        Removes the LazyTensor from the current computation graph.
        (In practice, this function removes all Tensors that make up the
        LazyTensor from the computation graph.)
        """
        return self.clone().detach_()

    def detach_(self):
        """
        An in-place version of `detach`.
        """
        for arg in self._args:
            if hasattr(arg, 'detach'):
                arg.detach_()
        for val in self._kwargs.values():
            if hasattr(val, 'detach'):
                val.detach_()
        return self

    def diag(self):
        """
        As :func:`torch.diag`, returns the diagonal of the matrix :math:`K` this LazyTensor represents as a vector.

        :rtype: torch.tensor
        :return: The diagonal of :math:`K`. If :math:`K` is :math:`n \\times n`, this will be a length
            n vector. If this LazyTensor represents a batch (e.g., is :math:`b \\times n \\times n`), this will be a
            :math:`b \\times n` matrix of diagonals, one for each matrix in the batch.
        """
        if settings.debug.on():
            if not self.is_square:
                raise RuntimeError('Diag works on square matrices (or batches)'
                    )
        row_col_iter = torch.arange(0, self.matrix_shape[-1], dtype=torch.
            long, device=self.device)
        return self[..., row_col_iter, row_col_iter]

    def dim(self):
        """
        Alias of :meth:`~gpytorch.lazy.LazyTensor.ndimension`
        """
        return self.ndimension()

    @property
    def dtype(self):
        return self._args[0].dtype

    def expand(self, *sizes):
        if len(sizes) == 1 and hasattr(sizes, '__iter__'):
            sizes = sizes[0]
        if len(sizes) < 2 or tuple(sizes[-2:]) != self.matrix_shape:
            raise RuntimeError(
                'Invalid expand arguments {}. Currently, repeat only works to create repeated batches of a 2D LazyTensor.'
                .format(tuple(sizes)))
        elif all(isinstance(size, int) for size in sizes):
            shape = torch.Size(sizes)
        else:
            raise RuntimeError('Invalid arguments {} to expand.'.format(sizes))
        res = self._expand_batch(batch_shape=shape[:-2])
        return res

    @cached
    def evaluate(self):
        """
        Explicitly evaluates the matrix this LazyTensor represents. This function
        should return a Tensor storing an exact representation of this LazyTensor.
        """
        num_rows, num_cols = self.matrix_shape
        if num_rows < num_cols:
            eye = torch.eye(num_rows, dtype=self.dtype, device=self.device)
            eye = eye.expand(*self.batch_shape, num_rows, num_rows)
            res = self.transpose(-1, -2).matmul(eye).transpose(-1, -2
                ).contiguous()
        else:
            eye = torch.eye(num_cols, dtype=self.dtype, device=self.device)
            eye = eye.expand(*self.batch_shape, num_cols, num_cols)
            res = self.matmul(eye)
        return res

    def evaluate_kernel(self):
        """
        Return a new LazyTensor representing the same one as this one, but with
        all lazily evaluated kernels actually evaluated.
        """
        return self.representation_tree()(*self.representation())

    def inv_matmul(self, right_tensor, left_tensor=None):
        """
        Computes a linear solve (w.r.t self = :math:`A`) with several right hand sides :math:`R`.
        I.e. computes

        ... math::

            \\begin{equation}
                A^{-1} R,
            \\end{equation}

        where :math:`R` is :attr:`right_tensor` and :math:`A` is the LazyTensor.

        If :attr:`left_tensor` is supplied, computes

        ... math::

            \\begin{equation}
                L A^{-1} R,
            \\end{equation}

        where :math:`L` is :attr:`left_tensor`. Supplying this can reduce the number of
        CG calls required.

        Args:
            - :obj:`torch.tensor` (n x k) - Matrix :math:`R` right hand sides
            - :obj:`torch.tensor` (m x n) - Optional matrix :math:`L` to perform left multiplication with

        Returns:
            - :obj:`torch.tensor` - :math:`A^{-1}R` or :math:`LA^{-1}R`.
        """
        if not self.is_square:
            raise RuntimeError(
                'inv_matmul only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'
                .format(self.__class__.__name__, self.size()))
        if self.dim() == 2 and right_tensor.dim() == 1:
            if self.shape[-1] != right_tensor.numel():
                raise RuntimeError(
                    'LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'
                    .format(self.shape, right_tensor.shape))
        func = InvMatmul
        if left_tensor is None:
            return func.apply(self.representation_tree(), False,
                right_tensor, *self.representation())
        else:
            return func.apply(self.representation_tree(), True, left_tensor,
                right_tensor, *self.representation())

    def inv_quad(self, tensor, reduce_inv_quad=True):
        """
        Computes an inverse quadratic form (w.r.t self) with several right hand sides.
        I.e. computes tr( tensor^T self^{-1} tensor )

        NOTE: Don't overwrite this function!
        Instead, overwrite inv_quad_logdet

        Args:
            - tensor (tensor nxk) - Vector (or matrix) for inverse quad

        Returns:
            - tensor - tr( tensor^T (self)^{-1} tensor )
        """
        if not self.is_square:
            raise RuntimeError(
                'inv_quad only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'
                .format(self.__class__.__name__, self.size()))
        if self.dim() == 2 and tensor.dim() == 1:
            if self.shape[-1] != tensor.numel():
                raise RuntimeError(
                    'LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'
                    .format(self.shape, tensor.shape))
        elif self.dim() != tensor.dim():
            raise RuntimeError(
                'LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number of dimensions.'
                .format(self.shape, tensor.shape))
        elif self.shape[-1] != tensor.shape[-2]:
            raise RuntimeError(
                'LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'
                .format(self.shape, tensor.shape))
        args = (tensor,) + self.representation()
        func = InvQuad.apply
        inv_quad_term = func(self.representation_tree(), *args)
        if reduce_inv_quad:
            inv_quad_term = inv_quad_term.sum(-1)
        return inv_quad_term

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False,
        reduce_inv_quad=True):
        """
        Computes an inverse quadratic form (w.r.t self) with several right hand sides.
        I.e. computes tr( tensor^T self^{-1} tensor )
        In addition, computes an (approximate) log determinant of the the matrix

        Args:
            - tensor (tensor nxk) - Vector (or matrix) for inverse quad

        Returns:
            - scalar - tr( tensor^T (self)^{-1} tensor )
            - scalar - log determinant
        """
        if settings.fast_computations.log_prob.off() or self.size(-1
            ) <= settings.max_cholesky_size.value():
            from .chol_lazy_tensor import CholLazyTensor
            cholesky = CholLazyTensor(self.cholesky())
            return cholesky.inv_quad_logdet(inv_quad_rhs=inv_quad_rhs,
                logdet=logdet, reduce_inv_quad=reduce_inv_quad)
        if not self.is_square:
            raise RuntimeError(
                'inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'
                .format(self.__class__.__name__, self.size()))
        if inv_quad_rhs is not None:
            if self.dim() == 2 and inv_quad_rhs.dim() == 1:
                if self.shape[-1] != inv_quad_rhs.numel():
                    raise RuntimeError(
                        'LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'
                        .format(self.shape, inv_quad_rhs.shape))
            elif self.dim() != inv_quad_rhs.dim():
                raise RuntimeError(
                    'LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number of dimensions.'
                    .format(self.shape, inv_quad_rhs.shape))
            elif self.batch_shape != inv_quad_rhs.shape[:-2] or self.shape[-1
                ] != inv_quad_rhs.shape[-2]:
                raise RuntimeError(
                    'LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'
                    .format(self.shape, inv_quad_rhs.shape))
        args = self.representation()
        if inv_quad_rhs is not None:
            args = [inv_quad_rhs] + list(args)
        probe_vectors, probe_vector_norms = self._probe_vectors_and_norms()
        func = InvQuadLogDet.apply
        inv_quad_term, logdet_term = func(self.representation_tree(), self.
            dtype, self.device, self.matrix_shape, self.batch_shape, 
            inv_quad_rhs is not None, logdet, probe_vectors,
            probe_vector_norms, *args)
        if inv_quad_term.numel() and reduce_inv_quad:
            inv_quad_term = inv_quad_term.sum(-1)
        return inv_quad_term, logdet_term

    @property
    def is_square(self):
        return self.matrix_shape[0] == self.matrix_shape[1]

    def logdet(self):
        """
        Computes an (approximate) log determinant of the matrix

        NOTE: Don't overwrite this function!
        Instead, overwrite inv_quad_logdet

        Returns:
            - scalar: log determinant
        """
        _, res = self.inv_quad_logdet(inv_quad_rhs=None, logdet=True)
        return res

    def matmul(self, other):
        """
        Multiplies self by a matrix

        Args:
            other (:obj:`torch.tensor`): Matrix or vector to multiply with. Can be either a :obj:`torch.tensor`
                or a :obj:`gpytorch.lazy.LazyTensor`.

        Returns:
            :obj:`torch.tensor`: Tensor or LazyTensor containing the result of the matrix multiplication :math:`KM`,
            where :math:`K` is the (batched) matrix that this :obj:`gpytorch.lazy.LazyTensor` represents, and :math:`M`
            is the (batched) matrix input to this method.
        """
        _matmul_broadcast_shape(self.shape, other.shape)
        if isinstance(other, LazyTensor):
            from .matmul_lazy_tensor import MatmulLazyTensor
            return MatmulLazyTensor(self, other)
        func = Matmul()
        return func.apply(self.representation_tree(), other, *self.
            representation())

    @property
    def matrix_shape(self):
        """
        Returns the shape of the matrix being represented (without batching).
        """
        return torch.Size(self.shape[-2:])

    def mul(self, other):
        """
        Multiplies the matrix by a constant, or elementwise the matrix by another matrix

        Args:
            other (:obj:`torch.tensor` or :obj:`~gpytorch.lazy.LazyTensor`): constant or matrix to elementwise
            multiply by.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`: Another lazy tensor representing the result of the multiplication. if
            other was a constant (or batch of constants), this will likely be a
            :obj:`gpytorch.lazy.ConstantMulLazyTensor`. If other was
            another matrix, this will likely be a :obj:`gpytorch.lazy.MulLazyTensor`.
        """
        from .zero_lazy_tensor import ZeroLazyTensor
        from .non_lazy_tensor import lazify
        if isinstance(other, ZeroLazyTensor):
            return other
        if not (torch.is_tensor(other) or isinstance(other, LazyTensor)):
            other = torch.tensor(other, dtype=self.dtype, device=self.device)
        try:
            _mul_broadcast_shape(self.shape, other.shape)
        except RuntimeError:
            raise RuntimeError(
                'Cannot multiply LazyTensor of size {} by an object of size {}'
                .format(self.shape, other.shape))
        if torch.is_tensor(other):
            if other.numel() == 1:
                return self._mul_constant(other.squeeze())
            elif other.shape[-2:] == torch.Size((1, 1)):
                return self._mul_constant(other.view(*other.shape[:-2]))
        return self._mul_matrix(lazify(other))

    def ndimension(self):
        """
        Returns the number of dimensions
        """
        return len(self.size())

    def numel(self):
        """
        Returns the number of elements
        """
        return self.shape.numel()

    def numpy(self):
        """
        Return self as an evaluated numpy array
        """
        return self.evaluate().detach().cpu().numpy()

    def permute(self, *dims):
        num_dims = self.dim()
        orig_dims = dims
        dims = tuple(dim if dim >= 0 else dim + num_dims for dim in dims)
        if settings.debug.on():
            if len(dims) != num_dims:
                raise RuntimeError("number of dims don't match in permute")
            if sorted(set(dims)) != sorted(dims):
                raise RuntimeError('repeated dim in permute')
            for dim, orig_dim in zip(dims, orig_dims):
                if dim >= num_dims:
                    raise RuntimeError(
                        'Dimension out of range (expected to be in range of [{}, {}], but got {}.'
                        .format(-num_dims, num_dims - 1, orig_dim))
        if dims[-2:] != (num_dims - 2, num_dims - 1):
            raise ValueError(
                'At the moment, cannot permute the non-batch dimensions of LazyTensors.'
                )
        return self._permute_batch(*dims[:-2])

    def prod(self, dim=None):
        """
        For a `b x n x m` LazyTensor, compute the product over the batch dimension.

        The `mul_batch_size` controls whether or not the batch dimension is grouped when multiplying.
            * `mul_batch_size=None` (default): The entire batch dimension is multiplied. Returns a `n x n` LazyTensor.
            * `mul_batch_size=k`: Creates `b/k` groups, and muls the `k` entries of this group.
                (The LazyTensor is reshaped as a `b/k x k x n x m` LazyTensor and the `k` dimension is multiplied over.
                Returns a `b/k x n x m` LazyTensor.

        Args:
            :attr:`mul_batch_size` (int or None):
                Controls the number of groups that are multiplied over (default: None).

        Returns:
            :obj:`~gpytorch.lazy.LazyTensor`

        Example:
            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.tensor([
                    [[2, 4], [1, 2]],
                    [[1, 1], [0, -1]],
                    [[2, 1], [1, 0]],
                    [[3, 2], [2, -1]],
                ]))
            >>> lazy_tensor.mul_batch().evaluate()
            >>> # Returns: torch.Tensor([[12, 8], [0, 0]])
            >>> lazy_tensor.mul_batch(mul_batch_size=2)
            >>> # Returns: torch.Tensor([[[2, 4], [0, -2]], [[6, 2], [2, 0]]])
        """
        if dim is None:
            raise ValueError(
                'At the moment, LazyTensor.prod requires a dim argument (got None)'
                )
        orig_dim = dim
        if dim < 0:
            dim = self.dim() + dim
        if dim >= len(self.batch_shape):
            raise ValueError(
                'At the moment, LazyTensor.prod only works on batch dimensions. Got dim={} for LazyTensor of shape {}'
                .format(orig_dim, self.shape))
        return self._prod_batch(dim)

    def repeat(self, *sizes):
        """
        Repeats this tensor along the specified dimensions.

        Currently, this only works to create repeated batches of a 2D LazyTensor.
        I.e. all calls should be `lazy_tensor.repeat(<size>, 1, 1)`.

        Example:
            >>> lazy_tensor = gpytorch.lazy.ToeplitzLazyTensor(torch.tensor([4. 1., 0.5]))
            >>> lazy_tensor.repeat(2, 1, 1).evaluate()
            tensor([[[4.0000, 1.0000, 0.5000],
                     [1.0000, 4.0000, 1.0000],
                     [0.5000, 1.0000, 4.0000]],
                    [[4.0000, 1.0000, 0.5000],
                     [1.0000, 4.0000, 1.0000],
                     [0.5000, 1.0000, 4.0000]]])
        """
        from .batch_repeat_lazy_tensor import BatchRepeatLazyTensor
        if len(sizes) < 3 or tuple(sizes[-2:]) != (1, 1):
            raise RuntimeError(
                'Invalid repeat arguments {}. Currently, repeat only works to create repeated batches of a 2D LazyTensor.'
                .format(tuple(sizes)))
        return BatchRepeatLazyTensor(self, batch_repeat=torch.Size(sizes[:-2]))

    def representation(self):
        """
        Returns the Tensors that are used to define the LazyTensor
        """
        representation = []
        for arg in self._args:
            if torch.is_tensor(arg):
                representation.append(arg)
            elif hasattr(arg, 'representation') and callable(arg.representation
                ):
                representation += list(arg.representation())
            else:
                raise RuntimeError(
                    'Representation of a LazyTensor should consist only of Tensors'
                    )
        return tuple(representation)

    def representation_tree(self):
        """
        Returns a :obj:`gpytorch.lazy.LazyTensorRepresentationTree` tree object that recursively encodes the
        representation of this lazy tensor. In particular, if the definition of this lazy tensor depends on other
        lazy tensors, the tree is an object that can be used to reconstruct the full structure of this lazy tensor,
        including all subobjects. This is used internally.
        """
        return LazyTensorRepresentationTree(self)

    @property
    def requires_grad(self):
        return any(arg.requires_grad for arg in tuple(self._args) + tuple(
            self._kwargs.values()) if hasattr(arg, 'requires_grad'))

    @requires_grad.setter
    def requires_grad(self, val):
        for arg in self._args:
            if hasattr(arg, 'requires_grad'):
                if arg.dtype in (torch.float, torch.double, torch.half):
                    arg.requires_grad = val
        for arg in self._kwargs.values():
            if hasattr(arg, 'requires_grad'):
                arg.requires_grad = val

    def requires_grad_(self, val):
        """
        Sets `requires_grad=val` on all the Tensors that make up the LazyTensor
        This is an inplace operation.
        """
        self.requires_grad = val
        return self

    @cached(name='root_decomposition')
    def root_decomposition(self):
        """
        Returns a (usually low-rank) root decomposition lazy tensor of a PSD matrix.
        This can be used for sampling from a Gaussian distribution, or for obtaining a
        low-rank version of a matrix
        """
        from .chol_lazy_tensor import CholLazyTensor
        from .root_lazy_tensor import RootLazyTensor
        if not self.is_square:
            raise RuntimeError(
                'root_decomposition only operates on (batches of) square (symmetric) LazyTensors. Got a {} of size {}.'
                .format(self.__class__.__name__, self.size()))
        if self.size(-1) <= settings.max_cholesky_size.value(
            ) or settings.fast_computations.covar_root_decomposition.off():
            try:
                res = self.cholesky()
                return CholLazyTensor(res)
            except RuntimeError as e:
                warnings.warn(
                    'Runtime Error when computing Cholesky decomposition: {}. Using RootDecomposition.'
                    .format(e), NumericalWarning)
        res = self._root_decomposition()
        return RootLazyTensor(res)

    @cached(name='root_inv_decomposition')
    def root_inv_decomposition(self, initial_vectors=None, test_vectors=None):
        """
        Returns a (usually low-rank) root decomposotion lazy tensor of a PSD matrix.
        This can be used for sampling from a Gaussian distribution, or for obtaining a
        low-rank version of a matrix
        """
        from .root_lazy_tensor import RootLazyTensor
        from .non_lazy_tensor import lazify
        if self.shape[-2:].numel() == 1:
            return RootLazyTensor(1 / self.evaluate().sqrt())
        if self.size(-1) <= settings.max_cholesky_size.value(
            ) or settings.fast_computations.covar_root_decomposition.off():
            try:
                L = delazify(self.cholesky())
                Eye = torch.eye(L.shape[-2], device=L.device, dtype=L.dtype)
                Linv = torch.triangular_solve(Eye, L, upper=False)[0]
                res = lazify(Linv.transpose(-1, -2))
                return RootLazyTensor(res)
            except RuntimeError as e:
                warnings.warn(
                    'Runtime Error when computing Cholesky decomposition: {}. Using RootDecomposition.'
                    .format(e), NumericalWarning)
        if not self.is_square:
            raise RuntimeError(
                'root_inv_decomposition only operates on (batches of) square (symmetric) LazyTensors. Got a {} of size {}.'
                .format(self.__class__.__name__, self.size()))
        if initial_vectors is not None:
            if self.dim() == 2 and initial_vectors.dim() == 1:
                if self.shape[-1] != initial_vectors.numel():
                    raise RuntimeError(
                        'LazyTensor (size={}) cannot be multiplied with initial_vectors (size={}).'
                        .format(self.shape, initial_vectors.shape))
            elif self.dim() != initial_vectors.dim():
                raise RuntimeError(
                    'LazyTensor (size={}) and initial_vectors (size={}) should have the same number of dimensions.'
                    .format(self.shape, initial_vectors.shape))
            elif self.batch_shape != initial_vectors.shape[:-2] or self.shape[
                -1] != initial_vectors.shape[-2]:
                raise RuntimeError(
                    'LazyTensor (size={}) cannot be multiplied with initial_vectors (size={}).'
                    .format(self.shape, initial_vectors.shape))
        inv_roots = self._root_inv_decomposition(initial_vectors)
        if initial_vectors is not None and initial_vectors.size(-1) > 1:
            num_probes = initial_vectors.size(-1)
            test_vectors = test_vectors.unsqueeze(0)
            solves = inv_roots.matmul(inv_roots.transpose(-1, -2).matmul(
                test_vectors))
            solves = solves.permute(*range(1, self.dim() + 1), 0).contiguous(
                ).view(*self.batch_shape, self.matrix_shape[-1], -1)
            mat_times_solves = self.matmul(solves)
            mat_times_solves = mat_times_solves.view(*self.batch_shape,
                self.matrix_shape[-1], -1, num_probes).permute(-1, *range(0,
                self.dim()))
            residuals = (mat_times_solves - test_vectors).norm(2, dim=-2)
            residuals = residuals.view(residuals.size(0), -1).sum(-1)
            _, best_solve_index = residuals.min(0)
            inv_root = inv_roots[best_solve_index].squeeze(0)
        else:
            inv_root = inv_roots
        return RootLazyTensor(inv_root)

    def size(self, val=None):
        """
        Returns the size of the resulting Tensor that the lazy tensor represents
        """
        size = self._size()
        if val is not None:
            return size[val]
        return size

    def squeeze(self, dim):
        if self.size(dim) != 1:
            return self
        else:
            index = [_noop_index] * self.dim()
            index[dim] = 0
            index = tuple(index)
            return self[index]

    @property
    def shape(self):
        return self.size()

    def sum(self, dim=None):
        """
        Sum the LazyTensor across a dimension.
        The `dim` controls which batch dimension is summed over.
        If set to None, then sums all dimensions

        Args:
            :attr:`dim` (int):
                Which dimension is being summed over (default=None)

        Returns:
            :obj:`~gpytorch.lazy.LazyTensor` or Tensor.

        Example:
            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.tensor([
                    [[2, 4], [1, 2]],
                    [[1, 1], [0, -1]],
                    [[2, 1], [1, 0]],
                    [[3, 2], [2, -1]],
                ]))
            >>> lazy_tensor.sum(0).evaluate()
        """
        if dim is None:
            ones = torch.ones(self.size(-2), 1, dtype=self.dtype, device=
                self.device)
            return (self @ ones).sum()
        orig_dim = dim
        if dim < 0:
            dim = self.dim() + dim
        if dim == self.dim() - 1:
            ones = torch.ones(self.size(-1), 1, dtype=self.dtype, device=
                self.device)
            return (self @ ones).squeeze(-1)
        elif dim == self.dim() - 2:
            ones = torch.ones(self.size(-2), 1, dtype=self.dtype, device=
                self.device)
            return (self.transpose(-1, -2) @ ones).squeeze(-1)
        elif dim < self.dim():
            return self._sum_batch(dim)
        else:
            raise ValueError('Invalid dim ({}) for LazyTensor of size {}'.
                format(orig_dim, self.shape))

    def to(self, device_id):
        """
        A device-agnostic method of moving the lazy_tensor to the specified device.

        Args:
            device_id (:obj: `torch.device`): Which device to use (GPU or CPU).
        Returns:
            :obj:`~gpytorch.lazy.LazyTensor`: New LazyTensor identical to self on specified device
        """
        new_args = []
        new_kwargs = {}
        for arg in self._args:
            if hasattr(arg, 'to'):
                new_args.append(arg.to(device_id))
            else:
                new_args.append(arg)
        for name, val in self._kwargs.items():
            if hasattr(val, 'to'):
                new_kwargs[name] = val.to(device_id)
            else:
                new_kwargs[name] = val
        return self.__class__(*new_args, **new_kwargs)

    def t(self):
        """
        Alias of :meth:`~gpytorch.lazy.LazyTensor.transpose` for 2D LazyTensor.
        (Tranposes the two dimensions.)
        """
        if self.ndimension() != 2:
            raise RuntimeError('Cannot call t for more than 2 dimensions')
        return self.transpose(0, 1)

    def transpose(self, dim1, dim2):
        """
        Transpose the dimensions `dim1` and `dim2` of the LazyTensor.

        Example:
            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.randn(3, 5))
            >>> lazy_tensor.transpose(0, 1)
        """
        ndimension = self.ndimension()
        if dim1 < 0:
            dim1 = ndimension + dim1
        if dim2 < 0:
            dim2 = ndimension + dim2
        if dim1 >= ndimension or dim2 >= ndimension or not isinstance(dim1, int
            ) or not isinstance(dim2, int):
            raise RuntimeError('Invalid dimension')
        if dim1 < ndimension - 2 and dim2 < ndimension - 2:
            small_dim = dim1 if dim1 < dim2 else dim2
            large_dim = dim2 if dim1 < dim2 else dim1
            res = self._permute_batch(*range(small_dim), large_dim, *range(
                small_dim + 1, large_dim), small_dim, *range(large_dim + 1,
                ndimension - 2))
        elif dim1 >= ndimension - 2 and dim2 >= ndimension - 2:
            res = self._transpose_nonbatch()
        else:
            raise RuntimeError(
                'Cannot transpose batch dimension with non-batch dimension')
        return res

    def unsqueeze(self, dim):
        positive_dim = self.dim() + dim + 1 if dim < 0 else dim
        if positive_dim > len(self.batch_shape):
            raise ValueError(
                'Can only unsqueeze batch dimensions of {} (size {}). Got dim={}.'
                .format(self.__class__.__name__, self.shape, dim))
        res = self._unsqueeze_batch(positive_dim)
        return res

    def zero_mean_mvn_samples(self, num_samples):
        """
        Assumes that self is a covariance matrix, or a batch of covariance matrices.
        Returns samples from a zero-mean MVN, defined by self (as covariance matrix)

        Self should be symmetric, either (batch_size x num_dim x num_dim) or (num_dim x num_dim)

        Args:
            :attr:`num_samples` (int):
                Number of samples to draw.

        Returns:
            :obj:`torch.tensor`:
                Samples from MVN (num_samples x batch_size x num_dim) or (num_samples x num_dim)
        """
        if self.size()[-2:] == torch.Size([1, 1]):
            covar_root = self.evaluate().sqrt()
        else:
            covar_root = self.root_decomposition().root
        base_samples = torch.randn(*self.batch_shape, covar_root.size(-1),
            num_samples, dtype=self.dtype, device=self.device)
        samples = covar_root.matmul(base_samples).permute(-1, *range(self.
            dim() - 1)).contiguous()
        return samples

    def __add__(self, other):
        """
        Return a :obj:`gpytorch.lazy.LazyTensor` that represents the sum of this lazy tensor and another matrix
        or lazy tensor.

        Args:
            :attr:`other` (:obj:`torch.tensor` or :obj:`gpytorch.lazy.LazyTensor`):
                Matrix to add to this one.

        Returns:
            :obj:`gpytorch.lazy.SumLazyTensor`:
                A sum lazy tensor representing the sum of this lazy tensor and other.
        """
        from .sum_lazy_tensor import SumLazyTensor
        from .zero_lazy_tensor import ZeroLazyTensor
        from .diag_lazy_tensor import DiagLazyTensor
        from .added_diag_lazy_tensor import AddedDiagLazyTensor
        from .non_lazy_tensor import lazify
        from torch import Tensor
        if isinstance(other, ZeroLazyTensor):
            return self
        elif isinstance(other, DiagLazyTensor):
            return AddedDiagLazyTensor(self, other)
        elif isinstance(other, Tensor):
            other = lazify(other)
            shape = _mul_broadcast_shape(self.shape, other.shape)
            return SumLazyTensor(self.expand(shape), other.expand(shape))
        else:
            return SumLazyTensor(self, other)

    def __div__(self, other):
        """
        Return a :obj:`gpytorch.lazy.LazyTensor` that represents the product of this lazy tensor and
        the elementwise reciprocal of another matrix or lazy tensor.

        Args:
            :attr:`other` (:obj:`torch.tensor` or :obj:`gpytorch.lazy.LazyTensor`):
                Matrix to divide this one by.

        Returns:
            :obj:`gpytorch.lazy.MulLazyTensor`:
                Result of division.
        """
        from .zero_lazy_tensor import ZeroLazyTensor
        if isinstance(other, ZeroLazyTensor):
            raise RuntimeError(
                'Attempted to divide by a ZeroLazyTensor (divison by zero)')
        return self.mul(1.0 / other)

    def __getitem__(self, index):
        """
        Supports subindexing of the matrix this LazyTensor represents. This may return either another
        :obj:`gpytorch.lazy.LazyTensor` or a :obj:`torch.tensor` depending on the exact implementation.
        """
        ndimension = self.ndimension()
        index = index if isinstance(index, tuple) else (index,)
        index = tuple(torch.tensor(idx) if isinstance(idx, list) else idx for
            idx in index)
        index = tuple(idx.item() if torch.is_tensor(idx) and not len(idx.
            shape) else idx for idx in index)
        ellipsis_locs = tuple(index for index, item in enumerate(index) if 
            item is Ellipsis)
        if settings.debug.on():
            if len(ellipsis_locs) > 1:
                raise RuntimeError(
                    'Cannot have multiple ellipsis in a __getitem__ call. LazyTensor {}  received index {}.'
                    .format(self, index))
        if len(ellipsis_locs) == 1:
            ellipsis_loc = ellipsis_locs[0]
            num_to_fill_in = ndimension - (len(index) - 1)
            index = index[:ellipsis_loc] + tuple(_noop_index for _ in range
                (num_to_fill_in)) + index[ellipsis_loc + 1:]
        index = index + tuple(_noop_index for _ in range(ndimension - len(
            index)))
        *batch_indices, row_index, col_index = index
        batch_has_tensor_index = bool(len(batch_indices)) and any(torch.
            is_tensor(index) for index in batch_indices)
        row_has_tensor_index = torch.is_tensor(row_index)
        col_has_tensor_index = torch.is_tensor(col_index)
        row_col_are_absorbed = any((batch_has_tensor_index and (
            row_has_tensor_index or col_has_tensor_index), not
            batch_has_tensor_index and (row_has_tensor_index and
            col_has_tensor_index)))
        squeeze_row = False
        squeeze_col = False
        if isinstance(row_index, int):
            row_index = slice(row_index, row_index + 1, None)
            squeeze_row = True
        if isinstance(col_index, int):
            col_index = slice(col_index, col_index + 1, None)
            squeeze_col = True
        if row_col_are_absorbed:
            *batch_indices, row_index, col_index = _convert_indices_to_tensors(
                self, (*batch_indices, row_index, col_index))
            res = self._get_indices(row_index, col_index, *batch_indices)
        else:
            res = self._getitem(row_index, col_index, *batch_indices)
        if squeeze_row or squeeze_col or row_col_are_absorbed:
            res = delazify(res)
        if squeeze_row:
            res = res.squeeze(-2)
        if squeeze_col:
            res = res.squeeze(-1)
        if settings.debug.on() and self.__class__._check_size:
            expected_shape = _compute_getitem_size(self, index)
            if expected_shape != res.shape:
                raise RuntimeError(
                    '{}.__getitem__ failed! Expected a final shape of size {}, got {}. This is a bug with GPyTorch, or your custom LazyTensor.'
                    .format(self.__class__.__name__, expected_shape, res.shape)
                    )
        return res

    def __matmul__(self, other):
        return self.matmul(other)

    def __mul__(self, other):
        return self.mul(other)

    def __radd__(self, other):
        return self + other

    def __rmul__(self, other):
        return self.mul(other)

    def __sub__(self, other):
        return self + other.mul(-1)


class BatchRepeatLazyTensor(LazyTensor):

    def __init__(self, base_lazy_tensor, batch_repeat=torch.Size((1,))):
        if settings.debug.on():
            if not isinstance(batch_repeat, torch.Size):
                raise RuntimeError(
                    'batch_repeat must be a torch.Size, got a {} instead'.
                    format(batch_repeat.__class__.__name__))
            if isinstance(base_lazy_tensor, BatchRepeatLazyTensor):
                raise RuntimeError(
                    """BatchRepeatLazyTensor recieved the following args:
base_lazy_tensor: {} (size: {}), batch_repeat: {}."""
                    .format(base_lazy_tensor, base_lazy_tensor.shape,
                    batch_repeat))
        for _ in range(len(batch_repeat) + 2 - base_lazy_tensor.dim()):
            base_lazy_tensor = base_lazy_tensor.unsqueeze(0)
        super().__init__(base_lazy_tensor, batch_repeat=batch_repeat)
        self.base_lazy_tensor = base_lazy_tensor
        self.batch_repeat = batch_repeat

    @cached(name='cholesky')
    def _cholesky(self):
        res = self.base_lazy_tensor._cholesky()
        res = res.repeat(*self.batch_repeat, 1, 1)
        return res

    def _cholesky_solve(self, rhs):
        output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)
        if rhs.shape != output_shape:
            rhs = rhs.expand(*output_shape)
        rhs = self._move_repeat_batches_to_columns(rhs, output_shape)
        res = self.base_lazy_tensor._cholesky_solve(rhs)
        res = self._move_repeat_batches_back(res, output_shape)
        return res

    def _compute_batch_repeat_size(self, current_batch_shape,
        desired_batch_shape):
        batch_repeat = torch.Size(desired_batch_size // current_batch_size for
            desired_batch_size, current_batch_size in zip(
            desired_batch_shape, current_batch_shape))
        return batch_repeat

    def _expand_batch(self, batch_shape):
        padding_dims = torch.Size(tuple(1 for _ in range(max(len(
            batch_shape) + 2 - self.base_lazy_tensor.dim(), 0))))
        current_batch_shape = padding_dims + self.base_lazy_tensor.batch_shape
        return self.__class__(self.base_lazy_tensor, batch_repeat=self.
            _compute_batch_repeat_size(current_batch_shape, batch_shape))

    def _get_indices(self, row_index, col_index, *batch_indices):
        num_true_batch_indices = self.base_lazy_tensor.dim() - 2
        batch_indices = batch_indices[len(batch_indices) -
            num_true_batch_indices:]
        batch_indices = [batch_index.fmod(size) for batch_index, size in
            zip(batch_indices, self.base_lazy_tensor.batch_shape)]
        res = self.base_lazy_tensor._get_indices(row_index, col_index, *
            batch_indices)
        return res

    def _getitem(self, row_index, col_index, *batch_indices):
        args = []
        kwargs = self.base_lazy_tensor._kwargs
        num_base_batch_dims = len(self.base_lazy_tensor.batch_shape)
        for arg in self.base_lazy_tensor._args:
            if torch.is_tensor(arg) or isinstance(arg, LazyTensor):
                arg_base_shape_len = max(arg.dim() - num_base_batch_dims, 0)
                args.append(arg.repeat(*self.batch_repeat, *[(1) for _ in
                    range(arg_base_shape_len)]))
            else:
                args.append(arg)
        new_lazy_tensor = self.base_lazy_tensor.__class__(*args, **kwargs)
        return new_lazy_tensor._getitem(row_index, col_index, *batch_indices)

    def _matmul(self, rhs):
        output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)
        if self.is_square:
            if rhs.shape != output_shape:
                rhs = rhs.expand(*output_shape)
            rhs = self._move_repeat_batches_to_columns(rhs, output_shape)
            res = self.base_lazy_tensor._matmul(rhs)
            res = self._move_repeat_batches_back(res, output_shape)
            return res
        else:
            res = self.base_lazy_tensor._matmul(rhs)
            if res.shape != output_shape:
                res = res.expand(*output_shape)
            return res

    def _move_repeat_batches_back(self, batch_matrix, output_shape):
        """
        The opposite of _move_repeat_batches_to_columns

        Takes a b x m x nr tensor, and moves the batches associated with repeating
        So that the tensor is now rb x m x n.
        """
        if hasattr(self, '_batch_move_memo'):
            padded_base_batch_shape, batch_repeat = self.__batch_move_memo
            del self.__batch_move_memo
        else:
            padding_dims = torch.Size(tuple(1 for _ in range(max(len(
                output_shape) - self.base_lazy_tensor.dim(), 0))))
            padded_base_batch_shape = (padding_dims + self.base_lazy_tensor
                .batch_shape)
            batch_repeat = self._compute_batch_repeat_size(
                padded_base_batch_shape, output_shape[:-2])
        batch_matrix = batch_matrix.view(*padded_base_batch_shape,
            output_shape[-2], -1, *batch_repeat)
        output_dims = len(output_shape)
        dims = tuple(itertools.chain.from_iterable([i + output_dims, i] for
            i in range(len(padded_base_batch_shape)))) + (output_dims - 2, 
            output_dims - 1)
        batch_matrix = batch_matrix.permute(*dims).contiguous()
        batch_matrix = batch_matrix.view(*output_shape)
        return batch_matrix

    def _move_repeat_batches_to_columns(self, batch_matrix, output_shape):
        """
        Takes a rb x m x n tensor, and moves the batches associated with repeating
        So that the tensor is now b x m x nr.
        This allows us to use the base_lazy_tensor routines.
        """
        padding_dims = torch.Size(tuple(1 for _ in range(max(len(
            output_shape) - self.base_lazy_tensor.dim(), 0))))
        padded_base_batch_shape = (padding_dims + self.base_lazy_tensor.
            batch_shape)
        batch_repeat = self._compute_batch_repeat_size(padded_base_batch_shape,
            output_shape[:-2])
        split_shape = torch.Size(tuple(itertools.chain.from_iterable([
            repeat, size] for repeat, size in zip(batch_repeat,
            padded_base_batch_shape))) + output_shape[-2:])
        batch_matrix = batch_matrix.view(*split_shape)
        repeat_dims = range(0, len(batch_repeat) * 2, 2)
        batch_dims = range(1, len(batch_repeat) * 2, 2)
        batch_matrix = batch_matrix.permute(*batch_dims, -2, -1, *repeat_dims
            ).contiguous()
        batch_matrix = batch_matrix.view(*self.base_lazy_tensor.batch_shape,
            output_shape[-2], -1)
        self.__batch_move_memo = (output_shape, padded_base_batch_shape,
            batch_repeat)
        return batch_matrix

    def _permute_batch(self, *dims):
        new_batch_repeat = torch.Size(tuple(self.batch_repeat[dim] for dim in
            dims))
        res = self.__class__(self.base_lazy_tensor._permute_batch(*dims),
            batch_repeat=new_batch_repeat)
        return res

    def _quad_form_derivative(self, left_vectors, right_vectors):
        if self.is_square:
            left_output_shape = _matmul_broadcast_shape(self.shape,
                left_vectors.shape)
            if left_output_shape != left_vectors.shape:
                left_vectors = left_vectors.expand(left_output_shape)
            right_output_shape = _matmul_broadcast_shape(self.shape,
                right_vectors.shape)
            if right_output_shape != right_vectors.shape:
                right_vectors = right_vectors.expand(right_output_shape)
            left_vectors = self._move_repeat_batches_to_columns(left_vectors,
                left_output_shape)
            right_vectors = self._move_repeat_batches_to_columns(right_vectors,
                right_output_shape)
            return self.base_lazy_tensor._quad_form_derivative(left_vectors,
                right_vectors)
        else:
            return super()._quad_form_derivative(left_vectors, right_vectors)

    def _root_decomposition(self):
        return self.base_lazy_tensor._root_decomposition().repeat(*self.
            batch_repeat, 1, 1)

    def _root_inv_decomposition(self, initial_vectors=None):
        return self.base_lazy_tensor._root_inv_decomposition().repeat(*self
            .batch_repeat, 1, 1)

    def _size(self):
        repeated_batch_shape = torch.Size(size * repeat for size, repeat in
            zip(self.base_lazy_tensor.batch_shape, self.batch_repeat))
        res = torch.Size(repeated_batch_shape + self.base_lazy_tensor.
            matrix_shape)
        return res

    def _transpose_nonbatch(self):
        return self.__class__(self.base_lazy_tensor._transpose_nonbatch(),
            batch_repeat=self.batch_repeat)

    def _unsqueeze_batch(self, dim):
        base_lazy_tensor = self.base_lazy_tensor
        batch_repeat = list(self.batch_repeat)
        batch_repeat.insert(dim, 1)
        batch_repeat = torch.Size(batch_repeat)
        base_unsqueeze_dim = dim - (len(self.base_lazy_tensor.batch_shape) -
            len(self.base_lazy_tensor.batch_shape))
        if base_unsqueeze_dim > 0:
            base_lazy_tensor = base_lazy_tensor._unsqueeze_batch(
                base_unsqueeze_dim)
        return self.__class__(base_lazy_tensor, batch_repeat=batch_repeat)

    def add_jitter(self, jitter_val=0.001):
        return self.__class__(self.base_lazy_tensor.add_jitter(jitter_val=
            jitter_val), batch_repeat=self.batch_repeat)

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False,
        reduce_inv_quad=True):
        if not self.is_square:
            raise RuntimeError(
                'inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'
                .format(self.__class__.__name__, self.size()))
        if inv_quad_rhs is not None:
            if self.dim() != inv_quad_rhs.dim():
                raise RuntimeError(
                    'LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number of dimensions.'
                    .format(self.shape, inv_quad_rhs.shape))
            elif self.batch_shape != inv_quad_rhs.shape[:-2] or self.shape[-1
                ] != inv_quad_rhs.shape[-2]:
                raise RuntimeError(
                    'LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'
                    .format(self.shape, inv_quad_rhs.shape))
        if inv_quad_rhs is not None:
            output_shape = _matmul_broadcast_shape(self.shape, inv_quad_rhs
                .shape)
            inv_quad_rhs = self._move_repeat_batches_to_columns(inv_quad_rhs,
                output_shape)
        inv_quad_term, logdet_term = self.base_lazy_tensor.inv_quad_logdet(
            inv_quad_rhs, logdet, reduce_inv_quad=False)
        if inv_quad_term is not None and inv_quad_term.numel():
            inv_quad_term = inv_quad_term.view(*inv_quad_term.shape[:-1], -
                1, 1, self.batch_repeat.numel())
            output_shape = list(output_shape)
            output_shape[-2] = 1
            inv_quad_term = self._move_repeat_batches_back(inv_quad_term,
                output_shape).squeeze(-2)
            if reduce_inv_quad:
                inv_quad_term = inv_quad_term.sum(-1)
        if logdet_term is not None and logdet_term.numel():
            logdet_term = logdet_term.repeat(*self.batch_repeat)
        return inv_quad_term, logdet_term

    def repeat(self, *sizes):
        if len(sizes) < 3 or tuple(sizes[-2:]) != (1, 1):
            raise RuntimeError(
                'Invalid repeat arguments {}. Currently, repeat only works to create repeated batches of a 2D LazyTensor.'
                .format(tuple(sizes)))
        padded_batch_repeat = tuple(1 for _ in range(len(sizes) - 2 - len(
            self.batch_repeat))) + self.batch_repeat
        return self.__class__(self.base_lazy_tensor, batch_repeat=torch.
            Size(orig_repeat_size * new_repeat_size for orig_repeat_size,
            new_repeat_size in zip(padded_batch_repeat, sizes[:-2])))


def lazify(obj):
    """
    A function which ensures that `obj` is a LazyTensor.

    If `obj` is a LazyTensor, this function does nothing.
    If `obj` is a (normal) Tensor, this function wraps it with a `NonLazyTensor`.
    """
    if torch.is_tensor(obj):
        return NonLazyTensor(obj)
    elif isinstance(obj, LazyTensor):
        return obj
    else:
        raise TypeError('object of class {} cannot be made into a LazyTensor'
            .format(obj.__class__.__name__))


class MatmulLazyTensor(LazyTensor):

    def __init__(self, left_lazy_tensor, right_lazy_tensor):
        left_lazy_tensor = lazify(left_lazy_tensor)
        right_lazy_tensor = lazify(right_lazy_tensor)
        super().__init__(left_lazy_tensor, right_lazy_tensor)
        self.left_lazy_tensor = left_lazy_tensor
        self.right_lazy_tensor = right_lazy_tensor

    def _expand_batch(self, batch_shape):
        return self.__class__(self.left_lazy_tensor._expand_batch(
            batch_shape), self.right_lazy_tensor._expand_batch(batch_shape))

    def _get_indices(self, row_index, col_index, *batch_indices):
        row_index = row_index.unsqueeze(-1)
        col_index = col_index.unsqueeze(-1)
        batch_indices = tuple(batch_index.unsqueeze(-1) for batch_index in
            batch_indices)
        inner_index = torch.arange(0, self.left_lazy_tensor.size(-1),
            device=self.device)
        inner_index = _pad_with_singletons(inner_index, row_index.dim() - 1, 0)
        left_tensor = self.left_lazy_tensor._get_indices(row_index,
            inner_index, *batch_indices)
        right_tensor = self.right_lazy_tensor._get_indices(inner_index,
            col_index, *batch_indices)
        res = (left_tensor * right_tensor).sum(-1)
        return res

    def _getitem(self, row_index, col_index, *batch_indices):
        if torch.is_tensor(row_index) and torch.is_tensor(col_index):
            num_indices = row_index.numel()
            if num_indices > self.matrix_shape.numel():
                return lazify(self.evaluate())._getitem(row_index,
                    col_index, *batch_indices)
        left_tensor = self.left_lazy_tensor._getitem(row_index, _noop_index,
            *batch_indices)
        right_tensor = self.right_lazy_tensor._getitem(_noop_index,
            col_index, *batch_indices)
        res = MatmulLazyTensor(left_tensor, right_tensor)
        return res

    def _matmul(self, right_lazy_tensor):
        return self.left_lazy_tensor._matmul(self.right_lazy_tensor._matmul
            (right_lazy_tensor))

    def _t_matmul(self, right_lazy_tensor):
        return self.right_lazy_tensor._t_matmul(self.left_lazy_tensor.
            _t_matmul(right_lazy_tensor))

    def _quad_form_derivative(self, left_vecs, right_vecs):
        if left_vecs.ndimension() == 1:
            left_vecs = left_vecs.unsqueeze(1)
            right_vecs = right_vecs.unsqueeze(1)
        right_vecs_times_right_lazy_tensor = self.right_lazy_tensor._matmul(
            right_vecs)
        left_vecs_times_left_lazy_tensor_t = self.left_lazy_tensor._t_matmul(
            left_vecs)
        left_grad = self.left_lazy_tensor._quad_form_derivative(left_vecs,
            right_vecs_times_right_lazy_tensor)
        right_grad = self.right_lazy_tensor._quad_form_derivative(
            left_vecs_times_left_lazy_tensor_t, right_vecs)
        left_grad = (left_grad,) if not isinstance(left_grad, tuple
            ) else left_grad
        right_grad = (right_grad,) if not isinstance(right_grad, tuple
            ) else right_grad
        return left_grad + right_grad

    def _size(self):
        return _matmul_broadcast_shape(self.left_lazy_tensor.shape, self.
            right_lazy_tensor.shape)

    def _transpose_nonbatch(self, *args):
        return self.__class__(self.right_lazy_tensor._transpose_nonbatch(),
            self.left_lazy_tensor._transpose_nonbatch())

    def diag(self):
        if isinstance(self.left_lazy_tensor, NonLazyTensor) and isinstance(self
            .right_lazy_tensor, NonLazyTensor):
            return (self.left_lazy_tensor.tensor * self.right_lazy_tensor.
                tensor.transpose(-1, -2)).sum(-1)
        elif isinstance(self.left_lazy_tensor, DiagLazyTensor) or isinstance(
            self.right_lazy_tensor, DiagLazyTensor):
            return self.left_lazy_tensor.diag() * self.right_lazy_tensor.diag()
        else:
            return super().diag()

    @cached
    def evaluate(self):
        return torch.matmul(self.left_lazy_tensor.evaluate(), self.
            right_lazy_tensor.evaluate())


def _equal_indices(a, b):
    """
    Helper which checks whether two index components (int, slice, tensor) are equal
    """
    if torch.is_tensor(a) and torch.is_tensor(b):
        return torch.equal(a, b)
    elif not torch.is_tensor(a) and not torch.is_tensor(b):
        return a == b
    else:
        return False


class RootLazyTensor(LazyTensor):

    def __init__(self, root):
        root = lazify(root)
        super(RootLazyTensor, self).__init__(root)
        self.root = root

    def _expand_batch(self, batch_shape):
        return self.__class__(self.root._expand_batch(batch_shape))

    def _get_indices(self, row_index, col_index, *batch_indices):
        row_index = row_index.unsqueeze(-1)
        col_index = col_index.unsqueeze(-1)
        batch_indices = tuple(batch_index.unsqueeze(-1) for batch_index in
            batch_indices)
        inner_index = torch.arange(0, self.root.size(-1), device=self.device)
        inner_index = _pad_with_singletons(inner_index, row_index.dim() - 1, 0)
        left_tensor = self.root._get_indices(row_index, inner_index, *
            batch_indices)
        if torch.equal(row_index, col_index):
            res = left_tensor.pow(2).sum(-1)
        else:
            right_tensor = self.root._get_indices(col_index, inner_index, *
                batch_indices)
            res = (left_tensor * right_tensor).sum(-1)
        return res

    def _getitem(self, row_index, col_index, *batch_indices):
        if torch.is_tensor(row_index) and torch.is_tensor(col_index):
            num_indices = row_index.numel()
            if num_indices > self.matrix_shape.numel():
                return lazify(self.evaluate())._getitem(row_index,
                    col_index, *batch_indices)
        left_tensor = self.root._getitem(row_index, _noop_index, *batch_indices
            )
        if _equal_indices(row_index, col_index):
            res = self.__class__(left_tensor)
        else:
            right_tensor = self.root._getitem(col_index, _noop_index, *
                batch_indices)
            res = MatmulLazyTensor(left_tensor, right_tensor.transpose(-1, -2))
        return res

    def _matmul(self, rhs):
        return self.root._matmul(self.root._t_matmul(rhs))

    def _t_matmul(self, rhs):
        return self._matmul(rhs)

    def _quad_form_derivative(self, left_vecs, right_vecs):
        right_vecs_times_rhs = self.root._t_matmul(right_vecs)
        left_vecs_times_lhs_t = self.root._t_matmul(left_vecs)
        deriv_part_1 = self.root._quad_form_derivative(left_vecs,
            right_vecs_times_rhs)
        deriv_part_2 = self.root._quad_form_derivative(right_vecs,
            left_vecs_times_lhs_t)
        deriv = []
        for item_part_1, item_part_2 in zip(deriv_part_1, deriv_part_2):
            deriv.append(item_part_1 + item_part_2)
        return tuple(deriv)

    def _root_decomposition(self):
        return self.root

    def _root_decomposition_size(self):
        return self.root.size(-1)

    def _size(self):
        return torch.Size((*self.root.batch_shape, self.root.size(-2), self
            .root.size(-2)))

    def _transpose_nonbatch(self):
        return self

    def diag(self):
        if isinstance(self.root, NonLazyTensor):
            return (self.root.tensor ** 2).sum(-1)
        else:
            return super(RootLazyTensor, self).diag()

    @cached
    def evaluate(self):
        eval_root = self.root.evaluate()
        return torch.matmul(eval_root, eval_root.transpose(-1, -2))


class ZeroLazyTensor(LazyTensor):
    """
    Special LazyTensor representing zero.
    """

    def __init__(self, *sizes, dtype=None, device=None):
        super(ZeroLazyTensor, self).__init__(*sizes)
        self.sizes = list(sizes)
        self._dtype = dtype or torch.get_default_dtype()
        self._device = device or torch.device('cpu')

    @property
    def dtype(self):
        return self._dtype

    @property
    def device(self):
        return self._device

    def _expand_batch(self, batch_shape):
        return self.__class__(*batch_shape, *self.sizes[-2:], dtype=self.
            _dtype, device=self._device)

    def _get_indices(self, row_index, col_index, *batch_indices):
        new_size = _compute_getitem_size(self, batch_indices + (row_index,
            col_index))
        return ZeroLazyTensor(*new_size)

    def _getitem(self, row_index, col_index, *batch_indices):
        new_size = _compute_getitem_size(self, batch_indices + (row_index,
            col_index))
        return ZeroLazyTensor(*new_size)

    def _matmul(self, rhs):
        rhs_size_ind = -2 if rhs.ndimension() > 1 else -1
        if self.size(-1) != rhs.size(rhs_size_ind):
            raise RuntimeError('Size mismatch, self: {}, rhs: {}'.format(
                self.size(), rhs.size()))
        return rhs * 0

    def _prod_batch(self, dim):
        sizes = list(self.sizes)
        del sizes[dim]
        return self.__class__(*sizes, dtype=self._dtype, device=self._device)

    def _quad_form_derivative(self, left_vecs, right_vecs):
        raise RuntimeError('Backwards through a ZeroLazyTensor is not possible'
            )

    def _root_decomposition(self):
        raise RuntimeError('ZeroLazyTensors are not positive definite!')

    def _root_inv_decomposition(self, initial_vectors=None):
        raise RuntimeError('ZeroLazyTensors are not positive definite!')

    def _root_decomposition_size(self):
        raise RuntimeError('ZeroLazyTensors are not positive definite!')

    def _size(self):
        return torch.Size(self.sizes)

    def _sum_batch(self, dim):
        sizes = list(self.sizes)
        del sizes[dim]
        return self.__class__(*sizes, dtype=self._dtype, device=self._device)

    def _t_matmul(self, rhs):
        rhs_size_ind = -2 if rhs.ndimension() > 1 else -1
        if self.size(-1) != rhs.size(rhs_size_ind):
            raise RuntimeError('Size mismatch, self: {}, rhs: {}'.format(
                self.size(), rhs.size()))
        return rhs * 0

    def _transpose_nonbatch(self):
        return self.transpose(-2, -1)

    def _unsqueeze_batch(self, dim):
        sizes = self.sizes.copy()
        sizes.insert(dim, 1)
        return self.__class__(*sizes, dtype=self._dtype, device=self._device)

    def add_diag(self, diag):
        from .diag_lazy_tensor import DiagLazyTensor
        if self.size(-1) != self.size(-2):
            raise RuntimeError('add_diag only defined for square matrices')
        if self.ndimension() == 3:
            if diag.ndimension() == 0:
                diag = diag.view(1, 1).expand(self.size(0), self.size(1))
            elif diag.ndimension() == 1:
                diag = diag.unsqueeze(0).expand(self.size(0), self.size(1))
            elif diag.ndimension() == 2:
                diag = diag.expand(self.size(0), self.size(1))
            else:
                raise RuntimeError(
                    'For a 3D tensor ({}), add_diag expects a 1D or 2D diag. Got size ({})'
                    .format(self.size(), diag.size()))
        elif diag.ndimension() == 0:
            diag = diag.view(1).expand(self.size(0))
        elif diag.ndimension() == 1:
            diag = diag.expand(self.size(0))
        else:
            raise RuntimeError(
                'For a 3D tensor ({}), add_diag expects a 1D or 2D diag. Got size ({})'
                .format(self.size(), diag.size()))
        res = DiagLazyTensor(diag)
        if res.size() != self.size():
            raise RuntimeError(
                'Diag dimensions are incompatible with the base LazyTensor dimensions. Diag size corresponds to a {} Tensor - expected {}'
                .format(res.size(), self.size()))
        return res

    def diag(self):
        shape = self.shape
        if shape[-1] != shape[-2]:
            raise RuntimeError('diag works on square matrices (or batches)')
        return torch.zeros(shape[:-1], dtype=self.dtype, device=self.device)

    @cached
    def evaluate(self):
        return torch.zeros(*self.sizes)

    def inv_matmul(self, right_tensor, left_tensor=None):
        raise RuntimeError('ZeroLazyTensors are not invertible!')

    def inv_quad(self, tensor):
        raise RuntimeError('ZeroLazyTensors are not invertible!')

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False,
        reduce_inv_quad=True):
        raise RuntimeError('ZeroLazyTensors are not invertible!')

    def logdet(self):
        return torch.log(torch.tensor(0.0))

    def matmul(self, tensor):
        tensor_size_ind = -2 if tensor.ndimension() > 1 else -1
        if self.size(-1) != tensor.size(tensor_size_ind):
            raise RuntimeError('Size mismatch, self: {}, tensor: {}'.format
                (self.size(), tensor.size()))
        return tensor * 0

    def mul(self, other):
        shape = _mul_broadcast_shape(self.shape, other.shape)
        return self.__class__(*shape, dtype=self._dtype, device=self._device)

    def transpose(self, dim1, dim2):
        sizes = self.sizes.copy()
        tmp = sizes[dim1]
        sizes[dim1] = sizes[dim2]
        sizes[dim2] = tmp
        return ZeroLazyTensor(*sizes)

    def __add__(self, other):
        return other

    def __div__(self, other):
        return self

    def __mul__(self, other):
        return self


def clear_cache_hook(module, *args, **kwargs):
    module._memoize_cache = {}


class DefaultPredictionStrategy(object):

    def __init__(self, train_inputs, train_prior_dist, train_labels,
        likelihood, root=None, inv_root=None):
        train_shape = train_prior_dist.event_shape
        train_labels = train_labels.view(*train_labels.shape[:-len(
            train_shape)], train_shape.numel())
        self.train_inputs = train_inputs
        self.train_prior_dist = train_prior_dist
        self.train_labels = train_labels
        self.likelihood = likelihood
        self._last_test_train_covar = None
        mvn = self.likelihood(train_prior_dist, train_inputs)
        self.lik_train_train_covar = mvn.lazy_covariance_matrix
        if root is not None:
            add_to_cache(self.lik_train_train_covar, 'root_decomposition',
                RootLazyTensor(root))
        if inv_root is not None:
            add_to_cache(self.lik_train_train_covar,
                'root_inv_decomposition', RootLazyTensor(inv_root))

    def __deepcopy__(self, memo):
        pass

    def _exact_predictive_covar_inv_quad_form_cache(self,
        train_train_covar_inv_root, test_train_covar):
        """
        Computes a cache for K_X*X (K_XX + sigma^2 I)^-1 K_X*X if possible. By default, this does no work and returns
        the first argument.

        Args:
            train_train_covar_inv_root (:obj:`torch.tensor`): a root of (K_XX + sigma^2 I)^-1
            test_train_covar (:obj:`torch.tensor`): the observed noise (from the likelihood)

        Returns
            - A precomputed cache
        """
        res = train_train_covar_inv_root
        if settings.detach_test_caches.on():
            res = res.detach()
        if res.grad_fn is not None:
            wrapper = functools.partial(clear_cache_hook, self)
            functools.update_wrapper(wrapper, clear_cache_hook)
            res.grad_fn.register_hook(wrapper)
        return res

    def _exact_predictive_covar_inv_quad_form_root(self, precomputed_cache,
        test_train_covar):
        """
        Computes :math:`K_{X^{*}X} S` given a precomputed cache
        Where :math:`S` is a tensor such that :math:`SS^{\\top} = (K_{XX} + \\sigma^2 I)^{-1}`

        Args:
            precomputed_cache (:obj:`torch.tensor`): What was computed in _exact_predictive_covar_inv_quad_form_cache
            test_train_covar (:obj:`torch.tensor`): The observed noise (from the likelihood)

        Returns
            :obj:`~gpytorch.lazy.LazyTensor`: :math:`K_{X^{*}X} S`
        """
        return test_train_covar.matmul(precomputed_cache)

    def get_fantasy_strategy(self, inputs, targets, full_inputs,
        full_targets, full_output, **kwargs):
        """
        Returns a new PredictionStrategy that incorporates the specified inputs and targets as new training data.

        This method is primary responsible for updating the mean and covariance caches. To add fantasy data to a
        GP model, use the :meth:`~gpytorch.models.ExactGP.get_fantasy_model` method.

        Args:
            - :attr:`inputs` (Tensor `b1 x ... x bk x m x d` or `f x b1 x ... x bk x m x d`): Locations of fantasy
                observations.
            - :attr:`targets` (Tensor `b1 x ... x bk x m` or `f x b1 x ... x bk x m`): Labels of fantasy observations.
            - :attr:`full_inputs` (Tensor `b1 x ... x bk x n+m x d` or `f x b1 x ... x bk x n+m x d`): Training data
                concatenated with fantasy inputs
            - :attr:`full_targets` (Tensor `b1 x ... x bk x n+m` or `f x b1 x ... x bk x n+m`): Training labels
                concatenated with fantasy labels.
            - :attr:`full_output` (:class:`gpytorch.distributions.MultivariateNormal`): Prior called on full_inputs

        Returns:
            - :class:`DefaultPredictionStrategy`
                A `DefaultPredictionStrategy` model with `n + m` training examples, where the `m` fantasy examples have
                been added and all test-time caches have been updated.
        """
        full_mean, full_covar = (full_output.mean, full_output.
            lazy_covariance_matrix)
        batch_shape = full_inputs[0].shape[:-2]
        full_mean = full_mean.view(*batch_shape, -1)
        num_train = self.num_train
        fant_fant_covar = full_covar[(...), num_train:, num_train:]
        fant_mean = full_mean[(...), num_train:]
        mvn = self.train_prior_dist.__class__(fant_mean, fant_fant_covar)
        fant_likelihood = self.likelihood.get_fantasy_likelihood(**kwargs)
        mvn_obs = fant_likelihood(mvn, inputs, **kwargs)
        fant_fant_covar = mvn_obs.covariance_matrix
        fant_train_covar = delazify(full_covar[(...), num_train:, :num_train])
        self.fantasy_inputs = inputs
        self.fantasy_targets = targets
        """
        Compute a new mean cache given the old mean cache.

        We have \\alpha = K^{-1}y, and we want to solve [K U; U' S][a; b] = [y; y_f], where U' is fant_train_covar,
        S is fant_fant_covar, and y_f is (targets - fant_mean)

        To do this, we solve the bordered linear system of equations for [a; b]:
            AQ = U  # Q = fant_solve
            [S - U'Q]b = y_f - U'\\alpha   ==> b = [S - U'Q]^{-1}(y_f - U'\\alpha)
            a = \\alpha - Qb
        """
        K_inverse = self.lik_train_train_covar.root_inv_decomposition()
        fant_solve = K_inverse.matmul(fant_train_covar.transpose(-2, -1))
        schur_complement = fant_fant_covar - fant_train_covar.matmul(fant_solve
            )
        prefix = string.ascii_lowercase[:max(fant_train_covar.dim() - self.
            mean_cache.dim() - 1, 0)]
        ftcm = torch.einsum(prefix + '...yz,...z->' + prefix + '...y', [
            fant_train_covar, self.mean_cache])
        small_system_rhs = targets - fant_mean - ftcm
        small_system_rhs = small_system_rhs.unsqueeze(-1)
        schur_cholesky = psd_safe_cholesky(schur_complement, jitter=
            settings.cholesky_jitter.value())
        fant_cache_lower = torch.cholesky_solve(small_system_rhs,
            schur_cholesky)
        fant_cache_upper = self.mean_cache.unsqueeze(-1) - fant_solve.matmul(
            fant_cache_lower)
        fant_cache_upper = fant_cache_upper.squeeze(-1)
        fant_cache_lower = fant_cache_lower.squeeze(-1)
        fant_mean_cache = torch.cat((fant_cache_upper, fant_cache_lower),
            dim=-1)
        """
        Compute a new covariance cache given the old covariance cache.

        We have access to K \\approx LL' and K^{-1} \\approx R^{-1}R^{-T}, where L and R are low rank matrices
        resulting from Lanczos (see the LOVE paper).

        To update R^{-1}, we first update L:
            [K U; U' S] = [L 0; A B][L' A'; 0 B']
        Solving this matrix equation, we get:
            K = LL' ==>       L = L
            U = LA' ==>       A = UR^{-1}
            S = AA' + BB' ==> B = cholesky(S - AA')

        Once we've computed Z = [L 0; A B], we have that the new kernel matrix [K U; U' S] pprox ZZ'. Therefore,
        we can form a pseudo-inverse of Z directly to approximate [K U; U' S]^{-1/2}.
        """
        batch_shape = fant_train_covar.shape[:-2]
        L_inverse = self.covar_cache
        L = delazify(self.lik_train_train_covar.root_decomposition().root)
        m, n = L.shape[-2:]
        lower_left = fant_train_covar.matmul(L_inverse)
        schur = fant_fant_covar - lower_left.matmul(lower_left.transpose(-2,
            -1))
        schur_root = psd_safe_cholesky(schur, jitter=settings.
            cholesky_jitter.value())
        num_fant = schur_root.size(-2)
        m, n = L.shape[-2:]
        new_root = torch.zeros(*batch_shape, m + num_fant, n + num_fant,
            device=L.device, dtype=L.dtype)
        new_root[(...), :m, :n] = L
        new_root[(...), m:, :n] = lower_left
        new_root[(...), m:, n:] = schur_root
        Q, R = torch.qr(new_root)
        Rdiag = torch.diagonal(R, dim1=-2, dim2=-1)
        zeroish = Rdiag.abs() < 1e-06
        if torch.any(zeroish):
            jitter_diag = 1e-06 * torch.sign(Rdiag) * zeroish.to(Rdiag)
            R = R + torch.diag_embed(jitter_diag)
        new_covar_cache = torch.triangular_solve(Q.transpose(-2, -1), R)[0
            ].transpose(-2, -1)
        if full_inputs[0].dim() <= full_targets.dim():
            fant_batch_shape = full_targets.shape[:1]
            n_batch = len(full_mean.shape[:-1])
            repeat_shape = fant_batch_shape + torch.Size([1] * n_batch)
            full_inputs = [fi.expand(fant_batch_shape + fi.shape) for fi in
                full_inputs]
            full_mean = full_mean.expand(fant_batch_shape + full_mean.shape)
            full_covar = BatchRepeatLazyTensor(full_covar, repeat_shape)
            new_root = BatchRepeatLazyTensor(NonLazyTensor(new_root),
                repeat_shape)
        fant_strat = self.__class__(train_inputs=full_inputs,
            train_prior_dist=self.train_prior_dist.__class__(full_mean,
            full_covar), train_labels=full_targets, likelihood=
            fant_likelihood, root=new_root, inv_root=new_covar_cache)
        fant_strat._memoize_cache = {'mean_cache': fant_mean_cache,
            'covar_cache': new_covar_cache}
        return fant_strat

    @property
    @cached(name='covar_cache')
    def covar_cache(self):
        train_train_covar = self.lik_train_train_covar
        train_train_covar_inv_root = delazify(train_train_covar.
            root_inv_decomposition().root)
        return self._exact_predictive_covar_inv_quad_form_cache(
            train_train_covar_inv_root, self._last_test_train_covar)

    @property
    @cached(name='mean_cache')
    def mean_cache(self):
        mvn = self.likelihood(self.train_prior_dist, self.train_inputs)
        train_mean, train_train_covar = mvn.loc, mvn.lazy_covariance_matrix
        train_labels_offset = (self.train_labels - train_mean).unsqueeze(-1)
        mean_cache = train_train_covar.inv_matmul(train_labels_offset).squeeze(
            -1)
        if settings.detach_test_caches.on():
            mean_cache = mean_cache.detach()
        if mean_cache.grad_fn is not None:
            wrapper = functools.partial(clear_cache_hook, self)
            functools.update_wrapper(wrapper, clear_cache_hook)
            mean_cache.grad_fn.register_hook(wrapper)
        return mean_cache

    @property
    def num_train(self):
        return self.train_prior_dist.event_shape.numel()

    @property
    def train_shape(self):
        return self.train_prior_dist.event_shape

    def exact_prediction(self, joint_mean, joint_covar):
        test_mean = joint_mean[(...), self.num_train:]
        if joint_covar.size(-1) <= settings.max_eager_kernel_size.value():
            test_covar = joint_covar[(...), self.num_train:, :].evaluate()
            test_test_covar = test_covar[(...), self.num_train:]
            test_train_covar = test_covar[(...), :self.num_train]
        else:
            test_test_covar = joint_covar[(...), self.num_train:, self.
                num_train:]
            test_train_covar = joint_covar[(...), self.num_train:, :self.
                num_train]
        return self.exact_predictive_mean(test_mean, test_train_covar
            ), self.exact_predictive_covar(test_test_covar, test_train_covar)

    def exact_predictive_mean(self, test_mean, test_train_covar):
        """
        Computes the posterior predictive covariance of a GP

        Args:
            test_mean (:obj:`torch.tensor`): The test prior mean
            test_train_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test and train inputs

        Returns:
            :obj:`torch.tensor`: The predictive posterior mean of the test points
        """
        res = (test_train_covar @ self.mean_cache.unsqueeze(-1)).squeeze(-1)
        res = res + test_mean
        return res

    def exact_predictive_covar(self, test_test_covar, test_train_covar):
        """
        Computes the posterior predictive covariance of a GP

        Args:
            test_train_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test and train inputs
            test_test_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test inputs

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`: A LazyTensor representing the predictive posterior covariance of the
                                               test points
        """
        if settings.fast_pred_var.on():
            self._last_test_train_covar = test_train_covar
        if settings.skip_posterior_variances.on():
            return ZeroLazyTensor(*test_test_covar.size())
        if settings.fast_pred_var.off():
            dist = self.train_prior_dist.__class__(torch.zeros_like(self.
                train_prior_dist.mean), self.train_prior_dist.
                lazy_covariance_matrix)
            if settings.detach_test_caches.on():
                train_train_covar = self.likelihood(dist, self.train_inputs
                    ).lazy_covariance_matrix.detach()
            else:
                train_train_covar = self.likelihood(dist, self.train_inputs
                    ).lazy_covariance_matrix
            test_train_covar = delazify(test_train_covar)
            train_test_covar = test_train_covar.transpose(-1, -2)
            covar_correction_rhs = train_train_covar.inv_matmul(
                train_test_covar)
            if torch.is_tensor(test_test_covar):
                if test_test_covar.dim() == 2:
                    return lazify(torch.addmm(test_test_covar,
                        test_train_covar, covar_correction_rhs, beta=1,
                        alpha=-1))
                else:
                    return lazify(test_test_covar + test_train_covar @
                        covar_correction_rhs.mul(-1))
            else:
                return test_test_covar + MatmulLazyTensor(test_train_covar,
                    covar_correction_rhs.mul(-1))
        precomputed_cache = self.covar_cache
        covar_inv_quad_form_root = (self.
            _exact_predictive_covar_inv_quad_form_root(precomputed_cache,
            test_train_covar))
        if torch.is_tensor(test_test_covar):
            return lazify(torch.add(test_test_covar, 
                covar_inv_quad_form_root @ covar_inv_quad_form_root.
                transpose(-1, -2), alpha=-1))
        else:
            return test_test_covar + MatmulLazyTensor(covar_inv_quad_form_root,
                covar_inv_quad_form_root.transpose(-1, -2).mul(-1))


class SumLazyTensor(LazyTensor):

    def __init__(self, *lazy_tensors, **kwargs):
        try:
            lazy_tensors = tuple(lazify(lt) for lt in lazy_tensors)
        except TypeError:
            raise TypeError(
                'All arguments of a SumLazyTensor should be LazyTensors or Tensors'
                )
        batch_shape = _mul_broadcast_shape(*[lt.batch_shape for lt in
            lazy_tensors])
        lazy_tensors = tuple(lt._expand_batch(batch_shape) if lt.
            batch_shape != batch_shape else lt for lt in lazy_tensors)
        super(SumLazyTensor, self).__init__(*lazy_tensors, **kwargs)
        self.lazy_tensors = lazy_tensors

    def _expand_batch(self, batch_shape):
        expanded_tensors = [lazy_tensor._expand_batch(batch_shape) for
            lazy_tensor in self.lazy_tensors]
        return self.__class__(*expanded_tensors)

    def _get_indices(self, row_index, col_index, *batch_indices):
        results = [lazy_tensor._get_indices(row_index, col_index, *
            batch_indices) for lazy_tensor in self.lazy_tensors]
        return sum(results)

    def _getitem(self, row_index, col_index, *batch_indices):
        results = [lazy_tensor._getitem(row_index, col_index, *
            batch_indices) for lazy_tensor in self.lazy_tensors]
        return SumLazyTensor(*results)

    def _matmul(self, rhs):
        return sum(lazy_tensor._matmul(rhs) for lazy_tensor in self.
            lazy_tensors)

    def _quad_form_derivative(self, left_vecs, right_vecs):
        return tuple(var for lazy_tensor in self.lazy_tensors for var in
            lazy_tensor._quad_form_derivative(left_vecs, right_vecs))

    def _size(self):
        return _mul_broadcast_shape(*[lt.shape for lt in self.lazy_tensors])

    def _sum_batch(self, dim):
        return self.__class__(*(lazy_tensor._sum_batch(dim) for lazy_tensor in
            self.lazy_tensors))

    def _t_matmul(self, rhs):
        return sum(lazy_tensor._t_matmul(rhs) for lazy_tensor in self.
            lazy_tensors)

    def _transpose_nonbatch(self):
        lazy_tensors_t = [lazy_tensor.transpose(-1, -2) for lazy_tensor in
            self.lazy_tensors]
        return self.__class__(*lazy_tensors_t)

    @cached
    def evaluate(self):
        return sum(lazy_tensor.evaluate() for lazy_tensor in self.lazy_tensors)

    def __add__(self, other):
        from .diag_lazy_tensor import DiagLazyTensor
        from .added_diag_lazy_tensor import AddedDiagLazyTensor
        if isinstance(other, ZeroLazyTensor):
            return self
        elif isinstance(other, DiagLazyTensor):
            return AddedDiagLazyTensor(self, other)
        elif isinstance(other, SumLazyTensor):
            return SumLazyTensor(*(list(self.lazy_tensors) + list(other.
                lazy_tensors)))
        elif isinstance(other, LazyTensor):
            return SumLazyTensor(*(list(self.lazy_tensors) + [other]))
        elif isinstance(other, Tensor):
            broadcasted_shape = _mul_broadcast_shape(self.shape, other.shape)
            broadcasted_other = lazify(other.expand(broadcasted_shape))
            if broadcasted_shape != self.shape:
                broadcasted_lts = [lt.expand(*broadcasted_shape, 1).squeeze
                    (-1).transpose(-1, -2) for lt in self.lazy_tensors]
            else:
                broadcasted_lts = list(self.lazy_tensors)
            return SumLazyTensor(*(broadcasted_lts + [broadcasted_other]))
        else:
            raise AttributeError('other must be a LazyTensor')

    def diag(self):
        return sum(lazy_tensor.diag().contiguous() for lazy_tensor in self.
            lazy_tensors)


def prediction_strategy(train_inputs, train_prior_dist, train_labels,
    likelihood):
    train_train_covar = train_prior_dist.lazy_covariance_matrix
    if isinstance(train_train_covar, LazyEvaluatedKernelTensor):
        cls = train_train_covar.kernel.prediction_strategy
    else:
        cls = DefaultPredictionStrategy
    return cls(train_inputs, train_prior_dist, train_labels, likelihood)


class SumPredictionStrategy(DefaultPredictionStrategy):

    @property
    def _sub_strategies(self):
        sub_strategies = []
        for lazy_tensor in self.train_prior_dist.lazy_covariance_matrix.evaluate_kernel(
            ).lazy_tensors:
            pred_strat = prediction_strategy(self.train_inputs, self.
                train_prior_dist.__class__(self.train_prior_dist.mean,
                lazy_tensor), self.train_labels, self.likelihood)
            sub_strategies.append(pred_strat)
        return sub_strategies

    def _exact_predictive_covar_inv_quad_form_cache(self,
        train_train_covar_inv_root, test_train_covar):
        test_train_covar = test_train_covar.evaluate_kernel()
        if not isinstance(test_train_covar, SumLazyTensor):
            return super(SumPredictionStrategy, self
                )._exact_predictive_covar_inv_quad_form_cache(
                train_train_covar_inv_root, test_train_covar)
        else:
            return tuple(sub_strat.
                _exact_predictive_covar_inv_quad_form_cache(
                train_train_covar_inv_root, test_train_covar_comp) for 
                sub_strat, test_train_covar_comp in zip(self.
                _sub_strategies, test_train_covar.lazy_tensors))

    def _exact_predictive_covar_inv_quad_form_root(self, precomputed_cache,
        test_train_covar):
        test_train_covar = test_train_covar.evaluate_kernel()
        if not isinstance(test_train_covar, SumLazyTensor):
            return super(SumPredictionStrategy, self
                )._exact_predictive_covar_inv_quad_form_root(precomputed_cache,
                test_train_covar)
        else:
            return sum(sub_strat._exact_predictive_covar_inv_quad_form_root
                (cache_comp, test_train_covar_comp) for sub_strat,
                cache_comp, test_train_covar_comp in zip(self.
                _sub_strategies, precomputed_cache, test_train_covar.
                evaluate_kernel().lazy_tensors))


class GreaterThan(Interval):

    def __init__(self, lower_bound, transform=softplus, inv_transform=
        inv_softplus, initial_value=None):
        super().__init__(lower_bound=lower_bound, upper_bound=math.inf,
            transform=transform, inv_transform=inv_transform, initial_value
            =initial_value)

    def __repr__(self):
        if self.lower_bound.numel() == 1:
            return self._get_name() + f'({self.lower_bound:.3E})'
        else:
            return super().__repr__()

    def transform(self, tensor):
        transformed_tensor = self._transform(tensor
            ) + self.lower_bound if self.enforced else tensor
        return transformed_tensor

    def inverse_transform(self, transformed_tensor):
        tensor = self._inv_transform(transformed_tensor - self.lower_bound
            ) if self.enforced else transformed_tensor
        return tensor


class Positive(GreaterThan):

    def __init__(self, transform=softplus, inv_transform=inv_softplus,
        initial_value=None):
        super().__init__(lower_bound=0.0, transform=transform,
            inv_transform=inv_transform, initial_value=initial_value)

    def __repr__(self):
        return self._get_name() + '()'

    def transform(self, tensor):
        transformed_tensor = self._transform(tensor
            ) if self.enforced else tensor
        return transformed_tensor

    def inverse_transform(self, transformed_tensor):
        tensor = self._inv_transform(transformed_tensor
            ) if self.enforced else transformed_tensor
        return tensor


class Kernel(Module):
    """
    Kernels in GPyTorch are implemented as a :class:`gpytorch.Module` that, when called on two :obj:`torch.tensor`
    objects `x1` and `x2` returns either a :obj:`torch.tensor` or a :obj:`gpytorch.lazy.LazyTensor` that represents
    the covariance matrix between `x1` and `x2`.

    In the typical use case, to extend this class means to implement the :func:`~gpytorch.kernels.Kernel.forward`
    method.

    .. note::
        The :func:`~gpytorch.kernels.Kernel.__call__` does some additional internal work. In particular,
        all kernels are lazily evaluated so that, in some cases, we can index in to the kernel matrix before actually
        computing it. Furthermore, many built in kernel modules return LazyTensors that allow for more efficient
        inference than if we explicitly computed the kernel matrix itself.

        As a result, if you want to use a :obj:`gpytorch.kernels.Kernel` object just to get an actual
        :obj:`torch.tensor` representing the covariance matrix, you may need to call the
        :func:`gpytorch.lazy.LazyTensor.evaluate` method on the output.

    This base :class:`Kernel` class includes a lengthscale parameter
    :math:`\\Theta`, which is used by many common kernel functions.
    There are a few options for the lengthscale:

    * Default: No lengthscale (i.e. :math:`\\Theta` is the identity matrix).

    * Single lengthscale: One lengthscale can be applied to all input dimensions/batches
      (i.e. :math:`\\Theta` is a constant diagonal matrix).
      This is controlled by setting the attribute `has_lengthscale=True`.

    * ARD: Each input dimension gets its own separate lengthscale
      (i.e. :math:`\\Theta` is a non-constant diagonal matrix).
      This is controlled by the `ard_num_dims` keyword argument (as well as `has_lengthscale=True`).

    In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each
    batch of data can have its own lengthscale parameter by setting the `batch_shape`
    keyword argument to the appropriate number of batches.

    .. note::

        The :attr:`lengthscale` parameter is parameterized on a log scale to constrain it to be positive.
        You can set a prior on this parameter using the :attr:`lengthscale_prior` argument.

    Base Args:
        :attr:`ard_num_dims` (int, optional):
            Set this if you want a separate lengthscale for each input
            dimension. It should be `d` if :attr:`x1` is a `n x d` matrix.  Default: `None`
        :attr:`batch_shape` (torch.Size, optional):
            Set this if you want a separate lengthscale for each batch of input
            data. It should be `b1 x ... x bk` if :attr:`x1` is a `b1 x ... x bk x n x d` tensor.
        :attr:`active_dims` (tuple of ints, optional):
            Set this if you want to compute the covariance of only a few input dimensions. The ints
            corresponds to the indices of the dimensions. Default: `None`.
        :attr:`lengthscale_prior` (Prior, optional):
            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`
        :attr:`lengthscale_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.
        :attr:`eps` (float):
            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.

    Base Attributes:
        :attr:`lengthscale` (Tensor):
            The lengthscale parameter. Size/shape of parameter depends on the
            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.

    Example:
        >>> covar_module = gpytorch.kernels.LinearKernel()
        >>> x1 = torch.randn(50, 3)
        >>> lazy_covar_matrix = covar_module(x1) # Returns a RootLazyTensor
        >>> tensor_covar_matrix = lazy_covar_matrix.evaluate() # Gets the actual tensor for this kernel matrix
    """
    has_lengthscale = False

    def __init__(self, ard_num_dims=None, batch_shape=torch.Size([]),
        active_dims=None, lengthscale_prior=None, lengthscale_constraint=
        None, eps=1e-06, **kwargs):
        super(Kernel, self).__init__()
        self._batch_shape = batch_shape
        if active_dims is not None and not torch.is_tensor(active_dims):
            active_dims = torch.tensor(active_dims, dtype=torch.long)
        self.register_buffer('active_dims', active_dims)
        self.ard_num_dims = ard_num_dims
        self.eps = eps
        param_transform = kwargs.get('param_transform')
        if lengthscale_constraint is None:
            lengthscale_constraint = Positive()
        if param_transform is not None:
            warnings.warn(
                "The 'param_transform' argument is now deprecated. If you want to use a different transformation, specify a different 'lengthscale_constraint' instead."
                , DeprecationWarning)
        if self.has_lengthscale:
            lengthscale_num_dims = 1 if ard_num_dims is None else ard_num_dims
            self.register_parameter(name='raw_lengthscale', parameter=torch
                .nn.Parameter(torch.zeros(*self.batch_shape, 1,
                lengthscale_num_dims)))
            if lengthscale_prior is not None:
                self.register_prior('lengthscale_prior', lengthscale_prior,
                    lambda : self.lengthscale, lambda v: self.
                    _set_lengthscale(v))
            self.register_constraint('raw_lengthscale', lengthscale_constraint)
        self.distance_module = None
        self.__pdist_supports_batch = True

    @abstractmethod
    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):
        """
        Computes the covariance between x1 and x2.
        This method should be imlemented by all Kernel subclasses.

        Args:
            :attr:`x1` (Tensor `n x d` or `b x n x d`):
                First set of data
            :attr:`x2` (Tensor `m x d` or `b x m x d`):
                Second set of data
            :attr:`diag` (bool):
                Should the Kernel compute the whole kernel, or just the diag?
            :attr:`last_dim_is_batch` (tuple, optional):
                If this is true, it treats the last dimension of the data as another batch dimension.
                (Useful for additive structure over the dimensions). Default: False

        Returns:
            :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.
                The exact size depends on the kernel's evaluation mode:

                * `full_covar`: `n x m` or `b x n x m`
                * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`
                * `diag`: `n` or `b x n`
                * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`
        """
        raise NotImplementedError()

    @property
    def batch_shape(self):
        kernels = list(self.sub_kernels())
        if len(kernels):
            return _mul_broadcast_shape(self._batch_shape, *[k.batch_shape for
                k in kernels])
        else:
            return self._batch_shape

    @batch_shape.setter
    def batch_shape(self, val):
        self._batch_shape = val

    @property
    def dtype(self):
        if self.has_lengthscale:
            return self.lengthscale.dtype
        else:
            for param in self.parameters():
                return param.dtype
            return torch.get_default_dtype()

    @property
    def is_stationary(self) ->bool:
        """
        Property to indicate whether kernel is stationary or not.
        """
        return self.has_lengthscale

    @property
    def lengthscale(self):
        if self.has_lengthscale:
            return self.raw_lengthscale_constraint.transform(self.
                raw_lengthscale)
        else:
            return None

    @lengthscale.setter
    def lengthscale(self, value):
        self._set_lengthscale(value)

    def _set_lengthscale(self, value):
        if not self.has_lengthscale:
            raise RuntimeError('Kernel has no lengthscale.')
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_lengthscale=self.raw_lengthscale_constraint.
            inverse_transform(value))

    def local_load_samples(self, samples_dict, memo, prefix):
        num_samples = next(iter(samples_dict.values())).size(0)
        self.batch_shape = torch.Size([num_samples]) + self.batch_shape
        super().local_load_samples(samples_dict, memo, prefix)

    def covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False,
        square_dist=False, dist_postprocess_func=default_postprocess_script,
        postprocess=True, **params):
        """
        This is a helper method for computing the Euclidean distance between
        all pairs of points in x1 and x2.

        Args:
            :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):
                First set of data.
            :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):
                Second set of data.
            :attr:`diag` (bool):
                Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.
            :attr:`last_dim_is_batch` (tuple, optional):
                Is the last dimension of the data a batch dimension or not?
            :attr:`square_dist` (bool):
                Should we square the distance matrix before returning?

        Returns:
            (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.
            The shape depends on the kernel's mode
            * `diag=False`
            * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)
            * `diag=True`
            * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)
        """
        if last_dim_is_batch:
            x1 = x1.transpose(-1, -2).unsqueeze(-1)
            x2 = x2.transpose(-1, -2).unsqueeze(-1)
        x1_eq_x2 = torch.equal(x1, x2)
        postprocess = torch.tensor(postprocess)
        res = None
        if (not self.distance_module or self.distance_module._postprocess !=
            dist_postprocess_func):
            self.distance_module = Distance(dist_postprocess_func)
        if diag:
            if x1_eq_x2:
                res = torch.zeros(*x1.shape[:-2], x1.shape[-2], dtype=x1.
                    dtype, device=x1.device)
                if postprocess:
                    res = dist_postprocess_func(res)
                return res
            else:
                res = torch.norm(x1 - x2, p=2, dim=-1)
                if square_dist:
                    res = res.pow(2)
            if postprocess:
                res = dist_postprocess_func(res)
            return res
        elif square_dist:
            res = self.distance_module._sq_dist(x1, x2, postprocess, x1_eq_x2)
        else:
            res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)
        return res

    def named_sub_kernels(self):
        for name, module in self._modules.items():
            if isinstance(module, Kernel):
                yield name, module

    def num_outputs_per_input(self, x1, x2):
        """
        How many outputs are produced per input (default 1)
        if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel
        will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`
        Default: 1
        """
        return 1

    def prediction_strategy(self, train_inputs, train_prior_dist,
        train_labels, likelihood):
        return DefaultPredictionStrategy(train_inputs, train_prior_dist,
            train_labels, likelihood)

    def sub_kernels(self):
        for _, kernel in self.named_sub_kernels():
            yield kernel

    def __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **
        params):
        x1_, x2_ = x1, x2
        if self.active_dims is not None:
            x1_ = x1_.index_select(-1, self.active_dims)
            if x2_ is not None:
                x2_ = x2_.index_select(-1, self.active_dims)
        if x1_.ndimension() == 1:
            x1_ = x1_.unsqueeze(1)
        if x2_ is not None:
            if x2_.ndimension() == 1:
                x2_ = x2_.unsqueeze(1)
            if not x1_.size(-1) == x2_.size(-1):
                raise RuntimeError(
                    'x1_ and x2_ must have the same number of dimensions!')
        if x2_ is None:
            x2_ = x1_
        if settings.debug.on():
            if self.ard_num_dims is not None and self.ard_num_dims != x1_.size(
                -1):
                raise RuntimeError(
                    'Expected the input to have {} dimensionality (based on the ard_num_dims argument). Got {}.'
                    .format(self.ard_num_dims, x1_.size(-1)))
        if diag:
            res = super(Kernel, self).__call__(x1_, x2_, diag=True,
                last_dim_is_batch=last_dim_is_batch, **params)
            if not isinstance(res, LazyEvaluatedKernelTensor):
                if res.dim() == x1_.dim() and res.shape[-2:] == torch.Size((
                    x1_.size(-2), x2_.size(-2))):
                    res = res.diag()
            return res
        else:
            if settings.lazily_evaluate_kernels.on():
                res = LazyEvaluatedKernelTensor(x1_, x2_, kernel=self,
                    last_dim_is_batch=last_dim_is_batch, **params)
            else:
                res = lazify(super(Kernel, self).__call__(x1_, x2_,
                    last_dim_is_batch=last_dim_is_batch, **params))
            return res

    def __getstate__(self):
        self.distance_module = None
        return self.__dict__

    def __add__(self, other):
        return AdditiveKernel(self, other)

    def __mul__(self, other):
        return ProductKernel(self, other)

    def __setstate__(self, d):
        self.__dict__ = d

    def __getitem__(self, index):
        if len(self.batch_shape) == 0:
            return self
        new_kernel = deepcopy(self)
        index = index if isinstance(index, tuple) else (index,)
        for param_name, param in self._parameters.items():
            new_kernel._parameters[param_name].data = param.__getitem__(index)
            ndim_removed = len(param.shape) - len(new_kernel._parameters[
                param_name].shape)
            new_batch_shape_len = len(self.batch_shape) - ndim_removed
            new_kernel.batch_shape = new_kernel._parameters[param_name].shape[:
                new_batch_shape_len]
        for sub_module_name, sub_module in self.named_sub_kernels():
            self._modules[sub_module_name] = sub_module.__getitem__(index)
        return new_kernel


class CatLazyTensor(LazyTensor):
    """
    A `LazyTensor` that represents the concatenation of other lazy tensors.
    Each LazyTensor must have the same shape except in the concatenating
    dimension.

    Args:
        - :attr:`lazy_tensors` (list of LazyTensors):
            A list of LazyTensors whose sizes are the same except in
            concatenating dimension :attr:`dim`
        - :attr:`dim` (int):
            The concatenating dimension which can be a batch dimension.
        - :attr:`output_device` (torch.device):
            The CatLazyTensor will appear to appear on :attr:`output_device`
            and place any output `torch.Tensors` on :attr:`output_device`
    """

    def _check_args(self, *lazy_tensors, dim=0, output_device=None):
        if len(lazy_tensors) == 0:
            raise RuntimeError('List of LazyTensors must be non-empty')
        elif len(lazy_tensors) == 1:
            raise RuntimeError(
                'Why are we trying to concatenate a single LazyTensor?')
        if not all([isinstance(t, LazyTensor) for t in lazy_tensors]):
            raise RuntimeError(
                'CatLazyTensor requires a list of all LazyTensors')
        rep_tensor = lazy_tensors[0]
        rep_tensor_noncat_shape = list(rep_tensor.shape)
        del rep_tensor_noncat_shape[dim]
        for t in lazy_tensors:
            if t.dim() != rep_tensor.dim():
                raise RuntimeError(
                    'All tensors must have the same number of dimensions')
            t_noncat_shape = list(t.shape)
            del t_noncat_shape[dim]
            if t_noncat_shape != rep_tensor_noncat_shape:
                raise RuntimeError(
                    'All LazyTensors must have the same size in the non-concatenation dimension'
                    )

    def __init__(self, *lazy_tensors, dim=0, output_device=None):
        rep_tensor = lazy_tensors[0]
        ndims = rep_tensor.ndimension()
        if dim >= 0:
            positive_dim = dim
            dim = dim - ndims
        else:
            positive_dim = ndims + dim
        super().__init__(*lazy_tensors, dim=dim, output_device=output_device)
        self.lazy_tensors = lazy_tensors
        self.cat_dim = dim
        self.output_device = output_device
        cat_dim_sizes = torch.tensor([t.size(dim) for t in lazy_tensors],
            device=output_device)
        cat_dim_cum_sizes = torch.zeros(len(lazy_tensors) + 1, dtype=torch.
            long, device=output_device)
        torch.cumsum(cat_dim_sizes, dim=-1, out=cat_dim_cum_sizes[1:])
        idx_to_tensor_idx = torch.empty(cat_dim_cum_sizes[-1].item(), dtype
            =torch.long, device=output_device)
        for tsr_idx, (start_idx, end_idx) in enumerate(zip(
            cat_dim_cum_sizes[:-1], cat_dim_cum_sizes[1:])):
            idx_to_tensor_idx[start_idx.item():end_idx.item()].fill_(tsr_idx)
        self.cat_dim_sizes = cat_dim_sizes
        self.cat_dim_cum_sizes = cat_dim_cum_sizes
        self.idx_to_tensor_idx = idx_to_tensor_idx
        self._shape = torch.Size((*rep_tensor.shape[:positive_dim],
            cat_dim_cum_sizes[-1].item(), *rep_tensor.shape[positive_dim + 1:])
            )

    def _split_slice(self, slice_idx):
        """
        Splits a slice(a, b, None) in to a list of slices [slice(a1, b1, None), slice(a2, b2, None), ...]
        so that each slice in the list slices in to a single tensor that we have concatenated with this LazyTensor.
        """
        if slice_idx.step is not None:
            raise RuntimeError(
                'Slicing a CatLazyTensor with a step is not currently supported!'
                )
        start_idx = slice_idx.start if slice_idx.start is not None else 0
        stop_idx = slice_idx.stop if slice_idx.stop is not None else self.size(
            self.cat_dim)
        first_tensor_idx = self.idx_to_tensor_idx[start_idx].item()
        last_tensor_idx = self.idx_to_tensor_idx[stop_idx - 1].item()
        first_tensor_start_index = start_idx - self.cat_dim_cum_sizes[
            first_tensor_idx].item()
        last_tensor_stop_index = stop_idx - self.cat_dim_cum_sizes[
            last_tensor_idx].item()
        if first_tensor_idx == last_tensor_idx:
            return [first_tensor_idx], [slice(first_tensor_start_index,
                last_tensor_stop_index, None)]
        else:
            num_middle_tensors = last_tensor_idx - first_tensor_idx - 1
            first_slice = slice(first_tensor_start_index, None, None)
            last_slice = slice(None, last_tensor_stop_index, None)
            return list(range(first_tensor_idx, last_tensor_idx + 1)), [
                first_slice] + [_noop_index] * num_middle_tensors + [last_slice
                ]

    def _expand_batch(self, batch_shape):
        lazy_tensors = [lazy_tensor._expand_batch(batch_shape) for
            lazy_tensor in self.lazy_tensors]
        res = self.__class__(*lazy_tensors, dim=self.cat_dim, output_device
            =self.output_device)
        return res

    def _get_indices(self, row_index, col_index, *batch_indices):
        indices = [*batch_indices, row_index, col_index]
        target_shape = _mul_broadcast_shape(*[index.shape for index in indices]
            )
        indices = [index.expand(target_shape).reshape(-1) for index in indices]
        cat_dim_indices = indices[self.cat_dim]
        target_tensors = self.idx_to_tensor_idx[cat_dim_indices]
        does_switch_tensor = torch.ones(target_tensors.numel() + 1, dtype=
            bool_compat, device=self.device)
        torch.ne(target_tensors[:-1], target_tensors[1:], out=
            does_switch_tensor[1:-1])
        lazy_tensor_indices = target_tensors[does_switch_tensor[:-1]].tolist()
        lazy_tensors = [self.lazy_tensors[idx] for idx in lazy_tensor_indices]
        switch_tensor = does_switch_tensor.nonzero().squeeze(-1)
        split_sizes = (switch_tensor[1:] - switch_tensor[:-1]).tolist()
        sub_indices = zip(*[(list(index.split(split_sizes)) if torch.
            is_tensor(index) else [index] * len(split_sizes)) for index in
            indices])
        sub_indices = [list(sub_index) for sub_index in sub_indices]
        for lazy_tensor_idx, sub_index in zip(lazy_tensor_indices, sub_indices
            ):
            sub_index[self.cat_dim] = sub_index[self.cat_dim
                ] - self.cat_dim_cum_sizes[lazy_tensor_idx]
        res_list = [lazy_tensor._get_indices(sub_index[-2], sub_index[-1],
            *sub_index[:-2]) for lazy_tensor, sub_index in zip(lazy_tensors,
            sub_indices)]
        if len(res_list) == 1:
            return res_list[0].view(target_shape).to(self.device)
        else:
            return torch.cat(res_list).view(target_shape).to(self.device)

    def _getitem(self, row_index, col_index, *batch_indices):
        indices = [*batch_indices, row_index, col_index]
        cat_dim_indices = indices[self.cat_dim]
        if isinstance(cat_dim_indices, slice):
            if cat_dim_indices == _noop_index:
                res_list = [lazy_tensor._getitem(row_index, col_index, *
                    batch_indices) for lazy_tensor in self.lazy_tensors]
            else:
                res_list = []
                tensor_idxs, target_slices = self._split_slice(cat_dim_indices)
                for tensor_idx, target_slice in zip(tensor_idxs, target_slices
                    ):
                    indices[self.cat_dim] = target_slice
                    res = self.lazy_tensors[tensor_idx]._getitem(indices[-2
                        ], indices[-1], *indices[:-2])
                    res_list.append(res)
        elif torch.is_tensor(cat_dim_indices):
            target_tensors = self.idx_to_tensor_idx[cat_dim_indices]
            does_switch_tensor = torch.ones(target_tensors.numel() + 1,
                dtype=bool_compat, device=self.device)
            torch.ne(target_tensors[:-1], target_tensors[1:], out=
                does_switch_tensor[1:-1])
            lazy_tensor_indices = target_tensors[does_switch_tensor[:-1]
                ].tolist()
            lazy_tensors = [self.lazy_tensors[idx] for idx in
                lazy_tensor_indices]
            switch_tensor = does_switch_tensor.nonzero().squeeze(-1)
            split_sizes = (switch_tensor[1:] - switch_tensor[:-1]).tolist()
            sub_indices = zip(*[(list(index.split(split_sizes)) if torch.
                is_tensor(index) else [index] * len(split_sizes)) for index in
                indices])
            sub_indices = [list(sub_index) for sub_index in sub_indices]
            for lazy_tensor_idx, sub_index in zip(lazy_tensor_indices,
                sub_indices):
                sub_index[self.cat_dim] = sub_index[self.cat_dim
                    ] - self.cat_dim_cum_sizes[lazy_tensor_idx]
            res_list = [lazy_tensor._getitem(sub_index[-2], sub_index[-1],
                *sub_index[:-2]) for lazy_tensor, sub_index in zip(
                lazy_tensors, sub_indices)]
        elif isinstance(cat_dim_indices, int):
            target_tensor = self.idx_to_tensor_idx[cat_dim_indices].item()
            cat_dim_indices = cat_dim_indices - self.cat_dim_cum_sizes[
                target_tensor]
            indices[self.cat_dim] = cat_dim_indices
            res_list = [self.lazy_tensors[target_tensor]._getitem(indices[-
                2], indices[-1], *indices[:-2])]
        if len(res_list) == 1:
            return res_list[0].to(self.output_device)
        else:
            res = self.__class__(*res_list, dim=self.cat_dim, output_device
                =self.output_device)
            return res

    def _matmul(self, rhs):
        output_device = self.device if self.device is not None else rhs.device
        rhs_ = []
        for d in self.devices:
            if d != rhs.device:
                rhs_.append(rhs.to(d))
            else:
                rhs_.append(rhs)
        if self.cat_dim == -2:
            res_list = [t._matmul(rhs) for t, rhs in zip(self.lazy_tensors,
                rhs_)]
            res_list = [x.to(output_device) for x in res_list]
            res = torch.cat(res_list, dim=-2)
        elif self.cat_dim == -1:
            curr_idx = 0
            res_list = []
            index = [slice(None, None, None) for _ in range(rhs.ndimension())]
            for t, size, rhs in zip(self.lazy_tensors, self.cat_dim_sizes, rhs_
                ):
                index[-2] = slice(curr_idx, curr_idx + size, None)
                res_list.append(t._matmul(rhs[index]))
                curr_idx += size
            res_list = [x.to(output_device) for x in res_list]
            res = 0.0
            for x in res_list:
                res = res + x
        else:
            output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)
            rhs = rhs.expand(*output_shape[:-2], *rhs.shape[-2:])
            curr_idx = 0
            res_list = []
            for t, size in zip(self.lazy_tensors, self.cat_dim_sizes):
                sub_rhs = rhs.narrow(self.cat_dim, curr_idx, size)
                res_list.append(t._matmul(sub_rhs))
                curr_idx += size
            res_list = [x.to(output_device) for x in res_list]
            res = torch.cat(res_list, dim=self.cat_dim)
        return res

    def _permute_batch(self, *dims):
        lazy_tensors = [lazy_tensor._permute_batch(*dims) for lazy_tensor in
            self.lazy_tensors]
        if self.cat_dim < -2:
            positive_cat_dim = self.dim() + self.cat_dim
            new_cat_dim = dims.index(positive_cat_dim)
        else:
            new_cat_dim = self.cat_dim
        return self.__class__(*lazy_tensors, dim=new_cat_dim, output_device
            =self.output_device)

    def _size(self):
        return self._shape

    def _transpose_nonbatch(self):
        if self.cat_dim == -2:
            new_dim = -1
        elif self.cat_dim == -1:
            new_dim = -2
        else:
            new_dim = self.cat_dim
        return self.__class__(*[t._transpose_nonbatch() for t in self.
            lazy_tensors], dim=new_dim, output_device=self.output_device)

    def _unsqueeze_batch(self, dim):
        cat_dim = self.dim() + self.cat_dim
        lazy_tensors = [lazy_tensor._unsqueeze_batch(dim) for lazy_tensor in
            self.lazy_tensors]
        res = self.__class__(*lazy_tensors, dim=cat_dim + 1 if dim <=
            cat_dim else cat_dim, output_device=self.output_device)
        return res

    def diag(self):
        if settings.debug.on():
            if not self.is_square:
                raise RuntimeError('Diag works on square matrices (or batches)'
                    )
        if self.cat_dim == -2:
            res = []
            curr_col = 0
            for t in self.lazy_tensors:
                n_rows, n_cols = t.shape[-2:]
                rows = torch.arange(0, n_rows, dtype=torch.long, device=t.
                    device)
                cols = torch.arange(curr_col, curr_col + n_rows, dtype=
                    torch.long, device=t.device)
                res.append(t[..., rows, cols].to(self.device))
                curr_col += n_rows
            res = torch.cat(res, dim=-1)
        elif self.cat_dim == -1:
            res = []
            curr_row = 0
            for t in self.lazy_tensors:
                n_rows, n_cols = t.shape[-2:]
                rows = torch.arange(curr_row, curr_row + n_cols, dtype=
                    torch.long, device=t.device)
                cols = torch.arange(0, n_cols, dtype=torch.long, device=t.
                    device)
                curr_row += n_cols
                res.append(t[..., rows, cols].to(self.device))
            res = torch.cat(res, dim=-1)
        else:
            res = torch.cat([t.diag().to(self.device) for t in self.
                lazy_tensors], dim=self.cat_dim + 1)
        return res

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False,
        reduce_inv_quad=True):
        res = super().inv_quad_logdet(inv_quad_rhs, logdet, reduce_inv_quad)
        return tuple(r.to(self.device) for r in res)

    @property
    def device(self):
        return self.output_device

    @property
    def devices(self):
        return [t.device for t in self.lazy_tensors]

    @property
    def device_count(self):
        return len(set(self.devices))

    def to(self, device_id):
        """
        returns a new CatLazyTensor with device_id as the output_device
        Warning: this does not move the LazyTensors in this CatLazyTensor to
        device_id
        """
        new_kwargs = dict(self._kwargs)
        new_kwargs['output_device'] = device_id
        return self.__class__(*self._args, **new_kwargs)

    def all_to(self, device_id):
        """
        Create a new CatLazyTensor with all LazyTensors in CatLazyTensor moved
        to one device device. The new CatLazyTensor also has device_id as the
        output_device.
        """
        new_args = []
        new_kwargs = {}
        for arg in self._args:
            if hasattr(arg, 'to'):
                new_args.append(arg.to(device_id))
            else:
                new_args.append(arg)
        for name, val in self._kwargs.items():
            if hasattr(val, 'to'):
                new_kwargs[name] = val.to(device_id)
            else:
                new_kwargs[name] = val
        new_kwargs['output_device'] = device_id
        return self.__class__(*new_args, **new_kwargs)


class MultiDeviceKernel(DataParallel, Kernel):
    """
    Allocates the covariance matrix on distributed devices, e.g. multiple GPUs.

    Args:
        - :attr:`base_kernel`: Base kernel to distribute
        - :attr:`device_ids`: list of `torch.device` objects to place kernel chunks on
        - :attr:`output_device`: Device where outputs will be placed
    """

    def __init__(self, base_kernel, device_ids, output_device=None,
        create_cuda_context=True, **kwargs):
        if create_cuda_context:
            for d in device_ids:
                _ = torch.tensor([], device=d)
        DataParallel.__init__(self, module=base_kernel, device_ids=
            device_ids, output_device=output_device, dim=-2)
        self.output_device = output_device if output_device else device_ids[0]
        self.__cached_x1 = torch.empty(1)
        self.__cached_x2 = torch.empty(1)

    @property
    def base_kernel(self):
        return self.module

    def forward(self, x1, x2, diag=False, **kwargs):
        if diag:
            return self.module.forward(x1, x2, diag=True, **kwargs)
        if x1.size(-2) < len(self.device_ids) + 1:
            return self.module.forward(x1, x2, diag=diag, **kwargs)
        if not x1.device == self.__cached_x1.device or not torch.equal(x1,
            self.__cached_x1):
            self._x1_scattered, self._kwargs = self.scatter((x1,), kwargs,
                self.device_ids)
            self.__cached_x1 = x1
        if not x2.device == self.__cached_x2.device or not torch.equal(x2,
            self.__cached_x2):
            self._x2_subs = [x2 for x1_ in self._x1_scattered]
            self.__cached_x2 = x2
        inputs = tuple((x1_[0], x2_) for x1_, x2_ in zip(self._x1_scattered,
            self._x2_subs))
        if not self.device_ids:
            return self.module.forward(*inputs, **self._kwargs)
        if len(self.device_ids) == 1:
            return self.module.forward(*inputs[0], **self._kwargs[0])

        def set_distance_module_to_none(module):
            if hasattr(module, 'distance_module'):
                module.distance_module = None
        self.module.apply(set_distance_module_to_none)
        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
        with settings.lazily_evaluate_kernels(False):
            outputs = self.parallel_apply(replicas, inputs, self._kwargs)
        return self.gather(outputs, self.output_device)

    def gather(self, outputs, output_device):
        return CatLazyTensor(*[lazify(o) for o in outputs], dim=self.dim,
            output_device=self.output_device)

    def num_outputs_per_input(self, x1, x2):
        return self.base_kernel.num_outputs_per_input(x1, x2)


class Noise(Module):
    pass


class DeprecationError(Exception):
    pass


def _extract_named_added_loss_terms(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_added_loss_terms'):
        for name, strategy in module._added_loss_terms.items():
            if strategy is not None and strategy not in memo:
                memo.add(strategy)
                yield prefix + ('.' if prefix else '') + name, strategy
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, strategy in _extract_named_added_loss_terms(module=
            module_, memo=memo, prefix=submodule_prefix):
            yield name, strategy


def _extract_named_constraints(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_constraints'):
        for name, constraint in module._constraints.items():
            if constraint is not None and constraint not in memo:
                memo.add(constraint)
                full_name = ('.' if prefix else '').join([prefix, name])
                yield full_name, constraint
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, constraint in _extract_named_constraints(module_, memo=
            memo, prefix=submodule_prefix):
            yield name, constraint


def _extract_named_priors(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_priors'):
        for name, (prior, closure, inv_closure) in module._priors.items():
            if prior is not None and prior not in memo:
                memo.add(prior)
                full_name = ('.' if prefix else '').join([prefix, name])
                yield full_name, prior, closure, inv_closure
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, prior, closure, inv_closure in _extract_named_priors(module_,
            memo=memo, prefix=submodule_prefix):
            yield name, prior, closure, inv_closure


def _pyro_load_from_samples(module, samples_dict, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_priors'):
        module.local_load_samples(samples_dict, memo, prefix)
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        _pyro_load_from_samples(module_, samples_dict, memo=memo, prefix=
            submodule_prefix)


def _pyro_sample_from_prior(module, memo=None, prefix=''):
    try:
        import pyro
    except ImportError:
        raise RuntimeError(
            'Cannot call pyro_sample_from_prior without pyro installed!')
    if memo is None:
        memo = set()
    if hasattr(module, '_priors'):
        for prior_name, (prior, closure, setting_closure
            ) in module._priors.items():
            if prior is not None and prior not in memo:
                if setting_closure is None:
                    raise RuntimeError(
                        f'Cannot use Pyro for sampling without a setting_closure for each prior, but the following prior had none: {prior_name}, {prior}.'
                        )
                memo.add(prior)
                prior = prior.expand(closure().shape)
                value = pyro.sample(prefix + ('.' if prefix else '') +
                    prior_name, prior)
                setting_closure(value)
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        _pyro_sample_from_prior(module=module_, memo=memo, prefix=
            submodule_prefix)


def _set_strict(module, value, memo=None):
    if memo is None:
        memo = set()
    if hasattr(module, '_strict_init'):
        module._strict_init = value
    for mname, module_ in module.named_children():
        _set_strict(module_, value)


def _validate_module_outputs(outputs):
    if isinstance(outputs, tuple):
        if not all(torch.is_tensor(output) or isinstance(output,
            Distribution) or isinstance(output, LazyTensor) for output in
            outputs):
            raise RuntimeError(
                'All outputs must be a Distribution, torch.Tensor, or LazyTensor. Got {}'
                .format([output.__class__.__name__ for output in outputs]))
        if len(outputs) == 1:
            outputs = outputs[0]
        return outputs
    elif torch.is_tensor(outputs) or isinstance(outputs, Distribution
        ) or isinstance(outputs, LazyTensor):
        return outputs
    else:
        raise RuntimeError(
            'Output must be a Distribution, torch.Tensor, or LazyTensor. Got {}'
            .format(outputs.__class__.__name__))


class Module(nn.Module):

    def __init__(self):
        super().__init__()
        self._added_loss_terms = OrderedDict()
        self._priors = OrderedDict()
        self._constraints = OrderedDict()
        self._strict_init = True
        self._load_strict_shapes = True
        self._register_load_state_dict_pre_hook(self.
            _load_state_hook_ignore_shapes)

    def __call__(self, *inputs, **kwargs):
        outputs = self.forward(*inputs, **kwargs)
        if isinstance(outputs, list):
            return [_validate_module_outputs(output) for output in outputs]
        return _validate_module_outputs(outputs)

    def _get_module_and_name(self, parameter_name):
        """Get module and name from full parameter name."""
        module, name = parameter_name.split('.', 1)
        if module in self._modules:
            return self.__getattr__(module), name
        else:
            raise AttributeError(
                'Invalid parameter name {}. {} has no module {}'.format(
                parameter_name, type(self).__name__, module))

    def _strict(self, value):
        _set_strict(self, value)

    def added_loss_terms(self):
        for _, strategy in self.named_added_loss_terms():
            yield strategy

    def forward(self, *inputs, **kwargs):
        raise NotImplementedError

    def constraints(self):
        for _, constraint in self.named_constraints():
            yield constraint

    def hyperparameters(self):
        for _, param in self.named_hyperparameters():
            yield param

    def initialize(self, **kwargs):
        """
        Set a value for a parameter

        kwargs: (param_name, value) - parameter to initialize.
        Can also initialize recursively by passing in the full name of a
        parameter. For example if model has attribute model.likelihood,
        we can initialize the noise with either
        `model.initialize(**{'likelihood.noise': 0.1})`
        or
        `model.likelihood.initialize(noise=0.1)`.
        The former method would allow users to more easily store the
        initialization values as one object.

        Value can take the form of a tensor, a float, or an int
        """
        for name, val in kwargs.items():
            if isinstance(val, int):
                val = float(val)
            if '.' in name:
                module, name = self._get_module_and_name(name)
                module.initialize(**{name: val})
            elif not hasattr(self, name):
                raise AttributeError('Unknown parameter {p} for {c}'.format
                    (p=name, c=self.__class__.__name__))
            elif name not in self._parameters and name not in self._buffers:
                setattr(self, name, val)
            elif torch.is_tensor(val):
                constraint = self.constraint_for_parameter_name(name)
                if (constraint is not None and constraint.enforced and not
                    constraint.check_raw(val)):
                    raise RuntimeError(
                        f"""Attempting to manually set a parameter value that is out of bounds of its current constraints, {constraint}. Most likely, you want to do the following:
 likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(better_lower_bound))"""
                        )
                try:
                    self.__getattr__(name).data.copy_(val.expand_as(self.
                        __getattr__(name)))
                except RuntimeError:
                    if not self._strict_init:
                        self.__getattr__(name).data = val
                    else:
                        self.__getattr__(name).data.copy_(val.view_as(self.
                            __getattr__(name)))
            elif isinstance(val, float):
                constraint = self.constraint_for_parameter_name(name)
                if constraint is not None and not constraint.check_raw(val):
                    raise RuntimeError(
                        f"""Attempting to manually set a parameter value that is out of bounds of its current constraints, {constraint}. Most likely, you want to do the following:
 likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(better_lower_bound))"""
                        )
                self.__getattr__(name).data.fill_(val)
            else:
                raise AttributeError(
                    'Type {t} not valid for initializing parameter {p}'.
                    format(t=type(val), p=name))
            prior_name = '_'.join([name, 'prior'])
            if prior_name in self._priors:
                prior, closure, _ = self._priors[prior_name]
                try:
                    prior._validate_sample(closure())
                except ValueError as e:
                    raise ValueError(
                        'Invalid input value for prior {}. Error:\n{}'.
                        format(prior_name, e))
        return self

    def named_added_loss_terms(self):
        """Returns an iterator over module variational strategies, yielding both
        the name of the variational strategy as well as the strategy itself.

        Yields:
            (string, VariationalStrategy): Tuple containing the name of the
                strategy and the strategy

        """
        return _extract_named_added_loss_terms(module=self, memo=None,
            prefix='')

    def named_hyperparameters(self):
        for name, param in self.named_parameters():
            if 'variational_' not in name:
                yield name, param

    def named_priors(self, memo=None, prefix=''):
        """Returns an iterator over the module's priors, yielding the name of the prior,
        the prior, the associated parameter names, and the transformation callable.

        Yields:
            (string, Prior, tuple((Parameter, callable)), callable): Tuple containing:
                - the name of the prior
                - the prior
                - a tuple of tuples (param, transform), one for each of the parameters associated with the prior
                - the prior's transform to be called on the parameters
        """
        return _extract_named_priors(module=self, memo=None, prefix='')

    def named_constraints(self, memo=None, prefix=''):
        return _extract_named_constraints(module=self, memo=None, prefix='')

    def named_variational_parameters(self):
        for name, param in self.named_parameters():
            if 'variational_' in name:
                yield name, param

    def register_added_loss_term(self, name):
        self._added_loss_terms[name] = None

    def register_parameter(self, name, parameter, prior=None):
        """
        Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.

        Args:
            :attr:`name` (str):
                The name of the parameter
            :attr:`parameter` (torch.nn.Parameter):
                The parameter
        """
        if prior is not None:
            raise DeprecationError(
                "Setting a prior upon registering a parameter is deprecated. Please use .register_prior('{name}_prior', prior, '{name}') instead."
                .format(name=name))
        if '_parameters' not in self.__dict__:
            raise AttributeError(
                'Cannot assign parameter before Module.__init__() call')
        super().register_parameter(name, parameter)

    def register_prior(self, name, prior, param_or_closure, setting_closure
        =None):
        """
        Adds a prior to the module. The prior can be accessed as an attribute using the given name.

        Args:
            :attr:`name` (str):
                The name of the prior
            :attr:`prior` (Prior):
                The prior to be registered`
            :attr:`param_or_closure` (string or callable):
                Either the name of the parameter, or a closure (which upon calling evalutes a function on
                one or more parameters):
                single parameter without a transform: `.register_prior("foo_prior", foo_prior, "foo_param")`
                transform a single parameter (e.g. put a log-Normal prior on it):
                `.register_prior("foo_prior", NormalPrior(0, 1), lambda: torch.log(self.foo_param))`
                function of multiple parameters:
                `.register_prior("foo2_prior", foo2_prior, lambda: f(self.param1, self.param2)))`
            :attr:`setting_closure` (callable, optional):
                A function taking in a tensor in (transformed) parameter space and initializing the
                internal parameter representation to the proper value by applying the inverse transform.
                Enables setting parametres directly in the transformed space, as well as sampling
                parameter values from priors (see `sample_from_prior`)

        """
        if isinstance(param_or_closure, str):
            if param_or_closure not in self._parameters and not hasattr(self,
                param_or_closure):
                raise AttributeError(
                    'Unknown parameter {name} for {module}'.format(name=
                    param_or_closure, module=self.__class__.__name__) +
                    ' Make sure the parameter is registered before registering a prior.'
                    )

            def closure():
                return getattr(self, param_or_closure)
            if setting_closure is not None:
                raise RuntimeError(
                    'Must specify a closure instead of a parameter name when providing setting_closure'
                    )

            def setting_closure(val):
                return self.initialize(**{param_or_closure: val})
        else:
            closure = param_or_closure
        self.add_module(name, prior)
        self._priors[name] = prior, closure, setting_closure

    def register_constraint(self, param_name, constraint, replace=True):
        if param_name not in self._parameters:
            raise RuntimeError(
                'Attempting to register constraint for nonexistent parameter.')
        constraint_name = param_name + '_constraint'
        if constraint_name in self._constraints:
            current_constraint = self._constraints[constraint_name]
        else:
            current_constraint = None
        if isinstance(current_constraint, Interval) and not replace:
            new_constraint = constraint.intersect(current_constraint)
        else:
            new_constraint = constraint
        self.add_module(constraint_name, new_constraint)
        self._constraints[constraint_name] = new_constraint
        if new_constraint.initial_value is not None:
            self.initialize(**{param_name: new_constraint.initial_value})

    def constraint_for_parameter_name(self, param_name):
        base_module = self
        base_name = param_name
        while '.' in base_name:
            components = base_name.split('.')
            submodule_name = components[0]
            submodule = getattr(base_module, submodule_name)
            base_module = submodule
            base_name = '.'.join(components[1:])
        try:
            constraint_name = base_name + '_constraint'
            return base_module._constraints.get(constraint_name)
        except AttributeError:
            return None

    def _load_state_hook_ignore_shapes(self, state_dict, prefix,
        local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
        if not self._load_strict_shapes:
            local_name_params = itertools.chain(self._parameters.items(),
                self._buffers.items())
            local_state = {k: v for k, v in local_name_params if v is not None}
            for name, param in local_state.items():
                key = prefix + name
                if key in state_dict:
                    param.data = state_dict[key].data

    def load_strict_shapes(self, value):

        def apply_fn(module):
            module._load_strict_shapes = value
        self.apply(apply_fn)

    def named_parameters_and_constraints(self):
        for name, param in self.named_parameters():
            yield name, param, self.constraint_for_parameter_name(name)

    def sample_from_prior(self, prior_name):
        """Sample parameter values from prior. Modifies the module's parameters in-place."""
        if prior_name not in self._priors:
            raise RuntimeError("Unknown prior name '{}'".format(prior_name))
        prior, _, setting_closure = self._priors[prior_name]
        if setting_closure is None:
            raise RuntimeError(
                'Must provide inverse transform to be able to sample from prior.'
                )
        setting_closure(prior.sample())

    def pyro_sample_from_prior(self):
        """
        For each parameter in this Module and submodule that have defined priors, sample a value for that parameter
        from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.

        This method can be used in a Pyro model to conveniently define pyro sample sites for all
        parameters of the model that have GPyTorch priors registered to them.
        """
        return _pyro_sample_from_prior(module=self, memo=None, prefix='')

    def local_load_samples(self, samples_dict, memo, prefix):
        """
        Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro
        sampling mechanism.

        The default behavior here should almost always be called from any overriding class. However, a class may
        want to add additional functionality, such as reshaping things to account for the fact that parameters will
        acquire an extra batch dimension corresponding to the number of samples drawn.
        """
        self._strict(False)
        for name, (prior, closure, setting_closure) in self._priors.items():
            if prior is not None and prior not in memo:
                memo.add(prior)
                setting_closure(samples_dict[prefix + ('.' if prefix else
                    '') + name])
        self._strict(True)

    def pyro_load_from_samples(self, samples_dict):
        """
        Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`
        is typically produced by a Pyro sampling mechanism.

        Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather
        than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with
        the prior to properly set the unconstrained parameter.

        Args:
            :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.
        """
        return _pyro_load_from_samples(module=self, samples_dict=
            samples_dict, memo=None, prefix='')

    def update_added_loss_term(self, name, added_loss_term):
        if not isinstance(added_loss_term, AddedLossTerm):
            raise RuntimeError('added_loss_term must be a AddedLossTerm')
        if name not in self._added_loss_terms.keys():
            raise RuntimeError('added_loss_term {} not registered'.format(name)
                )
        self._added_loss_terms[name] = added_loss_term

    def variational_parameters(self):
        for _, param in self.named_variational_parameters():
            yield param

    def __getattr__(self, name):
        try:
            return super().__getattr__(name)
        except AttributeError as e:
            try:
                return super().__getattribute__(name)
            except AttributeError:
                raise e


class Prior(Distribution, Module, ABC):
    """
    Base class for Priors in GPyTorch.
    In GPyTorch, a parameter can be assigned a prior by passing it as the `prior` argument to
    :func:`~gpytorch.module.register_parameter`. GPyTorch performs internal bookkeeping of priors,
    and for each parameter with a registered prior includes the log probability of the parameter under its
    respective prior in computing the Marginal Log-Likelihood.
    """

    def transform(self, x):
        return self._transform(x) if self._transform is not None else x

    def log_prob(self, x):
        """Returns the log-probability of the parameter value under the prior."""
        return super(Prior, self).log_prob(self.transform(x))


class _VariationalStrategy(Module, ABC):
    """
    Abstract base class for all Variational Strategies.
    """

    def __init__(self, model, inducing_points, variational_distribution,
        learn_inducing_locations=True):
        super().__init__()
        object.__setattr__(self, 'model', model)
        inducing_points = inducing_points.clone()
        if inducing_points.dim() == 1:
            inducing_points = inducing_points.unsqueeze(-1)
        if learn_inducing_locations:
            self.register_parameter(name='inducing_points', parameter=torch
                .nn.Parameter(inducing_points))
        else:
            self.register_buffer('inducing_points', inducing_points)
        self._variational_distribution = variational_distribution
        self.register_buffer('variational_params_initialized', torch.tensor(0))

    @abstractproperty
    @cached(name='prior_distribution_memo')
    def prior_distribution(self):
        """
        The :func:`~gpytorch.variational.VariationalStrategy.prior_distribution` method determines how to compute the
        GP prior distribution of the inducing points, e.g. :math:`p(u) \\sim N(\\mu(X_u), K(X_u, X_u))`. Most commonly,
        this is done simply by calling the user defined GP prior on the inducing point data directly.

        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`
        :return: The distribution :math:`p( \\mathbf u)`
        """
        raise NotImplementedError

    @property
    @cached(name='variational_distribution_memo')
    def variational_distribution(self):
        return self._variational_distribution()

    def forward(self, x, inducing_points, inducing_values,
        variational_inducing_covar=None):
        """
        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the
        inducing point function values. Specifically, forward defines how to transform a variational distribution
        over the inducing point values, :math:`q(u)`, in to a variational distribution over the function values at
        specified locations x, :math:`q(f|x)`, by integrating :math:`\\int p(f|x, u)q(u)du`

        :param torch.Tensor x: Locations :math:`\\mathbf X` to get the
            variational posterior of the function values at.
        :param torch.Tensor inducing_points: Locations :math:`\\mathbf Z` of the inducing points
        :param torch.Tensor inducing_values: Samples of the inducing function values :math:`\\mathbf u`
            (or the mean of the distribution :math:`q(\\mathbf u)` if q is a Gaussian.
        :param ~gpytorch.lazy.LazyTensor variational_inducing_covar: If the distribuiton :math:`q(\\mathbf u)`
            is Gaussian, then this variable is the covariance matrix of that Gaussian. Otherwise, it will be
            :attr:`None`.

        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`
        :return: The distribution :math:`q( \\mathbf f(\\mathbf X))`
        """
        raise NotImplementedError

    def kl_divergence(self):
        """
        Compute the KL divergence between the variational inducing distribution :math:`q(\\mathbf u)`
        and the prior inducing distribution :math:`p(\\mathbf u)`.

        :rtype: torch.Tensor
        """
        with settings.max_preconditioner_size(0):
            kl_divergence = torch.distributions.kl.kl_divergence(self.
                variational_distribution, self.prior_distribution)
        return kl_divergence

    def train(self, mode=True):
        if self.training and not mode or mode:
            if hasattr(self, '_memoize_cache'):
                delattr(self, '_memoize_cache')
        return super().train(mode=mode)

    def __call__(self, x, prior=False):
        if prior:
            return self.model.forward(x)
        if self.training:
            if hasattr(self, '_memoize_cache'):
                delattr(self, '_memoize_cache')
                self._memoize_cache = dict()
        if not self.variational_params_initialized.item():
            prior_dist = self.prior_distribution
            self._variational_distribution.initialize_variational_distribution(
                prior_dist)
            self.variational_params_initialized.fill_(1)
        inducing_points = self.inducing_points
        if inducing_points.shape[:-2] != x.shape[:-2]:
            batch_shape = _mul_broadcast_shape(inducing_points.shape[:-2],
                x.shape[:-2])
            inducing_points = inducing_points.expand(*batch_shape, *
                inducing_points.shape[-2:])
            x = x.expand(*batch_shape, *x.shape[-2:])
        variational_dist_u = self.variational_distribution
        if isinstance(variational_dist_u, MultivariateNormal):
            return super().__call__(x, inducing_points, inducing_values=
                variational_dist_u.mean, variational_inducing_covar=
                variational_dist_u.lazy_covariance_matrix)
        elif isinstance(variational_dist_u, Delta):
            return super().__call__(x, inducing_points, inducing_values=
                variational_dist_u.mean, variational_inducing_covar=None)
        else:
            raise RuntimeError(
                f'Invalid variational distribuition ({type(variational_dist_u)}). Expected a multivariate normal or a delta distribution.'
                )


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile

class Test_cornellius_gp_gpytorch(_paritybench_base):
    pass
    @_fails_compile()
    def test_000(self):
        self._check(_DenseBlock(*[], **{'num_layers': 1, 'num_input_features': 4, 'bn_size': 4, 'growth_rate': 4, 'drop_rate': 0.5}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_001(self):
        self._check(_DenseLayer(*[], **{'num_input_features': 4, 'growth_rate': 4, 'bn_size': 4, 'drop_rate': 0.5}), [torch.rand([4, 4, 4, 4])], {})

    def test_002(self):
        self._check(_Transition(*[], **{'num_input_features': 4, 'num_output_features': 4}), [torch.rand([4, 4, 4, 4])], {})

