import sys
_module = sys.modules[__name__]
del sys
conf = _module
densenet = _module
LBFGS = _module
gpytorch = _module
beta_features = _module
constraints = _module
constraints = _module
distributions = _module
delta = _module
distribution = _module
multitask_multivariate_normal = _module
multivariate_normal = _module
functions = _module
_dsmm = _module
_inv_matmul = _module
_inv_quad = _module
_inv_quad_log_det = _module
_log_normal_cdf = _module
_matmul = _module
_root_decomposition = _module
matern_covariance = _module
rbf_covariance = _module
kernels = _module
additive_structure_kernel = _module
arc_kernel = _module
cosine_kernel = _module
cylindrical_kernel = _module
grid_interpolation_kernel = _module
grid_kernel = _module
index_kernel = _module
inducing_point_kernel = _module
keops = _module
keops_kernel = _module
matern_kernel = _module
rbf_kernel = _module
kernel = _module
lcm_kernel = _module
linear_kernel = _module
matern_kernel = _module
multi_device_kernel = _module
multitask_kernel = _module
newton_girard_additive_kernel = _module
periodic_kernel = _module
polynomial_kernel = _module
polynomial_kernel_grad = _module
product_structure_kernel = _module
rbf_kernel = _module
rbf_kernel_grad = _module
rff_kernel = _module
rq_kernel = _module
scale_kernel = _module
spectral_mixture_kernel = _module
lazy = _module
added_diag_lazy_tensor = _module
batch_repeat_lazy_tensor = _module
block_diag_lazy_tensor = _module
block_interleaved_lazy_tensor = _module
block_lazy_tensor = _module
cached_cg_lazy_tensor = _module
cat_lazy_tensor = _module
chol_lazy_tensor = _module
constant_mul_lazy_tensor = _module
diag_lazy_tensor = _module
interpolated_lazy_tensor = _module
keops_lazy_tensor = _module
kronecker_product_lazy_tensor = _module
lazy_evaluated_kernel_tensor = _module
lazy_tensor = _module
lazy_tensor_representation_tree = _module
matmul_lazy_tensor = _module
mul_lazy_tensor = _module
non_lazy_tensor = _module
psd_sum_lazy_tensor = _module
root_lazy_tensor = _module
sum_batch_lazy_tensor = _module
sum_lazy_tensor = _module
toeplitz_lazy_tensor = _module
zero_lazy_tensor = _module
likelihoods = _module
bernoulli_likelihood = _module
gaussian_likelihood = _module
likelihood = _module
likelihood_list = _module
multitask_gaussian_likelihood = _module
noise_models = _module
softmax_likelihood = _module
means = _module
constant_mean = _module
constant_mean_grad = _module
linear_mean = _module
mean = _module
multitask_mean = _module
zero_mean = _module
mlls = _module
_approximate_mll = _module
added_loss_term = _module
deep_approximate_mll = _module
exact_marginal_log_likelihood = _module
gamma_robust_variational_elbo = _module
inducing_point_kernel_added_loss_term = _module
marginal_log_likelihood = _module
noise_model_added_loss_term = _module
predictive_log_likelihood = _module
sum_marginal_log_likelihood = _module
variational_elbo = _module
models = _module
approximate_gp = _module
deep_gps = _module
deep_gp = _module
exact_gp = _module
exact_prediction_strategies = _module
gp = _module
model_list = _module
pyro = _module
_pyro_mixin = _module
pyro_gp = _module
module = _module
priors = _module
horseshoe_prior = _module
lkj_prior = _module
prior = _module
smoothed_box_prior = _module
torch_priors = _module
utils = _module
wishart_prior = _module
settings = _module
test = _module
base_kernel_test_case = _module
base_likelihood_test_case = _module
base_mean_test_case = _module
base_test_case = _module
lazy_tensor_test_case = _module
model_test_case = _module
utils = _module
variational_test_case = _module
broadcasting = _module
cholesky = _module
deprecation = _module
errors = _module
fft = _module
getitem = _module
grid = _module
interpolation = _module
lanczos = _module
linear_cg = _module
memoize = _module
pivoted_cholesky = _module
quadrature = _module
sparse = _module
stochastic_lq = _module
toeplitz = _module
transforms = _module
warnings = _module
variational = _module
_variational_distribution = _module
_variational_strategy = _module
additive_grid_interpolation_variational_strategy = _module
cholesky_variational_distribution = _module
delta_variational_distribution = _module
grid_interpolation_variational_strategy = _module
mean_field_variational_distribution = _module
multitask_variational_strategy = _module
orthogonally_decoupled_variational_strategy = _module
unwhitened_variational_strategy = _module
variational_strategy = _module
whitened_variational_strategy = _module
setup = _module
test_constraints = _module
test_delta = _module
test_multitask_multivariate_normal = _module
test_multivariate_normal = _module
examples = _module
test_batch_gp_regression = _module
test_batch_multitask_gp_regression = _module
test_batch_svgp_gp_regression = _module
test_decoupled_svgp_regression = _module
test_fixed_noise_fanatasy_updates = _module
test_grid_gp_regression = _module
test_hadamard_multitask_gp_regression = _module
test_independent_multitask_gp_regression = _module
test_kissgp_additive_classification = _module
test_kissgp_additive_regression = _module
test_kissgp_dkl_regression = _module
test_kissgp_gp_classification = _module
test_kissgp_gp_regression = _module
test_kissgp_kronecker_product_classification = _module
test_kissgp_kronecker_product_regression = _module
test_kissgp_multiplicative_regression = _module
test_kissgp_variational_regression = _module
test_kissgp_white_noise_regression = _module
test_kronecker_multitask_gp_regression = _module
test_kronecker_multitask_ski_gp_regression = _module
test_lcm_kernel_regression = _module
test_model_list_gp_regression = _module
test_pyro_integration = _module
test_sgpr_regression = _module
test_simple_gp_classification = _module
test_simple_gp_regression = _module
test_spectral_mixture_gp_regression = _module
test_svgp_gp_classification = _module
test_svgp_gp_regression = _module
test_unwhitened_svgp_regression = _module
test_white_noise_regression = _module
test_dsmm = _module
test_inv_matmul = _module
test_inv_quad = _module
test_inv_quad_log_det = _module
test_log_normal_cdf = _module
test_matern_covariance = _module
test_matmul = _module
test_rbf_covariance = _module
test_root_decomposition = _module
test_matern_kernel = _module
test_rbf_kernel = _module
test_additive_kernel = _module
test_arc_kernel = _module
test_cosine_kernel = _module
test_cylindrical_kernel = _module
test_grid_interpolation_kernel = _module
test_grid_kernel = _module
test_linear_kernel = _module
test_matern_kernel = _module
test_newton_girard_additive_kernel = _module
test_periodic_kernel = _module
test_polynomial_kernel = _module
test_polynomial_kernel_grad = _module
test_rbf_kernel = _module
test_rbf_kernel_grad = _module
test_rff_kernel = _module
test_rq_kernel = _module
test_scale_kernel = _module
test_spectral_mixture_kernel = _module
test_added_diag_lazy_tensor = _module
test_batch_repeat_lazy_tensor = _module
test_block_diag_lazy_tensor = _module
test_block_interleaved_lazy_tensor = _module
test_cached_cg_lazy_tensor = _module
test_cat_lazy_tensor = _module
test_chol_lazy_tensor = _module
test_constant_mul_lazy_tensor = _module
test_diag_lazy_tensor = _module
test_interpolated_lazy_tensor = _module
test_kronecker_product_lazy_tensor = _module
test_lazy_evaluated_kernel_tensor = _module
test_matmul_lazy_tensor = _module
test_mul_lazy_tensor = _module
test_non_lazy_tensor = _module
test_psd_sum_lazy_tensor = _module
test_root_lazy_tensor = _module
test_sum_batch_lazy_tensor = _module
test_sum_lazy_tensor = _module
test_toeplitz_lazy_tensor = _module
test_zero_lazy_tensor = _module
test_bernoulli_likelihood = _module
test_gaussian_likelihood = _module
test_general_multitask_gaussian_likelihood = _module
test_multitask_gaussian_likelihood = _module
test_softmax_likelihood = _module
test_constant_mean = _module
test_constant_mean_grad = _module
test_linear_mean = _module
test_multitask_mean = _module
test_zero_mean = _module
test_exact_gp = _module
test_model_list = _module
test_variational_gp = _module
test_gamma_prior = _module
test_horseshoe_prior = _module
test_lkj_prior = _module
test_multivariate_normal_prior = _module
test_normal_prior = _module
test_smoothed_box_prior = _module
test_cholesky = _module
test_fft = _module
test_getitem = _module
test_grid = _module
test_interpolation = _module
test_lanczos = _module
test_linear_cg = _module
test_pivoted_cholesky = _module
test_quadrature = _module
test_sparse = _module
test_toeplitz = _module
test_grid_interpolation_variational_strategy = _module
test_multitask_variational_strategy = _module
test_orthogonally_decoupled_variational_strategy = _module
test_unwhitened_variational_strategy = _module
test_variational_strategy = _module
test_whitened_variational_strategy = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, numbers, numpy, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import re


import torch


import torch.nn as nn


import torch.nn.functional as F


from collections import OrderedDict


import numpy as np


from functools import reduce


from copy import deepcopy


from torch.optim import Optimizer


import math


from torch import sigmoid


from torch.nn import Module


from torch.nn.functional import softplus


import numbers


from torch.distributions.kl import register_kl


from torch.distributions import Distribution as TDistribution


from torch.distributions import MultivariateNormal as TMultivariateNormal


from torch.distributions.utils import _standard_normal


from torch.distributions.utils import lazy_property


from torch.autograd import Function


import warnings


from torch.distributions import Normal


from math import pi


from typing import Optional


from typing import List


from typing import Tuple


from typing import Union


from torch import Tensor


import copy


from abc import abstractmethod


from torch.nn import ModuleList


from torch.nn.parallel import DataParallel


import logging


import itertools


from abc import ABC


from typing import Any


from torch.nn import Parameter


import functools


import string


from abc import abstractproperty


from torch import nn


from torch.distributions import Distribution


from numbers import Number


from torch.distributions import HalfCauchy


from torch.distributions import constraints


from torch.nn import Module as TModule


from torch.distributions.utils import broadcast_all


from torch.distributions import Gamma


from torch.distributions import LogNormal


from torch.distributions import MultivariateNormal


from torch.distributions import Uniform


import random


from itertools import combinations


from itertools import product


from typing import Generator


from torch import optim


from math import exp


from torch.utils.data import TensorDataset


from torch.utils.data import DataLoader


import time


class _DenseLayer(nn.Sequential):

    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super(_DenseLayer, self).__init__()
        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),
        self.add_module('relu1', nn.ReLU(inplace=True)),
        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)),
        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),
        self.add_module('relu2', nn.ReLU(inplace=True)),
        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)),
        self.drop_rate = drop_rate

    def forward(self, x):
        new_features = super(_DenseLayer, self).forward(x)
        if self.drop_rate > 0:
            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)
        return torch.cat([x, new_features], 1)


class _Transition(nn.Sequential):

    def __init__(self, num_input_features, num_output_features):
        super(_Transition, self).__init__()
        self.add_module('norm', nn.BatchNorm2d(num_input_features))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))
        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))


class _DenseBlock(nn.Sequential):

    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):
        super(_DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)
            self.add_module('denselayer%d' % (i + 1), layer)


class DenseNet(nn.Module):
    """Densenet-BC model class, based on
    `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
    Args:
        growth_rate (int) - how many filters to add each layer (`k` in paper)
        block_config (list of 3 or 4 ints) - how many layers in each pooling block
        num_init_features (int) - the number of filters to learn in the first convolution layer
        bn_size (int) - multiplicative factor for number of bottle neck layers
            (i.e. bn_size * k features in the bottleneck layer)
        drop_rate (float) - dropout rate after each dense layer
        num_classes (int) - number of classification classes
    """

    def __init__(self, growth_rate=12, block_config=(16, 16, 16), compression=0.5, num_init_features=24, bn_size=4, drop_rate=0, avgpool_size=8, num_classes=10):
        super(DenseNet, self).__init__()
        assert 0 < compression <= 1, 'compression of densenet should be between 0 and 1'
        self.avgpool_size = avgpool_size
        self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False))]))
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = _Transition(num_input_features=num_features, num_output_features=int(num_features * compression))
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = int(num_features * compression)
        self.features.add_module('norm_final', nn.BatchNorm2d(num_features))
        self.classifier = nn.Linear(num_features, num_classes)

    def forward(self, x):
        features = self.features(x)
        out = F.relu(features, inplace=True)
        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(features.size(0), -1)
        out = self.classifier(out)
        return out


def inv_sigmoid(x):
    return torch.log(x) - torch.log(1 - x)


def inv_softplus(x):
    return x + torch.log(-torch.expm1(-x))


TRANSFORM_REGISTRY = {torch.exp: torch.log, torch.nn.functional.softplus: inv_softplus, torch.sigmoid: inv_sigmoid}


def _get_inv_param_transform(param_transform, inv_param_transform=None):
    reg_inv_tf = TRANSFORM_REGISTRY.get(param_transform, None)
    if reg_inv_tf is None:
        if inv_param_transform is None:
            raise RuntimeError('Must specify inv_param_transform for custom param_transforms')
        return inv_param_transform
    elif inv_param_transform is not None and reg_inv_tf != inv_param_transform:
        raise RuntimeError('TODO')
    return reg_inv_tf


class Interval(Module):

    def __init__(self, lower_bound, upper_bound, transform=sigmoid, inv_transform=inv_sigmoid, initial_value=None):
        """
        Defines an interval constraint for GP model parameters, specified by a lower bound and upper bound. For usage
        details, see the documentation for :meth:`~gpytorch.module.Module.register_constraint`.

        Args:
            lower_bound (float or torch.Tensor): The lower bound on the parameter.
            upper_bound (float or torch.Tensor): The upper bound on the parameter.
        """
        lower_bound = torch.as_tensor(lower_bound)
        upper_bound = torch.as_tensor(upper_bound)
        if torch.any(torch.ge(lower_bound, upper_bound)):
            raise RuntimeError('Got parameter bounds with empty intervals.')
        super().__init__()
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self._transform = transform
        self._inv_transform = inv_transform
        self._initial_value = initial_value
        if transform is not None and inv_transform is None:
            self._inv_transform = _get_inv_param_transform(transform)

    def _apply(self, fn):
        self.lower_bound = fn(self.lower_bound)
        self.upper_bound = fn(self.upper_bound)
        return super()._apply(fn)

    @property
    def enforced(self):
        return self._transform is not None

    def check(self, tensor):
        return bool(torch.all(tensor <= self.upper_bound) and torch.all(tensor >= self.lower_bound))

    def check_raw(self, tensor):
        return bool(torch.all(self.transform(tensor) <= self.upper_bound) and torch.all(self.transform(tensor) >= self.lower_bound))

    def intersect(self, other):
        """
        Returns a new Interval constraint that is the intersection of this one and another specified one.

        Args:
            other (Interval): Interval constraint to intersect with

        Returns:
            Interval: intersection if this interval with the other one.
        """
        if self.transform != other.transform:
            raise RuntimeError('Cant intersect Interval constraints with conflicting transforms!')
        lower_bound = torch.max(self.lower_bound, other.lower_bound)
        upper_bound = torch.min(self.upper_bound, other.upper_bound)
        return Interval(lower_bound, upper_bound)

    def transform(self, tensor):
        """
        Transforms a tensor to satisfy the specified bounds.

        If upper_bound is finite, we assume that `self.transform` saturates at 1 as tensor -> infinity. Similarly,
        if lower_bound is finite, we assume that `self.transform` saturates at 0 as tensor -> -infinity.

        Example transforms for one of the bounds being finite include torch.exp and torch.nn.functional.softplus.
        An example transform for the case where both are finite is torch.nn.functional.sigmoid.
        """
        if not self.enforced:
            return tensor
        if settings.debug.on():
            max_bound = torch.max(self.upper_bound)
            min_bound = torch.min(self.lower_bound)
            if max_bound == math.inf or min_bound == -math.inf:
                raise RuntimeError('Cannot make an Interval directly with non-finite bounds. Use a derived class like GreaterThan or LessThan instead.')
        transformed_tensor = self._transform(tensor) * (self.upper_bound - self.lower_bound) + self.lower_bound
        return transformed_tensor

    def inverse_transform(self, transformed_tensor):
        """
        Applies the inverse transformation.
        """
        if not self.enforced:
            return transformed_tensor
        if settings.debug.on():
            max_bound = torch.max(self.upper_bound)
            min_bound = torch.min(self.lower_bound)
            if max_bound == math.inf or min_bound == -math.inf:
                raise RuntimeError('Cannot make an Interval directly with non-finite bounds. Use a derived class like GreaterThan or LessThan instead.')
        tensor = self._inv_transform((transformed_tensor - self.lower_bound) / (self.upper_bound - self.lower_bound))
        return tensor

    @property
    def initial_value(self):
        """
        The initial parameter value (if specified, None otherwise)
        """
        return self._initial_value

    def __repr__(self):
        if self.lower_bound.numel() == 1 and self.upper_bound.numel() == 1:
            return self._get_name() + f'({self.lower_bound:.3E}, {self.upper_bound:.3E})'
        else:
            return super().__repr__()

    def __iter__(self):
        yield self.lower_bound
        yield self.upper_bound


class GreaterThan(Interval):

    def __init__(self, lower_bound, transform=softplus, inv_transform=inv_softplus, initial_value=None):
        super().__init__(lower_bound=lower_bound, upper_bound=math.inf, transform=transform, inv_transform=inv_transform, initial_value=initial_value)

    def __repr__(self):
        if self.lower_bound.numel() == 1:
            return self._get_name() + f'({self.lower_bound:.3E})'
        else:
            return super().__repr__()

    def transform(self, tensor):
        transformed_tensor = self._transform(tensor) + self.lower_bound if self.enforced else tensor
        return transformed_tensor

    def inverse_transform(self, transformed_tensor):
        tensor = self._inv_transform(transformed_tensor - self.lower_bound) if self.enforced else transformed_tensor
        return tensor


class Positive(GreaterThan):

    def __init__(self, transform=softplus, inv_transform=inv_softplus, initial_value=None):
        super().__init__(lower_bound=0.0, transform=transform, inv_transform=inv_transform, initial_value=initial_value)

    def __repr__(self):
        return self._get_name() + '()'

    def transform(self, tensor):
        transformed_tensor = self._transform(tensor) if self.enforced else tensor
        return transformed_tensor

    def inverse_transform(self, transformed_tensor):
        tensor = self._inv_transform(transformed_tensor) if self.enforced else transformed_tensor
        return tensor


class LessThan(Interval):

    def __init__(self, upper_bound, transform=softplus, inv_transform=inv_softplus):
        super().__init__(lower_bound=-math.inf, upper_bound=upper_bound, transform=transform, inv_transform=inv_transform)

    def transform(self, tensor):
        transformed_tensor = -self._transform(-tensor) + self.upper_bound if self.enforced else tensor
        return transformed_tensor

    def inverse_transform(self, transformed_tensor):
        tensor = -self._inv_transform(-(transformed_tensor - self.upper_bound)) if self.enforced else transformed_tensor
        return tensor

    def __repr__(self):
        return self._get_name() + f'({self.upper_bound:.3E})'


def default_postprocess_script(x):
    return x


class Distance(torch.nn.Module):

    def __init__(self, postprocess_script=default_postprocess_script):
        super().__init__()
        self._postprocess = postprocess_script

    def _sq_dist(self, x1, x2, postprocess, x1_eq_x2=False):
        adjustment = x1.mean(-2, keepdim=True)
        x1 = x1 - adjustment
        x2 = x2 - adjustment
        x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)
        x1_pad = torch.ones_like(x1_norm)
        if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:
            x2_norm, x2_pad = x1_norm, x1_pad
        else:
            x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)
            x2_pad = torch.ones_like(x2_norm)
        x1_ = torch.cat([-2.0 * x1, x1_norm, x1_pad], dim=-1)
        x2_ = torch.cat([x2, x2_pad, x2_norm], dim=-1)
        res = x1_.matmul(x2_.transpose(-2, -1))
        if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:
            res.diagonal(dim1=-2, dim2=-1).fill_(0)
        res.clamp_min_(0)
        return self._postprocess(res) if postprocess else res

    def _dist(self, x1, x2, postprocess, x1_eq_x2=False):
        res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)
        res = res.clamp_min_(1e-30).sqrt_()
        return self._postprocess(res) if postprocess else res


def _solve(lazy_tsr, rhs):
    if settings.fast_computations.solves.off() or settings.fast_computations.log_prob.off() or lazy_tsr.size(-1) <= settings.max_cholesky_size.value():
        return lazy_tsr._cholesky()._cholesky_solve(rhs)
    else:
        with torch.no_grad():
            preconditioner = lazy_tsr.detach()._inv_matmul_preconditioner()
        return lazy_tsr._solve(rhs, preconditioner)


class InvMatmul(Function):

    @staticmethod
    def forward(ctx, representation_tree, has_left, *args):
        left_tensor = None
        right_tensor = None
        matrix_args = None
        ctx.representation_tree = representation_tree
        ctx.has_left = has_left
        if ctx.has_left:
            left_tensor, right_tensor, *matrix_args = args
        else:
            right_tensor, *matrix_args = args
        orig_right_tensor = right_tensor
        lazy_tsr = ctx.representation_tree(*matrix_args)
        ctx.is_vector = False
        if right_tensor.ndimension() == 1:
            right_tensor = right_tensor.unsqueeze(-1)
            ctx.is_vector = True
        if ctx.has_left:
            rhs = torch.cat([left_tensor.transpose(-1, -2), right_tensor], -1)
            solves = _solve(lazy_tsr, rhs)
            res = solves[(...), left_tensor.size(-2):]
            res = left_tensor @ res
        else:
            solves = _solve(lazy_tsr, right_tensor)
            res = solves
        if ctx.is_vector:
            res = res.squeeze(-1)
        if ctx.has_left:
            args = [solves, left_tensor, orig_right_tensor] + list(matrix_args)
        else:
            args = [solves, orig_right_tensor] + list(matrix_args)
        ctx.save_for_backward(*args)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return res

    @staticmethod
    def backward(ctx, grad_output):
        if ctx.has_left:
            solves, left_tensor, right_tensor, *matrix_args = ctx.saved_tensors
            left_solves = solves[(...), :left_tensor.size(-2)]
            right_solves = solves[(...), left_tensor.size(-2):]
        else:
            right_solves, right_tensor, *matrix_args = ctx.saved_tensors
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        arg_grads = [None] * len(matrix_args)
        left_grad = None
        right_grad = None
        if any(ctx.needs_input_grad):
            if ctx.is_vector:
                right_tensor = right_tensor.unsqueeze(-1)
                grad_output = grad_output.unsqueeze(-1)
            if not ctx.has_left:
                left_solves = InvMatmul.apply(ctx.representation_tree, False, grad_output, *matrix_args)
                if any(ctx.needs_input_grad[3:]):
                    arg_grads = lazy_tsr._quad_form_derivative(torch.cat([left_solves, right_solves], -1), torch.cat([right_solves, left_solves], -1).mul(-0.5))
                if ctx.needs_input_grad[2]:
                    right_grad = left_solves
                    if ctx.is_vector:
                        right_grad.squeeze_(-1)
                return tuple([None, None] + [right_grad] + list(arg_grads))
            else:
                left_solves = left_solves @ grad_output
                if ctx.needs_input_grad[3]:
                    left_grad = grad_output @ right_solves.transpose(-1, -2)
                if any(ctx.needs_input_grad[4:]):
                    arg_grads = lazy_tsr._quad_form_derivative(torch.cat([left_solves, right_solves], -1), torch.cat([right_solves, left_solves], -1).mul(-0.5))
                if ctx.needs_input_grad[2]:
                    right_grad = left_solves
                    if ctx.is_vector:
                        right_grad.squeeze_(-1)
                return tuple([None, None] + [left_grad, right_grad] + list(arg_grads))


class InvQuad(Function):
    """
    Given a PSD matrix A (or a batch of PSD matrices A), this function computes b A^{-1} b
    where b is a vector or batch of vectors
    """

    @staticmethod
    def forward(ctx, representation_tree, *args):
        """
        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)
        If inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)
        - the RHS of the matrix solves.

        Returns:
        - (Scalar) The inverse quadratic form (or None, if inv_quad is False)
        - (Scalar) The log determinant (or None, if logdet is False)
        """
        inv_quad_rhs, *matrix_args = args
        ctx.representation_tree = representation_tree
        lazy_tsr = ctx.representation_tree(*matrix_args)
        ctx.is_vector = False
        if inv_quad_rhs.ndimension() == 1:
            inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)
            ctx.is_vector = True
        inv_quad_solves = _solve(lazy_tsr, inv_quad_rhs)
        inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)
        to_save = matrix_args + [inv_quad_solves]
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return inv_quad_term

    @staticmethod
    def backward(ctx, inv_quad_grad_output):
        *matrix_args, inv_quad_solves = ctx.saved_tensors
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)
        neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(inv_quad_grad_output).mul(-1)
        matrix_arg_grads = [None] * len(matrix_args)
        if any(ctx.needs_input_grad[2:]):
            left_factors = neg_inv_quad_solves_times_grad_out
            right_factors = inv_quad_solves
            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors, right_factors)
        if ctx.needs_input_grad[1]:
            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul(-2)
        else:
            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)
        if ctx.is_vector:
            inv_quad_rhs_grad.squeeze_(-1)
        res = tuple([None] + [inv_quad_rhs_grad] + list(matrix_arg_grads))
        return tuple(res)


def lanczos_tridiag(matmul_closure, max_iter, dtype, device, matrix_shape, batch_shape=torch.Size(), init_vecs=None, num_init_vecs=1, tol=1e-05):
    """
    """
    multiple_init_vecs = False
    if not callable(matmul_closure):
        raise RuntimeError('matmul_closure should be a function callable object that multiples a (Lazy)Tensor by a vector. Got a {} instead.'.format(matmul_closure.__class__.__name__))
    if init_vecs is None:
        init_vecs = torch.randn(matrix_shape[-1], num_init_vecs, dtype=dtype, device=device)
        init_vecs = init_vecs.expand(*batch_shape, matrix_shape[-1], num_init_vecs)
    else:
        if settings.debug.on():
            if dtype != init_vecs.dtype:
                raise RuntimeError('Supplied dtype {} and init_vecs.dtype {} do not agree!'.format(dtype, init_vecs.dtype))
            if device != init_vecs.device:
                raise RuntimeError('Supplied device {} and init_vecs.device {} do not agree!'.format(device, init_vecs.device))
            if batch_shape != init_vecs.shape[:-2]:
                raise RuntimeError('batch_shape {} and init_vecs.shape {} do not agree!'.format(batch_shape, init_vecs.shape))
            if matrix_shape[-1] != init_vecs.size(-2):
                raise RuntimeError('matrix_shape {} and init_vecs.shape {} do not agree!'.format(matrix_shape, init_vecs.shape))
        num_init_vecs = init_vecs.size(-1)
    num_iter = min(max_iter, matrix_shape[-1])
    dim_dimension = -2
    q_mat = torch.zeros(num_iter, *batch_shape, matrix_shape[-1], num_init_vecs, dtype=dtype, device=device)
    t_mat = torch.zeros(num_iter, num_iter, *batch_shape, num_init_vecs, dtype=dtype, device=device)
    q_0_vec = init_vecs / torch.norm(init_vecs, 2, dim=dim_dimension).unsqueeze(dim_dimension)
    q_mat[0].copy_(q_0_vec)
    r_vec = matmul_closure(q_0_vec)
    alpha_0 = q_0_vec.mul(r_vec).sum(dim_dimension)
    r_vec.sub_(alpha_0.unsqueeze(dim_dimension).mul(q_0_vec))
    beta_0 = torch.norm(r_vec, 2, dim=dim_dimension)
    t_mat[0, 0].copy_(alpha_0)
    t_mat[0, 1].copy_(beta_0)
    t_mat[1, 0].copy_(beta_0)
    q_mat[1].copy_(r_vec.div_(beta_0.unsqueeze(dim_dimension)))
    for k in range(1, num_iter):
        q_prev_vec = q_mat[k - 1]
        q_curr_vec = q_mat[k]
        beta_prev = t_mat[k, k - 1].unsqueeze(dim_dimension)
        r_vec = matmul_closure(q_curr_vec) - q_prev_vec.mul(beta_prev)
        alpha_curr = q_curr_vec.mul(r_vec).sum(dim_dimension, keepdim=True)
        t_mat[k, k].copy_(alpha_curr.squeeze(dim_dimension))
        if k + 1 < num_iter:
            r_vec.sub_(alpha_curr.mul(q_curr_vec))
            correction = r_vec.unsqueeze(0).mul(q_mat[:k + 1]).sum(dim_dimension, keepdim=True)
            correction = q_mat[:k + 1].mul(correction).sum(0)
            r_vec.sub_(correction)
            r_vec_norm = torch.norm(r_vec, 2, dim=dim_dimension, keepdim=True)
            r_vec.div_(r_vec_norm)
            beta_curr = r_vec_norm.squeeze_(dim_dimension)
            t_mat[k, k + 1].copy_(beta_curr)
            t_mat[k + 1, k].copy_(beta_curr)
            inner_products = q_mat[:k + 1].mul(r_vec.unsqueeze(0)).sum(dim_dimension)
            could_reorthogonalize = False
            for _ in range(10):
                if not torch.sum(inner_products > tol):
                    could_reorthogonalize = True
                    break
                correction = r_vec.unsqueeze(0).mul(q_mat[:k + 1]).sum(dim_dimension, keepdim=True)
                correction = q_mat[:k + 1].mul(correction).sum(0)
                r_vec.sub_(correction)
                r_vec_norm = torch.norm(r_vec, 2, dim=dim_dimension, keepdim=True)
                r_vec.div_(r_vec_norm)
                inner_products = q_mat[:k + 1].mul(r_vec.unsqueeze(0)).sum(dim_dimension)
            q_mat[k + 1].copy_(r_vec)
            if torch.sum(beta_curr.abs() > 1e-06) == 0 or not could_reorthogonalize:
                break
    num_iter = k + 1
    q_mat = q_mat[:num_iter + 1].permute(-1, *range(1, 1 + len(batch_shape)), -2, 0).contiguous()
    t_mat = t_mat[:num_iter + 1, :num_iter + 1].permute(-1, *range(2, 2 + len(batch_shape)), 0, 1).contiguous()
    if not multiple_init_vecs:
        q_mat.squeeze_(0)
        t_mat.squeeze_(0)
    return q_mat, t_mat


class StochasticLQ(object):
    """
    Implements an approximate log determinant calculation for symmetric positive definite matrices
    using stochastic Lanczos quadrature. For efficient calculation of derivatives, We additionally
    compute the trace of the inverse using the same probe vector the log determinant was computed
    with. For more details, see Dong et al. 2017 (in submission).
    """

    def __init__(self, max_iter=15, num_random_probes=10):
        """
        The nature of stochastic Lanczos quadrature is that the calculation of tr(f(A)) is both inaccurate and
        stochastic. An instance of StochasticLQ has two parameters that control these tradeoffs. Increasing either
        parameter increases the running time of the algorithm.

        Args:
            - cls - Tensor constructor - to ensure correct type (default - default tensor)
            - max_iter (scalar) - The number of Lanczos iterations to perform. Increasing this makes the estimate of
                tr(f(A)) more accurate in expectation -- that is, the average value returned has lower error.
            - num_random_probes (scalar) - The number of random probes to use in the stochastic trace estimation.
                Increasing this makes the estimate of tr(f(A)) lower variance -- that is, the value
                returned is more consistent.
        """
        self.max_iter = max_iter
        self.num_random_probes = num_random_probes

    def lanczos_batch(self, matmul_closure, rhs_vectors):
        return lanczos_tridiag(matmul_closure, self.max_iter, init_vecs=rhs_vectors, dtype=rhs_vectors.dtype, device=rhs_vectors.device, batch_shape=rhs_vectors.shape[-2:], matrix_shape=torch.Size((rhs_vectors.size(-2), rhs_vectors.size(-2))))

    def evaluate(self, matrix_shape, eigenvalues, eigenvectors, funcs):
        """
        Computes tr(f(A)) for an arbitrary list of functions, where f(A) is equivalent to applying the function
        elementwise to the eigenvalues of A, i.e., if A = V\\LambdaV^{T}, then f(A) = Vf(\\Lambda)V^{T}, where
        f(\\Lambda) is applied elementwise.
        Note that calling this function with a list of functions to apply is significantly more efficient than
        calling it multiple times with one function -- each additional function after the first requires negligible
        additional computation.

        Args:
            - matrix_shape (torch.Size()) - size of underlying matrix (not including batch dimensions)
            - eigenvalues (Tensor n_probes x ...batch_shape x k) - batches of eigenvalues from Lanczos tridiag mats
            - eigenvectors (Tensor n_probes x ...batch_shape x k x k) - batches of eigenvectors from " " "
            - funcs (list of closures) - A list of functions [f_1,...,f_k]. tr(f_i(A)) is computed for each function.
                Each function in the closure should expect to take a torch vector of eigenvalues as input and apply
                the function elementwise. For example, to compute logdet(A) = tr(log(A)), [lambda x: x.log()] would
                be a reasonable value of funcs.

        Returns:
            - results (list of scalars) - The trace of each supplied function applied to the matrix, e.g.,
                [tr(f_1(A)),tr(f_2(A)),...,tr(f_k(A))].
        """
        batch_shape = torch.Size(eigenvalues.shape[1:-1])
        results = [torch.zeros(batch_shape, dtype=eigenvalues.dtype, device=eigenvalues.device) for _ in funcs]
        num_random_probes = eigenvalues.size(0)
        for j in range(num_random_probes):
            eigenvalues_for_probe = eigenvalues[j]
            eigenvectors_for_probe = eigenvectors[j]
            for i, func in enumerate(funcs):
                eigenvecs_first_component = eigenvectors_for_probe[(...), (0), :]
                func_eigenvalues = func(eigenvalues_for_probe)
                dot_products = (eigenvecs_first_component.pow(2) * func_eigenvalues).sum(-1)
                results[i] = results[i] + matrix_shape[-1] / float(num_random_probes) * dot_products
        return results


def lanczos_tridiag_to_diag(t_mat):
    """
    Given a num_init_vecs x num_batch x k x k tridiagonal matrix t_mat,
    returns a num_init_vecs x num_batch x k set of eigenvalues
    and a num_init_vecs x num_batch x k x k set of eigenvectors.

    TODO: make the eigenvalue computations done in batch mode.
    """
    orig_device = t_mat.device
    if t_mat.size(-1) < 32:
        retr = torch.symeig(t_mat.cpu(), eigenvectors=True)
    else:
        retr = torch.symeig(t_mat, eigenvectors=True)
    evals, evecs = retr
    mask = evals.ge(0)
    evecs = evecs * mask.type_as(evecs).unsqueeze(-2)
    evals = evals.masked_fill_(~mask, 1)
    return evals, evecs


class InvQuadLogDet(Function):
    """
    Given a PSD matrix A (or a batch of PSD matrices A), this function computes one or both
    of the following
    - The matrix solves A^{-1} b
    - logdet(A)
    """

    @staticmethod
    def forward(ctx, representation_tree, dtype, device, matrix_shape, batch_shape=torch.Size(), inv_quad=False, logdet=False, probe_vectors=None, probe_vector_norms=None, *args):
        """
        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)
        If self.inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)
        - the RHS of the matrix solves.

        Returns:
        - (Scalar) The inverse quadratic form (or None, if self.inv_quad is False)
        - (Scalar) The log determinant (or None, self.if logdet is False)
        """
        if not (inv_quad or logdet):
            raise RuntimeError('Either inv_quad or logdet must be true (or both)')
        ctx.representation_tree = representation_tree
        ctx.dtype = dtype
        ctx.device = device
        ctx.matrix_shape = matrix_shape
        ctx.batch_shape = batch_shape
        ctx.inv_quad = inv_quad
        ctx.logdet = logdet
        matrix_args = None
        inv_quad_rhs = None
        if ctx.inv_quad:
            matrix_args = args[1:]
            inv_quad_rhs = args[0]
        else:
            matrix_args = args
        lazy_tsr = ctx.representation_tree(*matrix_args)
        with torch.no_grad():
            preconditioner, precond_lt, logdet_correction = lazy_tsr._preconditioner()
        ctx.preconditioner = preconditioner
        if (probe_vectors is None or probe_vector_norms is None) and logdet:
            num_random_probes = settings.num_trace_samples.value()
            if preconditioner is None:
                if settings.deterministic_probes.on():
                    warnings.warn("Deterministic probes will currently work only if you aren't training multiple independent models simultaneously.", UserWarning)
                    if settings.deterministic_probes.probe_vectors is None:
                        probe_vectors = torch.empty(matrix_shape[-1], num_random_probes, dtype=dtype, device=device)
                        probe_vectors.bernoulli_().mul_(2).add_(-1)
                        settings.deterministic_probes.probe_vectors = probe_vectors
                    else:
                        probe_vectors = settings.deterministic_probes.probe_vectors
                else:
                    probe_vectors = torch.empty(matrix_shape[-1], num_random_probes, dtype=dtype, device=device)
                    probe_vectors.bernoulli_().mul_(2).add_(-1)
                probe_vector_norms = torch.norm(probe_vectors, 2, dim=-2, keepdim=True)
                if batch_shape is not None:
                    probe_vectors = probe_vectors.expand(*batch_shape, matrix_shape[-1], num_random_probes)
                    probe_vector_norms = probe_vector_norms.expand(*batch_shape, 1, num_random_probes)
            else:
                if precond_lt.size()[-2:] == torch.Size([1, 1]):
                    covar_root = precond_lt.evaluate().sqrt()
                else:
                    covar_root = precond_lt.root_decomposition().root
                if settings.deterministic_probes.on():
                    warnings.warn("Deterministic probes will currently work only if you aren't training multiple independent models simultaneously.", UserWarning)
                    base_samples = settings.deterministic_probes.probe_vectors
                    if base_samples is None or covar_root.size(-1) != base_samples.size(-2):
                        base_samples = torch.randn(*precond_lt.batch_shape, covar_root.size(-1), num_random_probes, dtype=precond_lt.dtype, device=precond_lt.device)
                        settings.deterministic_probes.probe_vectors = base_samples
                    probe_vectors = covar_root.matmul(base_samples).permute(-1, *range(precond_lt.dim() - 1))
                else:
                    base_samples = torch.randn(*precond_lt.batch_shape, covar_root.size(-1), num_random_probes, dtype=precond_lt.dtype, device=precond_lt.device)
                    probe_vectors = precond_lt.zero_mean_mvn_samples(num_random_probes)
                probe_vectors = probe_vectors.unsqueeze(-2).transpose(0, -2).squeeze(0).transpose(-2, -1).contiguous()
                probe_vector_norms = torch.norm(probe_vectors, p=2, dim=-2, keepdim=True)
            probe_vectors = probe_vectors.div(probe_vector_norms)
        ctx.probe_vectors = probe_vectors
        ctx.probe_vector_norms = probe_vector_norms
        if ctx.logdet and not ctx.probe_vectors.numel():
            raise RuntimeError('Probe vectors were not supplied for logdet computation')
        rhs_list = []
        num_random_probes = 0
        num_inv_quad_solves = 0
        if ctx.logdet:
            rhs_list.append(ctx.probe_vectors)
            num_random_probes = ctx.probe_vectors.size(-1)
        ctx.is_vector = False
        if ctx.inv_quad:
            if inv_quad_rhs.ndimension() == 1:
                inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)
                ctx.is_vector = True
            rhs_list.append(inv_quad_rhs)
            num_inv_quad_solves = inv_quad_rhs.size(-1)
        rhs = torch.cat(rhs_list, -1)
        t_mat = None
        if ctx.logdet and settings.skip_logdet_forward.off():
            solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)
        else:
            solves = lazy_tsr._solve(rhs, preconditioner, num_tridiag=0)
        logdet_term = torch.zeros(lazy_tsr.batch_shape, dtype=ctx.dtype, device=ctx.device)
        inv_quad_term = torch.zeros(lazy_tsr.batch_shape, dtype=ctx.dtype, device=ctx.device)
        if ctx.logdet and settings.skip_logdet_forward.off():
            if torch.any(torch.isnan(t_mat)).item():
                logdet_term = torch.tensor(float('nan'), dtype=ctx.dtype, device=ctx.device)
            else:
                if ctx.batch_shape is None:
                    t_mat = t_mat.unsqueeze(1)
                eigenvalues, eigenvectors = lanczos_tridiag_to_diag(t_mat)
                slq = StochasticLQ()
                logdet_term, = slq.evaluate(ctx.matrix_shape, eigenvalues, eigenvectors, [lambda x: x.log()])
                if logdet_correction is not None:
                    logdet_term = logdet_term + logdet_correction
        if ctx.inv_quad:
            inv_quad_solves = solves.narrow(-1, num_random_probes, num_inv_quad_solves)
            inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)
        ctx.num_random_probes = num_random_probes
        ctx.num_inv_quad_solves = num_inv_quad_solves
        to_save = list(matrix_args) + [solves]
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return inv_quad_term, logdet_term

    @staticmethod
    def backward(ctx, inv_quad_grad_output, logdet_grad_output):
        matrix_arg_grads = None
        inv_quad_rhs_grad = None
        compute_inv_quad_grad = inv_quad_grad_output.abs().sum() and ctx.inv_quad
        compute_logdet_grad = logdet_grad_output.abs().sum() and ctx.logdet
        matrix_args = ctx.saved_tensors[:-1]
        solves = ctx.saved_tensors[-1]
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        if ctx.inv_quad:
            inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)
        if compute_logdet_grad:
            logdet_grad_output = logdet_grad_output.unsqueeze(-1)
            logdet_grad_output.unsqueeze_(-1)
        probe_vector_solves = None
        inv_quad_solves = None
        neg_inv_quad_solves_times_grad_out = None
        if compute_logdet_grad:
            coef = 1.0 / ctx.probe_vectors.size(-1)
            probe_vector_solves = solves.narrow(-1, 0, ctx.num_random_probes).mul(coef)
            probe_vector_solves.mul_(ctx.probe_vector_norms).mul_(logdet_grad_output)
            probe_vectors = ctx.probe_vectors.mul(ctx.probe_vector_norms)
        if ctx.inv_quad:
            inv_quad_solves = solves.narrow(-1, ctx.num_random_probes, ctx.num_inv_quad_solves)
            neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(inv_quad_grad_output).mul_(-1)
        if any(ctx.needs_input_grad):
            left_factors_list = []
            right_factors_list = []
            if compute_logdet_grad:
                left_factors_list.append(probe_vector_solves)
                if ctx.preconditioner is not None:
                    probe_vectors = ctx.preconditioner(probe_vectors)
                right_factors_list.append(probe_vectors)
            if compute_inv_quad_grad:
                left_factors_list.append(neg_inv_quad_solves_times_grad_out)
                right_factors_list.append(inv_quad_solves)
            left_factors = torch.cat(left_factors_list, -1)
            right_factors = torch.cat(right_factors_list, -1)
            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors, right_factors)
        if compute_inv_quad_grad and ctx.needs_input_grad[9]:
            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul_(-2)
        elif ctx.inv_quad:
            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)
        if ctx.is_vector:
            inv_quad_rhs_grad.squeeze_(-1)
        if ctx.inv_quad:
            res = [inv_quad_rhs_grad] + list(matrix_arg_grads)
        else:
            res = list(matrix_arg_grads)
        return tuple([None] * 9 + res)


class LazyTensorRepresentationTree(object):

    def __init__(self, lazy_tsr):
        self._cls = lazy_tsr.__class__
        self._kwargs = lazy_tsr._kwargs
        counter = 0
        self.children = []
        for arg in lazy_tsr._args:
            if hasattr(arg, 'representation') and callable(arg.representation):
                representation_size = len(arg.representation())
                self.children.append((slice(counter, counter + representation_size, None), arg.representation_tree()))
                counter += representation_size
            else:
                self.children.append((counter, None))
                counter += 1

    def __call__(self, *flattened_representation):
        unflattened_representation = []
        for index, subtree in self.children:
            if subtree is None:
                unflattened_representation.append(flattened_representation[index])
            else:
                sub_representation = flattened_representation[index]
                unflattened_representation.append(subtree(*sub_representation))
        return self._cls(*unflattened_representation, **self._kwargs)


class Matmul(Function):

    @staticmethod
    def forward(ctx, representation_tree, rhs, *matrix_args):
        ctx.representation_tree = representation_tree
        orig_rhs = rhs
        if rhs.ndimension() == 1:
            is_vector = True
            rhs = rhs.unsqueeze(-1)
        else:
            is_vector = False
        lazy_tsr = ctx.representation_tree(*matrix_args)
        res = lazy_tsr._matmul(rhs)
        to_save = [orig_rhs] + list(matrix_args)
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        if is_vector:
            res = res.squeeze(-1)
        return res

    @staticmethod
    def backward(ctx, grad_output):
        rhs = ctx.saved_tensors[0]
        matrix_args = ctx.saved_tensors[1:]
        rhs_shape = rhs.shape
        rhs_grad = None
        arg_grads = [None] * len(matrix_args)
        if any(ctx.needs_input_grad[2:]):
            rhs = rhs.unsqueeze(-1) if rhs.ndimension() == 1 else rhs
            grad_output_matrix = grad_output.unsqueeze(-1) if grad_output.ndimension() == 1 else grad_output
            arg_grads = ctx.representation_tree(*matrix_args)._quad_form_derivative(grad_output_matrix, rhs)
        if ctx.needs_input_grad[1]:
            if hasattr(ctx, '_lazy_tsr'):
                lazy_tsr = ctx._lazy_tsr
            else:
                lazy_tsr = ctx.representation_tree(*matrix_args)
            if grad_output.dim() == 1:
                rhs_grad = lazy_tsr._t_matmul(grad_output.unsqueeze(-1)).squeeze(-1)
            else:
                rhs_grad = lazy_tsr._t_matmul(grad_output)
            if rhs_grad.dim() > len(rhs_shape):
                rhs_grad = rhs_grad.reshape(-1, *rhs_shape).sum(0)
        return tuple([None] + [rhs_grad] + list(arg_grads))


class NumericalWarning(RuntimeWarning):
    """
    Warning thrown when convergence criteria are not met, or when comptuations require extra stability.
    """
    pass


class RootDecomposition(Function):

    @staticmethod
    def forward(ctx, representation_tree, max_iter, dtype, device, batch_shape, matrix_shape, root, inverse, initial_vectors, *matrix_args):
        """
        :param list matrix_args: The arguments representing the symmetric matrix A (or batch of PSD matrices A)

        :rtype: (torch.Tensor, torch.Tensor)
        :return: :attr:`R`, such that :math:`R R^T \\approx A`, and :attr:`R_inv`, such that
            :math:`R_{inv} R_{inv}^T \\approx A^{-1}` (will only be populated if self.inverse = True)
        """
        ctx.representation_tree = representation_tree
        ctx.device = device
        ctx.dtype = dtype
        ctx.matrix_shape = matrix_shape
        ctx.max_iter = max_iter
        ctx.batch_shape = batch_shape
        ctx.root = root
        ctx.inverse = inverse
        ctx.initial_vectors = initial_vectors
        lazy_tsr = ctx.representation_tree(*matrix_args)
        matmul_closure = lazy_tsr._matmul
        q_mat, t_mat = lanczos.lanczos_tridiag(matmul_closure, ctx.max_iter, dtype=ctx.dtype, device=ctx.device, matrix_shape=ctx.matrix_shape, batch_shape=ctx.batch_shape, init_vecs=ctx.initial_vectors)
        if ctx.batch_shape is None:
            q_mat = q_mat.unsqueeze(-3)
            t_mat = t_mat.unsqueeze(-3)
        if t_mat.ndimension() == 3:
            q_mat = q_mat.unsqueeze(0)
            t_mat = t_mat.unsqueeze(0)
        n_probes = t_mat.size(0)
        mins = lazify(t_mat).diag().min(dim=-1, keepdim=True)[0].unsqueeze(-1)
        jitter_mat = settings.tridiagonal_jitter.value() * mins * torch.eye(t_mat.size(-1), device=t_mat.device, dtype=t_mat.dtype).expand_as(t_mat)
        eigenvalues, eigenvectors = lanczos.lanczos_tridiag_to_diag(t_mat + jitter_mat)
        q_mat = q_mat.matmul(eigenvectors)
        root_evals = eigenvalues.sqrt()
        root = torch.empty(0, dtype=q_mat.dtype, device=q_mat.device)
        inverse = torch.empty(0, dtype=q_mat.dtype, device=q_mat.device)
        if ctx.inverse:
            inverse = q_mat / root_evals.unsqueeze(-2)
        if ctx.root:
            root = q_mat * root_evals.unsqueeze(-2)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        if ctx.batch_shape is None:
            root = root.squeeze(1) if root.numel() else root
            q_mat = q_mat.squeeze(1)
            t_mat = t_mat.squeeze(1)
            root_evals = root_evals.squeeze(1)
            inverse = inverse.squeeze(1) if inverse.numel() else inverse
        if n_probes == 1:
            root = root.squeeze(0) if root.numel() else root
            q_mat = q_mat.squeeze(0)
            t_mat = t_mat.squeeze(0)
            root_evals = root_evals.squeeze(0)
            inverse = inverse.squeeze(0) if inverse.numel() else inverse
        to_save = list(matrix_args) + [q_mat, root_evals, inverse]
        ctx.save_for_backward(*to_save)
        return root, inverse

    @staticmethod
    def backward(ctx, root_grad_output, inverse_grad_output):
        if any(ctx.needs_input_grad):

            def is_empty(tensor):
                return tensor.numel() == 0 or tensor.numel() == 1 and tensor[0] == 0
            if is_empty(root_grad_output):
                root_grad_output = None
            if is_empty(inverse_grad_output):
                inverse_grad_output = None
            matrix_args = ctx.saved_tensors[:-3]
            q_mat = ctx.saved_tensors[-3]
            root_evals = ctx.saved_tensors[-2]
            inverse = ctx.saved_tensors[-1]
            is_batch = False
            if root_grad_output is not None:
                if root_grad_output.ndimension() == 2 and q_mat.ndimension() > 2:
                    root_grad_output = root_grad_output.unsqueeze(0)
                    is_batch = True
                if root_grad_output.ndimension() == 3 and q_mat.ndimension() > 3:
                    root_grad_output = root_grad_output.unsqueeze(0)
                    is_batch = True
            if inverse_grad_output is not None:
                if inverse_grad_output.ndimension() == 2 and q_mat.ndimension() > 2:
                    inverse_grad_output = inverse_grad_output.unsqueeze(0)
                    is_batch = True
                if inverse_grad_output.ndimension() == 3 and q_mat.ndimension() > 3:
                    inverse_grad_output = inverse_grad_output.unsqueeze(0)
                    is_batch = True
            if hasattr(ctx, '_lazy_tsr'):
                lazy_tsr = ctx._lazy_tsr
            else:
                lazy_tsr = ctx.representation_tree(*matrix_args)
            if not ctx.inverse:
                inverse = q_mat / root_evals.unsqueeze(-2)
            left_factor = torch.zeros_like(inverse)
            if root_grad_output is not None:
                left_factor.add_(root_grad_output)
            if inverse_grad_output is not None:
                left_factor.sub_(torch.matmul(inverse, inverse_grad_output.transpose(-1, -2)).matmul(inverse))
            right_factor = inverse.div(2.0)
            if is_batch:
                left_factor = left_factor.permute(1, 0, 2, 3).contiguous()
                left_factor = left_factor.view(inverse.size(1), -1, left_factor.size(-1))
                right_factor = right_factor.permute(1, 0, 2, 3).contiguous()
                right_factor = right_factor.view(inverse.size(1), -1, right_factor.size(-1))
            else:
                left_factor = left_factor.contiguous()
                right_factor = right_factor.contiguous()
            res = lazy_tsr._quad_form_derivative(left_factor, right_factor)
            return tuple([None] * 9 + list(res))
        else:
            pass


def _mul_broadcast_shape(*shapes, error_msg=None):
    """Compute dimension suggested by multiple tensor indices (supports broadcasting)"""
    num_dims = max(len(shape) for shape in shapes)
    shapes = tuple([1] * (num_dims - len(shape)) + list(shape) for shape in shapes)
    final_size = []
    for size_by_dim in zip(*shapes):
        non_singleton_sizes = tuple(size for size in size_by_dim if size != 1)
        if len(non_singleton_sizes):
            if any(size != non_singleton_sizes[0] for size in non_singleton_sizes):
                if error_msg is None:
                    raise RuntimeError('Shapes are not broadcastable for mul operation')
                else:
                    raise RuntimeError(error_msg)
            final_size.append(non_singleton_sizes[0])
        else:
            final_size.append(1)
    return torch.Size(final_size)


_noop_index = slice(None, None, None)


def _compute_getitem_size(obj, indices):
    """
    Given an object and a tuple of indices, computes the final size of the
    Indices is a tuple containing ints, slices, and tensors

    .. note::
        The length of indices must match the dimensionality of obj

    Args:
        obj - tensor or LazyTensor
        indices - tuple of ints, slices, tensors

    Returns:
        :class:`torch.Size`
    """
    if obj.dim() != len(indices):
        raise RuntimeError('_compute_getitem_size assumes that obj (size: {}) and indices (len: {}) have the same dimensionality.'.format(obj.shape, len(indices)))
    final_shape = []
    tensor_idx = None
    tensor_idx_shape = None
    slice_after_tensor_idx = False
    for i, (size, idx) in enumerate(zip(obj.shape, indices)):
        if isinstance(idx, slice):
            if idx == _noop_index:
                final_shape.append(size)
            else:
                final_shape.append(len(range(*idx.indices(size))))
            if tensor_idx is not None:
                slice_after_tensor_idx = True
        elif isinstance(idx, int):
            if settings.debug.on():
                try:
                    range(size)[idx]
                except IndexError:
                    raise IndexError('index element {} ({}) is invalid: out of range for obj of size {}.'.format(i, idx, obj.shape))
        elif torch.is_tensor(idx):
            if tensor_idx_shape is None:
                tensor_idx_shape = idx.shape
                tensor_idx = len(final_shape)
            else:
                try:
                    tensor_idx_shape = _mul_broadcast_shape(tensor_idx_shape, idx.shape)
                except RuntimeError:
                    raise IndexError('Incompatible tensor indices in index - got shapes of {} .'.format([idx.shape for idx in indices if torch.is_tensor(idx)]))
                if slice_after_tensor_idx:
                    tensor_idx = 0
    if tensor_idx is not None:
        final_shape = final_shape[:tensor_idx] + list(tensor_idx_shape) + final_shape[tensor_idx:]
    return torch.Size(final_shape)


def _is_tensor_index_moved_to_start(indices):
    """
    Given an index, determine if the indexed part of the getitem is moved to the zero'th dimension
    """
    has_tensor_index = False
    continuous_tensor_index = True
    if torch.is_tensor(indices[0]):
        return True
    for index in indices[1:]:
        if torch.is_tensor(index):
            if not has_tensor_index:
                has_tensor_index = True
            elif not continuous_tensor_index:
                return True
        elif isinstance(index, slice):
            if has_tensor_index:
                continuous_tensor_index = False
    return False


def _pad_with_singletons(obj, num_singletons_before=0, num_singletons_after=0):
    """
    Pad obj with singleton dimensions on the left and right

    Example:
        >>> x = torch.randn(10, 5)
        >>> _pad_width_singletons(x, 2, 3).shape
        >>> # [1, 1, 10, 5, 1, 1, 1]
    """
    new_shape = [1] * num_singletons_before + list(obj.shape) + [1] * num_singletons_after
    return obj.view(*new_shape)


def _convert_indices_to_tensors(obj, indices):
    """
    Given an index made up of tensors/slices/ints, returns a tensor-only index that has the
    same outcome as the original index (when applied to the obj)

    .. note::
        The length of indices must match the dimensionality of obj

    Args:
        obj - tensor or LazyTensor
        indices - tuple of slices, tensors, ints

    Returns:
        tuple of tensor indices (shapes of tensors will involve broadcasting)

    Example:
        >>> x = torch.randn(3, 6, 4)
        >>> _convert_indices_to_tensors(x, (torch.tensor([0, 1]), 2, slice(None, None, None)))
        >>> # (torch.tensor([[[0]], [[1]]]), torch.tensor([[[2]]]), torch.tensor([[[0, 1, 2, 3]]]))
    """
    slice_indices = tuple(index for index in indices if isinstance(index, slice))
    tensor_indices = tuple(index for index in indices if torch.is_tensor(index))
    tensor_index_shape = _mul_broadcast_shape(*[tensor_index.shape for tensor_index in tensor_indices])
    num_final_dims = len(slice_indices) + len(tensor_index_shape)
    tensor_index_moved_to_start = _is_tensor_index_moved_to_start(indices)
    num_singletons_before = len(tensor_index_shape) if tensor_index_moved_to_start else 0
    num_singletons_after = num_final_dims - len(tensor_index_shape) if tensor_index_moved_to_start else num_final_dims
    num_singletons_before_tensor = 0 if tensor_index_moved_to_start else None
    num_singletons_after_tensor = num_final_dims - len(tensor_index_shape) if tensor_index_moved_to_start else None
    new_indices = []
    for dim, index in enumerate(indices):
        if isinstance(index, slice):
            num_singletons_after -= 1
            new_index = torch.arange(0, obj.size(dim), device=obj.device)[index]
            new_index = _pad_with_singletons(new_index, num_singletons_before, num_singletons_after)
            num_singletons_before += 1
        elif isinstance(index, int):
            new_index = torch.tensor(index, dtype=torch.long, device=obj.device)
            new_index = _pad_with_singletons(new_index, num_singletons_before, num_singletons_after)
        elif torch.is_tensor(index):
            if num_singletons_before_tensor is None:
                num_singletons_after -= len(tensor_index_shape)
                num_singletons_before_tensor = num_singletons_before
                num_singletons_after_tensor = num_singletons_after
                num_singletons_before += len(tensor_index_shape)
            new_index = _pad_with_singletons(index, num_singletons_before_tensor, num_singletons_after_tensor)
        new_indices.append(new_index)
    return tuple(new_indices)


def _is_noop_index(index):
    """
    Determine if a given index is a noop (e.g. ":")
    """
    return isinstance(index, slice) and index == _noop_index


def _matmul_broadcast_shape(shape_a, shape_b, error_msg=None):
    """Compute dimension of matmul operation on shapes (supports broadcasting)"""
    m, n, p = shape_a[-2], shape_a[-1], shape_b[-1]
    if len(shape_b) == 1:
        if n != p:
            if error_msg is None:
                raise RuntimeError(f'Incompatible dimensions for matmul: {shape_a} and {shape_b}')
            else:
                raise RuntimeError(error_msg)
        return shape_a[:-1]
    if n != shape_b[-2]:
        if error_msg is None:
            raise RuntimeError(f'Incompatible dimensions for matmul: {shape_a} and {shape_b}')
        else:
            raise RuntimeError(error_msg)
    tail_shape = torch.Size([m, p])
    batch_shape_a = shape_a[:-2]
    batch_shape_b = shape_b[:-2]
    if batch_shape_a == batch_shape_b:
        bc_shape = batch_shape_a
    else:
        bc_shape = _mul_broadcast_shape(batch_shape_a, batch_shape_b)
    return bc_shape + tail_shape


def add_to_cache(obj, name, val):
    """Add a result to the cache of an object."""
    if not hasattr(obj, '_memoize_cache'):
        obj._memoize_cache = dict()
    obj._memoize_cache[name] = val
    return obj


def is_in_cache(obj, name):
    return hasattr(obj, '_memoize_cache') and name in obj._memoize_cache


def get_from_cache(obj, name):
    """Get an item from the cache."""
    if not is_in_cache(obj, name):
        raise RuntimeError('Object does not have item {} stored in cache.'.format(name))
    return obj._memoize_cache[name]


def cached(method=None, name=None):
    """A decorator allowing for specifying the name of a cache, allowing it to be modified elsewhere."""
    if method is None:
        return functools.partial(cached, name=name)

    @functools.wraps(method)
    def g(self, *args, **kwargs):
        cache_name = name if name is not None else method
        if not is_in_cache(self, cache_name):
            add_to_cache(self, cache_name, method(self, *args, **kwargs))
        return get_from_cache(self, cache_name)
    return g


def delazify(obj):
    """
    A function which ensures that `obj` is a (normal) Tensor.

    If `obj` is a Tensor, this function does nothing.
    If `obj` is a LazyTensor, this function evaluates it.
    """
    if torch.is_tensor(obj):
        return obj
    elif isinstance(obj, LazyTensor):
        return obj.evaluate()
    else:
        raise TypeError('object of class {} cannot be made into a Tensor'.format(obj.__class__.__name__))


class NanError(RuntimeError):
    pass


def psd_safe_cholesky(A, upper=False, out=None, jitter=None):
    """Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.
    Args:
        :attr:`A` (Tensor):
            The tensor to compute the Cholesky decomposition of
        :attr:`upper` (bool, optional):
            See torch.cholesky
        :attr:`out` (Tensor, optional):
            See torch.cholesky
        :attr:`jitter` (float, optional):
            The jitter to add to the diagonal of A in case A is only p.s.d. If omitted, chosen
            as 1e-6 (float) or 1e-8 (double)
    """
    try:
        L = torch.cholesky(A, upper=upper, out=out)
        return L
    except RuntimeError as e:
        isnan = torch.isnan(A)
        if isnan.any():
            raise NanError(f'cholesky_cpu: {isnan.sum().item()} of {A.numel()} elements of the {A.shape} tensor are NaN.')
        if jitter is None:
            jitter = 1e-06 if A.dtype == torch.float32 else 1e-08
        Aprime = A.clone()
        jitter_prev = 0
        for i in range(3):
            jitter_new = jitter * 10 ** i
            Aprime.diagonal(dim1=-2, dim2=-1).add_(jitter_new - jitter_prev)
            jitter_prev = jitter_new
            try:
                L = torch.cholesky(Aprime, upper=upper, out=out)
                warnings.warn(f'A not p.d., added jitter of {jitter_new} to the diagonal', NumericalWarning)
                return L
            except RuntimeError:
                continue
        raise e


class LazyTensor(ABC):
    """
    Base class for LazyTensors in GPyTorch.

    In GPyTorch, nearly all covariance matrices for Gaussian processes are handled internally as some variety of
    LazyTensor. A LazyTensor is an object that represents a tensor object, similar to :class:`torch.tensor`, but
    typically differs in two ways:

    #. A tensor represented by a LazyTensor can typically be represented more efficiently than storing a full matrix.
       For example, a LazyTensor representing :math:`K=XX^{\\top}` where :math:`K` is :math:`n \\times n` but
       :math:`X` is :math:`n \\times d` might store :math:`X` instead of :math:`K` directly.
    #. A LazyTensor typically defines a matmul routine that performs :math:`KM` that is more efficient than storing
       the full matrix. Using the above example, performing :math:`KM=X(X^{\\top}M)` requires only :math:`O(nd)` time,
       rather than the :math:`O(n^2)` time required if we were storing :math:`K` directly.

    In order to define a new LazyTensor class that can be used as a covariance matrix in GPyTorch, a user must define
    at a minimum the following methods (in each example, :math:`K` denotes the matrix that the LazyTensor represents)

    * :func:`~gpytorch.lazy.LazyTensor._matmul`, which performs a matrix multiplication :math:`KM`
    * :func:`~gpytorch.lazy.LazyTensor._size`, which returns a :class:`torch.Size` containing the dimensions of
      :math:`K`.
    * :func:`~gpytorch.lazy.LazyTensor._transpose_nonbatch`, which returns a transposed version of the LazyTensor

    In addition to these, the following methods should be implemented for maximum efficiency

    * :func:`~gpytorch.lazy.LazyTensor._quad_form_derivative`, which computes the derivative of a quadratic form
      with the LazyTensor (e.g. :math:`d (a^T X b) / dX`).
    * :func:`~gpytorch.lazy.LazyTensor._get_indices`, which returns a :class:`torch.Tensor` containing elements that
      are given by various tensor indices.
    * :func:`~gpytorch.lazy.LazyTensor._expand_batch`, which expands the batch dimensions of LazyTensors.
    * :func:`~gpytorch.lazy.LazyTensor._check_args`, which performs error checking on the arguments supplied to the
      LazyTensor constructor.

    In addition to these, a LazyTensor *may* need to define the following functions if it does anything interesting
    with the batch dimensions (e.g. sums along them, adds additional ones, etc):
    :func:`~gpytorch.lazy.LazyTensor._unsqueeze_batch`, :func:`~gpytorch.lazy.LazyTensor._getitem`, and
    :func:`~gpytorch.lazy.LazyTensor._permute_batch`.
    See the documentation for these methods for details.

    .. note::
        The base LazyTensor class provides default implementations of many other operations in order to mimic the
        behavior of a standard tensor as closely as possible. For example, we provide default implementations of
        :func:`~gpytorch.lazy.LazyTensor.__getitem__`, :func:`~gpytorch.lazy.LazyTensor.__add__`, etc that either
        make use of other lazy tensors or exploit the functions that **must** be defined above.

        Rather than overriding the public methods, we recommend that you override the private versions associated
        with these methods (e.g. - write a custom `_getitem` verses a custom `__getitem__`). This is because the
        public methods do quite a bit of error checking and casing that doesn't need to be repeated.

    .. note::
        LazyTensors are designed by default to optionally represent batches of matrices. Thus, the size of a
        LazyTensor may be (for example) :math:`b \\times n \\times n`. Many of the methods are designed to efficiently
        operate on these batches if present.
    """

    def _check_args(self, *args, **kwargs):
        """
        (Optional) run checks to see that input arguments and kwargs are valid

        Return:
            None (if all checks pass) or str (error message to raise)
        """
        return None

    def __init__(self, *args, **kwargs):
        if settings.debug.on():
            err = self._check_args(*args, **kwargs)
            if err is not None:
                raise ValueError(err)
        self._args = args
        self._kwargs = kwargs

    @abstractmethod
    def _matmul(self, rhs):
        """
        Performs a matrix multiplication :math:`KM` with the matrix :math:`K` that this LazyTensor represents. Should
        behave as :func:`torch.matmul`. If the LazyTensor represents a batch of matrices, this method should therefore
        operate in batch mode as well.

        ..note::
            This method is intended to be used only internally by various Functions that support backpropagation
            (e.g., :class:`gpytorch.functions.Matmul`). Once this method is defined, it is strongly recommended that
            one use :func:`~gpytorch.lazy.LazyTensor.matmul` instead, which makes use of this method properly.

        Args:
            rhs (:obj:`torch.tensor`): the matrix :math:`M` to multiply with.

        Returns:
            :obj:`torch.tensor`: matrix * rhs
        """
        raise NotImplementedError('The class {} requires a _matmul function!'.format(self.__class__.__name__))

    @abstractmethod
    def _size(self):
        """
        Returns the size of the resulting Tensor that the lazy tensor represents.

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.size`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`torch.Size`: The size of the matrix :math:`K` represented by this LazyTensor
        """
        raise NotImplementedError('The class {} requires a _size function!'.format(self.__class__.__name__))

    @abstractmethod
    def _transpose_nonbatch(self):
        """
        Transposes non-batch dimensions (e.g. last two)
        Implement this method, rather than transpose() or t().

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.transpose`, which
            does some additional work. Calling this method directly is discouraged.
        """
        raise NotImplementedError('The class {} requires a _transpose_nonbatch function!'.format(self.__class__.__name__))

    def _permute_batch(self, *dims):
        """
        Permute the batch dimensions.
        This probably won't have to be overwritten by LazyTensors, unless they use batch dimensions
        in a special way (e.g. BlockDiagLazyTensor, SumBatchLazyTensor)

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.unsqueeze`,
            which does some additional work. Calling this method directly is discouraged.

        Args:
            dims (tuple of ints):
                The new order for the `self.dim() - 2` dimensions.
                It WILL contain each of the positive batch dimensions exactly once.
        """
        components = []
        for component in self._args:
            if torch.is_tensor(component):
                extra_dims = range(len(dims), component.dim())
                components.append(component.permute(*dims, *extra_dims))
            elif isinstance(component, LazyTensor):
                components.append(component._permute_batch(*dims))
            else:
                components.append(component)
        res = self.__class__(*components, **self._kwargs)
        return res

    def _getitem(self, row_index, col_index, *batch_indices):
        """
        Supports subindexing of the matrix this LazyTensor represents.

        The indices passed into this method will either be:
            Tensor indices
            Slices

        ..note::
            LazyTensor.__getitem__ uses this as a helper method. If you are writing your own custom LazyTensor,
            override this method rather than __getitem__ (so that you don't have to repeat the extra work)

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.__getitem__`,
            which does some additional work. Calling this method directly is discouraged.

        This method has a number of restrictions on the type of arguments that are passed in to reduce
        the complexity of __getitem__ calls in PyTorch. In particular:
            - This method only accepts slices and tensors for the row/column indices (no ints)
            - The row and column dimensions don't dissapear (e.g. from Tensor indexing). These cases are
              handled by the `_getindices` method

        Args:
            :attr:`row_index` (slice, Tensor):
                Index for the row of the LazyTensor
            :attr:`col_index` (slice, Tensor):
                Index for the col of the LazyTensor
            :attr:`batch_indices` (tuple of slice, int, Tensor):
                Indices for the batch dimensions

        Returns:
            `LazyTensor`
        """
        if _is_noop_index(row_index) and _is_noop_index(col_index):
            if len(batch_indices):
                components = [component[batch_indices] for component in self._args]
                res = self.__class__(*components, **self._kwargs)
                return res
            else:
                return self
        row_interp_indices = torch.arange(0, self.size(-2), dtype=torch.long, device=self.device).view(-1, 1)
        row_interp_indices = row_interp_indices.expand(*self.batch_shape, -1, 1)
        row_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(row_interp_indices)
        col_interp_indices = torch.arange(0, self.size(-1), dtype=torch.long, device=self.device).view(-1, 1)
        col_interp_indices = col_interp_indices.expand(*self.batch_shape, -1, 1)
        col_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(col_interp_indices)
        res = InterpolatedLazyTensor(self, row_interp_indices, row_interp_values, col_interp_indices, col_interp_values)
        return res._getitem(row_index, col_index, *batch_indices)

    def _unsqueeze_batch(self, dim):
        """
        Unsqueezes a batch dimension (positive-indexed only)
        This probably won't have to be overwritten by LazyTensors, unless they use batch dimensions
        in a special way (e.g. BlockDiagLazyTensor, SumBatchLazyTensor)

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.unsqueeze`,
            which does some additional work. Calling this method directly is discouraged.
        """
        components = [component.unsqueeze(dim) for component in self._args]
        res = self.__class__(*components, **self._kwargs)
        return res

    def _expand_batch(self, batch_shape):
        """
        Expands along batch dimensions.

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.expand`,
            which does some additional work. Calling this method directly is discouraged.
        """
        current_shape = torch.Size([(1) for _ in range(len(batch_shape) - self.dim() - 2)] + list(self.batch_shape))
        batch_repeat = torch.Size([(expand_size // current_size) for expand_size, current_size in zip(batch_shape, current_shape)])
        return self.repeat(*batch_repeat, 1, 1)

    def _get_indices(self, row_index, col_index, *batch_indices):
        """
        This method selects elements from the LazyTensor based on tensor indices for each dimension.
        All indices are tensor indices that are broadcastable.
        There will be exactly one index per dimension of the LazyTensor

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.__getitem__`,
            which does some additional work. Calling this method directly is discouraged.

        Args:
            row_index (LongTensor): indices to select from row of LazyTensor
            row_index (LongTensor): indices to select from col of LazyTensor
            batch_indices (tuple LongTensor): indices to select from batch dimensions.

        Returns:
            Tensor (size determined by broadcasted shape of indices) of selected values
        """
        final_shape = _mul_broadcast_shape(*(index.shape for index in batch_indices), row_index.shape, col_index.shape)
        row_index = row_index.expand(final_shape)
        col_index = col_index.expand(final_shape)
        batch_indices = tuple(index.expand(final_shape) for index in batch_indices)
        base_lazy_tensor = self._getitem(_noop_index, _noop_index, *batch_indices)._expand_batch(final_shape)
        row_interp_indices = torch.arange(0, self.size(-2), dtype=torch.long, device=self.device)
        row_interp_indices = row_interp_indices[row_index].unsqueeze_(-1).unsqueeze_(-1)
        row_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(row_interp_indices)
        col_interp_indices = torch.arange(0, self.size(-1), dtype=torch.long, device=self.device)
        col_interp_indices = col_interp_indices[col_index].unsqueeze_(-1).unsqueeze_(-1)
        col_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(col_interp_indices)
        res = InterpolatedLazyTensor(base_lazy_tensor, row_interp_indices, row_interp_values, col_interp_indices, col_interp_values).evaluate().squeeze(-2).squeeze(-1)
        return res

    def _quad_form_derivative(self, left_vecs, right_vecs):
        """
        Given u (left_vecs) and v (right_vecs),
        Computes the derivatives of (u^t K v) w.r.t. K

        ..note::
            This method is intended to be used only internally by various Functions that support backpropagation.
            For example, this method is used internally by :func:`~gpytorch.lazy.LazyTensor.inv_quad_logdet`. It is
            not likely that users will need to call this method directly.

        Returns:
            :obj:`torch.tensor`: derivative with respect to the arguments that are actually used to represent this
                                   this LazyTensor.
        """
        from collections import deque
        args = tuple(self.representation())
        args_with_grads = tuple(arg for arg in args if arg.requires_grad)
        if not len(args_with_grads):
            return tuple(None for _ in args)
        with torch.autograd.enable_grad():
            loss = (left_vecs * self._matmul(right_vecs)).sum()
            loss.requires_grad_(True)
            actual_grads = deque(torch.autograd.grad(loss, args_with_grads, allow_unused=True))
        grads = []
        for arg in args:
            if arg.requires_grad:
                grads.append(actual_grads.popleft())
            else:
                grads.append(None)
        return tuple(grads)
    _check_size = True

    @property
    def _args(self):
        return self._args_memo

    @_args.setter
    def _args(self, args):
        self._args_memo = args

    def _approx_diag(self):
        """
        (Optional) returns an (approximate) diagonal of the matrix

        Sometimes computing an exact diagonal is a bit computationally slow
        When we don't need an exact diagonal (e.g. for the pivoted cholesky
        decomposition, this function is called

        Defaults to calling the exact diagonal function

        Returns:
            tensor: - the diagonal (or batch of diagonals)
        """
        return self.diag()

    @cached(name='cholesky')
    def _cholesky(self):
        """
        (Optional) Cholesky-factorizes the LazyTensor

        ..note::
            This method is used as an internal helper. Calling this method directly is discouraged.

        Returns:
            (LazyTensor) Cholesky factor
        """
        evaluated_kern_mat = self.evaluate_kernel()
        if any(isinstance(sub_mat, KeOpsLazyTensor) for sub_mat in evaluated_kern_mat._args):
            raise RuntimeError('Cannot run Cholesky with KeOps: it will either be really slow or not work.')
        evaluated_mat = evaluated_kern_mat.evaluate()
        if evaluated_mat.size(-1) == 1:
            return NonLazyTensor(evaluated_mat.clamp_min(0.0).sqrt())
        cholesky = psd_safe_cholesky(evaluated_mat, jitter=settings.cholesky_jitter.value()).contiguous()
        return NonLazyTensor(cholesky)

    def _cholesky_solve(self, rhs):
        """
        (Optional) Assuming that `self` is a Cholesky factor, computes the cholesky solve

        ..note::
            This method is used as an internal helper. Calling this method directly is discouraged.

        Returns:
            (LazyTensor) Cholesky factor
        """
        return torch.cholesky_solve(rhs, self.evaluate())

    def _inv_matmul_preconditioner(self):
        """
        (Optional) define a preconditioner that can be used for linear systems, but not necessarily
        for log determinants. By default, this can call :meth:`~gpytorch.lazy.LazyTensor._preconditioner`.

        Returns:
            function: a function on x which performs P^{-1}(x)
        """
        base_precond, _, _ = self._preconditioner()
        if base_precond is not None:
            return base_precond
        elif gpytorch.beta_features.default_preconditioner.on():
            if hasattr(self, '_default_preconditioner_cache'):
                U, S, V = self._default_preconditioner_cache
            else:
                precond_basis_size = min(gpytorch.settings.max_preconditioner_size.value(), self.size(-1))
                random_basis = torch.randn(self.batch_shape + torch.Size((self.size(-2), precond_basis_size)), device=self.device, dtype=self.dtype)
                projected_mat = self._matmul(random_basis)
                proj_q = torch.qr(projected_mat)
                orthog_projected_mat = self._matmul(proj_q).transpose(-2, -1)
                U, S, V = torch.svd(orthog_projected_mat)
                U = proj_q.matmul(U)
                self._default_preconditioner_cache = U, S, V

            def preconditioner(v):
                res = V.transpose(-2, -1).matmul(v)
                res = (1 / S).unsqueeze(-1) * res
                res = U.matmul(res)
                return res
            return preconditioner
        else:
            return None

    def _mul_constant(self, other):
        """
        Multiplies the LazyTensor by a costant.

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.mul`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`
        """
        return ConstantMulLazyTensor(self, other)

    def _mul_matrix(self, other):
        """
        Multiplies the LazyTensor by a (batch of) matrices.

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.mul`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`
        """
        self = self.evaluate_kernel()
        other = other.evaluate_kernel()
        if isinstance(self, NonLazyTensor) or isinstance(other, NonLazyTensor):
            return NonLazyTensor(self.evaluate() * other.evaluate())
        else:
            left_lazy_tensor = self if self._root_decomposition_size() < other._root_decomposition_size() else other
            right_lazy_tensor = other if left_lazy_tensor is self else self
            return MulLazyTensor(left_lazy_tensor.root_decomposition(), right_lazy_tensor.root_decomposition())

    def _preconditioner(self):
        """
        (Optional) define a preconditioner (P) for linear conjugate gradients

        Returns:
            function: a function on x which performs P^{-1}(x)
            scalar: the log determinant of P
        """
        return None, None, None

    def _probe_vectors_and_norms(self):
        return None, None

    def _prod_batch(self, dim):
        """
        Multiply the LazyTensor across a batch dimension (supplied as a positive number).

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.prod`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`
        """
        if self.size(dim) == 1:
            return self.squeeze(dim)
        roots = self.root_decomposition().root.evaluate()
        num_batch = roots.size(dim)
        while True:
            if num_batch % 2:
                shape = list(roots.shape)
                shape[dim] = 1
                extra_root = torch.full(shape, dtype=self.dtype, device=self.device, fill_value=1.0 / math.sqrt(self.size(-2)))
                roots = torch.cat([roots, extra_root], dim)
                num_batch += 1
            part1_index = [_noop_index] * roots.dim()
            part1_index[dim] = slice(None, num_batch // 2, None)
            part1 = roots[tuple(part1_index)].contiguous()
            part2_index = [_noop_index] * roots.dim()
            part2_index[dim] = slice(num_batch // 2, None, None)
            part2 = roots[tuple(part2_index)].contiguous()
            if num_batch // 2 == 1:
                part1 = part1.squeeze(dim)
                part2 = part2.squeeze(dim)
                res = MulLazyTensor(RootLazyTensor(part1), RootLazyTensor(part2))
                break
            else:
                res = MulLazyTensor(RootLazyTensor(part1), RootLazyTensor(part2))
                roots = res.root_decomposition().root.evaluate()
                num_batch = num_batch // 2
        return res

    def _root_decomposition(self):
        """
        Returns the (usually low-rank) root of a lazy tensor of a PSD matrix.

        ..note::
            This method is used internally by the related function
            :func:`~gpytorch.lazy.LazyTensor.root_decomposition`, which does some additional work.
            Calling this method directly is discouraged.

        Returns:
            (Tensor or LazyTensor): The root of the root decomposition
        """
        func = RootDecomposition()
        res, _ = func.apply(self.representation_tree(), self._root_decomposition_size(), self.dtype, self.device, self.batch_shape, self.matrix_shape, True, False, None, *self.representation())
        return res

    def _root_decomposition_size(self):
        """
        This is the inner size of the root decomposition.
        This is primarily used to determine if it will be cheaper to compute a
        different root or not
        """
        return settings.max_root_decomposition_size.value()

    def _root_inv_decomposition(self, initial_vectors=None):
        """
        Returns the (usually low-rank) inverse root of a lazy tensor of a PSD matrix.

        ..note::
            This method is used internally by the related function
            :func:`~gpytorch.lazy.LazyTensor.root_inv_decomposition`, which does some additional work.
            Calling this method directly is discouraged.

        Returns:
            (Tensor or LazyTensor): The root of the inverse root decomposition
        """
        func = RootDecomposition()
        roots, inv_roots = func.apply(self.representation_tree(), self._root_decomposition_size(), self.dtype, self.device, self.batch_shape, self.matrix_shape, True, True, initial_vectors, *self.representation())
        if initial_vectors is not None and initial_vectors.size(-1) > 1:
            add_to_cache(self, 'root_decomposition', RootLazyTensor(roots[0]))
        else:
            add_to_cache(self, 'root_decomposition', RootLazyTensor(roots))
        return inv_roots

    def _solve(self, rhs, preconditioner, num_tridiag=0):
        return utils.linear_cg(self._matmul, rhs, n_tridiag=num_tridiag, max_iter=settings.max_cg_iterations.value(), max_tridiag_iter=settings.max_lanczos_quadrature_iterations.value(), preconditioner=preconditioner)

    def _sum_batch(self, dim):
        """
        Sum the LazyTensor across a batch dimension (supplied as a positive number).

        ..note::
            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.sum`,
            which does some additional work. Calling this method directly is discouraged.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`
        """
        return SumBatchLazyTensor(self, block_dim=dim)

    def _t_matmul(self, rhs):
        """
        Performs a transpose matrix multiplication :math:`K^{\\top}M` with the matrix :math:`K` that this
        LazyTensor represents.

        Args:
            rhs (:obj:`torch.tensor`): the matrix :math:`M` to multiply with.

        Returns:
            :obj:`torch.tensor`: matrix * rhs
        """
        return self.transpose(-1, -2)._matmul(rhs)

    def add_diag(self, diag):
        """
        Adds an element to the diagonal of the matrix.

        Args:
            - diag (Scalar Tensor)
        """
        if not self.is_square:
            raise RuntimeError('add_diag only defined for square matrices')
        try:
            expanded_diag = diag.expand(self.shape[:-1])
        except RuntimeError:
            raise RuntimeError('add_diag for LazyTensor of size {} received invalid diagonal of size {}.'.format(self.shape, diag.shape))
        return AddedDiagLazyTensor(self, DiagLazyTensor(expanded_diag))

    def add_jitter(self, jitter_val=0.001):
        """
        Adds jitter (i.e., a small diagonal component) to the matrix this
        LazyTensor represents. This could potentially be implemented as a no-op,
        however this could lead to numerical instabilities, so this should only
        be done at the user's risk.
        """
        diag = torch.tensor(jitter_val, dtype=self.dtype, device=self.device)
        return self.add_diag(diag)

    @property
    def batch_dim(self):
        """
        Returns the dimension of the shape over which the tensor is batched.
        """
        return len(self.batch_shape)

    @property
    def batch_shape(self):
        """
        Returns the shape over which the tensor is batched.
        """
        return self.shape[:-2]

    def cholesky(self, upper=False):
        """
        Cholesky-factorizes the LazyTensor

        Parameters:
            upper (bool) - upper triangular or lower triangular factor (default: False)

        Returns:
            (LazyTensor) Cholesky factor (lower triangular)
        """
        res = self._cholesky()
        if upper:
            res = res.transpose(-1, -2)
        return res

    def clone(self):
        """
        Clones the LazyTensor (creates clones of all underlying tensors)
        """
        args = [(arg.clone() if hasattr(arg, 'clone') else arg) for arg in self._args]
        kwargs = {key: (val.clone() if hasattr(val, 'clone') else val) for key, val in self._kwargs.items()}
        return self.__class__(*args, **kwargs)

    def cpu(self):
        """
        Returns:
            :obj:`~gpytorch.lazy.LazyTensor`: a new LazyTensor identical to ``self``, but on the CPU.
        """
        new_args = []
        new_kwargs = {}
        for arg in self._args:
            if hasattr(arg, 'cpu'):
                new_args.append(arg.cpu())
            else:
                new_args.append(arg)
        for name, val in self._kwargs.items():
            if hasattr(val, 'cpu'):
                new_kwargs[name] = val.cpu()
            else:
                new_kwargs[name] = val
        return self.__class__(*new_args, **new_kwargs)

    def cuda(self, device_id=None):
        """
        This method operates identically to :func:`torch.nn.Module.cuda`.

        Args:
            device_id (:obj:`str`, optional):
                Device ID of GPU to use.
        Returns:
            :obj:`~gpytorch.lazy.LazyTensor`:
                a new LazyTensor identical to ``self``, but on the GPU.
        """
        new_args = []
        new_kwargs = {}
        for arg in self._args:
            if hasattr(arg, 'cuda'):
                new_args.append(arg)
            else:
                new_args.append(arg)
        for name, val in self._kwargs.items():
            if hasattr(val, 'cuda'):
                new_kwargs[name] = val
            else:
                new_kwargs[name] = val
        return self.__class__(*new_args, **new_kwargs)

    @property
    def device(self):
        return self._args[0].device

    def detach(self):
        """
        Removes the LazyTensor from the current computation graph.
        (In practice, this function removes all Tensors that make up the
        LazyTensor from the computation graph.)
        """
        return self.clone().detach_()

    def detach_(self):
        """
        An in-place version of `detach`.
        """
        for arg in self._args:
            if hasattr(arg, 'detach'):
                arg.detach_()
        for val in self._kwargs.values():
            if hasattr(val, 'detach'):
                val.detach_()
        return self

    def diag(self):
        """
        As :func:`torch.diag`, returns the diagonal of the matrix :math:`K` this LazyTensor represents as a vector.

        :rtype: torch.tensor
        :return: The diagonal of :math:`K`. If :math:`K` is :math:`n \\times n`, this will be a length
            n vector. If this LazyTensor represents a batch (e.g., is :math:`b \\times n \\times n`), this will be a
            :math:`b \\times n` matrix of diagonals, one for each matrix in the batch.
        """
        if settings.debug.on():
            if not self.is_square:
                raise RuntimeError('Diag works on square matrices (or batches)')
        row_col_iter = torch.arange(0, self.matrix_shape[-1], dtype=torch.long, device=self.device)
        return self[..., row_col_iter, row_col_iter]

    def dim(self):
        """
        Alias of :meth:`~gpytorch.lazy.LazyTensor.ndimension`
        """
        return self.ndimension()

    @property
    def dtype(self):
        return self._args[0].dtype

    def expand(self, *sizes):
        if len(sizes) == 1 and hasattr(sizes, '__iter__'):
            sizes = sizes[0]
        if len(sizes) < 2 or tuple(sizes[-2:]) != self.matrix_shape:
            raise RuntimeError('Invalid expand arguments {}. Currently, repeat only works to create repeated batches of a 2D LazyTensor.'.format(tuple(sizes)))
        elif all(isinstance(size, int) for size in sizes):
            shape = torch.Size(sizes)
        else:
            raise RuntimeError('Invalid arguments {} to expand.'.format(sizes))
        res = self._expand_batch(batch_shape=shape[:-2])
        return res

    @cached
    def evaluate(self):
        """
        Explicitly evaluates the matrix this LazyTensor represents. This function
        should return a Tensor storing an exact representation of this LazyTensor.
        """
        num_rows, num_cols = self.matrix_shape
        if num_rows < num_cols:
            eye = torch.eye(num_rows, dtype=self.dtype, device=self.device)
            eye = eye.expand(*self.batch_shape, num_rows, num_rows)
            res = self.transpose(-1, -2).matmul(eye).transpose(-1, -2).contiguous()
        else:
            eye = torch.eye(num_cols, dtype=self.dtype, device=self.device)
            eye = eye.expand(*self.batch_shape, num_cols, num_cols)
            res = self.matmul(eye)
        return res

    def evaluate_kernel(self):
        """
        Return a new LazyTensor representing the same one as this one, but with
        all lazily evaluated kernels actually evaluated.
        """
        return self.representation_tree()(*self.representation())

    def inv_matmul(self, right_tensor, left_tensor=None):
        """
        Computes a linear solve (w.r.t self = :math:`A`) with several right hand sides :math:`R`.
        I.e. computes

        ... math::

            \\begin{equation}
                A^{-1} R,
            \\end{equation}

        where :math:`R` is :attr:`right_tensor` and :math:`A` is the LazyTensor.

        If :attr:`left_tensor` is supplied, computes

        ... math::

            \\begin{equation}
                L A^{-1} R,
            \\end{equation}

        where :math:`L` is :attr:`left_tensor`. Supplying this can reduce the number of
        CG calls required.

        Args:
            - :obj:`torch.tensor` (n x k) - Matrix :math:`R` right hand sides
            - :obj:`torch.tensor` (m x n) - Optional matrix :math:`L` to perform left multiplication with

        Returns:
            - :obj:`torch.tensor` - :math:`A^{-1}R` or :math:`LA^{-1}R`.
        """
        if not self.is_square:
            raise RuntimeError('inv_matmul only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'.format(self.__class__.__name__, self.size()))
        if self.dim() == 2 and right_tensor.dim() == 1:
            if self.shape[-1] != right_tensor.numel():
                raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, right_tensor.shape))
        func = InvMatmul
        if left_tensor is None:
            return func.apply(self.representation_tree(), False, right_tensor, *self.representation())
        else:
            return func.apply(self.representation_tree(), True, left_tensor, right_tensor, *self.representation())

    def inv_quad(self, tensor, reduce_inv_quad=True):
        """
        Computes an inverse quadratic form (w.r.t self) with several right hand sides.
        I.e. computes tr( tensor^T self^{-1} tensor )

        NOTE: Don't overwrite this function!
        Instead, overwrite inv_quad_logdet

        Args:
            - tensor (tensor nxk) - Vector (or matrix) for inverse quad

        Returns:
            - tensor - tr( tensor^T (self)^{-1} tensor )
        """
        if not self.is_square:
            raise RuntimeError('inv_quad only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'.format(self.__class__.__name__, self.size()))
        if self.dim() == 2 and tensor.dim() == 1:
            if self.shape[-1] != tensor.numel():
                raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, tensor.shape))
        elif self.dim() != tensor.dim():
            raise RuntimeError('LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number of dimensions.'.format(self.shape, tensor.shape))
        elif self.shape[-1] != tensor.shape[-2]:
            raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, tensor.shape))
        args = (tensor,) + self.representation()
        func = InvQuad.apply
        inv_quad_term = func(self.representation_tree(), *args)
        if reduce_inv_quad:
            inv_quad_term = inv_quad_term.sum(-1)
        return inv_quad_term

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):
        """
        Computes an inverse quadratic form (w.r.t self) with several right hand sides.
        I.e. computes tr( tensor^T self^{-1} tensor )
        In addition, computes an (approximate) log determinant of the the matrix

        Args:
            - tensor (tensor nxk) - Vector (or matrix) for inverse quad

        Returns:
            - scalar - tr( tensor^T (self)^{-1} tensor )
            - scalar - log determinant
        """
        if settings.fast_computations.log_prob.off() or self.size(-1) <= settings.max_cholesky_size.value():
            cholesky = CholLazyTensor(self.cholesky())
            return cholesky.inv_quad_logdet(inv_quad_rhs=inv_quad_rhs, logdet=logdet, reduce_inv_quad=reduce_inv_quad)
        if not self.is_square:
            raise RuntimeError('inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'.format(self.__class__.__name__, self.size()))
        if inv_quad_rhs is not None:
            if self.dim() == 2 and inv_quad_rhs.dim() == 1:
                if self.shape[-1] != inv_quad_rhs.numel():
                    raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, inv_quad_rhs.shape))
            elif self.dim() != inv_quad_rhs.dim():
                raise RuntimeError('LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number of dimensions.'.format(self.shape, inv_quad_rhs.shape))
            elif self.batch_shape != inv_quad_rhs.shape[:-2] or self.shape[-1] != inv_quad_rhs.shape[-2]:
                raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, inv_quad_rhs.shape))
        args = self.representation()
        if inv_quad_rhs is not None:
            args = [inv_quad_rhs] + list(args)
        probe_vectors, probe_vector_norms = self._probe_vectors_and_norms()
        func = InvQuadLogDet.apply
        inv_quad_term, logdet_term = func(self.representation_tree(), self.dtype, self.device, self.matrix_shape, self.batch_shape, inv_quad_rhs is not None, logdet, probe_vectors, probe_vector_norms, *args)
        if inv_quad_term.numel() and reduce_inv_quad:
            inv_quad_term = inv_quad_term.sum(-1)
        return inv_quad_term, logdet_term

    @property
    def is_square(self):
        return self.matrix_shape[0] == self.matrix_shape[1]

    def logdet(self):
        """
        Computes an (approximate) log determinant of the matrix

        NOTE: Don't overwrite this function!
        Instead, overwrite inv_quad_logdet

        Returns:
            - scalar: log determinant
        """
        _, res = self.inv_quad_logdet(inv_quad_rhs=None, logdet=True)
        return res

    def matmul(self, other):
        """
        Multiplies self by a matrix

        Args:
            other (:obj:`torch.tensor`): Matrix or vector to multiply with. Can be either a :obj:`torch.tensor`
                or a :obj:`gpytorch.lazy.LazyTensor`.

        Returns:
            :obj:`torch.tensor`: Tensor or LazyTensor containing the result of the matrix multiplication :math:`KM`,
            where :math:`K` is the (batched) matrix that this :obj:`gpytorch.lazy.LazyTensor` represents, and :math:`M`
            is the (batched) matrix input to this method.
        """
        _matmul_broadcast_shape(self.shape, other.shape)
        if isinstance(other, LazyTensor):
            return MatmulLazyTensor(self, other)
        func = Matmul()
        return func.apply(self.representation_tree(), other, *self.representation())

    @property
    def matrix_shape(self):
        """
        Returns the shape of the matrix being represented (without batching).
        """
        return torch.Size(self.shape[-2:])

    def mul(self, other):
        """
        Multiplies the matrix by a constant, or elementwise the matrix by another matrix

        Args:
            other (:obj:`torch.tensor` or :obj:`~gpytorch.lazy.LazyTensor`): constant or matrix to elementwise
            multiply by.

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`: Another lazy tensor representing the result of the multiplication. if
            other was a constant (or batch of constants), this will likely be a
            :obj:`gpytorch.lazy.ConstantMulLazyTensor`. If other was
            another matrix, this will likely be a :obj:`gpytorch.lazy.MulLazyTensor`.
        """
        if isinstance(other, ZeroLazyTensor):
            return other
        if not (torch.is_tensor(other) or isinstance(other, LazyTensor)):
            other = torch.tensor(other, dtype=self.dtype, device=self.device)
        try:
            _mul_broadcast_shape(self.shape, other.shape)
        except RuntimeError:
            raise RuntimeError('Cannot multiply LazyTensor of size {} by an object of size {}'.format(self.shape, other.shape))
        if torch.is_tensor(other):
            if other.numel() == 1:
                return self._mul_constant(other.squeeze())
            elif other.shape[-2:] == torch.Size((1, 1)):
                return self._mul_constant(other.view(*other.shape[:-2]))
        return self._mul_matrix(lazify(other))

    def ndimension(self):
        """
        Returns the number of dimensions
        """
        return len(self.size())

    def numel(self):
        """
        Returns the number of elements
        """
        return self.shape.numel()

    def numpy(self):
        """
        Return self as an evaluated numpy array
        """
        return self.evaluate().detach().cpu().numpy()

    def permute(self, *dims):
        num_dims = self.dim()
        orig_dims = dims
        dims = tuple(dim if dim >= 0 else dim + num_dims for dim in dims)
        if settings.debug.on():
            if len(dims) != num_dims:
                raise RuntimeError("number of dims don't match in permute")
            if sorted(set(dims)) != sorted(dims):
                raise RuntimeError('repeated dim in permute')
            for dim, orig_dim in zip(dims, orig_dims):
                if dim >= num_dims:
                    raise RuntimeError('Dimension out of range (expected to be in range of [{}, {}], but got {}.'.format(-num_dims, num_dims - 1, orig_dim))
        if dims[-2:] != (num_dims - 2, num_dims - 1):
            raise ValueError('At the moment, cannot permute the non-batch dimensions of LazyTensors.')
        return self._permute_batch(*dims[:-2])

    def prod(self, dim=None):
        """
        For a `b x n x m` LazyTensor, compute the product over the batch dimension.

        The `mul_batch_size` controls whether or not the batch dimension is grouped when multiplying.
            * `mul_batch_size=None` (default): The entire batch dimension is multiplied. Returns a `n x n` LazyTensor.
            * `mul_batch_size=k`: Creates `b/k` groups, and muls the `k` entries of this group.
                (The LazyTensor is reshaped as a `b/k x k x n x m` LazyTensor and the `k` dimension is multiplied over.
                Returns a `b/k x n x m` LazyTensor.

        Args:
            :attr:`mul_batch_size` (int or None):
                Controls the number of groups that are multiplied over (default: None).

        Returns:
            :obj:`~gpytorch.lazy.LazyTensor`

        Example:
            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.tensor([
                    [[2, 4], [1, 2]],
                    [[1, 1], [0, -1]],
                    [[2, 1], [1, 0]],
                    [[3, 2], [2, -1]],
                ]))
            >>> lazy_tensor.mul_batch().evaluate()
            >>> # Returns: torch.Tensor([[12, 8], [0, 0]])
            >>> lazy_tensor.mul_batch(mul_batch_size=2)
            >>> # Returns: torch.Tensor([[[2, 4], [0, -2]], [[6, 2], [2, 0]]])
        """
        if dim is None:
            raise ValueError('At the moment, LazyTensor.prod requires a dim argument (got None)')
        orig_dim = dim
        if dim < 0:
            dim = self.dim() + dim
        if dim >= len(self.batch_shape):
            raise ValueError('At the moment, LazyTensor.prod only works on batch dimensions. Got dim={} for LazyTensor of shape {}'.format(orig_dim, self.shape))
        return self._prod_batch(dim)

    def repeat(self, *sizes):
        """
        Repeats this tensor along the specified dimensions.

        Currently, this only works to create repeated batches of a 2D LazyTensor.
        I.e. all calls should be `lazy_tensor.repeat(<size>, 1, 1)`.

        Example:
            >>> lazy_tensor = gpytorch.lazy.ToeplitzLazyTensor(torch.tensor([4. 1., 0.5]))
            >>> lazy_tensor.repeat(2, 1, 1).evaluate()
            tensor([[[4.0000, 1.0000, 0.5000],
                     [1.0000, 4.0000, 1.0000],
                     [0.5000, 1.0000, 4.0000]],
                    [[4.0000, 1.0000, 0.5000],
                     [1.0000, 4.0000, 1.0000],
                     [0.5000, 1.0000, 4.0000]]])
        """
        if len(sizes) < 3 or tuple(sizes[-2:]) != (1, 1):
            raise RuntimeError('Invalid repeat arguments {}. Currently, repeat only works to create repeated batches of a 2D LazyTensor.'.format(tuple(sizes)))
        return BatchRepeatLazyTensor(self, batch_repeat=torch.Size(sizes[:-2]))

    def representation(self):
        """
        Returns the Tensors that are used to define the LazyTensor
        """
        representation = []
        for arg in self._args:
            if torch.is_tensor(arg):
                representation.append(arg)
            elif hasattr(arg, 'representation') and callable(arg.representation):
                representation += list(arg.representation())
            else:
                raise RuntimeError('Representation of a LazyTensor should consist only of Tensors')
        return tuple(representation)

    def representation_tree(self):
        """
        Returns a :obj:`gpytorch.lazy.LazyTensorRepresentationTree` tree object that recursively encodes the
        representation of this lazy tensor. In particular, if the definition of this lazy tensor depends on other
        lazy tensors, the tree is an object that can be used to reconstruct the full structure of this lazy tensor,
        including all subobjects. This is used internally.
        """
        return LazyTensorRepresentationTree(self)

    @property
    def requires_grad(self):
        return any(arg.requires_grad for arg in tuple(self._args) + tuple(self._kwargs.values()) if hasattr(arg, 'requires_grad'))

    @requires_grad.setter
    def requires_grad(self, val):
        for arg in self._args:
            if hasattr(arg, 'requires_grad'):
                if arg.dtype in (torch.float, torch.double, torch.half):
                    arg.requires_grad = val
        for arg in self._kwargs.values():
            if hasattr(arg, 'requires_grad'):
                arg.requires_grad = val

    def requires_grad_(self, val):
        """
        Sets `requires_grad=val` on all the Tensors that make up the LazyTensor
        This is an inplace operation.
        """
        self.requires_grad = val
        return self

    @cached(name='root_decomposition')
    def root_decomposition(self):
        """
        Returns a (usually low-rank) root decomposition lazy tensor of a PSD matrix.
        This can be used for sampling from a Gaussian distribution, or for obtaining a
        low-rank version of a matrix
        """
        if not self.is_square:
            raise RuntimeError('root_decomposition only operates on (batches of) square (symmetric) LazyTensors. Got a {} of size {}.'.format(self.__class__.__name__, self.size()))
        if self.size(-1) <= settings.max_cholesky_size.value() or settings.fast_computations.covar_root_decomposition.off():
            try:
                res = self.cholesky()
                return CholLazyTensor(res)
            except RuntimeError as e:
                warnings.warn('Runtime Error when computing Cholesky decomposition: {}. Using RootDecomposition.'.format(e), NumericalWarning)
        res = self._root_decomposition()
        return RootLazyTensor(res)

    @cached(name='root_inv_decomposition')
    def root_inv_decomposition(self, initial_vectors=None, test_vectors=None):
        """
        Returns a (usually low-rank) root decomposotion lazy tensor of a PSD matrix.
        This can be used for sampling from a Gaussian distribution, or for obtaining a
        low-rank version of a matrix
        """
        if self.shape[-2:].numel() == 1:
            return RootLazyTensor(1 / self.evaluate().sqrt())
        if self.size(-1) <= settings.max_cholesky_size.value() or settings.fast_computations.covar_root_decomposition.off():
            try:
                L = delazify(self.cholesky())
                Eye = torch.eye(L.shape[-2], device=L.device, dtype=L.dtype)
                Linv = torch.triangular_solve(Eye, L, upper=False)[0]
                res = lazify(Linv.transpose(-1, -2))
                return RootLazyTensor(res)
            except RuntimeError as e:
                warnings.warn('Runtime Error when computing Cholesky decomposition: {}. Using RootDecomposition.'.format(e), NumericalWarning)
        if not self.is_square:
            raise RuntimeError('root_inv_decomposition only operates on (batches of) square (symmetric) LazyTensors. Got a {} of size {}.'.format(self.__class__.__name__, self.size()))
        if initial_vectors is not None:
            if self.dim() == 2 and initial_vectors.dim() == 1:
                if self.shape[-1] != initial_vectors.numel():
                    raise RuntimeError('LazyTensor (size={}) cannot be multiplied with initial_vectors (size={}).'.format(self.shape, initial_vectors.shape))
            elif self.dim() != initial_vectors.dim():
                raise RuntimeError('LazyTensor (size={}) and initial_vectors (size={}) should have the same number of dimensions.'.format(self.shape, initial_vectors.shape))
            elif self.batch_shape != initial_vectors.shape[:-2] or self.shape[-1] != initial_vectors.shape[-2]:
                raise RuntimeError('LazyTensor (size={}) cannot be multiplied with initial_vectors (size={}).'.format(self.shape, initial_vectors.shape))
        inv_roots = self._root_inv_decomposition(initial_vectors)
        if initial_vectors is not None and initial_vectors.size(-1) > 1:
            num_probes = initial_vectors.size(-1)
            test_vectors = test_vectors.unsqueeze(0)
            solves = inv_roots.matmul(inv_roots.transpose(-1, -2).matmul(test_vectors))
            solves = solves.permute(*range(1, self.dim() + 1), 0).contiguous().view(*self.batch_shape, self.matrix_shape[-1], -1)
            mat_times_solves = self.matmul(solves)
            mat_times_solves = mat_times_solves.view(*self.batch_shape, self.matrix_shape[-1], -1, num_probes).permute(-1, *range(0, self.dim()))
            residuals = (mat_times_solves - test_vectors).norm(2, dim=-2)
            residuals = residuals.view(residuals.size(0), -1).sum(-1)
            _, best_solve_index = residuals.min(0)
            inv_root = inv_roots[best_solve_index].squeeze(0)
        else:
            inv_root = inv_roots
        return RootLazyTensor(inv_root)

    def size(self, val=None):
        """
        Returns the size of the resulting Tensor that the lazy tensor represents
        """
        size = self._size()
        if val is not None:
            return size[val]
        return size

    def squeeze(self, dim):
        if self.size(dim) != 1:
            return self
        else:
            index = [_noop_index] * self.dim()
            index[dim] = 0
            index = tuple(index)
            return self[index]

    @property
    def shape(self):
        return self.size()

    def sum(self, dim=None):
        """
        Sum the LazyTensor across a dimension.
        The `dim` controls which batch dimension is summed over.
        If set to None, then sums all dimensions

        Args:
            :attr:`dim` (int):
                Which dimension is being summed over (default=None)

        Returns:
            :obj:`~gpytorch.lazy.LazyTensor` or Tensor.

        Example:
            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.tensor([
                    [[2, 4], [1, 2]],
                    [[1, 1], [0, -1]],
                    [[2, 1], [1, 0]],
                    [[3, 2], [2, -1]],
                ]))
            >>> lazy_tensor.sum(0).evaluate()
        """
        if dim is None:
            ones = torch.ones(self.size(-2), 1, dtype=self.dtype, device=self.device)
            return (self @ ones).sum()
        orig_dim = dim
        if dim < 0:
            dim = self.dim() + dim
        if dim == self.dim() - 1:
            ones = torch.ones(self.size(-1), 1, dtype=self.dtype, device=self.device)
            return (self @ ones).squeeze(-1)
        elif dim == self.dim() - 2:
            ones = torch.ones(self.size(-2), 1, dtype=self.dtype, device=self.device)
            return (self.transpose(-1, -2) @ ones).squeeze(-1)
        elif dim < self.dim():
            return self._sum_batch(dim)
        else:
            raise ValueError('Invalid dim ({}) for LazyTensor of size {}'.format(orig_dim, self.shape))

    def to(self, device_id):
        """
        A device-agnostic method of moving the lazy_tensor to the specified device.

        Args:
            device_id (:obj: `torch.device`): Which device to use (GPU or CPU).
        Returns:
            :obj:`~gpytorch.lazy.LazyTensor`: New LazyTensor identical to self on specified device
        """
        new_args = []
        new_kwargs = {}
        for arg in self._args:
            if hasattr(arg, 'to'):
                new_args.append(arg)
            else:
                new_args.append(arg)
        for name, val in self._kwargs.items():
            if hasattr(val, 'to'):
                new_kwargs[name] = val
            else:
                new_kwargs[name] = val
        return self.__class__(*new_args, **new_kwargs)

    def t(self):
        """
        Alias of :meth:`~gpytorch.lazy.LazyTensor.transpose` for 2D LazyTensor.
        (Tranposes the two dimensions.)
        """
        if self.ndimension() != 2:
            raise RuntimeError('Cannot call t for more than 2 dimensions')
        return self.transpose(0, 1)

    def transpose(self, dim1, dim2):
        """
        Transpose the dimensions `dim1` and `dim2` of the LazyTensor.

        Example:
            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.randn(3, 5))
            >>> lazy_tensor.transpose(0, 1)
        """
        ndimension = self.ndimension()
        if dim1 < 0:
            dim1 = ndimension + dim1
        if dim2 < 0:
            dim2 = ndimension + dim2
        if dim1 >= ndimension or dim2 >= ndimension or not isinstance(dim1, int) or not isinstance(dim2, int):
            raise RuntimeError('Invalid dimension')
        if dim1 < ndimension - 2 and dim2 < ndimension - 2:
            small_dim = dim1 if dim1 < dim2 else dim2
            large_dim = dim2 if dim1 < dim2 else dim1
            res = self._permute_batch(*range(small_dim), large_dim, *range(small_dim + 1, large_dim), small_dim, *range(large_dim + 1, ndimension - 2))
        elif dim1 >= ndimension - 2 and dim2 >= ndimension - 2:
            res = self._transpose_nonbatch()
        else:
            raise RuntimeError('Cannot transpose batch dimension with non-batch dimension')
        return res

    def unsqueeze(self, dim):
        positive_dim = self.dim() + dim + 1 if dim < 0 else dim
        if positive_dim > len(self.batch_shape):
            raise ValueError('Can only unsqueeze batch dimensions of {} (size {}). Got dim={}.'.format(self.__class__.__name__, self.shape, dim))
        res = self._unsqueeze_batch(positive_dim)
        return res

    def zero_mean_mvn_samples(self, num_samples):
        """
        Assumes that self is a covariance matrix, or a batch of covariance matrices.
        Returns samples from a zero-mean MVN, defined by self (as covariance matrix)

        Self should be symmetric, either (batch_size x num_dim x num_dim) or (num_dim x num_dim)

        Args:
            :attr:`num_samples` (int):
                Number of samples to draw.

        Returns:
            :obj:`torch.tensor`:
                Samples from MVN (num_samples x batch_size x num_dim) or (num_samples x num_dim)
        """
        if self.size()[-2:] == torch.Size([1, 1]):
            covar_root = self.evaluate().sqrt()
        else:
            covar_root = self.root_decomposition().root
        base_samples = torch.randn(*self.batch_shape, covar_root.size(-1), num_samples, dtype=self.dtype, device=self.device)
        samples = covar_root.matmul(base_samples).permute(-1, *range(self.dim() - 1)).contiguous()
        return samples

    def __add__(self, other):
        """
        Return a :obj:`gpytorch.lazy.LazyTensor` that represents the sum of this lazy tensor and another matrix
        or lazy tensor.

        Args:
            :attr:`other` (:obj:`torch.tensor` or :obj:`gpytorch.lazy.LazyTensor`):
                Matrix to add to this one.

        Returns:
            :obj:`gpytorch.lazy.SumLazyTensor`:
                A sum lazy tensor representing the sum of this lazy tensor and other.
        """
        from torch import Tensor
        if isinstance(other, ZeroLazyTensor):
            return self
        elif isinstance(other, DiagLazyTensor):
            return AddedDiagLazyTensor(self, other)
        elif isinstance(other, Tensor):
            other = lazify(other)
            shape = _mul_broadcast_shape(self.shape, other.shape)
            return SumLazyTensor(self.expand(shape), other.expand(shape))
        else:
            return SumLazyTensor(self, other)

    def __div__(self, other):
        """
        Return a :obj:`gpytorch.lazy.LazyTensor` that represents the product of this lazy tensor and
        the elementwise reciprocal of another matrix or lazy tensor.

        Args:
            :attr:`other` (:obj:`torch.tensor` or :obj:`gpytorch.lazy.LazyTensor`):
                Matrix to divide this one by.

        Returns:
            :obj:`gpytorch.lazy.MulLazyTensor`:
                Result of division.
        """
        if isinstance(other, ZeroLazyTensor):
            raise RuntimeError('Attempted to divide by a ZeroLazyTensor (divison by zero)')
        return self.mul(1.0 / other)

    def __getitem__(self, index):
        """
        Supports subindexing of the matrix this LazyTensor represents. This may return either another
        :obj:`gpytorch.lazy.LazyTensor` or a :obj:`torch.tensor` depending on the exact implementation.
        """
        ndimension = self.ndimension()
        index = index if isinstance(index, tuple) else (index,)
        index = tuple(torch.tensor(idx) if isinstance(idx, list) else idx for idx in index)
        index = tuple(idx.item() if torch.is_tensor(idx) and not len(idx.shape) else idx for idx in index)
        ellipsis_locs = tuple(index for index, item in enumerate(index) if item is Ellipsis)
        if settings.debug.on():
            if len(ellipsis_locs) > 1:
                raise RuntimeError('Cannot have multiple ellipsis in a __getitem__ call. LazyTensor {}  received index {}.'.format(self, index))
        if len(ellipsis_locs) == 1:
            ellipsis_loc = ellipsis_locs[0]
            num_to_fill_in = ndimension - (len(index) - 1)
            index = index[:ellipsis_loc] + tuple(_noop_index for _ in range(num_to_fill_in)) + index[ellipsis_loc + 1:]
        index = index + tuple(_noop_index for _ in range(ndimension - len(index)))
        *batch_indices, row_index, col_index = index
        batch_has_tensor_index = bool(len(batch_indices)) and any(torch.is_tensor(index) for index in batch_indices)
        row_has_tensor_index = torch.is_tensor(row_index)
        col_has_tensor_index = torch.is_tensor(col_index)
        row_col_are_absorbed = any((batch_has_tensor_index and (row_has_tensor_index or col_has_tensor_index), not batch_has_tensor_index and (row_has_tensor_index and col_has_tensor_index)))
        squeeze_row = False
        squeeze_col = False
        if isinstance(row_index, int):
            row_index = slice(row_index, row_index + 1, None)
            squeeze_row = True
        if isinstance(col_index, int):
            col_index = slice(col_index, col_index + 1, None)
            squeeze_col = True
        if row_col_are_absorbed:
            *batch_indices, row_index, col_index = _convert_indices_to_tensors(self, (*batch_indices, row_index, col_index))
            res = self._get_indices(row_index, col_index, *batch_indices)
        else:
            res = self._getitem(row_index, col_index, *batch_indices)
        if squeeze_row or squeeze_col or row_col_are_absorbed:
            res = delazify(res)
        if squeeze_row:
            res = res.squeeze(-2)
        if squeeze_col:
            res = res.squeeze(-1)
        if settings.debug.on() and self.__class__._check_size:
            expected_shape = _compute_getitem_size(self, index)
            if expected_shape != res.shape:
                raise RuntimeError('{}.__getitem__ failed! Expected a final shape of size {}, got {}. This is a bug with GPyTorch, or your custom LazyTensor.'.format(self.__class__.__name__, expected_shape, res.shape))
        return res

    def __matmul__(self, other):
        return self.matmul(other)

    def __mul__(self, other):
        return self.mul(other)

    def __radd__(self, other):
        return self + other

    def __rmul__(self, other):
        return self.mul(other)

    def __sub__(self, other):
        return self + other.mul(-1)


class BatchRepeatLazyTensor(LazyTensor):

    def __init__(self, base_lazy_tensor, batch_repeat=torch.Size((1,))):
        if settings.debug.on():
            if not isinstance(batch_repeat, torch.Size):
                raise RuntimeError('batch_repeat must be a torch.Size, got a {} instead'.format(batch_repeat.__class__.__name__))
            if isinstance(base_lazy_tensor, BatchRepeatLazyTensor):
                raise RuntimeError('BatchRepeatLazyTensor recieved the following args:\nbase_lazy_tensor: {} (size: {}), batch_repeat: {}.'.format(base_lazy_tensor, base_lazy_tensor.shape, batch_repeat))
        for _ in range(len(batch_repeat) + 2 - base_lazy_tensor.dim()):
            base_lazy_tensor = base_lazy_tensor.unsqueeze(0)
        super().__init__(base_lazy_tensor, batch_repeat=batch_repeat)
        self.base_lazy_tensor = base_lazy_tensor
        self.batch_repeat = batch_repeat

    @cached(name='cholesky')
    def _cholesky(self):
        res = self.base_lazy_tensor._cholesky()
        res = res.repeat(*self.batch_repeat, 1, 1)
        return res

    def _cholesky_solve(self, rhs):
        output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)
        if rhs.shape != output_shape:
            rhs = rhs.expand(*output_shape)
        rhs = self._move_repeat_batches_to_columns(rhs, output_shape)
        res = self.base_lazy_tensor._cholesky_solve(rhs)
        res = self._move_repeat_batches_back(res, output_shape)
        return res

    def _compute_batch_repeat_size(self, current_batch_shape, desired_batch_shape):
        batch_repeat = torch.Size(desired_batch_size // current_batch_size for desired_batch_size, current_batch_size in zip(desired_batch_shape, current_batch_shape))
        return batch_repeat

    def _expand_batch(self, batch_shape):
        padding_dims = torch.Size(tuple(1 for _ in range(max(len(batch_shape) + 2 - self.base_lazy_tensor.dim(), 0))))
        current_batch_shape = padding_dims + self.base_lazy_tensor.batch_shape
        return self.__class__(self.base_lazy_tensor, batch_repeat=self._compute_batch_repeat_size(current_batch_shape, batch_shape))

    def _get_indices(self, row_index, col_index, *batch_indices):
        num_true_batch_indices = self.base_lazy_tensor.dim() - 2
        batch_indices = batch_indices[len(batch_indices) - num_true_batch_indices:]
        batch_indices = [batch_index.fmod(size) for batch_index, size in zip(batch_indices, self.base_lazy_tensor.batch_shape)]
        res = self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices)
        return res

    def _getitem(self, row_index, col_index, *batch_indices):
        args = []
        kwargs = self.base_lazy_tensor._kwargs
        num_base_batch_dims = len(self.base_lazy_tensor.batch_shape)
        for arg in self.base_lazy_tensor._args:
            if torch.is_tensor(arg) or isinstance(arg, LazyTensor):
                arg_base_shape_len = max(arg.dim() - num_base_batch_dims, 0)
                args.append(arg.repeat(*self.batch_repeat, *[(1) for _ in range(arg_base_shape_len)]))
            else:
                args.append(arg)
        new_lazy_tensor = self.base_lazy_tensor.__class__(*args, **kwargs)
        return new_lazy_tensor._getitem(row_index, col_index, *batch_indices)

    def _matmul(self, rhs):
        output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)
        if self.is_square:
            if rhs.shape != output_shape:
                rhs = rhs.expand(*output_shape)
            rhs = self._move_repeat_batches_to_columns(rhs, output_shape)
            res = self.base_lazy_tensor._matmul(rhs)
            res = self._move_repeat_batches_back(res, output_shape)
            return res
        else:
            res = self.base_lazy_tensor._matmul(rhs)
            if res.shape != output_shape:
                res = res.expand(*output_shape)
            return res

    def _move_repeat_batches_back(self, batch_matrix, output_shape):
        """
        The opposite of _move_repeat_batches_to_columns

        Takes a b x m x nr tensor, and moves the batches associated with repeating
        So that the tensor is now rb x m x n.
        """
        if hasattr(self, '_batch_move_memo'):
            padded_base_batch_shape, batch_repeat = self.__batch_move_memo
            del self.__batch_move_memo
        else:
            padding_dims = torch.Size(tuple(1 for _ in range(max(len(output_shape) - self.base_lazy_tensor.dim(), 0))))
            padded_base_batch_shape = padding_dims + self.base_lazy_tensor.batch_shape
            batch_repeat = self._compute_batch_repeat_size(padded_base_batch_shape, output_shape[:-2])
        batch_matrix = batch_matrix.view(*padded_base_batch_shape, output_shape[-2], -1, *batch_repeat)
        output_dims = len(output_shape)
        dims = tuple(itertools.chain.from_iterable([i + output_dims, i] for i in range(len(padded_base_batch_shape)))) + (output_dims - 2, output_dims - 1)
        batch_matrix = batch_matrix.permute(*dims).contiguous()
        batch_matrix = batch_matrix.view(*output_shape)
        return batch_matrix

    def _move_repeat_batches_to_columns(self, batch_matrix, output_shape):
        """
        Takes a rb x m x n tensor, and moves the batches associated with repeating
        So that the tensor is now b x m x nr.
        This allows us to use the base_lazy_tensor routines.
        """
        padding_dims = torch.Size(tuple(1 for _ in range(max(len(output_shape) - self.base_lazy_tensor.dim(), 0))))
        padded_base_batch_shape = padding_dims + self.base_lazy_tensor.batch_shape
        batch_repeat = self._compute_batch_repeat_size(padded_base_batch_shape, output_shape[:-2])
        split_shape = torch.Size(tuple(itertools.chain.from_iterable([repeat, size] for repeat, size in zip(batch_repeat, padded_base_batch_shape))) + output_shape[-2:])
        batch_matrix = batch_matrix.view(*split_shape)
        repeat_dims = range(0, len(batch_repeat) * 2, 2)
        batch_dims = range(1, len(batch_repeat) * 2, 2)
        batch_matrix = batch_matrix.permute(*batch_dims, -2, -1, *repeat_dims).contiguous()
        batch_matrix = batch_matrix.view(*self.base_lazy_tensor.batch_shape, output_shape[-2], -1)
        self.__batch_move_memo = output_shape, padded_base_batch_shape, batch_repeat
        return batch_matrix

    def _permute_batch(self, *dims):
        new_batch_repeat = torch.Size(tuple(self.batch_repeat[dim] for dim in dims))
        res = self.__class__(self.base_lazy_tensor._permute_batch(*dims), batch_repeat=new_batch_repeat)
        return res

    def _quad_form_derivative(self, left_vectors, right_vectors):
        if self.is_square:
            left_output_shape = _matmul_broadcast_shape(self.shape, left_vectors.shape)
            if left_output_shape != left_vectors.shape:
                left_vectors = left_vectors.expand(left_output_shape)
            right_output_shape = _matmul_broadcast_shape(self.shape, right_vectors.shape)
            if right_output_shape != right_vectors.shape:
                right_vectors = right_vectors.expand(right_output_shape)
            left_vectors = self._move_repeat_batches_to_columns(left_vectors, left_output_shape)
            right_vectors = self._move_repeat_batches_to_columns(right_vectors, right_output_shape)
            return self.base_lazy_tensor._quad_form_derivative(left_vectors, right_vectors)
        else:
            return super()._quad_form_derivative(left_vectors, right_vectors)

    def _root_decomposition(self):
        return self.base_lazy_tensor._root_decomposition().repeat(*self.batch_repeat, 1, 1)

    def _root_inv_decomposition(self, initial_vectors=None):
        return self.base_lazy_tensor._root_inv_decomposition().repeat(*self.batch_repeat, 1, 1)

    def _size(self):
        repeated_batch_shape = torch.Size(size * repeat for size, repeat in zip(self.base_lazy_tensor.batch_shape, self.batch_repeat))
        res = torch.Size(repeated_batch_shape + self.base_lazy_tensor.matrix_shape)
        return res

    def _transpose_nonbatch(self):
        return self.__class__(self.base_lazy_tensor._transpose_nonbatch(), batch_repeat=self.batch_repeat)

    def _unsqueeze_batch(self, dim):
        base_lazy_tensor = self.base_lazy_tensor
        batch_repeat = list(self.batch_repeat)
        batch_repeat.insert(dim, 1)
        batch_repeat = torch.Size(batch_repeat)
        base_unsqueeze_dim = dim - (len(self.base_lazy_tensor.batch_shape) - len(self.base_lazy_tensor.batch_shape))
        if base_unsqueeze_dim > 0:
            base_lazy_tensor = base_lazy_tensor._unsqueeze_batch(base_unsqueeze_dim)
        return self.__class__(base_lazy_tensor, batch_repeat=batch_repeat)

    def add_jitter(self, jitter_val=0.001):
        return self.__class__(self.base_lazy_tensor.add_jitter(jitter_val=jitter_val), batch_repeat=self.batch_repeat)

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):
        if not self.is_square:
            raise RuntimeError('inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'.format(self.__class__.__name__, self.size()))
        if inv_quad_rhs is not None:
            if self.dim() != inv_quad_rhs.dim():
                raise RuntimeError('LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number of dimensions.'.format(self.shape, inv_quad_rhs.shape))
            elif self.batch_shape != inv_quad_rhs.shape[:-2] or self.shape[-1] != inv_quad_rhs.shape[-2]:
                raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, inv_quad_rhs.shape))
        if inv_quad_rhs is not None:
            output_shape = _matmul_broadcast_shape(self.shape, inv_quad_rhs.shape)
            inv_quad_rhs = self._move_repeat_batches_to_columns(inv_quad_rhs, output_shape)
        inv_quad_term, logdet_term = self.base_lazy_tensor.inv_quad_logdet(inv_quad_rhs, logdet, reduce_inv_quad=False)
        if inv_quad_term is not None and inv_quad_term.numel():
            inv_quad_term = inv_quad_term.view(*inv_quad_term.shape[:-1], -1, 1, self.batch_repeat.numel())
            output_shape = list(output_shape)
            output_shape[-2] = 1
            inv_quad_term = self._move_repeat_batches_back(inv_quad_term, output_shape).squeeze(-2)
            if reduce_inv_quad:
                inv_quad_term = inv_quad_term.sum(-1)
        if logdet_term is not None and logdet_term.numel():
            logdet_term = logdet_term.repeat(*self.batch_repeat)
        return inv_quad_term, logdet_term

    def repeat(self, *sizes):
        if len(sizes) < 3 or tuple(sizes[-2:]) != (1, 1):
            raise RuntimeError('Invalid repeat arguments {}. Currently, repeat only works to create repeated batches of a 2D LazyTensor.'.format(tuple(sizes)))
        padded_batch_repeat = tuple(1 for _ in range(len(sizes) - 2 - len(self.batch_repeat))) + self.batch_repeat
        return self.__class__(self.base_lazy_tensor, batch_repeat=torch.Size(orig_repeat_size * new_repeat_size for orig_repeat_size, new_repeat_size in zip(padded_batch_repeat, sizes[:-2])))


def lazify(obj):
    """
    A function which ensures that `obj` is a LazyTensor.

    If `obj` is a LazyTensor, this function does nothing.
    If `obj` is a (normal) Tensor, this function wraps it with a `NonLazyTensor`.
    """
    if torch.is_tensor(obj):
        return NonLazyTensor(obj)
    elif isinstance(obj, LazyTensor):
        return obj
    else:
        raise TypeError('object of class {} cannot be made into a LazyTensor'.format(obj.__class__.__name__))


class MatmulLazyTensor(LazyTensor):

    def __init__(self, left_lazy_tensor, right_lazy_tensor):
        left_lazy_tensor = lazify(left_lazy_tensor)
        right_lazy_tensor = lazify(right_lazy_tensor)
        super().__init__(left_lazy_tensor, right_lazy_tensor)
        self.left_lazy_tensor = left_lazy_tensor
        self.right_lazy_tensor = right_lazy_tensor

    def _expand_batch(self, batch_shape):
        return self.__class__(self.left_lazy_tensor._expand_batch(batch_shape), self.right_lazy_tensor._expand_batch(batch_shape))

    def _get_indices(self, row_index, col_index, *batch_indices):
        row_index = row_index.unsqueeze(-1)
        col_index = col_index.unsqueeze(-1)
        batch_indices = tuple(batch_index.unsqueeze(-1) for batch_index in batch_indices)
        inner_index = torch.arange(0, self.left_lazy_tensor.size(-1), device=self.device)
        inner_index = _pad_with_singletons(inner_index, row_index.dim() - 1, 0)
        left_tensor = self.left_lazy_tensor._get_indices(row_index, inner_index, *batch_indices)
        right_tensor = self.right_lazy_tensor._get_indices(inner_index, col_index, *batch_indices)
        res = (left_tensor * right_tensor).sum(-1)
        return res

    def _getitem(self, row_index, col_index, *batch_indices):
        if torch.is_tensor(row_index) and torch.is_tensor(col_index):
            num_indices = row_index.numel()
            if num_indices > self.matrix_shape.numel():
                return lazify(self.evaluate())._getitem(row_index, col_index, *batch_indices)
        left_tensor = self.left_lazy_tensor._getitem(row_index, _noop_index, *batch_indices)
        right_tensor = self.right_lazy_tensor._getitem(_noop_index, col_index, *batch_indices)
        res = MatmulLazyTensor(left_tensor, right_tensor)
        return res

    def _matmul(self, right_lazy_tensor):
        return self.left_lazy_tensor._matmul(self.right_lazy_tensor._matmul(right_lazy_tensor))

    def _t_matmul(self, right_lazy_tensor):
        return self.right_lazy_tensor._t_matmul(self.left_lazy_tensor._t_matmul(right_lazy_tensor))

    def _quad_form_derivative(self, left_vecs, right_vecs):
        if left_vecs.ndimension() == 1:
            left_vecs = left_vecs.unsqueeze(1)
            right_vecs = right_vecs.unsqueeze(1)
        right_vecs_times_right_lazy_tensor = self.right_lazy_tensor._matmul(right_vecs)
        left_vecs_times_left_lazy_tensor_t = self.left_lazy_tensor._t_matmul(left_vecs)
        left_grad = self.left_lazy_tensor._quad_form_derivative(left_vecs, right_vecs_times_right_lazy_tensor)
        right_grad = self.right_lazy_tensor._quad_form_derivative(left_vecs_times_left_lazy_tensor_t, right_vecs)
        left_grad = (left_grad,) if not isinstance(left_grad, tuple) else left_grad
        right_grad = (right_grad,) if not isinstance(right_grad, tuple) else right_grad
        return left_grad + right_grad

    def _size(self):
        return _matmul_broadcast_shape(self.left_lazy_tensor.shape, self.right_lazy_tensor.shape)

    def _transpose_nonbatch(self, *args):
        return self.__class__(self.right_lazy_tensor._transpose_nonbatch(), self.left_lazy_tensor._transpose_nonbatch())

    def diag(self):
        if isinstance(self.left_lazy_tensor, NonLazyTensor) and isinstance(self.right_lazy_tensor, NonLazyTensor):
            return (self.left_lazy_tensor.tensor * self.right_lazy_tensor.tensor.transpose(-1, -2)).sum(-1)
        elif isinstance(self.left_lazy_tensor, DiagLazyTensor) or isinstance(self.right_lazy_tensor, DiagLazyTensor):
            return self.left_lazy_tensor.diag() * self.right_lazy_tensor.diag()
        else:
            return super().diag()

    @cached
    def evaluate(self):
        return torch.matmul(self.left_lazy_tensor.evaluate(), self.right_lazy_tensor.evaluate())


def _equal_indices(a, b):
    """
    Helper which checks whether two index components (int, slice, tensor) are equal
    """
    if torch.is_tensor(a) and torch.is_tensor(b):
        return torch.equal(a, b)
    elif not torch.is_tensor(a) and not torch.is_tensor(b):
        return a == b
    else:
        return False


class RootLazyTensor(LazyTensor):

    def __init__(self, root):
        root = lazify(root)
        super(RootLazyTensor, self).__init__(root)
        self.root = root

    def _expand_batch(self, batch_shape):
        return self.__class__(self.root._expand_batch(batch_shape))

    def _get_indices(self, row_index, col_index, *batch_indices):
        row_index = row_index.unsqueeze(-1)
        col_index = col_index.unsqueeze(-1)
        batch_indices = tuple(batch_index.unsqueeze(-1) for batch_index in batch_indices)
        inner_index = torch.arange(0, self.root.size(-1), device=self.device)
        inner_index = _pad_with_singletons(inner_index, row_index.dim() - 1, 0)
        left_tensor = self.root._get_indices(row_index, inner_index, *batch_indices)
        if torch.equal(row_index, col_index):
            res = left_tensor.pow(2).sum(-1)
        else:
            right_tensor = self.root._get_indices(col_index, inner_index, *batch_indices)
            res = (left_tensor * right_tensor).sum(-1)
        return res

    def _getitem(self, row_index, col_index, *batch_indices):
        if torch.is_tensor(row_index) and torch.is_tensor(col_index):
            num_indices = row_index.numel()
            if num_indices > self.matrix_shape.numel():
                return lazify(self.evaluate())._getitem(row_index, col_index, *batch_indices)
        left_tensor = self.root._getitem(row_index, _noop_index, *batch_indices)
        if _equal_indices(row_index, col_index):
            res = self.__class__(left_tensor)
        else:
            right_tensor = self.root._getitem(col_index, _noop_index, *batch_indices)
            res = MatmulLazyTensor(left_tensor, right_tensor.transpose(-1, -2))
        return res

    def _matmul(self, rhs):
        return self.root._matmul(self.root._t_matmul(rhs))

    def _t_matmul(self, rhs):
        return self._matmul(rhs)

    def _quad_form_derivative(self, left_vecs, right_vecs):
        right_vecs_times_rhs = self.root._t_matmul(right_vecs)
        left_vecs_times_lhs_t = self.root._t_matmul(left_vecs)
        deriv_part_1 = self.root._quad_form_derivative(left_vecs, right_vecs_times_rhs)
        deriv_part_2 = self.root._quad_form_derivative(right_vecs, left_vecs_times_lhs_t)
        deriv = []
        for item_part_1, item_part_2 in zip(deriv_part_1, deriv_part_2):
            deriv.append(item_part_1 + item_part_2)
        return tuple(deriv)

    def _root_decomposition(self):
        return self.root

    def _root_decomposition_size(self):
        return self.root.size(-1)

    def _size(self):
        return torch.Size((*self.root.batch_shape, self.root.size(-2), self.root.size(-2)))

    def _transpose_nonbatch(self):
        return self

    def diag(self):
        if isinstance(self.root, NonLazyTensor):
            return (self.root.tensor ** 2).sum(-1)
        else:
            return super(RootLazyTensor, self).diag()

    @cached
    def evaluate(self):
        eval_root = self.root.evaluate()
        return torch.matmul(eval_root, eval_root.transpose(-1, -2))


class ZeroLazyTensor(LazyTensor):
    """
    Special LazyTensor representing zero.
    """

    def __init__(self, *sizes, dtype=None, device=None):
        super(ZeroLazyTensor, self).__init__(*sizes)
        self.sizes = list(sizes)
        self._dtype = dtype or torch.get_default_dtype()
        self._device = device or torch.device('cpu')

    @property
    def dtype(self):
        return self._dtype

    @property
    def device(self):
        return self._device

    def _expand_batch(self, batch_shape):
        return self.__class__(*batch_shape, *self.sizes[-2:], dtype=self._dtype, device=self._device)

    def _get_indices(self, row_index, col_index, *batch_indices):
        new_size = _compute_getitem_size(self, batch_indices + (row_index, col_index))
        return ZeroLazyTensor(*new_size)

    def _getitem(self, row_index, col_index, *batch_indices):
        new_size = _compute_getitem_size(self, batch_indices + (row_index, col_index))
        return ZeroLazyTensor(*new_size)

    def _matmul(self, rhs):
        rhs_size_ind = -2 if rhs.ndimension() > 1 else -1
        if self.size(-1) != rhs.size(rhs_size_ind):
            raise RuntimeError('Size mismatch, self: {}, rhs: {}'.format(self.size(), rhs.size()))
        return rhs * 0

    def _prod_batch(self, dim):
        sizes = list(self.sizes)
        del sizes[dim]
        return self.__class__(*sizes, dtype=self._dtype, device=self._device)

    def _quad_form_derivative(self, left_vecs, right_vecs):
        raise RuntimeError('Backwards through a ZeroLazyTensor is not possible')

    def _root_decomposition(self):
        raise RuntimeError('ZeroLazyTensors are not positive definite!')

    def _root_inv_decomposition(self, initial_vectors=None):
        raise RuntimeError('ZeroLazyTensors are not positive definite!')

    def _root_decomposition_size(self):
        raise RuntimeError('ZeroLazyTensors are not positive definite!')

    def _size(self):
        return torch.Size(self.sizes)

    def _sum_batch(self, dim):
        sizes = list(self.sizes)
        del sizes[dim]
        return self.__class__(*sizes, dtype=self._dtype, device=self._device)

    def _t_matmul(self, rhs):
        rhs_size_ind = -2 if rhs.ndimension() > 1 else -1
        if self.size(-1) != rhs.size(rhs_size_ind):
            raise RuntimeError('Size mismatch, self: {}, rhs: {}'.format(self.size(), rhs.size()))
        return rhs * 0

    def _transpose_nonbatch(self):
        return self.transpose(-2, -1)

    def _unsqueeze_batch(self, dim):
        sizes = self.sizes.copy()
        sizes.insert(dim, 1)
        return self.__class__(*sizes, dtype=self._dtype, device=self._device)

    def add_diag(self, diag):
        if self.size(-1) != self.size(-2):
            raise RuntimeError('add_diag only defined for square matrices')
        if self.ndimension() == 3:
            if diag.ndimension() == 0:
                diag = diag.view(1, 1).expand(self.size(0), self.size(1))
            elif diag.ndimension() == 1:
                diag = diag.unsqueeze(0).expand(self.size(0), self.size(1))
            elif diag.ndimension() == 2:
                diag = diag.expand(self.size(0), self.size(1))
            else:
                raise RuntimeError('For a 3D tensor ({}), add_diag expects a 1D or 2D diag. Got size ({})'.format(self.size(), diag.size()))
        elif diag.ndimension() == 0:
            diag = diag.view(1).expand(self.size(0))
        elif diag.ndimension() == 1:
            diag = diag.expand(self.size(0))
        else:
            raise RuntimeError('For a 3D tensor ({}), add_diag expects a 1D or 2D diag. Got size ({})'.format(self.size(), diag.size()))
        res = DiagLazyTensor(diag)
        if res.size() != self.size():
            raise RuntimeError('Diag dimensions are incompatible with the base LazyTensor dimensions. Diag size corresponds to a {} Tensor - expected {}'.format(res.size(), self.size()))
        return res

    def diag(self):
        shape = self.shape
        if shape[-1] != shape[-2]:
            raise RuntimeError('diag works on square matrices (or batches)')
        return torch.zeros(shape[:-1], dtype=self.dtype, device=self.device)

    @cached
    def evaluate(self):
        return torch.zeros(*self.sizes)

    def inv_matmul(self, right_tensor, left_tensor=None):
        raise RuntimeError('ZeroLazyTensors are not invertible!')

    def inv_quad(self, tensor):
        raise RuntimeError('ZeroLazyTensors are not invertible!')

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):
        raise RuntimeError('ZeroLazyTensors are not invertible!')

    def logdet(self):
        return torch.log(torch.tensor(0.0))

    def matmul(self, tensor):
        tensor_size_ind = -2 if tensor.ndimension() > 1 else -1
        if self.size(-1) != tensor.size(tensor_size_ind):
            raise RuntimeError('Size mismatch, self: {}, tensor: {}'.format(self.size(), tensor.size()))
        return tensor * 0

    def mul(self, other):
        shape = _mul_broadcast_shape(self.shape, other.shape)
        return self.__class__(*shape, dtype=self._dtype, device=self._device)

    def transpose(self, dim1, dim2):
        sizes = self.sizes.copy()
        tmp = sizes[dim1]
        sizes[dim1] = sizes[dim2]
        sizes[dim2] = tmp
        return ZeroLazyTensor(*sizes)

    def __add__(self, other):
        return other

    def __div__(self, other):
        return self

    def __mul__(self, other):
        return self


def clear_cache_hook(module, *args, **kwargs):
    module._memoize_cache = {}


class DefaultPredictionStrategy(object):

    def __init__(self, train_inputs, train_prior_dist, train_labels, likelihood, root=None, inv_root=None):
        train_shape = train_prior_dist.event_shape
        train_labels = train_labels.view(*train_labels.shape[:-len(train_shape)], train_shape.numel())
        self.train_inputs = train_inputs
        self.train_prior_dist = train_prior_dist
        self.train_labels = train_labels
        self.likelihood = likelihood
        self._last_test_train_covar = None
        mvn = self.likelihood(train_prior_dist, train_inputs)
        self.lik_train_train_covar = mvn.lazy_covariance_matrix
        if root is not None:
            add_to_cache(self.lik_train_train_covar, 'root_decomposition', RootLazyTensor(root))
        if inv_root is not None:
            add_to_cache(self.lik_train_train_covar, 'root_inv_decomposition', RootLazyTensor(inv_root))

    def __deepcopy__(self, memo):
        pass

    def _exact_predictive_covar_inv_quad_form_cache(self, train_train_covar_inv_root, test_train_covar):
        """
        Computes a cache for K_X*X (K_XX + sigma^2 I)^-1 K_X*X if possible. By default, this does no work and returns
        the first argument.

        Args:
            train_train_covar_inv_root (:obj:`torch.tensor`): a root of (K_XX + sigma^2 I)^-1
            test_train_covar (:obj:`torch.tensor`): the observed noise (from the likelihood)

        Returns
            - A precomputed cache
        """
        res = train_train_covar_inv_root
        if settings.detach_test_caches.on():
            res = res.detach()
        if res.grad_fn is not None:
            wrapper = functools.partial(clear_cache_hook, self)
            functools.update_wrapper(wrapper, clear_cache_hook)
            res.grad_fn.register_hook(wrapper)
        return res

    def _exact_predictive_covar_inv_quad_form_root(self, precomputed_cache, test_train_covar):
        """
        Computes :math:`K_{X^{*}X} S` given a precomputed cache
        Where :math:`S` is a tensor such that :math:`SS^{\\top} = (K_{XX} + \\sigma^2 I)^{-1}`

        Args:
            precomputed_cache (:obj:`torch.tensor`): What was computed in _exact_predictive_covar_inv_quad_form_cache
            test_train_covar (:obj:`torch.tensor`): The observed noise (from the likelihood)

        Returns
            :obj:`~gpytorch.lazy.LazyTensor`: :math:`K_{X^{*}X} S`
        """
        return test_train_covar.matmul(precomputed_cache)

    def get_fantasy_strategy(self, inputs, targets, full_inputs, full_targets, full_output, **kwargs):
        """
        Returns a new PredictionStrategy that incorporates the specified inputs and targets as new training data.

        This method is primary responsible for updating the mean and covariance caches. To add fantasy data to a
        GP model, use the :meth:`~gpytorch.models.ExactGP.get_fantasy_model` method.

        Args:
            - :attr:`inputs` (Tensor `b1 x ... x bk x m x d` or `f x b1 x ... x bk x m x d`): Locations of fantasy
                observations.
            - :attr:`targets` (Tensor `b1 x ... x bk x m` or `f x b1 x ... x bk x m`): Labels of fantasy observations.
            - :attr:`full_inputs` (Tensor `b1 x ... x bk x n+m x d` or `f x b1 x ... x bk x n+m x d`): Training data
                concatenated with fantasy inputs
            - :attr:`full_targets` (Tensor `b1 x ... x bk x n+m` or `f x b1 x ... x bk x n+m`): Training labels
                concatenated with fantasy labels.
            - :attr:`full_output` (:class:`gpytorch.distributions.MultivariateNormal`): Prior called on full_inputs

        Returns:
            - :class:`DefaultPredictionStrategy`
                A `DefaultPredictionStrategy` model with `n + m` training examples, where the `m` fantasy examples have
                been added and all test-time caches have been updated.
        """
        full_mean, full_covar = full_output.mean, full_output.lazy_covariance_matrix
        batch_shape = full_inputs[0].shape[:-2]
        full_mean = full_mean.view(*batch_shape, -1)
        num_train = self.num_train
        fant_fant_covar = full_covar[(...), num_train:, num_train:]
        fant_mean = full_mean[(...), num_train:]
        mvn = self.train_prior_dist.__class__(fant_mean, fant_fant_covar)
        fant_likelihood = self.likelihood.get_fantasy_likelihood(**kwargs)
        mvn_obs = fant_likelihood(mvn, inputs, **kwargs)
        fant_fant_covar = mvn_obs.covariance_matrix
        fant_train_covar = delazify(full_covar[(...), num_train:, :num_train])
        self.fantasy_inputs = inputs
        self.fantasy_targets = targets
        """
        Compute a new mean cache given the old mean cache.

        We have \\alpha = K^{-1}y, and we want to solve [K U; U' S][a; b] = [y; y_f], where U' is fant_train_covar,
        S is fant_fant_covar, and y_f is (targets - fant_mean)

        To do this, we solve the bordered linear system of equations for [a; b]:
            AQ = U  # Q = fant_solve
            [S - U'Q]b = y_f - U'\\alpha   ==> b = [S - U'Q]^{-1}(y_f - U'\\alpha)
            a = \\alpha - Qb
        """
        K_inverse = self.lik_train_train_covar.root_inv_decomposition()
        fant_solve = K_inverse.matmul(fant_train_covar.transpose(-2, -1))
        schur_complement = fant_fant_covar - fant_train_covar.matmul(fant_solve)
        prefix = string.ascii_lowercase[:max(fant_train_covar.dim() - self.mean_cache.dim() - 1, 0)]
        ftcm = torch.einsum(prefix + '...yz,...z->' + prefix + '...y', [fant_train_covar, self.mean_cache])
        small_system_rhs = targets - fant_mean - ftcm
        small_system_rhs = small_system_rhs.unsqueeze(-1)
        schur_cholesky = psd_safe_cholesky(schur_complement, jitter=settings.cholesky_jitter.value())
        fant_cache_lower = torch.cholesky_solve(small_system_rhs, schur_cholesky)
        fant_cache_upper = self.mean_cache.unsqueeze(-1) - fant_solve.matmul(fant_cache_lower)
        fant_cache_upper = fant_cache_upper.squeeze(-1)
        fant_cache_lower = fant_cache_lower.squeeze(-1)
        fant_mean_cache = torch.cat((fant_cache_upper, fant_cache_lower), dim=-1)
        """
        Compute a new covariance cache given the old covariance cache.

        We have access to K \\approx LL' and K^{-1} \\approx R^{-1}R^{-T}, where L and R are low rank matrices
        resulting from Lanczos (see the LOVE paper).

        To update R^{-1}, we first update L:
            [K U; U' S] = [L 0; A B][L' A'; 0 B']
        Solving this matrix equation, we get:
            K = LL' ==>       L = L
            U = LA' ==>       A = UR^{-1}
            S = AA' + BB' ==> B = cholesky(S - AA')

        Once we've computed Z = [L 0; A B], we have that the new kernel matrix [K U; U' S] pprox ZZ'. Therefore,
        we can form a pseudo-inverse of Z directly to approximate [K U; U' S]^{-1/2}.
        """
        batch_shape = fant_train_covar.shape[:-2]
        L_inverse = self.covar_cache
        L = delazify(self.lik_train_train_covar.root_decomposition().root)
        m, n = L.shape[-2:]
        lower_left = fant_train_covar.matmul(L_inverse)
        schur = fant_fant_covar - lower_left.matmul(lower_left.transpose(-2, -1))
        schur_root = psd_safe_cholesky(schur, jitter=settings.cholesky_jitter.value())
        num_fant = schur_root.size(-2)
        m, n = L.shape[-2:]
        new_root = torch.zeros(*batch_shape, m + num_fant, n + num_fant, device=L.device, dtype=L.dtype)
        new_root[(...), :m, :n] = L
        new_root[(...), m:, :n] = lower_left
        new_root[(...), m:, n:] = schur_root
        Q, R = torch.qr(new_root)
        Rdiag = torch.diagonal(R, dim1=-2, dim2=-1)
        zeroish = Rdiag.abs() < 1e-06
        if torch.any(zeroish):
            jitter_diag = 1e-06 * torch.sign(Rdiag) * zeroish
            R = R + torch.diag_embed(jitter_diag)
        new_covar_cache = torch.triangular_solve(Q.transpose(-2, -1), R)[0].transpose(-2, -1)
        if full_inputs[0].dim() <= full_targets.dim():
            fant_batch_shape = full_targets.shape[:1]
            n_batch = len(full_mean.shape[:-1])
            repeat_shape = fant_batch_shape + torch.Size([1] * n_batch)
            full_inputs = [fi.expand(fant_batch_shape + fi.shape) for fi in full_inputs]
            full_mean = full_mean.expand(fant_batch_shape + full_mean.shape)
            full_covar = BatchRepeatLazyTensor(full_covar, repeat_shape)
            new_root = BatchRepeatLazyTensor(NonLazyTensor(new_root), repeat_shape)
        fant_strat = self.__class__(train_inputs=full_inputs, train_prior_dist=self.train_prior_dist.__class__(full_mean, full_covar), train_labels=full_targets, likelihood=fant_likelihood, root=new_root, inv_root=new_covar_cache)
        fant_strat._memoize_cache = {'mean_cache': fant_mean_cache, 'covar_cache': new_covar_cache}
        return fant_strat

    @property
    @cached(name='covar_cache')
    def covar_cache(self):
        train_train_covar = self.lik_train_train_covar
        train_train_covar_inv_root = delazify(train_train_covar.root_inv_decomposition().root)
        return self._exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, self._last_test_train_covar)

    @property
    @cached(name='mean_cache')
    def mean_cache(self):
        mvn = self.likelihood(self.train_prior_dist, self.train_inputs)
        train_mean, train_train_covar = mvn.loc, mvn.lazy_covariance_matrix
        train_labels_offset = (self.train_labels - train_mean).unsqueeze(-1)
        mean_cache = train_train_covar.inv_matmul(train_labels_offset).squeeze(-1)
        if settings.detach_test_caches.on():
            mean_cache = mean_cache.detach()
        if mean_cache.grad_fn is not None:
            wrapper = functools.partial(clear_cache_hook, self)
            functools.update_wrapper(wrapper, clear_cache_hook)
            mean_cache.grad_fn.register_hook(wrapper)
        return mean_cache

    @property
    def num_train(self):
        return self.train_prior_dist.event_shape.numel()

    @property
    def train_shape(self):
        return self.train_prior_dist.event_shape

    def exact_prediction(self, joint_mean, joint_covar):
        test_mean = joint_mean[(...), self.num_train:]
        if joint_covar.size(-1) <= settings.max_eager_kernel_size.value():
            test_covar = joint_covar[(...), self.num_train:, :].evaluate()
            test_test_covar = test_covar[(...), self.num_train:]
            test_train_covar = test_covar[(...), :self.num_train]
        else:
            test_test_covar = joint_covar[(...), self.num_train:, self.num_train:]
            test_train_covar = joint_covar[(...), self.num_train:, :self.num_train]
        return self.exact_predictive_mean(test_mean, test_train_covar), self.exact_predictive_covar(test_test_covar, test_train_covar)

    def exact_predictive_mean(self, test_mean, test_train_covar):
        """
        Computes the posterior predictive covariance of a GP

        Args:
            test_mean (:obj:`torch.tensor`): The test prior mean
            test_train_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test and train inputs

        Returns:
            :obj:`torch.tensor`: The predictive posterior mean of the test points
        """
        res = (test_train_covar @ self.mean_cache.unsqueeze(-1)).squeeze(-1)
        res = res + test_mean
        return res

    def exact_predictive_covar(self, test_test_covar, test_train_covar):
        """
        Computes the posterior predictive covariance of a GP

        Args:
            test_train_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test and train inputs
            test_test_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test inputs

        Returns:
            :obj:`gpytorch.lazy.LazyTensor`: A LazyTensor representing the predictive posterior covariance of the
                                               test points
        """
        if settings.fast_pred_var.on():
            self._last_test_train_covar = test_train_covar
        if settings.skip_posterior_variances.on():
            return ZeroLazyTensor(*test_test_covar.size())
        if settings.fast_pred_var.off():
            dist = self.train_prior_dist.__class__(torch.zeros_like(self.train_prior_dist.mean), self.train_prior_dist.lazy_covariance_matrix)
            if settings.detach_test_caches.on():
                train_train_covar = self.likelihood(dist, self.train_inputs).lazy_covariance_matrix.detach()
            else:
                train_train_covar = self.likelihood(dist, self.train_inputs).lazy_covariance_matrix
            test_train_covar = delazify(test_train_covar)
            train_test_covar = test_train_covar.transpose(-1, -2)
            covar_correction_rhs = train_train_covar.inv_matmul(train_test_covar)
            if torch.is_tensor(test_test_covar):
                if test_test_covar.dim() == 2:
                    return lazify(torch.addmm(test_test_covar, test_train_covar, covar_correction_rhs, beta=1, alpha=-1))
                else:
                    return lazify(test_test_covar + test_train_covar @ covar_correction_rhs.mul(-1))
            else:
                return test_test_covar + MatmulLazyTensor(test_train_covar, covar_correction_rhs.mul(-1))
        precomputed_cache = self.covar_cache
        covar_inv_quad_form_root = self._exact_predictive_covar_inv_quad_form_root(precomputed_cache, test_train_covar)
        if torch.is_tensor(test_test_covar):
            return lazify(torch.add(test_test_covar, covar_inv_quad_form_root @ covar_inv_quad_form_root.transpose(-1, -2), alpha=-1))
        else:
            return test_test_covar + MatmulLazyTensor(covar_inv_quad_form_root, covar_inv_quad_form_root.transpose(-1, -2).mul(-1))


class SumLazyTensor(LazyTensor):

    def __init__(self, *lazy_tensors, **kwargs):
        try:
            lazy_tensors = tuple(lazify(lt) for lt in lazy_tensors)
        except TypeError:
            raise TypeError('All arguments of a SumLazyTensor should be LazyTensors or Tensors')
        batch_shape = _mul_broadcast_shape(*[lt.batch_shape for lt in lazy_tensors])
        lazy_tensors = tuple(lt._expand_batch(batch_shape) if lt.batch_shape != batch_shape else lt for lt in lazy_tensors)
        super(SumLazyTensor, self).__init__(*lazy_tensors, **kwargs)
        self.lazy_tensors = lazy_tensors

    def _expand_batch(self, batch_shape):
        expanded_tensors = [lazy_tensor._expand_batch(batch_shape) for lazy_tensor in self.lazy_tensors]
        return self.__class__(*expanded_tensors)

    def _get_indices(self, row_index, col_index, *batch_indices):
        results = [lazy_tensor._get_indices(row_index, col_index, *batch_indices) for lazy_tensor in self.lazy_tensors]
        return sum(results)

    def _getitem(self, row_index, col_index, *batch_indices):
        results = [lazy_tensor._getitem(row_index, col_index, *batch_indices) for lazy_tensor in self.lazy_tensors]
        return SumLazyTensor(*results)

    def _matmul(self, rhs):
        return sum(lazy_tensor._matmul(rhs) for lazy_tensor in self.lazy_tensors)

    def _quad_form_derivative(self, left_vecs, right_vecs):
        return tuple(var for lazy_tensor in self.lazy_tensors for var in lazy_tensor._quad_form_derivative(left_vecs, right_vecs))

    def _size(self):
        return _mul_broadcast_shape(*[lt.shape for lt in self.lazy_tensors])

    def _sum_batch(self, dim):
        return self.__class__(*(lazy_tensor._sum_batch(dim) for lazy_tensor in self.lazy_tensors))

    def _t_matmul(self, rhs):
        return sum(lazy_tensor._t_matmul(rhs) for lazy_tensor in self.lazy_tensors)

    def _transpose_nonbatch(self):
        lazy_tensors_t = [lazy_tensor.transpose(-1, -2) for lazy_tensor in self.lazy_tensors]
        return self.__class__(*lazy_tensors_t)

    @cached
    def evaluate(self):
        return sum(lazy_tensor.evaluate() for lazy_tensor in self.lazy_tensors)

    def __add__(self, other):
        if isinstance(other, ZeroLazyTensor):
            return self
        elif isinstance(other, DiagLazyTensor):
            return AddedDiagLazyTensor(self, other)
        elif isinstance(other, SumLazyTensor):
            return SumLazyTensor(*(list(self.lazy_tensors) + list(other.lazy_tensors)))
        elif isinstance(other, LazyTensor):
            return SumLazyTensor(*(list(self.lazy_tensors) + [other]))
        elif isinstance(other, Tensor):
            broadcasted_shape = _mul_broadcast_shape(self.shape, other.shape)
            broadcasted_other = lazify(other.expand(broadcasted_shape))
            if broadcasted_shape != self.shape:
                broadcasted_lts = [lt.expand(*broadcasted_shape, 1).squeeze(-1).transpose(-1, -2) for lt in self.lazy_tensors]
            else:
                broadcasted_lts = list(self.lazy_tensors)
            return SumLazyTensor(*(broadcasted_lts + [broadcasted_other]))
        else:
            raise AttributeError('other must be a LazyTensor')

    def diag(self):
        return sum(lazy_tensor.diag().contiguous() for lazy_tensor in self.lazy_tensors)


def prediction_strategy(train_inputs, train_prior_dist, train_labels, likelihood):
    train_train_covar = train_prior_dist.lazy_covariance_matrix
    if isinstance(train_train_covar, LazyEvaluatedKernelTensor):
        cls = train_train_covar.kernel.prediction_strategy
    else:
        cls = DefaultPredictionStrategy
    return cls(train_inputs, train_prior_dist, train_labels, likelihood)


class SumPredictionStrategy(DefaultPredictionStrategy):

    @property
    def _sub_strategies(self):
        sub_strategies = []
        for lazy_tensor in self.train_prior_dist.lazy_covariance_matrix.evaluate_kernel().lazy_tensors:
            pred_strat = prediction_strategy(self.train_inputs, self.train_prior_dist.__class__(self.train_prior_dist.mean, lazy_tensor), self.train_labels, self.likelihood)
            sub_strategies.append(pred_strat)
        return sub_strategies

    def _exact_predictive_covar_inv_quad_form_cache(self, train_train_covar_inv_root, test_train_covar):
        test_train_covar = test_train_covar.evaluate_kernel()
        if not isinstance(test_train_covar, SumLazyTensor):
            return super(SumPredictionStrategy, self)._exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, test_train_covar)
        else:
            return tuple(sub_strat._exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, test_train_covar_comp) for sub_strat, test_train_covar_comp in zip(self._sub_strategies, test_train_covar.lazy_tensors))

    def _exact_predictive_covar_inv_quad_form_root(self, precomputed_cache, test_train_covar):
        test_train_covar = test_train_covar.evaluate_kernel()
        if not isinstance(test_train_covar, SumLazyTensor):
            return super(SumPredictionStrategy, self)._exact_predictive_covar_inv_quad_form_root(precomputed_cache, test_train_covar)
        else:
            return sum(sub_strat._exact_predictive_covar_inv_quad_form_root(cache_comp, test_train_covar_comp) for sub_strat, cache_comp, test_train_covar_comp in zip(self._sub_strategies, precomputed_cache, test_train_covar.evaluate_kernel().lazy_tensors))


class Kernel(Module):
    """
    Kernels in GPyTorch are implemented as a :class:`gpytorch.Module` that, when called on two :obj:`torch.tensor`
    objects `x1` and `x2` returns either a :obj:`torch.tensor` or a :obj:`gpytorch.lazy.LazyTensor` that represents
    the covariance matrix between `x1` and `x2`.

    In the typical use case, to extend this class means to implement the :func:`~gpytorch.kernels.Kernel.forward`
    method.

    .. note::
        The :func:`~gpytorch.kernels.Kernel.__call__` does some additional internal work. In particular,
        all kernels are lazily evaluated so that, in some cases, we can index in to the kernel matrix before actually
        computing it. Furthermore, many built in kernel modules return LazyTensors that allow for more efficient
        inference than if we explicitly computed the kernel matrix itself.

        As a result, if you want to use a :obj:`gpytorch.kernels.Kernel` object just to get an actual
        :obj:`torch.tensor` representing the covariance matrix, you may need to call the
        :func:`gpytorch.lazy.LazyTensor.evaluate` method on the output.

    This base :class:`Kernel` class includes a lengthscale parameter
    :math:`\\Theta`, which is used by many common kernel functions.
    There are a few options for the lengthscale:

    * Default: No lengthscale (i.e. :math:`\\Theta` is the identity matrix).

    * Single lengthscale: One lengthscale can be applied to all input dimensions/batches
      (i.e. :math:`\\Theta` is a constant diagonal matrix).
      This is controlled by setting the attribute `has_lengthscale=True`.

    * ARD: Each input dimension gets its own separate lengthscale
      (i.e. :math:`\\Theta` is a non-constant diagonal matrix).
      This is controlled by the `ard_num_dims` keyword argument (as well as `has_lengthscale=True`).

    In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each
    batch of data can have its own lengthscale parameter by setting the `batch_shape`
    keyword argument to the appropriate number of batches.

    .. note::

        The :attr:`lengthscale` parameter is parameterized on a log scale to constrain it to be positive.
        You can set a prior on this parameter using the :attr:`lengthscale_prior` argument.

    Base Args:
        :attr:`ard_num_dims` (int, optional):
            Set this if you want a separate lengthscale for each input
            dimension. It should be `d` if :attr:`x1` is a `n x d` matrix.  Default: `None`
        :attr:`batch_shape` (torch.Size, optional):
            Set this if you want a separate lengthscale for each batch of input
            data. It should be `b1 x ... x bk` if :attr:`x1` is a `b1 x ... x bk x n x d` tensor.
        :attr:`active_dims` (tuple of ints, optional):
            Set this if you want to compute the covariance of only a few input dimensions. The ints
            corresponds to the indices of the dimensions. Default: `None`.
        :attr:`lengthscale_prior` (Prior, optional):
            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`
        :attr:`lengthscale_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.
        :attr:`eps` (float):
            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.

    Base Attributes:
        :attr:`lengthscale` (Tensor):
            The lengthscale parameter. Size/shape of parameter depends on the
            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.

    Example:
        >>> covar_module = gpytorch.kernels.LinearKernel()
        >>> x1 = torch.randn(50, 3)
        >>> lazy_covar_matrix = covar_module(x1) # Returns a RootLazyTensor
        >>> tensor_covar_matrix = lazy_covar_matrix.evaluate() # Gets the actual tensor for this kernel matrix
    """
    has_lengthscale = False

    def __init__(self, ard_num_dims=None, batch_shape=torch.Size([]), active_dims=None, lengthscale_prior=None, lengthscale_constraint=None, eps=1e-06, **kwargs):
        super(Kernel, self).__init__()
        self._batch_shape = batch_shape
        if active_dims is not None and not torch.is_tensor(active_dims):
            active_dims = torch.tensor(active_dims, dtype=torch.long)
        self.register_buffer('active_dims', active_dims)
        self.ard_num_dims = ard_num_dims
        self.eps = eps
        param_transform = kwargs.get('param_transform')
        if lengthscale_constraint is None:
            lengthscale_constraint = Positive()
        if param_transform is not None:
            warnings.warn("The 'param_transform' argument is now deprecated. If you want to use a different transformation, specify a different 'lengthscale_constraint' instead.", DeprecationWarning)
        if self.has_lengthscale:
            lengthscale_num_dims = 1 if ard_num_dims is None else ard_num_dims
            self.register_parameter(name='raw_lengthscale', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, lengthscale_num_dims)))
            if lengthscale_prior is not None:
                self.register_prior('lengthscale_prior', lengthscale_prior, lambda : self.lengthscale, lambda v: self._set_lengthscale(v))
            self.register_constraint('raw_lengthscale', lengthscale_constraint)
        self.distance_module = None
        self.__pdist_supports_batch = True

    @abstractmethod
    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):
        """
        Computes the covariance between x1 and x2.
        This method should be imlemented by all Kernel subclasses.

        Args:
            :attr:`x1` (Tensor `n x d` or `b x n x d`):
                First set of data
            :attr:`x2` (Tensor `m x d` or `b x m x d`):
                Second set of data
            :attr:`diag` (bool):
                Should the Kernel compute the whole kernel, or just the diag?
            :attr:`last_dim_is_batch` (tuple, optional):
                If this is true, it treats the last dimension of the data as another batch dimension.
                (Useful for additive structure over the dimensions). Default: False

        Returns:
            :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.
                The exact size depends on the kernel's evaluation mode:

                * `full_covar`: `n x m` or `b x n x m`
                * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`
                * `diag`: `n` or `b x n`
                * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`
        """
        raise NotImplementedError()

    @property
    def batch_shape(self):
        kernels = list(self.sub_kernels())
        if len(kernels):
            return _mul_broadcast_shape(self._batch_shape, *[k.batch_shape for k in kernels])
        else:
            return self._batch_shape

    @batch_shape.setter
    def batch_shape(self, val):
        self._batch_shape = val

    @property
    def dtype(self):
        if self.has_lengthscale:
            return self.lengthscale.dtype
        else:
            for param in self.parameters():
                return param.dtype
            return torch.get_default_dtype()

    @property
    def is_stationary(self) ->bool:
        """
        Property to indicate whether kernel is stationary or not.
        """
        return self.has_lengthscale

    @property
    def lengthscale(self):
        if self.has_lengthscale:
            return self.raw_lengthscale_constraint.transform(self.raw_lengthscale)
        else:
            return None

    @lengthscale.setter
    def lengthscale(self, value):
        self._set_lengthscale(value)

    def _set_lengthscale(self, value):
        if not self.has_lengthscale:
            raise RuntimeError('Kernel has no lengthscale.')
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_lengthscale=self.raw_lengthscale_constraint.inverse_transform(value))

    def local_load_samples(self, samples_dict, memo, prefix):
        num_samples = next(iter(samples_dict.values())).size(0)
        self.batch_shape = torch.Size([num_samples]) + self.batch_shape
        super().local_load_samples(samples_dict, memo, prefix)

    def covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=default_postprocess_script, postprocess=True, **params):
        """
        This is a helper method for computing the Euclidean distance between
        all pairs of points in x1 and x2.

        Args:
            :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):
                First set of data.
            :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):
                Second set of data.
            :attr:`diag` (bool):
                Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.
            :attr:`last_dim_is_batch` (tuple, optional):
                Is the last dimension of the data a batch dimension or not?
            :attr:`square_dist` (bool):
                Should we square the distance matrix before returning?

        Returns:
            (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.
            The shape depends on the kernel's mode
            * `diag=False`
            * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)
            * `diag=True`
            * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)
        """
        if last_dim_is_batch:
            x1 = x1.transpose(-1, -2).unsqueeze(-1)
            x2 = x2.transpose(-1, -2).unsqueeze(-1)
        x1_eq_x2 = torch.equal(x1, x2)
        postprocess = torch.tensor(postprocess)
        res = None
        if not self.distance_module or self.distance_module._postprocess != dist_postprocess_func:
            self.distance_module = Distance(dist_postprocess_func)
        if diag:
            if x1_eq_x2:
                res = torch.zeros(*x1.shape[:-2], x1.shape[-2], dtype=x1.dtype, device=x1.device)
                if postprocess:
                    res = dist_postprocess_func(res)
                return res
            else:
                res = torch.norm(x1 - x2, p=2, dim=-1)
                if square_dist:
                    res = res.pow(2)
            if postprocess:
                res = dist_postprocess_func(res)
            return res
        elif square_dist:
            res = self.distance_module._sq_dist(x1, x2, postprocess, x1_eq_x2)
        else:
            res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)
        return res

    def named_sub_kernels(self):
        for name, module in self._modules.items():
            if isinstance(module, Kernel):
                yield name, module

    def num_outputs_per_input(self, x1, x2):
        """
        How many outputs are produced per input (default 1)
        if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel
        will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`
        Default: 1
        """
        return 1

    def prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood):
        return DefaultPredictionStrategy(train_inputs, train_prior_dist, train_labels, likelihood)

    def sub_kernels(self):
        for _, kernel in self.named_sub_kernels():
            yield kernel

    def __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params):
        x1_, x2_ = x1, x2
        if self.active_dims is not None:
            x1_ = x1_.index_select(-1, self.active_dims)
            if x2_ is not None:
                x2_ = x2_.index_select(-1, self.active_dims)
        if x1_.ndimension() == 1:
            x1_ = x1_.unsqueeze(1)
        if x2_ is not None:
            if x2_.ndimension() == 1:
                x2_ = x2_.unsqueeze(1)
            if not x1_.size(-1) == x2_.size(-1):
                raise RuntimeError('x1_ and x2_ must have the same number of dimensions!')
        if x2_ is None:
            x2_ = x1_
        if settings.debug.on():
            if self.ard_num_dims is not None and self.ard_num_dims != x1_.size(-1):
                raise RuntimeError('Expected the input to have {} dimensionality (based on the ard_num_dims argument). Got {}.'.format(self.ard_num_dims, x1_.size(-1)))
        if diag:
            res = super(Kernel, self).__call__(x1_, x2_, diag=True, last_dim_is_batch=last_dim_is_batch, **params)
            if not isinstance(res, LazyEvaluatedKernelTensor):
                if res.dim() == x1_.dim() and res.shape[-2:] == torch.Size((x1_.size(-2), x2_.size(-2))):
                    res = res.diag()
            return res
        else:
            if settings.lazily_evaluate_kernels.on():
                res = LazyEvaluatedKernelTensor(x1_, x2_, kernel=self, last_dim_is_batch=last_dim_is_batch, **params)
            else:
                res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))
            return res

    def __getstate__(self):
        self.distance_module = None
        return self.__dict__

    def __add__(self, other):
        return AdditiveKernel(self, other)

    def __mul__(self, other):
        return ProductKernel(self, other)

    def __setstate__(self, d):
        self.__dict__ = d

    def __getitem__(self, index):
        if len(self.batch_shape) == 0:
            return self
        new_kernel = deepcopy(self)
        index = index if isinstance(index, tuple) else (index,)
        for param_name, param in self._parameters.items():
            new_kernel._parameters[param_name].data = param.__getitem__(index)
            ndim_removed = len(param.shape) - len(new_kernel._parameters[param_name].shape)
            new_batch_shape_len = len(self.batch_shape) - ndim_removed
            new_kernel.batch_shape = new_kernel._parameters[param_name].shape[:new_batch_shape_len]
        for sub_module_name, sub_module in self.named_sub_kernels():
            self._modules[sub_module_name] = sub_module.__getitem__(index)
        return new_kernel


def left_interp(interp_indices, interp_values, rhs):
    """
    """
    is_vector = rhs.ndimension() == 1
    if is_vector:
        res = rhs.index_select(0, interp_indices.view(-1)).view(*interp_values.size())
        res = res.mul(interp_values)
        res = res.sum(-1)
        return res
    else:
        num_rows, num_interp = interp_indices.shape[-2:]
        num_data, num_columns = rhs.shape[-2:]
        interp_shape = torch.Size((*interp_indices.shape[:-1], num_data))
        output_shape = _matmul_broadcast_shape(interp_shape, rhs.shape)
        batch_shape = output_shape[:-2]
        interp_indices_expanded = interp_indices.unsqueeze(-1).expand(*batch_shape, num_rows, num_interp, num_columns)
        interp_values_expanded = interp_values.unsqueeze(-1).expand(*batch_shape, num_rows, num_interp, num_columns)
        rhs_expanded = rhs.unsqueeze(-2).expand(*batch_shape, num_data, num_interp, num_columns)
        res = rhs_expanded.gather(-3, interp_indices_expanded).mul(interp_values_expanded)
        return res.sum(-2)


def left_t_interp(interp_indices, interp_values, rhs, output_dim):
    """
    """
    is_vector = rhs.ndimension() == 1
    if is_vector:
        rhs = rhs.unsqueeze(-1)
    values = rhs.unsqueeze(-2) * interp_values.unsqueeze(-1)
    num_data, num_interp = interp_values.shape[-2:]
    num_cols = rhs.size(-1)
    interp_shape = torch.Size((*interp_indices.shape[:-2], output_dim, num_data))
    output_shape = _matmul_broadcast_shape(interp_shape, rhs.shape)
    batch_shape = output_shape[:-2]
    batch_size = batch_shape.numel()
    interp_indices = interp_indices.expand(*batch_shape, *interp_indices.shape[-2:]).contiguous()
    batch_indices = torch.arange(0, batch_size, dtype=torch.long, device=values.device).unsqueeze_(1)
    batch_indices = batch_indices.repeat(1, num_data * num_interp)
    column_indices = torch.arange(0, num_data * num_interp, dtype=torch.long, device=values.device).unsqueeze_(1)
    column_indices = column_indices.repeat(batch_size, 1)
    summing_matrix_indices = torch.stack([batch_indices.view(-1), interp_indices.view(-1), column_indices.view(-1)], 0)
    summing_matrix_values = torch.ones(batch_size * num_data * num_interp, dtype=interp_values.dtype, device=interp_values.device)
    size = torch.Size((batch_size, output_dim, num_data * num_interp))
    type_name = summing_matrix_values.type().split('.')[-1]
    if interp_values.is_cuda:
        cls = getattr(torch.sparse, type_name)
    else:
        cls = getattr(torch.sparse, type_name)
    summing_matrix = cls(summing_matrix_indices, summing_matrix_values, size)
    values = values.reshape(batch_size, num_data * num_interp, num_cols)
    res = dsmm(summing_matrix, values)
    res = res.view(*batch_shape, *res.shape[-2:])
    if is_vector:
        res = res.squeeze(-1)
    return res


class PsdSumLazyTensor(SumLazyTensor):
    """
    A SumLazyTensor, but where every component of the sum is positive semi-definite
    """

    def zero_mean_mvn_samples(self, num_samples):
        return sum(lazy_tensor.zero_mean_mvn_samples(num_samples) for lazy_tensor in self.lazy_tensors)


class IndexKernel(Kernel):
    """
    A kernel for discrete indices. Kernel is defined by a lookup table.

    .. math::

        \\begin{equation}
            k(i, j) = \\left(BB^\\top + \\text{diag}(\\mathbf v) \\right)_{i, j}
        \\end{equation}

    where :math:`B` is a low-rank matrix, and :math:`\\mathbf v` is a  non-negative vector.
    These parameters are learned.

    Args:
        :attr:`num_tasks` (int):
            Total number of indices.
        :attr:`batch_shape` (torch.Size, optional):
            Set if the MultitaskKernel is operating on batches of data (and you want different
            parameters for each batch)
        :attr:`rank` (int):
            Rank of :math:`B` matrix.
        :attr:`prior` (:obj:`gpytorch.priors.Prior`):
            Prior for :math:`B` matrix.
        :attr:`var_constraint` (Constraint, optional):
            Constraint for added diagonal component. Default: `Positive`.

    Attributes:
        covar_factor:
            The :math:`B` matrix.
        raw_var:
            The element-wise log of the :math:`\\mathbf v` vector.
    """

    def __init__(self, num_tasks, rank=1, prior=None, var_constraint=None, **kwargs):
        if rank > num_tasks:
            raise RuntimeError('Cannot create a task covariance matrix larger than the number of tasks')
        super().__init__(**kwargs)
        if var_constraint is None:
            var_constraint = Positive()
        self.register_parameter(name='covar_factor', parameter=torch.nn.Parameter(torch.randn(*self.batch_shape, num_tasks, rank)))
        self.register_parameter(name='raw_var', parameter=torch.nn.Parameter(torch.randn(*self.batch_shape, num_tasks)))
        if prior is not None:
            self.register_prior('IndexKernelPrior', prior, self._eval_covar_matrix)
        self.register_constraint('raw_var', var_constraint)

    @property
    def var(self):
        return self.raw_var_constraint.transform(self.raw_var)

    @var.setter
    def var(self, value):
        self._set_var(value)

    def _set_var(self, value):
        self.initialize(raw_var=self.raw_var_constraint.inverse_transform(value))

    def _eval_covar_matrix(self):
        cf = self.covar_factor
        return cf @ cf.transpose(-1, -2) + torch.diag_embed(self.var)

    @property
    def covar_matrix(self):
        var = self.var
        res = PsdSumLazyTensor(RootLazyTensor(self.covar_factor), DiagLazyTensor(var))
        return res

    def forward(self, i1, i2, **params):
        covar_matrix = self._eval_covar_matrix()
        batch_shape = _mul_broadcast_shape(i1.shape[:-2], self.batch_shape)
        index_shape = batch_shape + i1.shape[-2:]
        res = InterpolatedLazyTensor(base_lazy_tensor=covar_matrix, left_interp_indices=i1.expand(index_shape), right_interp_indices=i2.expand(index_shape))
        return res


def _prod(iterable):
    return reduce(operator.mul, iterable, 1)


class KroneckerProductLazyTensor(LazyTensor):

    def __init__(self, *lazy_tensors):
        try:
            lazy_tensors = tuple(lazify(lazy_tensor) for lazy_tensor in lazy_tensors)
        except TypeError:
            raise RuntimeError('KroneckerProductLazyTensor is intended to wrap lazy tensors.')
        for prev_lazy_tensor, curr_lazy_tensor in zip(lazy_tensors[:-1], lazy_tensors[1:]):
            if prev_lazy_tensor.batch_shape != curr_lazy_tensor.batch_shape:
                raise RuntimeError('KroneckerProductLazyTensor expects lazy tensors with the same batch shapes. Got {}.'.format([lv.batch_shape for lv in lazy_tensors]))
        super(KroneckerProductLazyTensor, self).__init__(*lazy_tensors)
        self.lazy_tensors = lazy_tensors

    def _get_indices(self, row_index, col_index, *batch_indices):
        row_factor = self.size(-2)
        col_factor = self.size(-1)
        res = None
        for lazy_tensor in self.lazy_tensors:
            sub_row_size = lazy_tensor.size(-2)
            sub_col_size = lazy_tensor.size(-1)
            row_factor //= sub_row_size
            col_factor //= sub_col_size
            sub_res = lazy_tensor._get_indices((row_index // row_factor).fmod(sub_row_size), (col_index // col_factor).fmod(sub_col_size), *batch_indices)
            res = sub_res if res is None else sub_res * res
        return res

    def _matmul(self, rhs):
        is_vec = rhs.ndimension() == 1
        if is_vec:
            rhs = rhs.unsqueeze(-1)
        res = _matmul(self.lazy_tensors, self.shape, rhs.contiguous())
        if is_vec:
            res = res.squeeze(-1)
        return res

    def _t_matmul(self, rhs):
        is_vec = rhs.ndimension() == 1
        if is_vec:
            rhs = rhs.unsqueeze(-1)
        res = _t_matmul(self.lazy_tensors, self.shape, rhs.contiguous())
        if is_vec:
            res = res.squeeze(-1)
        return res

    def _expand_batch(self, batch_shape):
        return self.__class__(*[lazy_tensor._expand_batch(batch_shape) for lazy_tensor in self.lazy_tensors])

    @cached(name='size')
    def _size(self):
        left_size = _prod(lazy_tensor.size(-2) for lazy_tensor in self.lazy_tensors)
        right_size = _prod(lazy_tensor.size(-1) for lazy_tensor in self.lazy_tensors)
        return torch.Size((*self.lazy_tensors[0].batch_shape, left_size, right_size))

    def _transpose_nonbatch(self):
        return self.__class__(*(lazy_tensor._transpose_nonbatch() for lazy_tensor in self.lazy_tensors), **self._kwargs)


class MultitaskKernel(Kernel):
    """
    Kernel supporting Kronecker style multitask Gaussian processes (where every data point is evaluated at every
    task) using :class:`gpytorch.kernels.IndexKernel` as a basic multitask kernel.

    Given a base covariance module to be used for the data, :math:`K_{XX}`, this kernel computes a task kernel of
    specified size :math:`K_{TT}` and returns :math:`K = K_{TT} \\otimes K_{XX}`. as an
    :obj:`gpytorch.lazy.KroneckerProductLazyTensor`.

    :param ~gpytorch.kernels.Kernel data_covar_module: Kernel to use as the data kernel.
    :param int num_tasks: Number of tasks
    :param int rank: (default 1) Rank of index kernel to use for task covariance matrix.
    :param ~gpytorch.priors.Prior task_covar_prior: (default None) Prior to use for task kernel.
        See :class:`gpytorch.kernels.IndexKernel` for details.
    :param dict kwargs: Additional arguments to pass to the kernel.
    """

    def __init__(self, data_covar_module, num_tasks, rank=1, task_covar_prior=None, **kwargs):
        """
        """
        super(MultitaskKernel, self).__init__(**kwargs)
        self.task_covar_module = IndexKernel(num_tasks=num_tasks, batch_shape=self.batch_shape, rank=rank, prior=task_covar_prior)
        self.data_covar_module = data_covar_module
        self.num_tasks = num_tasks

    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):
        if last_dim_is_batch:
            raise RuntimeError('MultitaskKernel does not accept the last_dim_is_batch argument.')
        covar_i = self.task_covar_module.covar_matrix
        if len(x1.shape[:-2]):
            covar_i = covar_i.repeat(*x1.shape[:-2], 1, 1)
        covar_x = lazify(self.data_covar_module.forward(x1, x2, **params))
        res = KroneckerProductLazyTensor(covar_x, covar_i)
        return res.diag() if diag else res

    def num_outputs_per_input(self, x1, x2):
        """
        Given `n` data points `x1` and `m` datapoints `x2`, this multitask
        kernel returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.
        """
        return self.num_tasks


class LCMKernel(Kernel):
    """
    This kernel supports the LCM kernel. It allows the user to specify a list of
    base kernels to use, and individual `MultitaskKernel` objects are fit to each
    of them. The final kernel is the linear sum of the Kronecker product of all
    these base kernels with their respective `MultitaskKernel` objects.

    The returned object is of type :obj:`gpytorch.lazy.KroneckerProductLazyTensor`.
    """

    def __init__(self, base_kernels, num_tasks, rank=1, task_covar_prior=None):
        """
        Args:
            base_kernels (:type: list of `Kernel` objects): A list of base kernels.
            num_tasks (int): The number of output tasks to fit.
            rank (int): Rank of index kernel to use for task covariance matrix for each
                        of the base kernels.
            task_covar_prior (:obj:`gpytorch.priors.Prior`): Prior to use for each
                task kernel. See :class:`gpytorch.kernels.IndexKernel` for details.
        """
        if len(base_kernels) < 1:
            raise ValueError('At least one base kernel must be provided.')
        for k in base_kernels:
            if not isinstance(k, Kernel):
                raise ValueError('base_kernels must only contain Kernel objects')
        super(LCMKernel, self).__init__()
        self.covar_module_list = ModuleList([MultitaskKernel(base_kernel, num_tasks=num_tasks, rank=rank, task_covar_prior=task_covar_prior) for base_kernel in base_kernels])

    def forward(self, x1, x2, **params):
        res = self.covar_module_list[0].forward(x1, x2, **params)
        for m in self.covar_module_list[1:]:
            res += m.forward(x1, x2, **params)
        return res

    def num_outputs_per_input(self, x1, x2):
        """
        Given `n` data points `x1` and `m` datapoints `x2`, this multitask kernel
        returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.
        """
        return self.covar_module_list[0].num_outputs_per_input(x1, x2)

    def __getitem__(self, index):
        new_kernel = deepcopy(self)
        new_kernel.covar_module_list = ModuleList([base_kernel.__getitem__(index) for base_kernel in self.covar_module_list])
        return new_kernel


class LinearKernel(Kernel):
    """
    Computes a covariance matrix based on the Linear kernel
    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:

    .. math::
        \\begin{equation*}
            k_\\text{Linear}(\\mathbf{x_1}, \\mathbf{x_2}) = v\\mathbf{x_1}^\\top
            \\mathbf{x_2}.
        \\end{equation*}

    where

    * :math:`v` is a :attr:`variance` parameter.


    .. note::

        To implement this efficiently, we use a :obj:`gpytorch.lazy.RootLazyTensor` during training and a
        :class:`gpytorch.lazy.MatmulLazyTensor` during test. These lazy tensors represent matrices of the form
        :math:`K = XX^{\\top}` and :math:`K = XZ^{\\top}`. This makes inference
        efficient because a matrix-vector product :math:`Kv` can be computed as
        :math:`Kv=X(X^{\\top}v)`, where the base multiply :math:`Xv` takes only
        :math:`O(nd)` time and space.

    Args:
        :attr:`variance_prior` (:class:`gpytorch.priors.Prior`):
            Prior over the variance parameter (default `None`).
        :attr:`variance_constraint` (Constraint, optional):
            Constraint to place on variance parameter. Default: `Positive`.
        :attr:`active_dims` (list):
            List of data dimensions to operate on.
            `len(active_dims)` should equal `num_dimensions`.
    """

    def __init__(self, num_dimensions=None, offset_prior=None, variance_prior=None, variance_constraint=None, **kwargs):
        super(LinearKernel, self).__init__(**kwargs)
        if variance_constraint is None:
            variance_constraint = Positive()
        if num_dimensions is not None:
            warnings.warn('The `num_dimensions` argument is deprecated and no longer used.', DeprecationWarning)
            self.register_parameter(name='offset', parameter=torch.nn.Parameter(torch.zeros(1, 1, num_dimensions)))
        if offset_prior is not None:
            warnings.warn('The `offset_prior` argument is deprecated and no longer used.', DeprecationWarning)
        self.register_parameter(name='raw_variance', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))
        if variance_prior is not None:
            self.register_prior('variance_prior', variance_prior, lambda : self.variance, lambda v: self._set_variance(v))
        self.register_constraint('raw_variance', variance_constraint)

    @property
    def variance(self):
        return self.raw_variance_constraint.transform(self.raw_variance)

    @variance.setter
    def variance(self, value):
        self._set_variance(value)

    def _set_variance(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_variance=self.raw_variance_constraint.inverse_transform(value))

    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):
        x1_ = x1 * self.variance.sqrt()
        if last_dim_is_batch:
            x1_ = x1_.transpose(-1, -2).unsqueeze(-1)
        if x1.size() == x2.size() and torch.equal(x1, x2):
            prod = RootLazyTensor(x1_)
        else:
            x2_ = x2 * self.variance.sqrt()
            if last_dim_is_batch:
                x2_ = x2_.transpose(-1, -2).unsqueeze(-1)
            prod = MatmulLazyTensor(x1_, x2_.transpose(-2, -1))
        if diag:
            return prod.diag()
        else:
            return prod


class MaternCovariance(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x1, x2, lengthscale, nu, dist_func):
        if any(ctx.needs_input_grad[:2]):
            raise RuntimeError('MaternCovariance cannot compute gradients with respect to x1 and x2')
        if lengthscale.size(-1) > 1:
            raise ValueError('MaternCovariance cannot handle multiple lengthscales')
        needs_grad = any(ctx.needs_input_grad)
        mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]
        x1_ = (x1 - mean).div(lengthscale)
        x2_ = (x2 - mean).div(lengthscale)
        scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))
        if nu == 0.5:
            scaled_unitless_dist_ = scaled_unitless_dist.clone() if needs_grad else scaled_unitless_dist
            exp_component = scaled_unitless_dist_.neg_().exp_()
            covar_mat = exp_component
            if needs_grad:
                d_output_d_input = scaled_unitless_dist.div_(lengthscale).mul_(exp_component)
        elif nu == 1.5:
            if needs_grad:
                scaled_unitless_dist_ = scaled_unitless_dist.clone()
            linear_term = scaled_unitless_dist.clone().add_(1)
            exp_component = scaled_unitless_dist.neg_().exp_()
            covar_mat = linear_term.mul_(exp_component)
            if needs_grad:
                d_output_d_input = scaled_unitless_dist_.pow_(2).div_(lengthscale).mul_(exp_component)
        elif nu == 2.5:
            linear_term = scaled_unitless_dist.clone().add_(1)
            quadratic_term = scaled_unitless_dist.clone().pow_(2).div_(3)
            exp_component = scaled_unitless_dist.neg_().exp_()
            if needs_grad:
                covar_mat = (linear_term + quadratic_term).mul_(exp_component)
                d_output_d_input = linear_term.mul_(quadratic_term).mul_(exp_component).div_(lengthscale)
            else:
                covar_mat = exp_component.mul_(linear_term.add_(quadratic_term))
        if needs_grad:
            ctx.save_for_backward(d_output_d_input)
        return covar_mat

    @staticmethod
    def backward(ctx, grad_output):
        d_output_d_input = ctx.saved_tensors[0]
        lengthscale_grad = grad_output * d_output_d_input
        return None, None, lengthscale_grad, None, None


class _feature_flag(object):
    _state = False

    @classmethod
    def on(cls):
        return cls._state

    @classmethod
    def off(cls):
        return not cls._state

    @classmethod
    def _set_state(cls, state):
        cls._state = state

    def __init__(self, state=True):
        self.prev = self.__class__.on()
        self.state = state

    def __enter__(self):
        self.__class__._set_state(self.state)

    def __exit__(self, *args):
        self.__class__._set_state(self.prev)
        return False


class trace_mode(_feature_flag):
    """
    If set to True, we will generally try to avoid calling our built in PyTorch functions, because these cannot
    be run through torch.jit.trace.

    Note that this will sometimes involve explicitly evaluating lazy tensors and various other slowdowns and
    inefficiencies. As a result, you really shouldn't use this feature context unless you are calling torch.jit.trace
    on a GPyTorch model.

    Our hope is that this flag will not be necessary long term, once https://github.com/pytorch/pytorch/issues/22329
    is fixed.
    """
    _state = False


class MaternKernel(Kernel):
    """
    Computes a covariance matrix based on the Matern kernel
    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:

    .. math::

       \\begin{equation*}
          k_{\\text{Matern}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)}
          \\left( \\sqrt{2 \\nu} d \\right) K_\\nu \\left( \\sqrt{2 \\nu} d \\right)
       \\end{equation*}

    where

    * :math:`d = (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-1} (\\mathbf{x_1} - \\mathbf{x_2})`
      is the distance between
      :math:`x_1` and :math:`x_2` scaled by the :attr:`lengthscale` parameter :math:`\\Theta`.
    * :math:`\\nu` is a smoothness parameter (takes values 1/2, 3/2, or 5/2). Smaller values are less smooth.
    * :math:`K_\\nu` is a modified Bessel function.

    There are a few options for the lengthscale parameter :math:`\\Theta`:
    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.

    .. note::

        This kernel does not have an `outputscale` parameter. To add a scaling parameter,
        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.

    Args:
        :attr:`nu` (float):
            The smoothness parameter: either 1/2, 3/2, or 5/2.
        :attr:`ard_num_dims` (int, optional):
            Set this if you want a separate lengthscale for each
            input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`
        :attr:`batch_shape` (torch.Size, optional):
            Set this if you want a separate lengthscale for each
             batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`
        :attr:`active_dims` (tuple of ints, optional):
            Set this if you want to
            compute the covariance of only a few input dimensions. The ints
            corresponds to the indices of the dimensions. Default: `None`.
        :attr:`lengthscale_prior` (Prior, optional):
            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`
        :attr:`lengthscale_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.
        :attr:`eps` (float):
            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.

    Attributes:
        :attr:`lengthscale` (Tensor):
            The lengthscale parameter. Size/shape of parameter depends on the
            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.

    Example:
        >>> x = torch.randn(10, 5)
        >>> # Non-batch: Simple option
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))
        >>> # Non-batch: ARD (different lengthscale for each input dimension)
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=5))
        >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)
        >>>
        >>> batch_x = torch.randn(2, 10, 5)
        >>> # Batch: Simple option
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))
        >>> # Batch: different lengthscale for each batch
        >>> covar_module = gpytorch.kernels.MaternKernel(nu=0.5, batch_shape=torch.Size([2])
        >>> covar = covar_module(x)  # Output: LazyVariable of size (2 x 10 x 10)
    """
    has_lengthscale = True

    def __init__(self, nu=2.5, **kwargs):
        if nu not in {0.5, 1.5, 2.5}:
            raise RuntimeError('nu expected to be 0.5, 1.5, or 2.5')
        super(MaternKernel, self).__init__(**kwargs)
        self.nu = nu

    def forward(self, x1, x2, diag=False, **params):
        if x1.requires_grad or x2.requires_grad or self.ard_num_dims is not None and self.ard_num_dims > 1 or diag or params.get('last_dim_is_batch', False) or trace_mode.on():
            mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]
            x1_ = (x1 - mean).div(self.lengthscale)
            x2_ = (x2 - mean).div(self.lengthscale)
            distance = self.covar_dist(x1_, x2_, diag=diag, **params)
            exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)
            if self.nu == 0.5:
                constant_component = 1
            elif self.nu == 1.5:
                constant_component = (math.sqrt(3) * distance).add(1)
            elif self.nu == 2.5:
                constant_component = (math.sqrt(5) * distance).add(1).add(5.0 / 3.0 * distance ** 2)
            return constant_component * exp_component
        return MaternCovariance().apply(x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params))


class CatLazyTensor(LazyTensor):
    """
    A `LazyTensor` that represents the concatenation of other lazy tensors.
    Each LazyTensor must have the same shape except in the concatenating
    dimension.

    Args:
        - :attr:`lazy_tensors` (list of LazyTensors):
            A list of LazyTensors whose sizes are the same except in
            concatenating dimension :attr:`dim`
        - :attr:`dim` (int):
            The concatenating dimension which can be a batch dimension.
        - :attr:`output_device` (torch.device):
            The CatLazyTensor will appear to appear on :attr:`output_device`
            and place any output `torch.Tensors` on :attr:`output_device`
    """

    def _check_args(self, *lazy_tensors, dim=0, output_device=None):
        if len(lazy_tensors) == 0:
            raise RuntimeError('List of LazyTensors must be non-empty')
        elif len(lazy_tensors) == 1:
            raise RuntimeError('Why are we trying to concatenate a single LazyTensor?')
        if not all([isinstance(t, LazyTensor) for t in lazy_tensors]):
            raise RuntimeError('CatLazyTensor requires a list of all LazyTensors')
        rep_tensor = lazy_tensors[0]
        rep_tensor_noncat_shape = list(rep_tensor.shape)
        del rep_tensor_noncat_shape[dim]
        for t in lazy_tensors:
            if t.dim() != rep_tensor.dim():
                raise RuntimeError('All tensors must have the same number of dimensions')
            t_noncat_shape = list(t.shape)
            del t_noncat_shape[dim]
            if t_noncat_shape != rep_tensor_noncat_shape:
                raise RuntimeError('All LazyTensors must have the same size in the non-concatenation dimension')

    def __init__(self, *lazy_tensors, dim=0, output_device=None):
        rep_tensor = lazy_tensors[0]
        ndims = rep_tensor.ndimension()
        if dim >= 0:
            positive_dim = dim
            dim = dim - ndims
        else:
            positive_dim = ndims + dim
        super().__init__(*lazy_tensors, dim=dim, output_device=output_device)
        self.lazy_tensors = lazy_tensors
        self.cat_dim = dim
        self.output_device = output_device
        cat_dim_sizes = torch.tensor([t.size(dim) for t in lazy_tensors], device=output_device)
        cat_dim_cum_sizes = torch.zeros(len(lazy_tensors) + 1, dtype=torch.long, device=output_device)
        torch.cumsum(cat_dim_sizes, dim=-1, out=cat_dim_cum_sizes[1:])
        idx_to_tensor_idx = torch.empty(cat_dim_cum_sizes[-1].item(), dtype=torch.long, device=output_device)
        for tsr_idx, (start_idx, end_idx) in enumerate(zip(cat_dim_cum_sizes[:-1], cat_dim_cum_sizes[1:])):
            idx_to_tensor_idx[start_idx.item():end_idx.item()].fill_(tsr_idx)
        self.cat_dim_sizes = cat_dim_sizes
        self.cat_dim_cum_sizes = cat_dim_cum_sizes
        self.idx_to_tensor_idx = idx_to_tensor_idx
        self._shape = torch.Size((*rep_tensor.shape[:positive_dim], cat_dim_cum_sizes[-1].item(), *rep_tensor.shape[positive_dim + 1:]))

    def _split_slice(self, slice_idx):
        """
        Splits a slice(a, b, None) in to a list of slices [slice(a1, b1, None), slice(a2, b2, None), ...]
        so that each slice in the list slices in to a single tensor that we have concatenated with this LazyTensor.
        """
        if slice_idx.step is not None:
            raise RuntimeError('Slicing a CatLazyTensor with a step is not currently supported!')
        start_idx = slice_idx.start if slice_idx.start is not None else 0
        stop_idx = slice_idx.stop if slice_idx.stop is not None else self.size(self.cat_dim)
        first_tensor_idx = self.idx_to_tensor_idx[start_idx].item()
        last_tensor_idx = self.idx_to_tensor_idx[stop_idx - 1].item()
        first_tensor_start_index = start_idx - self.cat_dim_cum_sizes[first_tensor_idx].item()
        last_tensor_stop_index = stop_idx - self.cat_dim_cum_sizes[last_tensor_idx].item()
        if first_tensor_idx == last_tensor_idx:
            return [first_tensor_idx], [slice(first_tensor_start_index, last_tensor_stop_index, None)]
        else:
            num_middle_tensors = last_tensor_idx - first_tensor_idx - 1
            first_slice = slice(first_tensor_start_index, None, None)
            last_slice = slice(None, last_tensor_stop_index, None)
            return list(range(first_tensor_idx, last_tensor_idx + 1)), [first_slice] + [_noop_index] * num_middle_tensors + [last_slice]

    def _expand_batch(self, batch_shape):
        lazy_tensors = [lazy_tensor._expand_batch(batch_shape) for lazy_tensor in self.lazy_tensors]
        res = self.__class__(*lazy_tensors, dim=self.cat_dim, output_device=self.output_device)
        return res

    def _get_indices(self, row_index, col_index, *batch_indices):
        indices = [*batch_indices, row_index, col_index]
        target_shape = _mul_broadcast_shape(*[index.shape for index in indices])
        indices = [index.expand(target_shape).reshape(-1) for index in indices]
        cat_dim_indices = indices[self.cat_dim]
        target_tensors = self.idx_to_tensor_idx[cat_dim_indices]
        does_switch_tensor = torch.ones(target_tensors.numel() + 1, dtype=bool_compat, device=self.device)
        torch.ne(target_tensors[:-1], target_tensors[1:], out=does_switch_tensor[1:-1])
        lazy_tensor_indices = target_tensors[does_switch_tensor[:-1]].tolist()
        lazy_tensors = [self.lazy_tensors[idx] for idx in lazy_tensor_indices]
        switch_tensor = does_switch_tensor.nonzero().squeeze(-1)
        split_sizes = (switch_tensor[1:] - switch_tensor[:-1]).tolist()
        sub_indices = zip(*[(list(index.split(split_sizes)) if torch.is_tensor(index) else [index] * len(split_sizes)) for index in indices])
        sub_indices = [list(sub_index) for sub_index in sub_indices]
        for lazy_tensor_idx, sub_index in zip(lazy_tensor_indices, sub_indices):
            sub_index[self.cat_dim] = sub_index[self.cat_dim] - self.cat_dim_cum_sizes[lazy_tensor_idx]
        res_list = [lazy_tensor._get_indices(sub_index[-2], sub_index[-1], *sub_index[:-2]) for lazy_tensor, sub_index in zip(lazy_tensors, sub_indices)]
        if len(res_list) == 1:
            return res_list[0].view(target_shape)
        else:
            return torch.cat(res_list).view(target_shape)

    def _getitem(self, row_index, col_index, *batch_indices):
        indices = [*batch_indices, row_index, col_index]
        cat_dim_indices = indices[self.cat_dim]
        if isinstance(cat_dim_indices, slice):
            if cat_dim_indices == _noop_index:
                res_list = [lazy_tensor._getitem(row_index, col_index, *batch_indices) for lazy_tensor in self.lazy_tensors]
            else:
                res_list = []
                tensor_idxs, target_slices = self._split_slice(cat_dim_indices)
                for tensor_idx, target_slice in zip(tensor_idxs, target_slices):
                    indices[self.cat_dim] = target_slice
                    res = self.lazy_tensors[tensor_idx]._getitem(indices[-2], indices[-1], *indices[:-2])
                    res_list.append(res)
        elif torch.is_tensor(cat_dim_indices):
            target_tensors = self.idx_to_tensor_idx[cat_dim_indices]
            does_switch_tensor = torch.ones(target_tensors.numel() + 1, dtype=bool_compat, device=self.device)
            torch.ne(target_tensors[:-1], target_tensors[1:], out=does_switch_tensor[1:-1])
            lazy_tensor_indices = target_tensors[does_switch_tensor[:-1]].tolist()
            lazy_tensors = [self.lazy_tensors[idx] for idx in lazy_tensor_indices]
            switch_tensor = does_switch_tensor.nonzero().squeeze(-1)
            split_sizes = (switch_tensor[1:] - switch_tensor[:-1]).tolist()
            sub_indices = zip(*[(list(index.split(split_sizes)) if torch.is_tensor(index) else [index] * len(split_sizes)) for index in indices])
            sub_indices = [list(sub_index) for sub_index in sub_indices]
            for lazy_tensor_idx, sub_index in zip(lazy_tensor_indices, sub_indices):
                sub_index[self.cat_dim] = sub_index[self.cat_dim] - self.cat_dim_cum_sizes[lazy_tensor_idx]
            res_list = [lazy_tensor._getitem(sub_index[-2], sub_index[-1], *sub_index[:-2]) for lazy_tensor, sub_index in zip(lazy_tensors, sub_indices)]
        elif isinstance(cat_dim_indices, int):
            target_tensor = self.idx_to_tensor_idx[cat_dim_indices].item()
            cat_dim_indices = cat_dim_indices - self.cat_dim_cum_sizes[target_tensor]
            indices[self.cat_dim] = cat_dim_indices
            res_list = [self.lazy_tensors[target_tensor]._getitem(indices[-2], indices[-1], *indices[:-2])]
        if len(res_list) == 1:
            return res_list[0]
        else:
            res = self.__class__(*res_list, dim=self.cat_dim, output_device=self.output_device)
            return res

    def _matmul(self, rhs):
        output_device = self.device if self.device is not None else rhs.device
        rhs_ = []
        for d in self.devices:
            if d != rhs.device:
                rhs_.append(rhs)
            else:
                rhs_.append(rhs)
        if self.cat_dim == -2:
            res_list = [t._matmul(rhs) for t, rhs in zip(self.lazy_tensors, rhs_)]
            res_list = [x for x in res_list]
            res = torch.cat(res_list, dim=-2)
        elif self.cat_dim == -1:
            curr_idx = 0
            res_list = []
            index = [slice(None, None, None) for _ in range(rhs.ndimension())]
            for t, size, rhs in zip(self.lazy_tensors, self.cat_dim_sizes, rhs_):
                index[-2] = slice(curr_idx, curr_idx + size, None)
                res_list.append(t._matmul(rhs[index]))
                curr_idx += size
            res_list = [x for x in res_list]
            res = 0.0
            for x in res_list:
                res = res + x
        else:
            output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)
            rhs = rhs.expand(*output_shape[:-2], *rhs.shape[-2:])
            curr_idx = 0
            res_list = []
            for t, size in zip(self.lazy_tensors, self.cat_dim_sizes):
                sub_rhs = rhs.narrow(self.cat_dim, curr_idx, size)
                res_list.append(t._matmul(sub_rhs))
                curr_idx += size
            res_list = [x for x in res_list]
            res = torch.cat(res_list, dim=self.cat_dim)
        return res

    def _permute_batch(self, *dims):
        lazy_tensors = [lazy_tensor._permute_batch(*dims) for lazy_tensor in self.lazy_tensors]
        if self.cat_dim < -2:
            positive_cat_dim = self.dim() + self.cat_dim
            new_cat_dim = dims.index(positive_cat_dim)
        else:
            new_cat_dim = self.cat_dim
        return self.__class__(*lazy_tensors, dim=new_cat_dim, output_device=self.output_device)

    def _size(self):
        return self._shape

    def _transpose_nonbatch(self):
        if self.cat_dim == -2:
            new_dim = -1
        elif self.cat_dim == -1:
            new_dim = -2
        else:
            new_dim = self.cat_dim
        return self.__class__(*[t._transpose_nonbatch() for t in self.lazy_tensors], dim=new_dim, output_device=self.output_device)

    def _unsqueeze_batch(self, dim):
        cat_dim = self.dim() + self.cat_dim
        lazy_tensors = [lazy_tensor._unsqueeze_batch(dim) for lazy_tensor in self.lazy_tensors]
        res = self.__class__(*lazy_tensors, dim=cat_dim + 1 if dim <= cat_dim else cat_dim, output_device=self.output_device)
        return res

    def diag(self):
        if settings.debug.on():
            if not self.is_square:
                raise RuntimeError('Diag works on square matrices (or batches)')
        if self.cat_dim == -2:
            res = []
            curr_col = 0
            for t in self.lazy_tensors:
                n_rows, n_cols = t.shape[-2:]
                rows = torch.arange(0, n_rows, dtype=torch.long, device=t.device)
                cols = torch.arange(curr_col, curr_col + n_rows, dtype=torch.long, device=t.device)
                res.append(t[..., rows, cols])
                curr_col += n_rows
            res = torch.cat(res, dim=-1)
        elif self.cat_dim == -1:
            res = []
            curr_row = 0
            for t in self.lazy_tensors:
                n_rows, n_cols = t.shape[-2:]
                rows = torch.arange(curr_row, curr_row + n_cols, dtype=torch.long, device=t.device)
                cols = torch.arange(0, n_cols, dtype=torch.long, device=t.device)
                curr_row += n_cols
                res.append(t[..., rows, cols])
            res = torch.cat(res, dim=-1)
        else:
            res = torch.cat([t.diag() for t in self.lazy_tensors], dim=self.cat_dim + 1)
        return res

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):
        res = super().inv_quad_logdet(inv_quad_rhs, logdet, reduce_inv_quad)
        return tuple(r for r in res)

    @property
    def device(self):
        return self.output_device

    @property
    def devices(self):
        return [t.device for t in self.lazy_tensors]

    @property
    def device_count(self):
        return len(set(self.devices))

    def to(self, device_id):
        """
        returns a new CatLazyTensor with device_id as the output_device
        Warning: this does not move the LazyTensors in this CatLazyTensor to
        device_id
        """
        new_kwargs = dict(self._kwargs)
        new_kwargs['output_device'] = device_id
        return self.__class__(*self._args, **new_kwargs)

    def all_to(self, device_id):
        """
        Create a new CatLazyTensor with all LazyTensors in CatLazyTensor moved
        to one device device. The new CatLazyTensor also has device_id as the
        output_device.
        """
        new_args = []
        new_kwargs = {}
        for arg in self._args:
            if hasattr(arg, 'to'):
                new_args.append(arg)
            else:
                new_args.append(arg)
        for name, val in self._kwargs.items():
            if hasattr(val, 'to'):
                new_kwargs[name] = val
            else:
                new_kwargs[name] = val
        new_kwargs['output_device'] = device_id
        return self.__class__(*new_args, **new_kwargs)


class MultiDeviceKernel(DataParallel, Kernel):
    """
    Allocates the covariance matrix on distributed devices, e.g. multiple GPUs.

    Args:
        - :attr:`base_kernel`: Base kernel to distribute
        - :attr:`device_ids`: list of `torch.device` objects to place kernel chunks on
        - :attr:`output_device`: Device where outputs will be placed
    """

    def __init__(self, base_kernel, device_ids, output_device=None, create_cuda_context=True, **kwargs):
        if create_cuda_context:
            for d in device_ids:
                _ = torch.tensor([], device=d)
        DataParallel.__init__(self, module=base_kernel, device_ids=device_ids, output_device=output_device, dim=-2)
        self.output_device = output_device if output_device else device_ids[0]
        self.__cached_x1 = torch.empty(1)
        self.__cached_x2 = torch.empty(1)

    @property
    def base_kernel(self):
        return self.module

    def forward(self, x1, x2, diag=False, **kwargs):
        if diag:
            return self.module.forward(x1, x2, diag=True, **kwargs)
        if x1.size(-2) < len(self.device_ids) + 1:
            return self.module.forward(x1, x2, diag=diag, **kwargs)
        if not x1.device == self.__cached_x1.device or not torch.equal(x1, self.__cached_x1):
            self._x1_scattered, self._kwargs = self.scatter((x1,), kwargs, self.device_ids)
            self.__cached_x1 = x1
        if not x2.device == self.__cached_x2.device or not torch.equal(x2, self.__cached_x2):
            self._x2_subs = [x2 for x1_ in self._x1_scattered]
            self.__cached_x2 = x2
        inputs = tuple((x1_[0], x2_) for x1_, x2_ in zip(self._x1_scattered, self._x2_subs))
        if not self.device_ids:
            return self.module.forward(*inputs, **self._kwargs)
        if len(self.device_ids) == 1:
            return self.module.forward(*inputs[0], **self._kwargs[0])

        def set_distance_module_to_none(module):
            if hasattr(module, 'distance_module'):
                module.distance_module = None
        self.module.apply(set_distance_module_to_none)
        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
        with settings.lazily_evaluate_kernels(False):
            outputs = self.parallel_apply(replicas, inputs, self._kwargs)
        return self.gather(outputs, self.output_device)

    def gather(self, outputs, output_device):
        return CatLazyTensor(*[lazify(o) for o in outputs], dim=self.dim, output_device=self.output_device)

    def num_outputs_per_input(self, x1, x2):
        return self.base_kernel.num_outputs_per_input(x1, x2)


class NewtonGirardAdditiveKernel(Kernel):

    def __init__(self, base_kernel, num_dims, max_degree=None, active_dims=None, **kwargs):
        """Create an Additive Kernel a la https://arxiv.org/abs/1112.4394 using Newton-Girard Formulae

        :param base_kernel: a base 1-dimensional kernel. NOTE: put ard_num_dims=d in the base kernel...
        :param max_degree: the maximum numbers of kernel degrees to compute
        :param active_dims:
        :param kwargs:
        """
        super(NewtonGirardAdditiveKernel, self).__init__(active_dims=active_dims, **kwargs)
        self.base_kernel = base_kernel
        self.num_dims = num_dims
        if max_degree is None:
            self.max_degree = self.num_dims
        elif max_degree > self.num_dims:
            self.max_degree = self.num_dims
        else:
            self.max_degree = max_degree
        self.register_parameter(name='raw_outputscale', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, self.max_degree)))
        outputscale_constraint = Positive()
        self.register_constraint('raw_outputscale', outputscale_constraint)
        self.outputscale_constraint = outputscale_constraint
        self.outputscale = [(1 / self.max_degree) for _ in range(self.max_degree)]

    @property
    def outputscale(self):
        return self.raw_outputscale_constraint.transform(self.raw_outputscale)

    @outputscale.setter
    def outputscale(self, value):
        self._set_outputscale(value)

    def _set_outputscale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))

    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):
        """Forward proceeds by Newton-Girard formulae"""
        if last_dim_is_batch:
            raise RuntimeError('NewtonGirardAdditiveKernel does not accept the last_dim_is_batch argument.')
        kern_values = delazify(self.base_kernel(x1, x2, diag=diag, last_dim_is_batch=True, **params))
        kernel_dim = -3 if not diag else -2
        shape = [(1) for _ in range(len(kern_values.shape) + 1)]
        shape[kernel_dim - 1] = -1
        kvals = torch.arange(1, self.max_degree + 1, device=kern_values.device).reshape(*shape)
        shape = [d_ for d_ in kern_values.shape]
        shape[kernel_dim] = self.max_degree + 1
        e_n = torch.empty(*shape, device=kern_values.device)
        if kernel_dim == -3:
            e_n[(...), (0), :, :] = 1.0
        else:
            e_n[(...), (0), :] = 1.0
        s_k = kern_values.unsqueeze(kernel_dim - 1).pow(kvals).sum(dim=kernel_dim)
        m1 = torch.tensor([-1], dtype=torch.float, device=kern_values.device)
        shape = [(1) for _ in range(len(kern_values.shape))]
        shape[kernel_dim] = -1
        for deg in range(1, self.max_degree + 1):
            ks = torch.arange(1, deg + 1, device=kern_values.device, dtype=torch.float).reshape(*shape)
            kslong = torch.arange(1, deg + 1, device=kern_values.device, dtype=torch.long)
            sum_ = (m1.pow(ks - 1) * e_n.index_select(kernel_dim, deg - kslong) * s_k.index_select(kernel_dim, kslong - 1)).sum(dim=kernel_dim) / deg
            if kernel_dim == -3:
                e_n[(...), (deg), :, :] = sum_
            else:
                e_n[(...), (deg), :] = sum_
        if kernel_dim == -3:
            return (self.outputscale.unsqueeze(-1).unsqueeze(-1) * e_n.narrow(kernel_dim, 1, self.max_degree)).sum(dim=kernel_dim)
        else:
            return (self.outputscale.unsqueeze(-1) * e_n.narrow(kernel_dim, 1, self.max_degree)).sum(dim=kernel_dim)


class PeriodicKernel(Kernel):
    """ Computes a covariance matrix based on the periodic kernel
    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:

    .. math::

       \\begin{equation*}
          k_{\\text{Periodic}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left(
            \\frac{2 \\sin^2 \\left( \\pi \\Vert \\mathbf{x_1} - \\mathbf{x_2} \\Vert_1 / p \\right) }
            { \\ell^2 } \\right)
       \\end{equation*}

    where

    * :math:`p` is the periord length parameter.
    * :math:`\\ell` is a lengthscale parameter.

    .. note::

        This kernel does not have an `outputscale` parameter. To add a scaling parameter,
        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.

    .. note::

        This kernel does not have an ARD lengthscale option.

    Args:
        :attr:`batch_shape` (torch.Size, optional):
            Set this if you want a separate lengthscale for each
             batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.
        :attr:`active_dims` (tuple of ints, optional):
            Set this if you want to compute the covariance of only a few input dimensions. The ints
            corresponds to the indices of the dimensions. Default: `None`.
        :attr:`period_length_prior` (Prior, optional):
            Set this if you want to apply a prior to the period length parameter.  Default: `None`.
        :attr:`lengthscale_prior` (Prior, optional):
            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.
        :attr:`lengthscale_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the value of the lengthscale. Default: `Positive`.
        :attr:`period_length_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the value of the period length. Default: `Positive`.
        :attr:`eps` (float):
            The minimum value that the lengthscale/period length can take
            (prevents divide by zero errors). Default: `1e-6`.

    Attributes:
        :attr:`lengthscale` (Tensor):
            The lengthscale parameter. Size = `*batch_shape x 1 x 1`.
        :attr:`period_length` (Tensor):
            The period length parameter. Size = `*batch_shape x 1 x 1`.

    Example:
        >>> x = torch.randn(10, 5)
        >>> # Non-batch: Simple option
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())
        >>>
        >>> batch_x = torch.randn(2, 10, 5)
        >>> # Batch: Simple option
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())
        >>> # Batch: different lengthscale for each batch
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel(batch_size=2))
        >>> covar = covar_module(x)  # Output: LazyVariable of size (2 x 10 x 10)
    """
    has_lengthscale = True

    def __init__(self, period_length_prior=None, period_length_constraint=None, **kwargs):
        super(PeriodicKernel, self).__init__(**kwargs)
        if period_length_constraint is None:
            period_length_constraint = Positive()
        self.register_parameter(name='raw_period_length', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))
        if period_length_prior is not None:
            self.register_prior('period_length_prior', period_length_prior, lambda : self.period_length, lambda v: self._set_period_length(v))
        self.register_constraint('raw_period_length', period_length_constraint)

    @property
    def period_length(self):
        return self.raw_period_length_constraint.transform(self.raw_period_length)

    @period_length.setter
    def period_length(self, value):
        self._set_period_length(value)

    def _set_period_length(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_period_length=self.raw_period_length_constraint.inverse_transform(value))

    def forward(self, x1, x2, diag=False, **params):
        x1_ = x1.div(self.period_length)
        x2_ = x2.div(self.period_length)
        diff = self.covar_dist(x1_, x2_, diag=diag, **params)
        res = torch.sin(diff.mul(math.pi)).pow(2).mul(-2 / self.lengthscale).exp_()
        if diag:
            res = res.squeeze(0)
        return res


class Prior(Distribution, Module, ABC):
    """
    Base class for Priors in GPyTorch.
    In GPyTorch, a parameter can be assigned a prior by passing it as the `prior` argument to
    :func:`~gpytorch.module.register_parameter`. GPyTorch performs internal bookkeeping of priors,
    and for each parameter with a registered prior includes the log probability of the parameter under its
    respective prior in computing the Marginal Log-Likelihood.
    """

    def transform(self, x):
        return self._transform(x) if self._transform is not None else x

    def log_prob(self, x):
        """Returns the log-probability of the parameter value under the prior."""
        return super(Prior, self).log_prob(self.transform(x))


class PolynomialKernel(Kernel):
    """
    Computes a covariance matrix based on the Polynomial kernel
    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:

    .. math::
        \\begin{equation*}
            k_\\text{Poly}(\\mathbf{x_1}, \\mathbf{x_2}) = (\\mathbf{x_1}^\\top
            \\mathbf{x_2} + c)^{d}.
        \\end{equation*}

    where

    * :math:`c` is an :attr:`offset` parameter.

    Args:
        :attr:`offset_prior` (:class:`gpytorch.priors.Prior`):
            Prior over the offset parameter (default `None`).
        :attr:`offset_constraint` (Constraint, optional):
            Constraint to place on offset parameter. Default: `Positive`.
        :attr:`active_dims` (list):
            List of data dimensions to operate on.
            `len(active_dims)` should equal `num_dimensions`.
    """

    def __init__(self, power: int, offset_prior: Optional[Prior]=None, offset_constraint: Optional[Interval]=None, **kwargs):
        super().__init__(**kwargs)
        if offset_constraint is None:
            offset_constraint = Positive()
        self.register_parameter(name='raw_offset', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1)))
        if torch.is_tensor(power):
            if power.numel() > 1:
                raise RuntimeError('Cant create a Polynomial kernel with more than one power')
            else:
                power = power.item()
        self.power = power
        if offset_prior is not None:
            self.register_prior('offset_prior', offset_prior, lambda : self.offset, lambda v: self._set_offset(v))
        self.register_constraint('raw_offset', offset_constraint)

    @property
    def offset(self) ->torch.Tensor:
        return self.raw_offset_constraint.transform(self.raw_offset)

    @offset.setter
    def offset(self, value: torch.Tensor) ->None:
        self._set_offset(value)

    def _set_offset(self, value: torch.Tensor) ->None:
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_offset=self.raw_offset_constraint.inverse_transform(value))

    def forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: Optional[bool]=False, last_dim_is_batch: Optional[bool]=False, **params) ->torch.Tensor:
        offset = self.offset.view(*self.batch_shape, 1, 1)
        if last_dim_is_batch:
            x1 = x1.transpose(-1, -2).unsqueeze(-1)
            x2 = x2.transpose(-1, -2).unsqueeze(-1)
        if diag:
            return ((x1 * x2).sum(dim=-1) + self.offset).pow(self.power)
        if x1.dim() == 2 and x2.dim() == 2:
            return torch.addmm(offset, x1, x2.transpose(-2, -1)).pow(self.power)
        else:
            return (torch.matmul(x1, x2.transpose(-2, -1)) + offset).pow(self.power)


class PolynomialKernelGrad(PolynomialKernel):

    def forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: Optional[bool]=False, last_dim_is_batch: Optional[bool]=False, **params) ->torch.Tensor:
        offset = self.offset.view(*self.batch_shape, 1, 1)
        batch_shape = x1.shape[:-2]
        n1, d = x1.shape[-2:]
        n2 = x2.shape[-2]
        if diag:
            base_diag = (x1 * x2).sum(dim=-1) + self.offset
            K11_diag = base_diag.pow(self.power)
            all_outers_diag = (x1 * x2).transpose(-2, -1).reshape(*batch_shape, -1)
            K22_base_diag = self.power * (self.power - 1) * base_diag.pow(self.power - 2)
            K12_base_diag = self.power * base_diag.pow(self.power - 1)
            K22_diag = all_outers_diag * K22_base_diag.repeat(d) + K12_base_diag.repeat(d)
            return torch.cat([K11_diag, K22_diag], dim=-1)
        else:
            base_inner_prod = torch.matmul(x1, x2.transpose(-2, -1)) + offset
            K11 = base_inner_prod.pow(self.power)
            K12_base = self.power * base_inner_prod.pow(self.power - 1)
            K12 = torch.zeros(*batch_shape, n1, n2 * d, dtype=x1.dtype, device=x1.device)
            ones_ = torch.ones(*batch_shape, d, 1, n2, dtype=x1.dtype, device=x1.device)
            K12_outer_prods = torch.matmul(x1.transpose(-2, -1).unsqueeze(-1), ones_)
            K12 = (K12_base.unsqueeze(-3) * K12_outer_prods).transpose(-3, -2).reshape(*batch_shape, n1, d * n2)
            ones_ = torch.ones(*batch_shape, d, n1, 1, dtype=x1.dtype, device=x1.device)
            K21_outer_prods = torch.matmul(ones_, x2.transpose(-2, -1).unsqueeze(-2))
            K21 = (K12_base.unsqueeze(-3) * K21_outer_prods).view(*batch_shape, d * n1, n2)
            K22_base = self.power * (self.power - 1) * base_inner_prod.pow(self.power - 2)
            K22 = torch.zeros(*batch_shape, n1 * d, n2 * d, dtype=x1.dtype, device=x1.device)
            all_outers = x1.unsqueeze(-2).unsqueeze(-2).transpose(-2, -1).matmul(x2.unsqueeze(-3).unsqueeze(-2))
            all_outers = all_outers.transpose(-4, -2).transpose(-3, -1)
            K22 = K22_base.unsqueeze(-3).unsqueeze(-3) * all_outers
            for i in range(d):
                K22[(...), (i), (i), :, :] = K22[(...), (i), (i), :, :] + K12_base
            K22 = K22.transpose(-4, -3).transpose(-3, -2).reshape(*batch_shape, n1 * d, n2 * d)
            K = torch.cat([torch.cat([K11, K12], dim=-1), torch.cat([K21, K22], dim=-1)], dim=-2)
            pi1 = torch.arange(n1 * (d + 1)).view(d + 1, n1).t().reshape(n1 * (d + 1))
            pi2 = torch.arange(n2 * (d + 1)).view(d + 1, n2).t().reshape(n2 * (d + 1))
            K = K[(...), (pi1), :][(...), :, (pi2)]
            return K

    def num_outputs_per_input(self, x1, x2):
        return x1.size(-1) + 1


class RBFCovariance(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x1, x2, lengthscale, sq_dist_func):
        if any(ctx.needs_input_grad[:2]):
            raise RuntimeError('RBFCovariance cannot compute gradients with respect to x1 and x2')
        if lengthscale.size(-1) > 1:
            raise ValueError('RBFCovariance cannot handle multiple lengthscales')
        needs_grad = any(ctx.needs_input_grad)
        x1_ = x1.div(lengthscale)
        x2_ = x2.div(lengthscale)
        unitless_sq_dist = sq_dist_func(x1_, x2_)
        unitless_sq_dist_ = unitless_sq_dist.clone() if needs_grad else unitless_sq_dist
        covar_mat = unitless_sq_dist_.div_(-2.0).exp_()
        if needs_grad:
            d_output_d_input = unitless_sq_dist.mul_(covar_mat).div_(lengthscale)
            ctx.save_for_backward(d_output_d_input)
        return covar_mat

    @staticmethod
    def backward(ctx, grad_output):
        d_output_d_input = ctx.saved_tensors[0]
        lengthscale_grad = grad_output * d_output_d_input
        return None, None, lengthscale_grad, None


def postprocess_rbf(dist_mat):
    return dist_mat.div_(-2).exp_()


class RBFKernel(Kernel):
    """
    Computes a covariance matrix based on the RBF (squared exponential) kernel
    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:

    .. math::

       \\begin{equation*}
          k_{\\text{RBF}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left( -\\frac{1}{2}
          (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)
       \\end{equation*}

    where :math:`\\Theta` is a :attr:`lengthscale` parameter.
    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.

    .. note::

        This kernel does not have an `outputscale` parameter. To add a scaling parameter,
        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.

    Args:
        :attr:`ard_num_dims` (int, optional):
            Set this if you want a separate lengthscale for each
            input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`
        :attr:`batch_shape` (torch.Size, optional):
            Set this if you want a separate lengthscale for each
            batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.
        :attr:`active_dims` (tuple of ints, optional):
            Set this if you want to compute the covariance of only a few input dimensions. The ints
            corresponds to the indices of the dimensions. Default: `None`.
        :attr:`lengthscale_prior` (Prior, optional):
            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.
        :attr:`lengthscale_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.
        :attr:`eps` (float):
            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.

    Attributes:
        :attr:`lengthscale` (Tensor):
            The lengthscale parameter. Size/shape of parameter depends on the
            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.

    Example:
        >>> x = torch.randn(10, 5)
        >>> # Non-batch: Simple option
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
        >>> # Non-batch: ARD (different lengthscale for each input dimension)
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=5))
        >>> covar = covar_module(x)  # Output: LazyTensor of size (10 x 10)
        >>>
        >>> batch_x = torch.randn(2, 10, 5)
        >>> # Batch: Simple option
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
        >>> # Batch: different lengthscale for each batch
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([2])))
        >>> covar = covar_module(x)  # Output: LazyTensor of size (2 x 10 x 10)
    """
    has_lengthscale = True

    def forward(self, x1, x2, diag=False, **params):
        if x1.requires_grad or x2.requires_grad or self.ard_num_dims is not None and self.ard_num_dims > 1 or diag or params.get('last_dim_is_batch', False) or trace_mode.on():
            x1_ = x1.div(self.lengthscale)
            x2_ = x2.div(self.lengthscale)
            return self.covar_dist(x1_, x2_, square_dist=True, diag=diag, dist_postprocess_func=postprocess_rbf, postprocess=True, **params)
        return RBFCovariance().apply(x1, x2, self.lengthscale, lambda x1, x2: self.covar_dist(x1, x2, square_dist=True, diag=False, dist_postprocess_func=postprocess_rbf, postprocess=False, **params))


class RBFKernelGrad(RBFKernel):
    """
    Computes a covariance matrix of the RBF kernel that models the covariance
    between the values and partial derivatives for inputs :math:`\\mathbf{x_1}`
    and :math:`\\mathbf{x_2}`.

    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.

    .. note::

        This kernel does not have an `outputscale` parameter. To add a scaling parameter,
        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.

    Args:
        :attr:`batch_shape` (torch.Size, optional):
            Set this if you want a separate lengthscale for each
             batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.
        :attr:`active_dims` (tuple of ints, optional):
            Set this if you want to compute the covariance of only a few input dimensions. The ints
            corresponds to the indices of the dimensions. Default: `None`.
        :attr:`lengthscale_prior` (Prior, optional):
            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.
        :attr:`lengthscale_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.
        :attr:`eps` (float):
            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.

    Attributes:
        :attr:`lengthscale` (Tensor):
            The lengthscale parameter. Size/shape of parameter depends on the
            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.

    Example:
        >>> x = torch.randn(10, 5)
        >>> # Non-batch: Simple option
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad())
        >>> covar = covar_module(x)  # Output: LazyTensor of size (60 x 60), where 60 = n * (d + 1)
        >>>
        >>> batch_x = torch.randn(2, 10, 5)
        >>> # Batch: Simple option
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad())
        >>> # Batch: different lengthscale for each batch
        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad(batch_shape=torch.Size([2])))
        >>> covar = covar_module(x)  # Output: LazyTensor of size (2 x 60 x 60)
    """

    def forward(self, x1, x2, diag=False, **params):
        batch_shape = x1.shape[:-2]
        n_batch_dims = len(batch_shape)
        n1, d = x1.shape[-2:]
        n2 = x2.shape[-2]
        K = torch.zeros(*batch_shape, n1 * (d + 1), n2 * (d + 1), device=x1.device, dtype=x1.dtype)
        if not diag:
            x1_ = x1.div(self.lengthscale)
            x2_ = x2.div(self.lengthscale)
            outer = x1_.view(*batch_shape, n1, 1, d) - x2_.view(*batch_shape, 1, n2, d)
            outer = outer / self.lengthscale.unsqueeze(-2)
            outer = torch.transpose(outer, -1, -2).contiguous()
            diff = self.covar_dist(x1_, x2_, square_dist=True, dist_postprocess_func=postprocess_rbf, **params)
            K_11 = diff
            K[(...), :n1, :n2] = K_11
            outer1 = outer.view(*batch_shape, n1, n2 * d)
            K[(...), :n1, n2:] = outer1 * K_11.repeat([*([1] * (n_batch_dims + 1)), d])
            outer2 = outer.transpose(-1, -3).reshape(*batch_shape, n2, n1 * d)
            outer2 = outer2.transpose(-1, -2)
            K[(...), n1:, :n2] = -outer2 * K_11.repeat([*([1] * n_batch_dims), d, 1])
            outer3 = outer1.repeat([*([1] * n_batch_dims), d, 1]) * outer2.repeat([*([1] * (n_batch_dims + 1)), d])
            kp = KroneckerProductLazyTensor(torch.eye(d, d, device=x1.device, dtype=x1.dtype).repeat(*batch_shape, 1, 1) / self.lengthscale.pow(2), torch.ones(n1, n2, device=x1.device, dtype=x1.dtype).repeat(*batch_shape, 1, 1))
            chain_rule = kp.evaluate() - outer3
            K[(...), n1:, n2:] = chain_rule * K_11.repeat([*([1] * n_batch_dims), d, d])
            if n1 == n2 and torch.eq(x1, x2).all():
                K = 0.5 * (K.transpose(-1, -2) + K)
            pi1 = torch.arange(n1 * (d + 1)).view(d + 1, n1).t().reshape(n1 * (d + 1))
            pi2 = torch.arange(n2 * (d + 1)).view(d + 1, n2).t().reshape(n2 * (d + 1))
            K = K[(...), (pi1), :][(...), :, (pi2)]
            return K
        else:
            if not (n1 == n2 and torch.eq(x1, x2).all()):
                raise RuntimeError('diag=True only works when x1 == x2')
            kernel_diag = super(RBFKernelGrad, self).forward(x1, x2, diag=True)
            grad_diag = torch.ones(*batch_shape, n2, d, device=x1.device, dtype=x1.dtype) / self.lengthscale.pow_(2)
            grad_diag = grad_diag.transpose(-1, -2).reshape(*batch_shape, n2 * d)
            k_diag = torch.cat((kernel_diag, grad_diag), dim=-1)
            pi = torch.arange(n2 * (d + 1)).view(d + 1, n2).t().reshape(n2 * (d + 1))
            return k_diag[..., pi]

    def num_outputs_per_input(self, x1, x2):
        return x1.size(-1) + 1


class ConstantMulLazyTensor(LazyTensor):
    """
    A LazyTensor that multiplies a base LazyTensor by a scalar constant:

    ```
    constant_mul_lazy_tensor = constant * base_lazy_tensor
    ```

    .. note::

        To element-wise multiply two lazy tensors, see :class:`gpytorch.lazy.MulLazyTensor`

    Args:
        base_lazy_tensor (LazyTensor) or (b x n x m)): The base_lazy tensor
        constant (Tensor): The constant

    If `base_lazy_tensor` represents a matrix (non-batch), then `constant` must be a
    0D tensor, or a 1D tensor with one element.

    If `base_lazy_tensor` represents a batch of matrices (b x m x n), then `constant` can be
    either:
    - A 0D tensor - the same constant is applied to all matrices in the batch
    - A 1D tensor with one element - the same constant is applied to all matrices
    - A 1D tensor with `b` elements - a different constant is applied to each matrix

    Example::

        >>> base_base_lazy_tensor = gpytorch.lazy.ToeplitzLazyTensor([1, 2, 3])
        >>> constant = torch.tensor(1.2)
        >>> new_base_lazy_tensor = gpytorch.lazy.ConstantMulLazyTensor(base_base_lazy_tensor, constant)
        >>> new_base_lazy_tensor.evaluate()
        >>> # Returns:
        >>> # [[ 1.2, 2.4, 3.6 ]
        >>> #  [ 2.4, 1.2, 2.4 ]
        >>> #  [ 3.6, 2.4, 1.2 ]]
        >>>
        >>> base_base_lazy_tensor = gpytorch.lazy.ToeplitzLazyTensor([[1, 2, 3], [2, 3, 4]])
        >>> constant = torch.tensor([1.2, 0.5])
        >>> new_base_lazy_tensor = gpytorch.lazy.ConstantMulLazyTensor(base_base_lazy_tensor, constant)
        >>> new_base_lazy_tensor.evaluate()
        >>> # Returns:
        >>> # [[[ 1.2, 2.4, 3.6 ]
        >>> #   [ 2.4, 1.2, 2.4 ]
        >>> #   [ 3.6, 2.4, 1.2 ]]
        >>> #  [[ 1, 1.5, 2 ]
        >>> #   [ 1.5, 1, 1.5 ]
        >>> #   [ 2, 1.5, 1 ]]]
    """

    def __init__(self, base_lazy_tensor, constant):
        if not torch.is_tensor(constant):
            constant = torch.tensor(constant, device=base_lazy_tensor.device, dtype=base_lazy_tensor.dtype)
        super(ConstantMulLazyTensor, self).__init__(base_lazy_tensor, constant)
        self.base_lazy_tensor = base_lazy_tensor
        self._constant = constant

    def _approx_diag(self):
        res = self.base_lazy_tensor._approx_diag()
        return res * self._constant.unsqueeze(-1)

    def _expand_batch(self, batch_shape):
        return self.__class__(self.base_lazy_tensor._expand_batch(batch_shape), self._constant.expand(*batch_shape))

    def _get_indices(self, row_index, col_index, *batch_indices):
        base_lazy_tensor = self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices)
        constant = self._constant.expand(self.batch_shape)[batch_indices]
        return base_lazy_tensor * constant

    def _getitem(self, row_index, col_index, *batch_indices):
        base_lazy_tensor = self.base_lazy_tensor._getitem(row_index, col_index, *batch_indices)
        constant = self._constant.expand(self.batch_shape)[batch_indices]
        constant = constant.view(*constant.shape, 1, 1)
        return base_lazy_tensor * constant

    def _matmul(self, rhs):
        res = self.base_lazy_tensor._matmul(rhs)
        res = res * self.expanded_constant
        return res

    def _permute_batch(self, *dims):
        return self.__class__(self.base_lazy_tensor._permute_batch(*dims), self._constant.expand(self.batch_shape).permute(*dims))

    def _quad_form_derivative(self, left_vecs, right_vecs):
        constant_deriv = left_vecs * self.base_lazy_tensor._matmul(right_vecs)
        constant_deriv = constant_deriv.sum(-2).sum(-1)
        while constant_deriv.dim() > self._constant.dim():
            constant_deriv = constant_deriv.sum(0)
        for i in range(self._constant.dim()):
            if self._constant.size(i) == 1:
                constant_deriv = constant_deriv.sum(i, keepdim=True)
        left_vecs = left_vecs * self.expanded_constant
        res = self.base_lazy_tensor._quad_form_derivative(left_vecs, right_vecs)
        return tuple(res) + (constant_deriv,)

    def _size(self):
        return self.base_lazy_tensor.size()

    def _t_matmul(self, rhs):
        res = self.base_lazy_tensor._t_matmul(rhs)
        res = res * self.expanded_constant
        return res

    def _transpose_nonbatch(self):
        return ConstantMulLazyTensor(self.base_lazy_tensor._transpose_nonbatch(), self._constant)

    @property
    def expanded_constant(self):
        try:
            constant = self._constant.view(*self._constant.shape, 1, 1)
        except RuntimeError:
            raise RuntimeError('ConstantMulLazyTensor of size {} received an invalid constant of size {}.'.format(self.base_lazy_tensor.shape, self._constant.shape))
        return constant

    def diag(self):
        res = self.base_lazy_tensor.diag()
        return res * self._constant.unsqueeze(-1)

    @cached
    def evaluate(self):
        res = self.base_lazy_tensor.evaluate()
        return res * self.expanded_constant


class RFFPredictionStrategy(DefaultPredictionStrategy):

    def __init__(self, train_inputs, train_prior_dist, train_labels, likelihood):
        super().__init__(train_inputs, train_prior_dist, train_labels, likelihood)
        self.train_prior_dist = self.train_prior_dist.__class__(self.train_prior_dist.mean, self.train_prior_dist.lazy_covariance_matrix.evaluate_kernel())

    def get_fantasy_strategy(self, inputs, targets, full_inputs, full_targets, full_output, **kwargs):
        raise NotImplementedError('Fantasy observation updates not yet supported for models using RFFs')

    @property
    @cached(name='covar_cache')
    def covar_cache(self):
        lt = self.train_prior_dist.lazy_covariance_matrix
        if isinstance(lt, ConstantMulLazyTensor):
            constant = lt.expanded_constant
            lt = lt.base_lazy_tensor
        else:
            constant = torch.tensor(1.0, dtype=lt.dtype, device=lt.device)
        train_factor = lt.root.evaluate()
        train_train_covar = self.lik_train_train_covar
        inner_term = torch.eye(train_factor.size(-1), dtype=train_factor.dtype, device=train_factor.device) - train_factor.transpose(-1, -2) @ train_train_covar.inv_matmul(train_factor) * constant
        return psd_safe_cholesky(inner_term)

    def exact_prediction(self, joint_mean, joint_covar):
        test_mean = joint_mean[(...), self.num_train:]
        test_test_covar = joint_covar[(...), self.num_train:, self.num_train:].evaluate_kernel()
        test_train_covar = joint_covar[(...), self.num_train:, :self.num_train].evaluate_kernel()
        return self.exact_predictive_mean(test_mean, test_train_covar), self.exact_predictive_covar(test_test_covar, test_train_covar)

    def exact_predictive_covar(self, test_test_covar, test_train_covar):
        if isinstance(test_test_covar, ConstantMulLazyTensor):
            constant = test_test_covar.expanded_constant
            test_test_covar = test_test_covar.base_lazy_tensor
        else:
            constant = torch.tensor(1.0, dtype=test_test_covar.dtype, device=test_test_covar.device)
        covar_cache = self.covar_cache
        factor = test_test_covar.root.evaluate() * constant.sqrt()
        res = RootLazyTensor(factor @ covar_cache)
        return res


class RFFKernel(Kernel):
    """
    Computes a covariance matrix based on Random Fourier Features with the RBFKernel.

    Random Fourier features was originally proposed in
    'Random Features for Large-Scale Kernel Machines' by Rahimi and Recht (2008).
    Instead of the shifted cosine features from Rahimi and Recht (2008), we use
    the sine and cosine features which is a lower-variance estimator --- see
    'On the Error of Random Fourier Features' by Sutherland and Schneider (2015).

    By Bochner's theorem, any continuous kernel :math:`k` is positive definite
    if and only if it is the Fourier transform of a non-negative measure :math:`p(\\omega)`, i.e.

    .. math::
        \\begin{equation}
            k(x, x') = k(x - x') = \\int p(\\omega) e^{i(\\omega^\\top (x - x'))} d\\omega.
        \\end{equation}

    where :math:`p(\\omega)` is a normalized probability measure if :math:`k(0)=1`.

    For the RBF kernel,

    .. math::
        \\begin{equation}
        k(\\Delta) = \\exp{(-\\frac{\\Delta^2}{2\\sigma^2})}$ and $p(\\omega) = \\exp{(-\\frac{\\sigma^2\\omega^2}{2})}
        \\end{equation}

    where :math:`\\Delta = x - x'`.

    Given datapoint :math:`x\\in \\mathbb{R}^d`, we can construct its random Fourier features
    :math:`z(x) \\in \\mathbb{R}^{2D}` by

    .. math::
        \\begin{equation}
        z(x) = \\sqrt{\\frac{1}{D}}
        \\begin{bmatrix}
            \\cos(\\omega_1^\\top x)\\\\
            \\sin(\\omega_1^\\top x)\\\\
            \\cdots \\\\
            \\cos(\\omega_D^\\top x)\\\\
            \\sin(\\omega_D^\\top x)
        \\end{bmatrix}, \\omega_1, \\ldots, \\omega_D \\sim p(\\omega)
        \\end{equation}

    such that we have an unbiased Monte Carlo estimator

    .. math::
        \\begin{equation}
            k(x, x') = k(x - x') \\approx z(x)^\\top z(x') = \\frac{1}{D}\\sum_{i=1}^D \\cos(\\omega_i^\\top (x - x')).
        \\end{equation}

    .. note::
        When this kernel is used in batch mode, the random frequencies are drawn
        independently across the batch dimension as well by default.

    :param num_samples: Number of random frequencies to draw. This is :math:`D` in the above
        papers. This will produce :math:`D` sine features and :math:`D` cosine
        features for a total of :math:`2D` random Fourier features.
    :type num_samples: int
    :param num_dims: (Default `None`.) Dimensionality of the data space.
        This is :math:`d` in the above papers. Note that if you want an
        independent lengthscale for each dimension, set `ard_num_dims` equal to
        `num_dims`. If unspecified, it will be inferred the first time `forward`
        is called.
    :type num_dims: int, optional

    :var torch.Tensor randn_weights: The random frequencies that are drawn once and then fixed.

    Example:

        >>> # This will infer `num_dims` automatically
        >>> kernel= gpytorch.kernels.RFFKernel(num_samples=5)
        >>> x = torch.randn(10, 3)
        >>> kxx = kernel(x, x).evaluate()
        >>> print(kxx.randn_weights.size())
        torch.Size([3, 5])

    """
    has_lengthscale = True

    def __init__(self, num_samples: int, num_dims: Optional[int]=None, **kwargs):
        super().__init__(**kwargs)
        self.num_samples = num_samples
        if num_dims is not None:
            self._init_weights(num_dims, num_samples)

    def _init_weights(self, num_dims: Optional[int]=None, num_samples: Optional[int]=None, randn_weights: Optional[Tensor]=None):
        if num_dims is not None and num_samples is not None:
            d = num_dims
            D = num_samples
        if randn_weights is None:
            randn_shape = torch.Size([*self._batch_shape, d, D])
            randn_weights = torch.randn(randn_shape, dtype=self.raw_lengthscale.dtype, device=self.raw_lengthscale.device)
        self.register_buffer('randn_weights', randn_weights)

    def forward(self, x1: Tensor, x2: Tensor, diag: bool=False, last_dim_is_batch: bool=False, **kwargs) ->Tensor:
        if last_dim_is_batch:
            x1 = x1.transpose(-1, -2).unsqueeze(-1)
            x2 = x2.transpose(-1, -2).unsqueeze(-1)
        num_dims = x1.size(-1)
        if not hasattr(self, 'randn_weights'):
            self._init_weights(num_dims, self.num_samples)
        x1_eq_x2 = torch.equal(x1, x2)
        z1 = self._featurize(x1, normalize=False)
        if not x1_eq_x2:
            z2 = self._featurize(x2, normalize=False)
        else:
            z2 = z1
        D = float(self.num_samples)
        if diag:
            return (z1 * z2).sum(-1) / D
        if x1_eq_x2:
            return RootLazyTensor(z1 / math.sqrt(D))
        else:
            return MatmulLazyTensor(z1 / D, z2.transpose(-1, -2))

    def _featurize(self, x: Tensor, normalize: bool=False) ->Tensor:
        x = x.matmul(self.randn_weights / self.lengthscale.transpose(-1, -2))
        z = torch.cat([torch.cos(x), torch.sin(x)], dim=-1)
        if normalize:
            D = self.num_samples
            z = z / math.sqrt(D)
        return z

    def prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood):
        return RFFPredictionStrategy(train_inputs, train_prior_dist, train_labels, likelihood)


class RQKernel(Kernel):
    """
    Computes a covariance matrix based on the rational quadratic kernel
    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:

    .. math::

       \\begin{equation*}
          k_{\\text{RQ}}(\\mathbf{x_1}, \\mathbf{x_2}) =  \\left(1 + \\frac{1}{2\\alpha}
          (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)^{-\\alpha}
       \\end{equation*}

    where :math:`\\Theta` is a :attr:`lengthscale` parameter, and :math:`\\alpha` is the
    rational quadratic relative weighting parameter.
    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.

    .. note::

        This kernel does not have an `outputscale` parameter. To add a scaling parameter,
        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.

    Args:
        :attr:`ard_num_dims` (int, optional):
            Set this if you want a separate lengthscale for each
            input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`
        :attr:`batch_shape` (torch.Size, optional):
            Set this if you want a separate lengthscale for each
            batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.
        :attr:`active_dims` (tuple of ints, optional):
            Set this if you want to compute the covariance of only a few input dimensions. The ints
            corresponds to the indices of the dimensions. Default: `None`.
        :attr:`lengthscale_prior` (Prior, optional):
            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.
        :attr:`lengthscale_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.
        :attr:`alpha_constraint` (Constraint, optional):
            Set this if you want to apply a constraint to the alpha parameter. Default: `Positive`.
        :attr:`eps` (float):
            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.

    Attributes:
        :attr:`lengthscale` (Tensor):
            The lengthscale parameter. Size/shape of parameter depends on the
            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.
        :attr:`alpha` (Tensor):
            The rational quadratic relative weighting parameter. Size/shape of parameter depends
            on the :attr:`batch_shape` argument
    """
    has_lengthscale = True

    def __init__(self, alpha_constraint=None, **kwargs):
        super(RQKernel, self).__init__(**kwargs)
        self.register_parameter(name='raw_alpha', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1)))
        if alpha_constraint is None:
            alpha_constraint = Positive()
        self.register_constraint('raw_alpha', alpha_constraint)

    def forward(self, x1, x2, diag=False, **params):

        def postprocess_rq(dist):
            alpha = self.alpha
            for _ in range(1, len(dist.shape) - len(self.batch_shape)):
                alpha = alpha.unsqueeze(-1)
            return (1 + dist.div(2 * alpha)).pow(-alpha)
        x1_ = x1.div(self.lengthscale)
        x2_ = x2.div(self.lengthscale)
        return self.covar_dist(x1_, x2_, square_dist=True, diag=diag, dist_postprocess_func=postprocess_rq, postprocess=True, **params)

    @property
    def alpha(self):
        return self.raw_alpha_constraint.transform(self.raw_alpha)

    @alpha.setter
    def alpha(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_alpha=self.raw_alpha_constraint.inverse_transform(value))


class ScaleKernel(Kernel):
    """
    Decorates an existing kernel object with an output scale, i.e.

    .. math::

       \\begin{equation*}
          K_{\\text{scaled}} = \\theta_\\text{scale} K_{\\text{orig}}
       \\end{equation*}

    where :math:`\\theta_\\text{scale}` is the `outputscale` parameter.

    In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each
    batch of data can have its own `outputscale` parameter by setting the `batch_shape`
    keyword argument to the appropriate number of batches.

    .. note::
        The :attr:`outputscale` parameter is parameterized on a log scale to constrain it to be positive.
        You can set a prior on this parameter using the :attr:`outputscale_prior` argument.

    Args:
        :attr:`base_kernel` (Kernel):
            The base kernel to be scaled.
        :attr:`batch_shape` (int, optional):
            Set this if you want a separate outputscale for each batch of input data. It should be `b`
            if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`
        :attr:`outputscale_prior` (Prior, optional): Set this if you want to apply a prior to the outputscale
            parameter.  Default: `None`
        :attr:`outputscale_constraint` (Constraint, optional): Set this if you want to apply a constraint to the
            outputscale parameter. Default: `Positive`.

    Attributes:
        :attr:`base_kernel` (Kernel):
            The kernel module to be scaled.
        :attr:`outputscale` (Tensor):
            The outputscale parameter. Size/shape of parameter depends on the :attr:`batch_shape` arguments.

    Example:
        >>> x = torch.randn(10, 5)
        >>> base_covar_module = gpytorch.kernels.RBFKernel()
        >>> scaled_covar_module = gpytorch.kernels.ScaleKernel(base_covar_module)
        >>> covar = scaled_covar_module(x)  # Output: LazyTensor of size (10 x 10)
    """

    @property
    def is_stationary(self) ->bool:
        """
        Kernel is stationary if base kernel is stationary.
        """
        return self.base_kernel.is_stationary

    def __init__(self, base_kernel, outputscale_prior=None, outputscale_constraint=None, **kwargs):
        if base_kernel.active_dims is not None:
            kwargs['active_dims'] = base_kernel.active_dims
        super(ScaleKernel, self).__init__(**kwargs)
        if outputscale_constraint is None:
            outputscale_constraint = Positive()
        self.base_kernel = base_kernel
        outputscale = torch.zeros(*self.batch_shape) if len(self.batch_shape) else torch.tensor(0.0)
        self.register_parameter(name='raw_outputscale', parameter=torch.nn.Parameter(outputscale))
        if outputscale_prior is not None:
            self.register_prior('outputscale_prior', outputscale_prior, lambda : self.outputscale, lambda v: self._set_outputscale(v))
        self.register_constraint('raw_outputscale', outputscale_constraint)

    @property
    def outputscale(self):
        return self.raw_outputscale_constraint.transform(self.raw_outputscale)

    @outputscale.setter
    def outputscale(self, value):
        self._set_outputscale(value)

    def _set_outputscale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_outputscale=self.raw_outputscale_constraint.inverse_transform(value))

    def forward(self, x1, x2, last_dim_is_batch=False, diag=False, **params):
        orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)
        outputscales = self.outputscale
        if last_dim_is_batch:
            outputscales = outputscales.unsqueeze(-1)
        if diag:
            outputscales = outputscales.unsqueeze(-1)
            return delazify(orig_output) * outputscales
        else:
            outputscales = outputscales.view(*outputscales.shape, 1, 1)
            return orig_output.mul(outputscales)

    def num_outputs_per_input(self, x1, x2):
        return self.base_kernel.num_outputs_per_input(x1, x2)


logger = logging.getLogger()


class SpectralMixtureKernel(Kernel):
    """
    Computes a covariance matrix based on the Spectral Mixture Kernel
    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:
    It was proposed in `Gaussian Process Kernels for Pattern Discovery and Extrapolation`_.

    .. note::

        Unlike other kernels,
        * :attr:`ard_num_dims` **must equal** the number of dimensions of the data
        * :attr:`batch_shape` **must equal** the batch size of the data (torch.Size([1]) if the data is not batched)
        * :attr:`batch_shape` **cannot** contain more than one batch dimension.
        * This kernel should not be combined with a :class:`gpytorch.kernels.ScaleKernel`.

    Args:
        :attr:`num_mixtures` (int, optional):
            The number of components in the mixture.
        :attr:`ard_num_dims` (int, optional):
            Set this to match the dimensionality of the input.
            It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `1`
        :attr:`batch_shape` (torch.Size, optional):
            Set this if the data is
             batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([1])`
        :attr:`active_dims` (tuple of ints, optional):
            Set this if you want to compute the covariance of only a few input dimensions. The ints
            corresponds to the indices of the dimensions. Default: `None`.
        :attr:`eps` (float):
            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.

    Attributes:
        :attr:`mixture_lengthscale` (Tensor):
            The lengthscale parameter. Given `k` mixture components, and `b x n x d` data, this will be of
            size `b x k x 1 x d`.
        :attr:`mixture_means` (Tensor):
            The mixture mean parameters (`b x k x 1 x d`).
        :attr:`mixture_weights` (Tensor):
            The mixture weight parameters (`b x k`).

    Example:
        >>> # Non-batch
        >>> x = torch.randn(10, 5)
        >>> covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=5)
        >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)
        >>>
        >>> # Batch
        >>> batch_x = torch.randn(2, 10, 5)
        >>> covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, batch_size=2, ard_num_dims=5)
        >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)


    .. _Gaussian Process Kernels for Pattern Discovery and Extrapolation:
        https://arxiv.org/pdf/1302.4245.pdf
    """
    is_stationary = True

    def __init__(self, num_mixtures=None, ard_num_dims=1, batch_shape=torch.Size([]), mixture_scales_prior=None, mixture_scales_constraint=None, mixture_means_prior=None, mixture_means_constraint=None, mixture_weights_prior=None, mixture_weights_constraint=None, **kwargs):
        if num_mixtures is None:
            raise RuntimeError('num_mixtures is a required argument')
        if mixture_means_prior is not None or mixture_scales_prior is not None or mixture_weights_prior is not None:
            logger.warning('Priors not implemented for SpectralMixtureKernel')
        super(SpectralMixtureKernel, self).__init__(ard_num_dims=ard_num_dims, batch_shape=batch_shape, **kwargs)
        self.num_mixtures = num_mixtures
        if mixture_scales_constraint is None:
            mixture_scales_constraint = Positive()
        if mixture_means_constraint is None:
            mixture_means_constraint = Positive()
        if mixture_weights_constraint is None:
            mixture_weights_constraint = Positive()
        self.register_parameter(name='raw_mixture_weights', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, self.num_mixtures)))
        ms_shape = torch.Size([*self.batch_shape, self.num_mixtures, 1, self.ard_num_dims])
        self.register_parameter(name='raw_mixture_means', parameter=torch.nn.Parameter(torch.zeros(ms_shape)))
        self.register_parameter(name='raw_mixture_scales', parameter=torch.nn.Parameter(torch.zeros(ms_shape)))
        self.register_constraint('raw_mixture_scales', mixture_scales_constraint)
        self.register_constraint('raw_mixture_means', mixture_means_constraint)
        self.register_constraint('raw_mixture_weights', mixture_weights_constraint)

    @property
    def mixture_scales(self):
        return self.raw_mixture_scales_constraint.transform(self.raw_mixture_scales)

    @mixture_scales.setter
    def mixture_scales(self, value):
        self._set_mixture_scales(value)

    def _set_mixture_scales(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_mixture_scales=self.raw_mixture_scales_constraint.inverse_transform(value))

    @property
    def mixture_means(self):
        return self.raw_mixture_means_constraint.transform(self.raw_mixture_means)

    @mixture_means.setter
    def mixture_means(self, value):
        self._set_mixture_means(value)

    def _set_mixture_means(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_mixture_means=self.raw_mixture_means_constraint.inverse_transform(value))

    @property
    def mixture_weights(self):
        return self.raw_mixture_weights_constraint.transform(self.raw_mixture_weights)

    @mixture_weights.setter
    def mixture_weights(self, value):
        self._set_mixture_weights(value)

    def _set_mixture_weights(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value)
        self.initialize(raw_mixture_weights=self.raw_mixture_weights_constraint.inverse_transform(value))

    def initialize_from_data_empspect(self, train_x, train_y):
        """
        Initialize mixture components based on the empirical spectrum of the data.

        This will often be better than the standard initialize_from_data method.
        """
        import numpy as np
        from scipy.fftpack import fft
        from scipy.integrate import cumtrapz
        N = train_x.size(-2)
        emp_spect = np.abs(fft(train_y.cpu().detach().numpy())) ** 2 / N
        M = math.floor(N / 2)
        freq1 = np.arange(M + 1)
        freq2 = np.arange(-M + 1, 0)
        freq = np.hstack((freq1, freq2)) / N
        freq = freq[:M + 1]
        emp_spect = emp_spect[:M + 1]
        total_area = np.trapz(emp_spect, freq)
        spec_cdf = np.hstack((np.zeros(1), cumtrapz(emp_spect, freq)))
        spec_cdf = spec_cdf / total_area
        a = np.random.rand(1000, self.ard_num_dims)
        p, q = np.histogram(a, spec_cdf)
        bins = np.digitize(a, q)
        slopes = (spec_cdf[bins] - spec_cdf[bins - 1]) / (freq[bins] - freq[bins - 1])
        intercepts = spec_cdf[bins - 1] - slopes * freq[bins - 1]
        inv_spec = (a - intercepts) / slopes
        from sklearn.mixture import GaussianMixture
        GMM = GaussianMixture(n_components=self.num_mixtures, covariance_type='diag').fit(inv_spec)
        means = GMM.means_
        varz = GMM.covariances_
        weights = GMM.weights_
        self.mixture_means = means
        self.mixture_scales = varz
        self.mixture_weights = weights

    def initialize_from_data(self, train_x, train_y, **kwargs):
        if not torch.is_tensor(train_x) or not torch.is_tensor(train_y):
            raise RuntimeError('train_x and train_y should be tensors')
        if train_x.ndimension() == 1:
            train_x = train_x.unsqueeze(-1)
        if train_x.ndimension() == 2:
            train_x = train_x.unsqueeze(0)
        train_x_sort = train_x.sort(1)[0]
        max_dist = train_x_sort[:, (-1), :] - train_x_sort[:, (0), :]
        min_dist_sort = (train_x_sort[:, 1:, :] - train_x_sort[:, :-1, :]).squeeze(0)
        min_dist = torch.zeros(1, self.ard_num_dims, dtype=train_x.dtype, device=train_x.device)
        for ind in range(self.ard_num_dims):
            min_dist[:, (ind)] = min_dist_sort[min_dist_sort[:, (ind)].nonzero()[0], ind]
        self.raw_mixture_scales.data.normal_().mul_(max_dist).abs_().pow_(-1)
        self.raw_mixture_scales.data = self.raw_mixture_scales_constraint.inverse_transform(self.raw_mixture_scales.data)
        self.raw_mixture_means.data.uniform_().mul_(0.5).div_(min_dist)
        self.raw_mixture_means.data = self.raw_mixture_means_constraint.inverse_transform(self.raw_mixture_means.data)
        self.raw_mixture_weights.data.fill_(train_y.std() / self.num_mixtures)
        self.raw_mixture_weights.data = self.raw_mixture_weights_constraint.inverse_transform(self.raw_mixture_weights.data)

    def _create_input_grid(self, x1, x2, diag=False, last_dim_is_batch=False, **params):
        """
        This is a helper method for creating a grid of the kernel's inputs.
        Use this helper rather than maually creating a meshgrid.

        The grid dimensions depend on the kernel's evaluation mode.

        Args:
            :attr:`x1` (Tensor `n x d` or `b x n x d`)
            :attr:`x2` (Tensor `m x d` or `b x m x d`) - for diag mode, these must be the same inputs

        Returns:
            (:class:`Tensor`, :class:`Tensor) corresponding to the gridded `x1` and `x2`.
            The shape depends on the kernel's mode

            * `full_covar`: (`b x n x 1 x d` and `b x 1 x m x d`)
            * `full_covar` with `last_dim_is_batch=True`: (`b x k x n x 1 x 1` and `b x k x 1 x m x 1`)
            * `diag`: (`b x n x d` and `b x n x d`)
            * `diag` with `last_dim_is_batch=True`: (`b x k x n x 1` and `b x k x n x 1`)
        """
        x1_, x2_ = x1, x2
        if last_dim_is_batch:
            x1_ = x1_.transpose(-1, -2).unsqueeze(-1)
            if torch.equal(x1, x2):
                x2_ = x1_
            else:
                x2_ = x2_.transpose(-1, -2).unsqueeze(-1)
        if diag:
            return x1_, x2_
        else:
            return x1_.unsqueeze(-2), x2_.unsqueeze(-3)

    def forward(self, x1, x2, last_dim_is_batch=False, **params):
        batch_shape = x1.shape[:-2]
        n, num_dims = x1.shape[-2:]
        if not num_dims == self.ard_num_dims:
            raise RuntimeError('The SpectralMixtureKernel expected the input to have {} dimensionality (based on the ard_num_dims argument). Got {}.'.format(self.ard_num_dims, num_dims))
        if not batch_shape == self.batch_shape:
            raise RuntimeError('The SpectralMixtureKernel expected the input to have a batch_size of {} (based on the batch_size argument). Got {}.'.format(self.batch_shape, batch_shape))
        x1_ = x1.unsqueeze(len(batch_shape))
        x2_ = x2.unsqueeze(len(batch_shape))
        x1_exp = x1_ * self.mixture_scales
        x2_exp = x2_ * self.mixture_scales
        x1_cos = x1_ * self.mixture_means
        x2_cos = x2_ * self.mixture_means
        x1_exp_, x2_exp_ = self._create_input_grid(x1_exp, x2_exp, last_dim_is_batch=last_dim_is_batch, **params)
        x1_cos_, x2_cos_ = self._create_input_grid(x1_cos, x2_cos, last_dim_is_batch=last_dim_is_batch, **params)
        exp_term = (x1_exp_ - x2_exp_).pow_(2).mul_(-2 * math.pi ** 2)
        cos_term = (x1_cos_ - x2_cos_).mul_(2 * math.pi)
        res = exp_term.exp_() * cos_term.cos_()
        if last_dim_is_batch:
            res = res.squeeze(-1)
        else:
            res = res.prod(-1)
        mixture_weights = self.mixture_weights
        if last_dim_is_batch:
            mixture_weights = mixture_weights.unsqueeze(-1)
        while mixture_weights.dim() < res.dim():
            mixture_weights = mixture_weights.unsqueeze(-1)
        res = (res * mixture_weights).sum(len(batch_shape))
        return res


class _Likelihood(Module, ABC):

    def __init__(self, max_plate_nesting=1):
        super().__init__()
        self.max_plate_nesting = max_plate_nesting

    def _draw_likelihood_samples(self, function_dist, *args, sample_shape=None, **kwargs):
        if sample_shape is None:
            sample_shape = torch.Size([settings.num_likelihood_samples.value()] + [1] * (self.max_plate_nesting - len(function_dist.batch_shape) - 1))
        else:
            sample_shape = sample_shape[:-len(function_dist.batch_shape) - 1]
        if self.training:
            num_event_dims = len(function_dist.event_shape)
            function_dist = base_distributions.Normal(function_dist.mean, function_dist.variance.sqrt())
            function_dist = base_distributions.Independent(function_dist, num_event_dims - 1)
        function_samples = function_dist.rsample(sample_shape)
        return self.forward(function_samples, *args, **kwargs)

    def expected_log_prob(self, observations, function_dist, *args, **kwargs):
        likelihood_samples = self._draw_likelihood_samples(function_dist, *args, **kwargs)
        res = likelihood_samples.log_prob(observations).mean(dim=0)
        return res

    @abstractmethod
    def forward(self, function_samples, *args, **kwargs):
        raise NotImplementedError

    def get_fantasy_likelihood(self, **kwargs):
        return deepcopy(self)

    def log_marginal(self, observations, function_dist, *args, **kwargs):
        likelihood_samples = self._draw_likelihood_samples(function_dist, *args, **kwargs)
        log_probs = likelihood_samples.log_prob(observations)
        res = log_probs.sub(math.log(log_probs.size(0))).logsumexp(dim=0)
        return res

    def marginal(self, function_dist, *args, **kwargs):
        res = self._draw_likelihood_samples(function_dist, *args, **kwargs)
        return res

    def __call__(self, input, *args, **kwargs):
        if torch.is_tensor(input):
            return super().__call__(input, *args, **kwargs)
        elif isinstance(input, MultivariateNormal):
            return self.marginal(input, *args, **kwargs)
        else:
            raise RuntimeError('Likelihoods expects a MultivariateNormal input to make marginal predictions, or a torch.Tensor for conditional predictions. Got a {}'.format(input.__class__.__name__))


class Noise(Module):
    pass


class GP(Module):
    pass


class MarginalLogLikelihood(Module):
    """
    These are modules to compute (or approximate/bound) the marginal log likelihood
    (MLL) of the GP model when applied to data.  I.e., given a GP :math:`f \\sim
    \\mathcal{GP}(\\mu, K)`, and data :math:`\\mathbf X, \\mathbf y`, these modules
    compute/approximate

    .. math::

       \\begin{equation*}
          \\mathcal{L} = p_f(\\mathbf y \\! \\mid \\! \\mathbf X)
          = \\int p \\left( \\mathbf y \\! \\mid \\! f(\\mathbf X) \\right) \\: p(f(\\mathbf X) \\! \\mid \\! \\mathbf X) \\: d f
       \\end{equation*}

    This is computed exactly when the GP inference is computed exactly (e.g. regression w/ a Gaussian likelihood).
    It is approximated/bounded for GP models that use approximate inference.

    These models are typically used as the "loss" functions for GP models (though note that the output of
    these functions must be negated for optimization).
    """

    def __init__(self, likelihood, model):
        super(MarginalLogLikelihood, self).__init__()
        if not isinstance(model, GP):
            raise RuntimeError('All MarginalLogLikelihood objects must be given a GP object as a model. If you are using a more complicated model involving a GP, pass the underlying GP object as the model, not a full PyTorch module.')
        self.likelihood = likelihood
        self.model = model

    def forward(self, output, target, **kwargs):
        """
        Computes the MLL given :math:`p(\\mathbf f)` and `\\mathbf y`

        :param ~gpytorch.distributions.MultivariateNormal output: the outputs of the latent function
            (the :obj:`~gpytorch.models.GP`)
        :param torch.Tensor target: :math:`\\mathbf y` The target values
        :param dict kwargs: Additional arguments to pass to the likelihood's :attr:`forward` function.
        """
        raise NotImplementedError

    def pyro_factor(self, output, target):
        """
        As forward, but register the MLL with pyro using the pyro.factor primitive.
        """
        raise NotImplementedError


class ExactMarginalLogLikelihood(MarginalLogLikelihood):
    """
    The exact marginal log likelihood (MLL) for an exact Gaussian process with a
    Gaussian likelihood.

    .. note::
        This module will not work with anything other than a :obj:`~gpytorch.likelihoods.GaussianLikelihood`
        and a :obj:`~gpytorch.models.ExactGP`. It also cannot be used in conjunction with
        stochastic optimization.

    :param ~gpytorch.likelihoods.GaussianLikelihood likelihood: The Gaussian likelihood for the model
    :param ~gpytorch.models.ExactGP model: The exact GP model

    Example:
        >>> # model is a gpytorch.models.ExactGP
        >>> # likelihood is a gpytorch.likelihoods.Likelihood
        >>> mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
        >>>
        >>> output = model(train_x)
        >>> loss = -mll(output, train_y)
        >>> loss.backward()
    """

    def __init__(self, likelihood, model):
        if not isinstance(likelihood, _GaussianLikelihoodBase):
            raise RuntimeError('Likelihood must be Gaussian for exact inference')
        super(ExactMarginalLogLikelihood, self).__init__(likelihood, model)

    def forward(self, function_dist, target, *params):
        """
        Computes the MLL given :math:`p(\\mathbf f)` and :math:`\\mathbf y`.

        :param ~gpytorch.distributions.MultivariateNormal function_dist: :math:`p(\\mathbf f)`
            the outputs of the latent function (the :obj:`gpytorch.models.ExactGP`)
        :param torch.Tensor target: :math:`\\mathbf y` The target values
        :rtype: torch.Tensor
        :return: Exact MLL. Output shape corresponds to batch shape of the model/input data.
        """
        if not isinstance(function_dist, MultivariateNormal):
            raise RuntimeError('ExactMarginalLogLikelihood can only operate on Gaussian random variables')
        output = self.likelihood(function_dist, *params)
        res = output.log_prob(target)
        for added_loss_term in self.model.added_loss_terms():
            res = res.add(added_loss_term.loss(*params))
        for _, prior, closure, _ in self.named_priors():
            res.add_(prior.log_prob(closure()).sum())
        num_data = target.size(-1)
        return res.div_(num_data)

    def pyro_factor(self, output, target, *params):
        mll = self(output, target, *params)
        pyro.factor('gp_mll', mll)
        return mll


class SumMarginalLogLikelihood(MarginalLogLikelihood):
    """Sum of marginal log likelihoods, to be used with Multi-Output models.

    Args:
        likelihood: A MultiOutputLikelihood
        model: A MultiOutputModel
        mll_cls: The Marginal Log Likelihood class (default: ExactMarginalLogLikelihood)

    In case the model outputs are independent, this provives the MLL of the multi-output model.

    """

    def __init__(self, likelihood, model, mll_cls=ExactMarginalLogLikelihood):
        super().__init__(model.likelihood, model)
        self.mlls = ModuleList([mll_cls(mdl.likelihood, mdl) for mdl in model.models])

    def forward(self, outputs, targets, *params):
        """
        Args:
            outputs: (Iterable[MultivariateNormal]) - the outputs of the latent function
            targets: (Iterable[Tensor]) - the target values
            params: (Iterable[Iterable[Tensor]]) - the arguments to be passed through
                (e.g. parameters in case of heteroskedastic likelihoods)
        """
        if len(params) == 0:
            sum_mll = sum(mll(output, target) for mll, output, target in zip(self.mlls, outputs, targets))
        else:
            sum_mll = sum(mll(output, target, *iparams) for mll, output, target, iparams in zip(self.mlls, outputs, targets, params))
        return sum_mll.div_(len(self.mlls))


class DeprecationError(Exception):
    pass


def _extract_named_added_loss_terms(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_added_loss_terms'):
        for name, strategy in module._added_loss_terms.items():
            if strategy is not None and strategy not in memo:
                memo.add(strategy)
                yield prefix + ('.' if prefix else '') + name, strategy
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, strategy in _extract_named_added_loss_terms(module=module_, memo=memo, prefix=submodule_prefix):
            yield name, strategy


def _extract_named_constraints(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_constraints'):
        for name, constraint in module._constraints.items():
            if constraint is not None and constraint not in memo:
                memo.add(constraint)
                full_name = ('.' if prefix else '').join([prefix, name])
                yield full_name, constraint
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, constraint in _extract_named_constraints(module_, memo=memo, prefix=submodule_prefix):
            yield name, constraint


def _extract_named_priors(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_priors'):
        for name, (prior, closure, inv_closure) in module._priors.items():
            if prior is not None and prior not in memo:
                memo.add(prior)
                full_name = ('.' if prefix else '').join([prefix, name])
                yield full_name, prior, closure, inv_closure
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, prior, closure, inv_closure in _extract_named_priors(module_, memo=memo, prefix=submodule_prefix):
            yield name, prior, closure, inv_closure


def _pyro_load_from_samples(module, samples_dict, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_priors'):
        module.local_load_samples(samples_dict, memo, prefix)
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        _pyro_load_from_samples(module_, samples_dict, memo=memo, prefix=submodule_prefix)


def _set_strict(module, value, memo=None):
    if memo is None:
        memo = set()
    if hasattr(module, '_strict_init'):
        module._strict_init = value
    for mname, module_ in module.named_children():
        _set_strict(module_, value)


def _validate_module_outputs(outputs):
    if isinstance(outputs, tuple):
        if not all(torch.is_tensor(output) or isinstance(output, Distribution) or isinstance(output, LazyTensor) for output in outputs):
            raise RuntimeError('All outputs must be a Distribution, torch.Tensor, or LazyTensor. Got {}'.format([output.__class__.__name__ for output in outputs]))
        if len(outputs) == 1:
            outputs = outputs[0]
        return outputs
    elif torch.is_tensor(outputs) or isinstance(outputs, Distribution) or isinstance(outputs, LazyTensor):
        return outputs
    else:
        raise RuntimeError('Output must be a Distribution, torch.Tensor, or LazyTensor. Got {}'.format(outputs.__class__.__name__))


class Module(nn.Module):

    def __init__(self):
        super().__init__()
        self._added_loss_terms = OrderedDict()
        self._priors = OrderedDict()
        self._constraints = OrderedDict()
        self._strict_init = True
        self._load_strict_shapes = True
        self._register_load_state_dict_pre_hook(self._load_state_hook_ignore_shapes)

    def __call__(self, *inputs, **kwargs):
        outputs = self.forward(*inputs, **kwargs)
        if isinstance(outputs, list):
            return [_validate_module_outputs(output) for output in outputs]
        return _validate_module_outputs(outputs)

    def _get_module_and_name(self, parameter_name):
        """Get module and name from full parameter name."""
        module, name = parameter_name.split('.', 1)
        if module in self._modules:
            return self.__getattr__(module), name
        else:
            raise AttributeError('Invalid parameter name {}. {} has no module {}'.format(parameter_name, type(self).__name__, module))

    def _strict(self, value):
        _set_strict(self, value)

    def added_loss_terms(self):
        for _, strategy in self.named_added_loss_terms():
            yield strategy

    def forward(self, *inputs, **kwargs):
        raise NotImplementedError

    def constraints(self):
        for _, constraint in self.named_constraints():
            yield constraint

    def hyperparameters(self):
        for _, param in self.named_hyperparameters():
            yield param

    def initialize(self, **kwargs):
        """
        Set a value for a parameter

        kwargs: (param_name, value) - parameter to initialize.
        Can also initialize recursively by passing in the full name of a
        parameter. For example if model has attribute model.likelihood,
        we can initialize the noise with either
        `model.initialize(**{'likelihood.noise': 0.1})`
        or
        `model.likelihood.initialize(noise=0.1)`.
        The former method would allow users to more easily store the
        initialization values as one object.

        Value can take the form of a tensor, a float, or an int
        """
        for name, val in kwargs.items():
            if isinstance(val, int):
                val = float(val)
            if '.' in name:
                module, name = self._get_module_and_name(name)
                module.initialize(**{name: val})
            elif not hasattr(self, name):
                raise AttributeError('Unknown parameter {p} for {c}'.format(p=name, c=self.__class__.__name__))
            elif name not in self._parameters and name not in self._buffers:
                setattr(self, name, val)
            elif torch.is_tensor(val):
                constraint = self.constraint_for_parameter_name(name)
                if constraint is not None and constraint.enforced and not constraint.check_raw(val):
                    raise RuntimeError(f'Attempting to manually set a parameter value that is out of bounds of its current constraints, {constraint}. Most likely, you want to do the following:\n likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(better_lower_bound))')
                try:
                    self.__getattr__(name).data.copy_(val.expand_as(self.__getattr__(name)))
                except RuntimeError:
                    if not self._strict_init:
                        self.__getattr__(name).data = val
                    else:
                        self.__getattr__(name).data.copy_(val.view_as(self.__getattr__(name)))
            elif isinstance(val, float):
                constraint = self.constraint_for_parameter_name(name)
                if constraint is not None and not constraint.check_raw(val):
                    raise RuntimeError(f'Attempting to manually set a parameter value that is out of bounds of its current constraints, {constraint}. Most likely, you want to do the following:\n likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(better_lower_bound))')
                self.__getattr__(name).data.fill_(val)
            else:
                raise AttributeError('Type {t} not valid for initializing parameter {p}'.format(t=type(val), p=name))
            prior_name = '_'.join([name, 'prior'])
            if prior_name in self._priors:
                prior, closure, _ = self._priors[prior_name]
                try:
                    prior._validate_sample(closure())
                except ValueError as e:
                    raise ValueError('Invalid input value for prior {}. Error:\n{}'.format(prior_name, e))
        return self

    def named_added_loss_terms(self):
        """Returns an iterator over module variational strategies, yielding both
        the name of the variational strategy as well as the strategy itself.

        Yields:
            (string, VariationalStrategy): Tuple containing the name of the
                strategy and the strategy

        """
        return _extract_named_added_loss_terms(module=self, memo=None, prefix='')

    def named_hyperparameters(self):
        for name, param in self.named_parameters():
            if 'variational_' not in name:
                yield name, param

    def named_priors(self, memo=None, prefix=''):
        """Returns an iterator over the module's priors, yielding the name of the prior,
        the prior, the associated parameter names, and the transformation callable.

        Yields:
            (string, Prior, tuple((Parameter, callable)), callable): Tuple containing:
                - the name of the prior
                - the prior
                - a tuple of tuples (param, transform), one for each of the parameters associated with the prior
                - the prior's transform to be called on the parameters
        """
        return _extract_named_priors(module=self, memo=None, prefix='')

    def named_constraints(self, memo=None, prefix=''):
        return _extract_named_constraints(module=self, memo=None, prefix='')

    def named_variational_parameters(self):
        for name, param in self.named_parameters():
            if 'variational_' in name:
                yield name, param

    def register_added_loss_term(self, name):
        self._added_loss_terms[name] = None

    def register_parameter(self, name, parameter, prior=None):
        """
        Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.

        Args:
            :attr:`name` (str):
                The name of the parameter
            :attr:`parameter` (torch.nn.Parameter):
                The parameter
        """
        if prior is not None:
            raise DeprecationError("Setting a prior upon registering a parameter is deprecated. Please use .register_prior('{name}_prior', prior, '{name}') instead.".format(name=name))
        if '_parameters' not in self.__dict__:
            raise AttributeError('Cannot assign parameter before Module.__init__() call')
        super().register_parameter(name, parameter)

    def register_prior(self, name, prior, param_or_closure, setting_closure=None):
        """
        Adds a prior to the module. The prior can be accessed as an attribute using the given name.

        Args:
            :attr:`name` (str):
                The name of the prior
            :attr:`prior` (Prior):
                The prior to be registered`
            :attr:`param_or_closure` (string or callable):
                Either the name of the parameter, or a closure (which upon calling evalutes a function on
                one or more parameters):
                single parameter without a transform: `.register_prior("foo_prior", foo_prior, "foo_param")`
                transform a single parameter (e.g. put a log-Normal prior on it):
                `.register_prior("foo_prior", NormalPrior(0, 1), lambda: torch.log(self.foo_param))`
                function of multiple parameters:
                `.register_prior("foo2_prior", foo2_prior, lambda: f(self.param1, self.param2)))`
            :attr:`setting_closure` (callable, optional):
                A function taking in a tensor in (transformed) parameter space and initializing the
                internal parameter representation to the proper value by applying the inverse transform.
                Enables setting parametres directly in the transformed space, as well as sampling
                parameter values from priors (see `sample_from_prior`)

        """
        if isinstance(param_or_closure, str):
            if param_or_closure not in self._parameters and not hasattr(self, param_or_closure):
                raise AttributeError('Unknown parameter {name} for {module}'.format(name=param_or_closure, module=self.__class__.__name__) + ' Make sure the parameter is registered before registering a prior.')

            def closure():
                return getattr(self, param_or_closure)
            if setting_closure is not None:
                raise RuntimeError('Must specify a closure instead of a parameter name when providing setting_closure')

            def setting_closure(val):
                return self.initialize(**{param_or_closure: val})
        else:
            closure = param_or_closure
        self.add_module(name, prior)
        self._priors[name] = prior, closure, setting_closure

    def register_constraint(self, param_name, constraint, replace=True):
        if param_name not in self._parameters:
            raise RuntimeError('Attempting to register constraint for nonexistent parameter.')
        constraint_name = param_name + '_constraint'
        if constraint_name in self._constraints:
            current_constraint = self._constraints[constraint_name]
        else:
            current_constraint = None
        if isinstance(current_constraint, Interval) and not replace:
            new_constraint = constraint.intersect(current_constraint)
        else:
            new_constraint = constraint
        self.add_module(constraint_name, new_constraint)
        self._constraints[constraint_name] = new_constraint
        if new_constraint.initial_value is not None:
            self.initialize(**{param_name: new_constraint.initial_value})

    def constraint_for_parameter_name(self, param_name):
        base_module = self
        base_name = param_name
        while '.' in base_name:
            components = base_name.split('.')
            submodule_name = components[0]
            submodule = getattr(base_module, submodule_name)
            base_module = submodule
            base_name = '.'.join(components[1:])
        try:
            constraint_name = base_name + '_constraint'
            return base_module._constraints.get(constraint_name)
        except AttributeError:
            return None

    def _load_state_hook_ignore_shapes(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
        if not self._load_strict_shapes:
            local_name_params = itertools.chain(self._parameters.items(), self._buffers.items())
            local_state = {k: v for k, v in local_name_params if v is not None}
            for name, param in local_state.items():
                key = prefix + name
                if key in state_dict:
                    param.data = state_dict[key].data

    def load_strict_shapes(self, value):

        def apply_fn(module):
            module._load_strict_shapes = value
        self.apply(apply_fn)

    def named_parameters_and_constraints(self):
        for name, param in self.named_parameters():
            yield name, param, self.constraint_for_parameter_name(name)

    def sample_from_prior(self, prior_name):
        """Sample parameter values from prior. Modifies the module's parameters in-place."""
        if prior_name not in self._priors:
            raise RuntimeError("Unknown prior name '{}'".format(prior_name))
        prior, _, setting_closure = self._priors[prior_name]
        if setting_closure is None:
            raise RuntimeError('Must provide inverse transform to be able to sample from prior.')
        setting_closure(prior.sample())

    def pyro_sample_from_prior(self):
        """
        For each parameter in this Module and submodule that have defined priors, sample a value for that parameter
        from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.

        This method can be used in a Pyro model to conveniently define pyro sample sites for all
        parameters of the model that have GPyTorch priors registered to them.
        """
        return _pyro_sample_from_prior(module=self, memo=None, prefix='')

    def local_load_samples(self, samples_dict, memo, prefix):
        """
        Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro
        sampling mechanism.

        The default behavior here should almost always be called from any overriding class. However, a class may
        want to add additional functionality, such as reshaping things to account for the fact that parameters will
        acquire an extra batch dimension corresponding to the number of samples drawn.
        """
        self._strict(False)
        for name, (prior, closure, setting_closure) in self._priors.items():
            if prior is not None and prior not in memo:
                memo.add(prior)
                setting_closure(samples_dict[prefix + ('.' if prefix else '') + name])
        self._strict(True)

    def pyro_load_from_samples(self, samples_dict):
        """
        Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`
        is typically produced by a Pyro sampling mechanism.

        Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather
        than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with
        the prior to properly set the unconstrained parameter.

        Args:
            :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.
        """
        return _pyro_load_from_samples(module=self, samples_dict=samples_dict, memo=None, prefix='')

    def update_added_loss_term(self, name, added_loss_term):
        if not isinstance(added_loss_term, AddedLossTerm):
            raise RuntimeError('added_loss_term must be a AddedLossTerm')
        if name not in self._added_loss_terms.keys():
            raise RuntimeError('added_loss_term {} not registered'.format(name))
        self._added_loss_terms[name] = added_loss_term

    def variational_parameters(self):
        for _, param in self.named_variational_parameters():
            yield param

    def __getattr__(self, name):
        try:
            return super().__getattr__(name)
        except AttributeError as e:
            try:
                return super().__getattribute__(name)
            except AttributeError:
                raise e


def _bufferize_attributes(module, attributes):
    attr_clones = {attr: getattr(module, attr).clone() for attr in attributes}
    for attr, value in attr_clones.items():
        delattr(module, attr)
        module.register_buffer(attr, value)


class NormalPrior(Prior, Normal):
    """
    Normal (Gaussian) Prior

    pdf(x) = (2 * pi * sigma^2)^-0.5 * exp(-(x - mu)^2 / (2 * sigma^2))

    where mu is the mean and sigma^2 is the variance.
    """

    def __init__(self, loc, scale, validate_args=False, transform=None):
        TModule.__init__(self)
        Normal.__init__(self, loc=loc, scale=scale, validate_args=validate_args)
        _bufferize_attributes(self, ('loc', 'scale'))
        self._transform = transform

    def expand(self, batch_shape):
        batch_shape = torch.Size(batch_shape)
        return NormalPrior(self.loc.expand(batch_shape), self.scale.expand(batch_shape))


class SmoothedBoxPrior(Prior):
    """A smoothed approximation of a uniform prior.

    Has full support on the reals and is differentiable everywhere.

    .. math::

        \\begin{equation*}
            B = {x: a_i <= x_i <= b_i}
            d(x, B) = min_{x' in B} |x - x'|
            pdf(x) ~ exp(- d(x, B)**2 / sqrt(2 * sigma^2))
        \\end{equation*}

    """
    arg_constraints = {'sigma': constraints.positive, 'a': constraints.real, 'b': constraints.real}
    support = constraints.real
    _validate_args = True

    def __init__(self, a, b, sigma=0.01, validate_args=False, transform=None):
        TModule.__init__(self)
        _a = torch.tensor(float(a)) if isinstance(a, Number) else a
        _a = _a.view(-1) if _a.dim() < 1 else _a
        _a, _b, _sigma = broadcast_all(_a, b, sigma)
        if not torch.all(constraints.less_than(_b).check(_a)):
            raise ValueError('must have that a < b (element-wise)')
        batch_shape, event_shape = _a.shape[:-1], _a.shape[-1:]
        self.a, self.b, self.sigma = _a, _b, _sigma
        super(SmoothedBoxPrior, self).__init__(batch_shape, event_shape, validate_args=validate_args)
        del self.a, self.b, self.sigma
        self.register_buffer('a', _a)
        self.register_buffer('b', _b)
        self.register_buffer('sigma', _sigma)
        self.tails = NormalPrior(torch.zeros_like(_a), _sigma, validate_args=validate_args)
        self._transform = transform

    @property
    def _c(self):
        return (self.a + self.b) / 2

    @property
    def _r(self):
        return (self.b - self.a) / 2

    @property
    def _M(self):
        return torch.log(1 + (self.b - self.a) / (math.sqrt(2 * math.pi) * self.sigma))

    def log_prob(self, x):
        return self._log_prob(self.transform(x))

    def _log_prob(self, x):
        X = ((x - self._c).abs_() - self._r).clamp(min=0)
        return (self.tails.log_prob(X) - self._M).sum(-1)


class LogNormalPrior(Prior, LogNormal):
    """
    Log Normal prior.
    """

    def __init__(self, loc, scale, validate_args=None, transform=None):
        TModule.__init__(self)
        LogNormal.__init__(self, loc=loc, scale=scale, validate_args=validate_args)
        self._transform = transform

    def expand(self, batch_shape):
        batch_shape = torch.Size(batch_shape)
        return LogNormalPrior(self.loc.expand(batch_shape), self.scale.expand(batch_shape))


class UniformPrior(Prior, Uniform):
    """
    Uniform prior.
    """

    def __init__(self, a, b, validate_args=None, transform=None):
        TModule.__init__(self)
        Uniform.__init__(self, a, b, validate_args=validate_args)
        self._transform = transform

    def expand(self, batch_shape):
        batch_shape = torch.Size(batch_shape)
        return UniformPrior(self.low.expand(batch_shape), self.high.expand(batch_shape))


class GammaPrior(Prior, Gamma):
    """Gamma Prior parameterized by concentration and rate

    pdf(x) = beta^alpha / Gamma(alpha) * x^(alpha - 1) * exp(-beta * x)

    were alpha > 0 and beta > 0 are the concentration and rate parameters, respectively.
    """

    def __init__(self, concentration, rate, validate_args=False, transform=None):
        TModule.__init__(self)
        Gamma.__init__(self, concentration=concentration, rate=rate, validate_args=validate_args)
        _bufferize_attributes(self, ('concentration', 'rate'))
        self._transform = transform

    def expand(self, batch_shape):
        batch_shape = torch.Size(batch_shape)
        return GammaPrior(self.concentration.expand(batch_shape), self.rate.expand(batch_shape))

    def __call__(self, *args, **kwargs):
        return super(Gamma, self).__call__(*args, **kwargs)


MVN_LAZY_PROPERTIES = 'covariance_matrix', 'scale_tril', 'precision_matrix'


def _del_attributes(module, attributes, raise_on_error=False):
    for attr in attributes:
        try:
            delattr(module, attr)
        except AttributeError as e:
            if raise_on_error:
                raise e
    return module


class MultivariateNormalPrior(Prior, MultivariateNormal):
    """Multivariate Normal prior

    pdf(x) = det(2 * pi * Sigma)^-0.5 * exp(-0.5 * (x - mu)' Sigma^-1 (x - mu))

    where mu is the mean and Sigma > 0 is the covariance matrix.
    """

    def __init__(self, loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=False, transform=None):
        TModule.__init__(self)
        MultivariateNormal.__init__(self, loc=loc, covariance_matrix=covariance_matrix, precision_matrix=precision_matrix, scale_tril=scale_tril, validate_args=validate_args)
        _bufferize_attributes(self, ('loc', '_unbroadcasted_scale_tril'))
        self._transform = transform

    def cuda(self, device=None):
        """Applies module-level cuda() call and resets all lazy properties"""
        module = self._apply(lambda t: t)
        _del_attributes(module, MVN_LAZY_PROPERTIES)
        return module

    def cpu(self):
        """Applies module-level cpu() call and resets all lazy properties"""
        module = self._apply(lambda t: t.cpu())
        _del_attributes(module, MVN_LAZY_PROPERTIES)
        return module

    def expand(self, batch_shape):
        batch_shape = torch.Size(batch_shape)
        cov_shape = batch_shape + self.event_shape
        new_loc = self.loc.expand(batch_shape)
        new_scale_tril = self.scale_tril.expand(cov_shape)
        return MultivariateNormalPrior(loc=new_loc, scale_tril=new_scale_tril)


class WishartPrior(Prior):
    """Wishart prior over n x n positive definite matrices

    pdf(Sigma) ~ |Sigma|^(nu - n - 1)/2 * exp(-0.5 * Trace(K^-1 Sigma))

    where nu > n - 1 are the degrees of freedom and K > 0 is the p x p scale matrix

    Reference: A. Shah, A. G. Wilson, and Z. Ghahramani. Student-t Processes as
        Alternatives to Gaussian Processes. ArXiv e-prints, Feb. 2014.
    """
    arg_constraints = {'K_inv': constraints.positive_definite, 'nu': constraints.positive}
    support = constraints.positive_definite
    _validate_args = True

    def __init__(self, nu, K, validate_args=False):
        TModule.__init__(self)
        if K.dim() < 2:
            raise ValueError('K must be at least 2-dimensional')
        n = K.shape[-1]
        if K.shape[-2] != K.shape[-1]:
            raise ValueError('K must be square')
        if isinstance(nu, Number):
            nu = torch.tensor(float(nu))
        if torch.any(nu <= n):
            raise ValueError('Must have nu > n - 1')
        self.n = torch.tensor(n, dtype=torch.long, device=nu.device)
        batch_shape = nu.shape
        event_shape = torch.Size([n, n])
        logdetK = torch.logdet(K)
        C = -(nu / 2) * (logdetK + n * math.log(2)) - torch.mvlgamma(nu / 2, n)
        K_inv = torch.inverse(K)
        self.nu = nu
        self.K_inv = K_inv
        self.C = C
        super(WishartPrior, self).__init__(batch_shape, event_shape, validate_args=validate_args)
        del self.nu, self.K_inv, self.C
        self.register_buffer('nu', nu)
        self.register_buffer('K_inv', K_inv)
        self.register_buffer('C', C)

    def log_prob(self, X):
        logdetp = torch.logdet(X)
        Kinvp = torch.matmul(self.K_inv, X)
        trKinvp = torch.diagonal(Kinvp, dim1=-2, dim2=-1).sum(-1)
        return self.C + 0.5 * (self.nu - self.n - 1) * logdetp - trKinvp


class InverseWishartPrior(Prior):
    """Inverse Wishart prior over n x n positive definite matrices

    pdf(Sigma) ~ |Sigma|^-(nu + 2 * n)/2 * exp(-0.5 * Trace(K Sigma^-1))

    where nu > 0 are the degrees of freedom and K > 0 is the p x p scale matrix

    Reference: A. Shah, A. G. Wilson, and Z. Ghahramani. Student-t Processes as
        Alternatives to Gaussian Processes. ArXiv e-prints, Feb. 2014.
    """
    arg_constraints = {'K': constraints.positive_definite, 'nu': constraints.positive}
    support = constraints.positive_definite
    _validate_args = True

    def __init__(self, nu, K, validate_args=False):
        TModule.__init__(self)
        if K.dim() < 2:
            raise ValueError('K must be at least 2-dimensional')
        n = K.shape[-1]
        if isinstance(nu, Number):
            nu = torch.tensor(float(nu))
        if torch.any(nu <= 0):
            raise ValueError('Must have nu > 0')
        self.n = torch.tensor(n, dtype=torch.long, device=nu.device)
        batch_shape = nu.shape
        event_shape = torch.Size([n, n])
        c = (nu + n - 1) / 2
        logdetK = torch.logdet(K)
        C = c * (logdetK - n * math.log(2)) - torch.mvlgamma(c, n)
        self.nu = nu
        self.K = K
        self.C = C
        super(InverseWishartPrior, self).__init__(batch_shape, event_shape, validate_args=validate_args)
        del self.nu, self.K, self.C
        self.register_buffer('nu', nu)
        self.register_buffer('K', K)
        self.register_buffer('C', C)

    def log_prob(self, X):
        logdetp = torch.logdet(X)
        pinvK = torch.solve(self.K, X)[0]
        trpinvK = torch.diagonal(pinvK, dim1=-2, dim2=-1).sum(-1)
        return self.C - 0.5 * ((self.nu + 2 * self.n) * logdetp + trpinvK)


class _VariationalDistribution(Module, ABC):
    """
    Abstract base class for all Variational Distributions.
    """

    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=0.001):
        super().__init__()
        self.num_inducing_points = num_inducing_points
        self.batch_shape = batch_shape
        self.mean_init_std = mean_init_std

    def forward(self):
        """
        Constructs and returns the variational distribution

        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`
        :return: The distribution :math:q(\\mathbf u)"
        """
        raise NotImplementedError

    @abstractmethod
    def initialize_variational_distribution(self, prior_dist):
        """
        Method for initializing the variational distribution, based on the prior distribution.

        :param ~gpytorch.distribution.Distribution prior_dist: The prior distribution :math:`p(\\mathbf u)`.
        """
        raise NotImplementedError

    def __call__(self):
        try:
            return self.forward()
        except NotImplementedError:
            warnings.warn('_VariationalDistribution.variational_distribution is deprecated. Please implement a `forward` method instead.', DeprecationWarning)
            return self.variational_distribution

    def __getattr__(self, attr):
        if attr == 'variational_distribution':
            warnings.warn('_VariationalDistribution.variational_distribution is deprecated. To get q(u), call the _VariationalDistribution object instead.', DeprecationWarning)
            return self.forward()
        else:
            return super().__getattr__(attr)


class _VariationalStrategy(Module, ABC):
    """
    Abstract base class for all Variational Strategies.
    """

    def __init__(self, model, inducing_points, variational_distribution, learn_inducing_locations=True):
        super().__init__()
        object.__setattr__(self, 'model', model)
        inducing_points = inducing_points.clone()
        if inducing_points.dim() == 1:
            inducing_points = inducing_points.unsqueeze(-1)
        if learn_inducing_locations:
            self.register_parameter(name='inducing_points', parameter=torch.nn.Parameter(inducing_points))
        else:
            self.register_buffer('inducing_points', inducing_points)
        self._variational_distribution = variational_distribution
        self.register_buffer('variational_params_initialized', torch.tensor(0))

    @abstractproperty
    @cached(name='prior_distribution_memo')
    def prior_distribution(self):
        """
        The :func:`~gpytorch.variational.VariationalStrategy.prior_distribution` method determines how to compute the
        GP prior distribution of the inducing points, e.g. :math:`p(u) \\sim N(\\mu(X_u), K(X_u, X_u))`. Most commonly,
        this is done simply by calling the user defined GP prior on the inducing point data directly.

        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`
        :return: The distribution :math:`p( \\mathbf u)`
        """
        raise NotImplementedError

    @property
    @cached(name='variational_distribution_memo')
    def variational_distribution(self):
        return self._variational_distribution()

    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):
        """
        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the
        inducing point function values. Specifically, forward defines how to transform a variational distribution
        over the inducing point values, :math:`q(u)`, in to a variational distribution over the function values at
        specified locations x, :math:`q(f|x)`, by integrating :math:`\\int p(f|x, u)q(u)du`

        :param torch.Tensor x: Locations :math:`\\mathbf X` to get the
            variational posterior of the function values at.
        :param torch.Tensor inducing_points: Locations :math:`\\mathbf Z` of the inducing points
        :param torch.Tensor inducing_values: Samples of the inducing function values :math:`\\mathbf u`
            (or the mean of the distribution :math:`q(\\mathbf u)` if q is a Gaussian.
        :param ~gpytorch.lazy.LazyTensor variational_inducing_covar: If the distribuiton :math:`q(\\mathbf u)`
            is Gaussian, then this variable is the covariance matrix of that Gaussian. Otherwise, it will be
            :attr:`None`.

        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`
        :return: The distribution :math:`q( \\mathbf f(\\mathbf X))`
        """
        raise NotImplementedError

    def kl_divergence(self):
        """
        Compute the KL divergence between the variational inducing distribution :math:`q(\\mathbf u)`
        and the prior inducing distribution :math:`p(\\mathbf u)`.

        :rtype: torch.Tensor
        """
        with settings.max_preconditioner_size(0):
            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)
        return kl_divergence

    def train(self, mode=True):
        if self.training and not mode or mode:
            if hasattr(self, '_memoize_cache'):
                delattr(self, '_memoize_cache')
        return super().train(mode=mode)

    def __call__(self, x, prior=False):
        if prior:
            return self.model.forward(x)
        if self.training:
            if hasattr(self, '_memoize_cache'):
                delattr(self, '_memoize_cache')
                self._memoize_cache = dict()
        if not self.variational_params_initialized.item():
            prior_dist = self.prior_distribution
            self._variational_distribution.initialize_variational_distribution(prior_dist)
            self.variational_params_initialized.fill_(1)
        inducing_points = self.inducing_points
        if inducing_points.shape[:-2] != x.shape[:-2]:
            batch_shape = _mul_broadcast_shape(inducing_points.shape[:-2], x.shape[:-2])
            inducing_points = inducing_points.expand(*batch_shape, *inducing_points.shape[-2:])
            x = x.expand(*batch_shape, *x.shape[-2:])
        variational_dist_u = self.variational_distribution
        if isinstance(variational_dist_u, MultivariateNormal):
            return super().__call__(x, inducing_points, inducing_values=variational_dist_u.mean, variational_inducing_covar=variational_dist_u.lazy_covariance_matrix)
        elif isinstance(variational_dist_u, Delta):
            return super().__call__(x, inducing_points, inducing_values=variational_dist_u.mean, variational_inducing_covar=None)
        else:
            raise RuntimeError(f'Invalid variational distribuition ({type(variational_dist_u)}). Expected a multivariate normal or a delta distribution.')


class CholLazyTensor(RootLazyTensor):

    def __init__(self, chol):
        if settings.debug.on():
            delazy_chol = delazify(chol) if not isinstance(chol, BatchRepeatLazyTensor) else delazify(chol.base_lazy_tensor)
            mask = torch.ones(delazy_chol.shape[-2:], dtype=delazy_chol.dtype, device=delazy_chol.device).triu_(1)
            if torch.max(delazy_chol.mul(mask)).item() > 0.001 and torch.equal(delazy_chol, delazy_chol):
                raise RuntimeError('CholLazyVariable should take a lower-triangular matrix in the constructor.')
        super(CholLazyTensor, self).__init__(chol)

    @property
    def _chol(self):
        if not hasattr(self, '_chol_memo'):
            self._chol_memo = self.root.evaluate()
        return self._chol_memo

    @property
    def _chol_diag(self):
        if not hasattr(self, '_chol_diag_memo'):
            self._chol_diag_memo = self._chol.diagonal(dim1=-2, dim2=-1).clone()
        return self._chol_diag_memo

    def _cholesky(self):
        return self.root

    def _solve(self, rhs, preconditioner, num_tridiag=0):
        if num_tridiag:
            return super()._solve(rhs, preconditioner, num_tridiag=num_tridiag)
        else:
            return self.root._cholesky_solve(rhs)

    def inv_matmul(self, right_tensor, left_tensor=None):
        with settings.fast_computations(solves=False):
            return super().inv_matmul(right_tensor, left_tensor=left_tensor)

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):
        if not self.is_square:
            raise RuntimeError('inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'.format(self.__class__.__name__, self.size()))
        if inv_quad_rhs is not None:
            if self.dim() == 2 and inv_quad_rhs.dim() == 1:
                if self.shape[-1] != inv_quad_rhs.numel():
                    raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, inv_quad_rhs.shape))
            elif self.dim() != inv_quad_rhs.dim():
                raise RuntimeError('LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number of dimensions.'.format(self.shape, inv_quad_rhs.shape))
            elif self.shape[-1] != inv_quad_rhs.shape[-2]:
                raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, inv_quad_rhs.shape))
        inv_quad_term = None
        logdet_term = None
        if inv_quad_rhs is not None:
            inv_quad_term = self.inv_quad(inv_quad_rhs, reduce_inv_quad=reduce_inv_quad)
        if logdet:
            logdet_term = self._chol_diag.pow(2).log().sum(-1)
        return inv_quad_term, logdet_term


class CholeskyVariationalDistribution(_VariationalDistribution):
    """
    A :obj:`~gpytorch.variational._VariationalDistribution` that is defined to be a multivariate normal distribution
    with a full covariance matrix.

    The most common way this distribution is defined is to parameterize it in terms of a mean vector and a covariance
    matrix. In order to ensure that the covariance matrix remains positive definite, we only consider the lower
    triangle.

    :param int num_inducing_points: Size of the variational distribution. This implies that the variational mean
        should be this size, and the variational covariance matrix should have this many rows and columns.
    :param torch.Size batch_shape: (Optional.) Specifies an optional batch size
        for the variational parameters. This is useful for example when doing additive variational inference.
    :param float mean_init_std: (default=1e-3) Standard deviation of gaussian noise to add to the mean initialization.
    """

    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=0.001, **kwargs):
        super().__init__(num_inducing_points=num_inducing_points, batch_shape=batch_shape, mean_init_std=mean_init_std)
        mean_init = torch.zeros(num_inducing_points)
        covar_init = torch.eye(num_inducing_points, num_inducing_points)
        mean_init = mean_init.repeat(*batch_shape, 1)
        covar_init = covar_init.repeat(*batch_shape, 1, 1)
        self.register_parameter(name='variational_mean', parameter=torch.nn.Parameter(mean_init))
        self.register_parameter(name='chol_variational_covar', parameter=torch.nn.Parameter(covar_init))

    def forward(self):
        chol_variational_covar = self.chol_variational_covar
        dtype = chol_variational_covar.dtype
        device = chol_variational_covar.device
        lower_mask = torch.ones(self.chol_variational_covar.shape[-2:], dtype=dtype, device=device).tril(0)
        chol_variational_covar = chol_variational_covar.mul(lower_mask)
        variational_covar = CholLazyTensor(chol_variational_covar)
        return MultivariateNormal(self.variational_mean, variational_covar)

    def initialize_variational_distribution(self, prior_dist):
        self.variational_mean.data.copy_(prior_dist.mean)
        self.variational_mean.data.add_(torch.randn_like(prior_dist.mean), alpha=self.mean_init_std)
        self.chol_variational_covar.data.copy_(prior_dist.lazy_covariance_matrix.cholesky().evaluate())


class DeltaVariationalDistribution(_VariationalDistribution):
    """
    This :obj:`~gpytorch.variational._VariationalDistribution` object replaces a variational distribution
    with a single particle. It is equivalent to doing MAP inference.

    :param int num_inducing_points: Size of the variational distribution. This implies that the variational mean
        should be this size.
    :param torch.Size batch_shape: (Optional.) Specifies an optional batch size
        for the variational parameters. This is useful for example when doing additive variational inference.
    :param float mean_init_std: (default=1e-3) Standard deviation of gaussian noise to add to the mean initialization.
    """

    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=0.001, **kwargs):
        super().__init__(num_inducing_points=num_inducing_points, batch_shape=batch_shape, mean_init_std=mean_init_std)
        mean_init = torch.zeros(num_inducing_points)
        mean_init = mean_init.repeat(*batch_shape, 1)
        self.register_parameter(name='variational_mean', parameter=torch.nn.Parameter(mean_init))

    def forward(self):
        return Delta(self.variational_mean)

    def initialize_variational_distribution(self, prior_dist):
        self.variational_mean.data.copy_(prior_dist.mean)
        self.variational_mean.data.add_(torch.randn_like(prior_dist.mean), alpha=self.mean_init_std)


def convert_legacy_grid(grid: torch.Tensor) ->List[torch.Tensor]:
    return [grid[:, (i)] for i in range(grid.size(-1))]


class Interpolation(object):

    def _cubic_interpolation_kernel(self, scaled_grid_dist):
        """
        Computes the interpolation kernel u() for points X given the scaled
        grid distances:
                                    (X-x_{t})/s
        where s is the distance between neighboring grid points. Note that,
        in this context, the word "kernel" is not used to mean a covariance
        function as in the rest of the package. For more details, see the
        original paper Keys et al., 1989, equation (4).

        scaled_grid_dist should be an n-by-g matrix of distances, where the
        (ij)th element is the distance between the ith data point in X and the
        jth element in the grid.

        Note that, although this method ultimately expects a scaled distance matrix,
        it is only intended to be used on single dimensional data.
        """
        U = scaled_grid_dist.abs()
        res = torch.zeros(U.size(), dtype=U.dtype, device=U.device)
        U_lt_1 = 1 - U.floor().clamp(0, 1)
        res = res + ((1.5 * U - 2.5).mul(U).mul(U) + 1) * U_lt_1
        U_ge_1_le_2 = 1 - U_lt_1
        res = res + (((-0.5 * U + 2.5).mul(U) - 4).mul(U) + 2) * U_ge_1_le_2
        return res

    def interpolate(self, x_grid: List[torch.Tensor], x_target: torch.Tensor, interp_points=range(-2, 2), eps=1e-10):
        if torch.is_tensor(x_grid):
            x_grid = convert_legacy_grid(x_grid)
        num_target_points = x_target.size(0)
        num_dim = x_target.size(-1)
        assert num_dim == len(x_grid)
        grid_sizes = [len(x_grid[i]) for i in range(num_dim)]
        x_target_max = x_target.max(0)[0]
        x_target_min = x_target.min(0)[0]
        grid_mins = torch.stack([x_grid[i].min() for i in range(num_dim)], dim=0)
        grid_maxs = torch.stack([x_grid[i].max() for i in range(num_dim)], dim=0)
        lt_min_mask = (x_target_min - grid_mins).lt(-1e-07)
        gt_max_mask = (x_target_max - grid_maxs).gt(1e-07)
        if lt_min_mask.sum().item():
            first_out_of_range = lt_min_mask.nonzero().squeeze(1)[0].item()
            raise RuntimeError('Received data that was out of bounds for the specified grid. Grid bounds were ({0:.3f}, {0:.3f}), but min = {0:.3f}, max = {0:.3f}'.format(grid_mins[first_out_of_range].item(), grid_maxs[first_out_of_range].item(), x_target_min[first_out_of_range].item(), x_target_max[first_out_of_range].item()))
        if gt_max_mask.sum().item():
            first_out_of_range = gt_max_mask.nonzero().squeeze(1)[0].item()
            raise RuntimeError('Received data that was out of bounds for the specified grid. Grid bounds were ({0:.3f}, {0:.3f}), but min = {0:.3f}, max = {0:.3f}'.format(grid_mins[first_out_of_range].item(), grid_maxs[first_out_of_range].item(), x_target_min[first_out_of_range].item(), x_target_max[first_out_of_range].item()))
        interp_points = torch.tensor(interp_points, dtype=x_grid[0].dtype, device=x_grid[0].device)
        interp_points_flip = interp_points.flip(0)
        num_coefficients = len(interp_points)
        interp_values = torch.ones(num_target_points, num_coefficients ** num_dim, dtype=x_grid[0].dtype, device=x_grid[0].device)
        interp_indices = torch.zeros(num_target_points, num_coefficients ** num_dim, dtype=torch.long, device=x_grid[0].device)
        for i in range(num_dim):
            num_grid_points = x_grid[i].size(0)
            grid_delta = (x_grid[i][1] - x_grid[i][0]).clamp_min_(eps)
            lower_grid_pt_idxs = torch.floor((x_target[:, (i)] - x_grid[i][0]) / grid_delta)
            lower_pt_rel_dists = (x_target[:, (i)] - x_grid[i][0]) / grid_delta - lower_grid_pt_idxs
            lower_grid_pt_idxs = lower_grid_pt_idxs - interp_points.max()
            lower_grid_pt_idxs.detach_()
            if len(lower_grid_pt_idxs.shape) == 0:
                lower_grid_pt_idxs = lower_grid_pt_idxs.unsqueeze(0)
            scaled_dist = lower_pt_rel_dists.unsqueeze(-1) + interp_points_flip.unsqueeze(-2)
            dim_interp_values = self._cubic_interpolation_kernel(scaled_dist)
            left_boundary_pts = (lower_grid_pt_idxs < 0).nonzero()
            num_left = len(left_boundary_pts)
            if num_left > 0:
                left_boundary_pts.squeeze_(1)
                x_grid_first = x_grid[i][:num_coefficients].unsqueeze(1).t().expand(num_left, num_coefficients)
                grid_targets = x_target.select(1, i)[left_boundary_pts].unsqueeze(1).expand(num_left, num_coefficients)
                dists = torch.abs(x_grid_first - grid_targets)
                closest_from_first = torch.min(dists, 1)[1]
                for j in range(num_left):
                    dim_interp_values[(left_boundary_pts[j]), :] = 0
                    dim_interp_values[left_boundary_pts[j], closest_from_first[j]] = 1
                    lower_grid_pt_idxs[left_boundary_pts[j]] = 0
            right_boundary_pts = (lower_grid_pt_idxs > num_grid_points - num_coefficients).nonzero()
            num_right = len(right_boundary_pts)
            if num_right > 0:
                right_boundary_pts.squeeze_(1)
                x_grid_last = x_grid[i][-num_coefficients:].unsqueeze(1).t().expand(num_right, num_coefficients)
                grid_targets = x_target.select(1, i)[right_boundary_pts].unsqueeze(1)
                grid_targets = grid_targets.expand(num_right, num_coefficients)
                dists = torch.abs(x_grid_last - grid_targets)
                closest_from_last = torch.min(dists, 1)[1]
                for j in range(num_right):
                    dim_interp_values[(right_boundary_pts[j]), :] = 0
                    dim_interp_values[right_boundary_pts[j], closest_from_last[j]] = 1
                    lower_grid_pt_idxs[right_boundary_pts[j]] = num_grid_points - num_coefficients
            offset = (interp_points - interp_points.min()).long().unsqueeze(-2)
            dim_interp_indices = lower_grid_pt_idxs.long().unsqueeze(-1) + offset
            n_inner_repeat = num_coefficients ** i
            n_outer_repeat = num_coefficients ** (num_dim - i - 1)
            index_coeff = reduce(mul, grid_sizes[i + 1:], 1)
            dim_interp_indices = dim_interp_indices.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)
            dim_interp_values = dim_interp_values.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)
            interp_indices = interp_indices.add(dim_interp_indices.view(num_target_points, -1).mul(index_coeff))
            interp_values = interp_values.mul(dim_interp_values.view(num_target_points, -1))
        return interp_indices, interp_values


class GridInterpolationVariationalStrategy(_VariationalStrategy):
    """
    This strategy constrains the inducing points to a grid and applies a deterministic
    relationship between :math:`\\mathbf f` and :math:`\\mathbf u`.
    It was introduced by `Wilson et al. (2016)`_.

    Here, the inducing points are not learned. Instead, the strategy
    automatically creates inducing points based on a set of grid sizes and grid
    bounds.

    .. _Wilson et al. (2016):
        https://arxiv.org/abs/1611.00336

    :param ~gpytorch.models.ApproximateGP model: Model this strategy is applied to.
        Typically passed in when the VariationalStrategy is created in the
        __init__ method of the user defined model.
    :param int grid_size: Size of the grid
    :param list grid_bounds: Bounds of each dimension of the grid (should be a list of (float, float) tuples)
    :param ~gpytorch.variational.VariationalDistribution variational_distribution: A
        VariationalDistribution object that represents the form of the variational distribution :math:`q(\\mathbf u)`
    """

    def __init__(self, model, grid_size, grid_bounds, variational_distribution):
        grid = torch.zeros(grid_size, len(grid_bounds))
        for i in range(len(grid_bounds)):
            grid_diff = float(grid_bounds[i][1] - grid_bounds[i][0]) / (grid_size - 2)
            grid[:, (i)] = torch.linspace(grid_bounds[i][0] - grid_diff, grid_bounds[i][1] + grid_diff, grid_size)
        inducing_points = torch.zeros(int(pow(grid_size, len(grid_bounds))), len(grid_bounds))
        prev_points = None
        for i in range(len(grid_bounds)):
            for j in range(grid_size):
                inducing_points[j * grid_size ** i:(j + 1) * grid_size ** i, (i)].fill_(grid[j, i])
                if prev_points is not None:
                    inducing_points[j * grid_size ** i:(j + 1) * grid_size ** i, :i].copy_(prev_points)
            prev_points = inducing_points[:grid_size ** (i + 1), :i + 1]
        super(GridInterpolationVariationalStrategy, self).__init__(model, inducing_points, variational_distribution, learn_inducing_locations=False)
        object.__setattr__(self, 'model', model)
        self.register_buffer('grid', grid)

    def _compute_grid(self, inputs):
        n_data, n_dimensions = inputs.size(-2), inputs.size(-1)
        batch_shape = inputs.shape[:-2]
        inputs = inputs.reshape(-1, n_dimensions)
        interp_indices, interp_values = Interpolation().interpolate(self.grid, inputs)
        interp_indices = interp_indices.view(*batch_shape, n_data, -1)
        interp_values = interp_values.view(*batch_shape, n_data, -1)
        if interp_indices.dim() - 2 != len(self._variational_distribution.batch_shape):
            batch_shape = _mul_broadcast_shape(interp_indices.shape[:-2], self._variational_distribution.batch_shape)
            interp_indices = interp_indices.expand(*batch_shape, *interp_indices.shape[-2:])
            interp_values = interp_values.expand(*batch_shape, *interp_values.shape[-2:])
        return interp_indices, interp_values

    @property
    @cached(name='prior_distribution_memo')
    def prior_distribution(self):
        out = self.model.forward(self.inducing_points)
        res = MultivariateNormal(out.mean, out.lazy_covariance_matrix.add_jitter())
        return res

    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):
        if variational_inducing_covar is None:
            raise RuntimeError(f'GridInterpolationVariationalStrategy is only compatible with Gaussian variational distributions. Got ({self.variational_distribution.__class__.__name__}.')
        variational_distribution = self.variational_distribution
        interp_indices, interp_values = self._compute_grid(x)
        predictive_mean = left_interp(interp_indices, interp_values, inducing_values.unsqueeze(-1))
        predictive_mean = predictive_mean.squeeze(-1)
        predictive_covar = InterpolatedLazyTensor(variational_distribution.lazy_covariance_matrix, interp_indices, interp_values, interp_indices, interp_values)
        output = MultivariateNormal(predictive_mean, predictive_covar)
        return output


class MeanFieldVariationalDistribution(_VariationalDistribution):
    """
    A :obj:`~gpytorch.variational._VariationalDistribution` that is defined to be a multivariate normal distribution
    with a diagonal covariance matrix. This will not be as flexible/expressive as a
    :obj:`~gpytorch.variational.CholeskyVariationalDistribution`.

    :param int num_inducing_points: Size of the variational distribution. This implies that the variational mean
        should be this size, and the variational covariance matrix should have this many rows and columns.
    :param torch.Size batch_shape: (Optional.) Specifies an optional batch size
        for the variational parameters. This is useful for example when doing additive variational inference.
    :param float mean_init_std: (default=1e-3) Standard deviation of gaussian noise to add to the mean initialization.
    """

    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=0.001, **kwargs):
        super().__init__(num_inducing_points=num_inducing_points, batch_shape=batch_shape, mean_init_std=mean_init_std)
        mean_init = torch.zeros(num_inducing_points)
        covar_init = torch.ones(num_inducing_points)
        mean_init = mean_init.repeat(*batch_shape, 1)
        covar_init = covar_init.repeat(*batch_shape, 1)
        self.register_parameter(name='variational_mean', parameter=torch.nn.Parameter(mean_init))
        self.register_parameter(name='_variational_stddev', parameter=torch.nn.Parameter(covar_init))

    @property
    def variational_stddev(self):
        mask = torch.ones_like(self._variational_stddev)
        return self._variational_stddev.mul(mask).abs().clamp_min(1e-08)

    def forward(self):
        mask = torch.ones_like(self._variational_stddev)
        variational_covar = DiagLazyTensor(self._variational_stddev.mul(mask).pow(2))
        return MultivariateNormal(self.variational_mean, variational_covar)

    def initialize_variational_distribution(self, prior_dist):
        self.variational_mean.data.copy_(prior_dist.mean)
        self.variational_mean.data.add_(torch.randn_like(prior_dist.mean), alpha=self.mean_init_std)
        self._variational_stddev.data.copy_(prior_dist.stddev)


class OrthogonallyDecoupledVariationalStrategy(_VariationalStrategy):
    """
    Implements orthogonally decoupled VGPs as defined in `Salimbeni et al. (2018)`_.
    This variational strategy uses a different set of inducing points for the mean and covariance functions.
    The idea is to use more inducing points for the (computationally efficient) mean and fewer inducing points for the
    (computationally expensive) covaraince.

    This variational strategy defines the inducing points/:obj:`~gpytorch.variational._VariationalDistribution`
    for the mean function.
    It then wraps a different :obj:`~gpytorch.variational._VariationalStrategy` which
    defines the covariance inducing points.

    Example:
        >>> mean_inducing_points = torch.randn(1000, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)
        >>> covar_inducing_points = torch.randn(100, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)
        >>>
        >>> covar_variational_strategy = gpytorch.variational.VariationalStrategy(
        >>>     model, covar_inducing_points,
        >>>     gpytorch.variational.CholeskyVariationalDistribution(covar_inducing_points.size(-2)),
        >>>     learn_inducing_locations=True
        >>> )
        >>>
        >>> variational_strategy = gpytorch.variational.OrthogonallyDecoupledVariationalStrategy(
        >>>     covar_variational_strategy, mean_inducing_points,
        >>>     gpytorch.variational.DeltaVariationalDistribution(mean_inducing_points.size(-2)),
        >>> )

    .. _Salimbeni et al. (2018):
        https://arxiv.org/abs/1809.08820
    """

    def __init__(self, model, inducing_points, variational_distribution):
        if not isinstance(variational_distribution, DeltaVariationalDistribution):
            raise NotImplementedError('OrthogonallyDecoupledVariationalStrategy currently works with DeltaVariationalDistribution')
        super().__init__(model, inducing_points, variational_distribution, learn_inducing_locations=True)
        self.base_variational_strategy = model

    @property
    @cached(name='prior_distribution_memo')
    def prior_distribution(self):
        out = self.model(self.inducing_points)
        res = MultivariateNormal(out.mean, out.lazy_covariance_matrix.add_jitter())
        return res

    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):
        if variational_inducing_covar is not None:
            raise NotImplementedError('OrthogonallyDecoupledVariationalStrategy currently works with DeltaVariationalDistribution')
        num_data = x.size(-2)
        full_output = self.model(torch.cat([x, inducing_points], dim=-2))
        full_mean = full_output.mean
        full_covar = full_output.lazy_covariance_matrix
        if self.training:
            induc_mean = full_mean[(...), num_data:]
            induc_induc_covar = full_covar[(...), num_data:, num_data:]
            self._memoize_cache['prior_distribution_memo'] = MultivariateNormal(induc_mean, induc_induc_covar)
        test_mean = full_mean[(...), :num_data]
        data_induc_covar = full_covar[(...), :num_data, num_data:]
        predictive_mean = (data_induc_covar @ inducing_values.unsqueeze(-1)).squeeze(-1).add(test_mean)
        predictive_covar = full_covar[(...), :num_data, :num_data]
        return MultivariateNormal(predictive_mean, predictive_covar)

    def kl_divergence(self):
        mean = self.variational_distribution.mean
        induc_induc_covar = self.prior_distribution.lazy_covariance_matrix
        kl = self.model.kl_divergence() + ((induc_induc_covar @ mean.unsqueeze(-1)).squeeze(-1) * mean).sum(-1).mul(0.5)
        return kl


class ExtraComputationWarning(UserWarning):
    """
    Warning thrown when a GP model does extra computation that it is not designed to do.
    This is mostly designed for :obj:`~gpytorch.variational.UnwhitenedVariationalStrategy`, which
    should cache most of its solves up front.
    """
    pass


class CachedCGLazyTensor(LazyTensor):
    """
    A LazyTensor wrapper that eagerly computes many CG calls in batch.
    This maximizes CG parallelism for fast inference.
    Used primarily for variational inference with GPs.

    Args:
        :attr:`base_lazy_tensor` (:class:`gpytorch.lazy.LazyTensor`):
            the LazyTensor to wrap
        :attr:`eager_rhss` (list of :class:`gpytorch.lazy.LazyTensor`):
            list of right-hand sides with eagerly-computed solves
        :attr:`solves` (list of :class:`gpytorch.lazy.LazyTensor`):
            list of solves associated with :attr:`eager_rhss`
        :attr:`probe_vectors` (:class:`gpytorch.lazy.LazyTensor`, optional):
            normalized probe vectors (for computing logdet with SLQ)
        :attr:`probe_vector_norms` (:class:`gpytorch.lazy.LazyTensor`, optional):
            norms associated with :attr:`probe_vectors` that will return :attr:`probe_vectors`
            to having identity covariance (for computing logdet with SLQ)
        :attr:`probe_vector_solves` (:class:`gpytorch.lazy.LazyTensor`, optional):
            solves associated with :attr:`probe_vectors` (for computing logdet with SLQ)
        :attr:`probe_vector_tmats` (:class:`gpytorch.lazy.LazyTensor`, optional):
            Lanczos tridiagonal matrices associated with :attr:`probe_vectors`
            (for computing logdet with SLQ)
    """

    @classmethod
    def precompute_terms(cls, base_lazy_tensor, eager_rhs, logdet_terms=True, include_tmats=True):
        """
        Computes the solves, probe vectors, probe_vector norms, probe vector solves, and probe vector
        tridiagonal matrices to construct a CachedCGLazyTensor

        Set logdet_terms to False if you are not going to compute the logdet of the LazyTensor
        """
        with torch.no_grad():
            if logdet_terms:
                num_random_probes = settings.num_trace_samples.value()
                probe_vectors = torch.empty(base_lazy_tensor.matrix_shape[-1], num_random_probes, dtype=base_lazy_tensor.dtype, device=base_lazy_tensor.device)
                probe_vectors.bernoulli_().mul_(2).add_(-1)
                probe_vectors = probe_vectors.expand(*base_lazy_tensor.batch_shape, base_lazy_tensor.matrix_shape[-1], num_random_probes)
                probe_vector_norms = torch.norm(probe_vectors, 2, dim=-2, keepdim=True)
                probe_vectors = probe_vectors.div(probe_vector_norms)
                if include_tmats:
                    all_solves, probe_vector_tmats = base_lazy_tensor._solve(torch.cat([probe_vectors, eager_rhs], -1), preconditioner=base_lazy_tensor._preconditioner()[0], num_tridiag=probe_vectors.size(-1))
                else:
                    all_solves = base_lazy_tensor._solve(torch.cat([probe_vectors, eager_rhs], -1), preconditioner=base_lazy_tensor._preconditioner()[0])
                    probe_vector_tmats = torch.tensor([])
                probe_vector_solves = all_solves[(...), :probe_vectors.size(-1)].detach()
                solves = all_solves[(...), probe_vectors.size(-1):]
                return solves.detach(), probe_vectors.detach(), probe_vector_norms.detach(), probe_vector_solves.detach(), probe_vector_tmats.detach()
            else:
                if settings.fast_computations.log_prob.on():
                    solves = base_lazy_tensor._solve(eager_rhs, preconditioner=base_lazy_tensor._preconditioner()[0])
                else:
                    solves = base_lazy_tensor._cholesky()._cholesky_solve(eager_rhs)
                dtype = solves.dtype
                device = solves.device
                return solves.detach(), torch.tensor([], dtype=dtype, device=device), torch.tensor([], dtype=dtype, device=device), torch.tensor([], dtype=dtype, device=device), torch.tensor([], dtype=dtype, device=device)

    def __init__(self, base_lazy_tensor, eager_rhss=[], solves=[], probe_vectors=torch.tensor([]), probe_vector_norms=torch.tensor([]), probe_vector_solves=torch.tensor([]), probe_vector_tmats=torch.tensor([])):
        super(CachedCGLazyTensor, self).__init__(base_lazy_tensor, eager_rhss=eager_rhss, solves=solves, probe_vectors=probe_vectors, probe_vector_norms=probe_vector_norms, probe_vector_solves=probe_vector_solves, probe_vector_tmats=probe_vector_tmats)
        self.base_lazy_tensor = base_lazy_tensor
        self.eager_rhss = [eager_rhs.detach() for eager_rhs in eager_rhss]
        self.solves = [solve.detach() for solve in solves]
        self.probe_vectors = probe_vectors.detach()
        self.probe_vector_norms = probe_vector_norms.detach()
        self.probe_vector_solves = probe_vector_solves.detach()
        self.probe_vector_tmats = probe_vector_tmats.detach()

    @property
    def requires_grad(self):
        return self.base_lazy_tensor.requires_grad

    @requires_grad.setter
    def requires_grad(self, val):
        self.base_lazy_tensor.requires_grad = val

    def _cholesky(self):
        res = self.__class__(self.base_lazy_tensor._cholesky(), eager_rhss=self.eager_rhss, solves=self.solves, probe_vectors=self.probe_vectors, probe_vector_norms=self.probe_vector_norms, probe_vector_solves=self.probe_vector_solves, probe_vector_tmats=self.probe_vector_tmats)
        return res

    def _cholesky_solve(self, rhs):
        for eager_rhs, solve in zip(self.eager_rhss, self.solves):
            if torch.equal(rhs, eager_rhs):
                return solve
        if settings.debug.on():
            warnings.warn('CachedCGLazyTensor had to run CG on a tensor of size {}. For best performance, this LazyTensor should pre-register all vectors to run CG against.'.format(rhs.shape), ExtraComputationWarning)
        return super(CachedCGLazyTensor, self)._cholesky_solve(rhs)

    def _expand_batch(self, batch_shape):
        return self.base_lazy_tensor._expand_batch(batch_shape)

    def _get_indices(self, row_index, col_index, *batch_indices):
        return self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices)

    def _getitem(self, row_index, col_index, *batch_indices):
        return self.base_lazy_tensor._getitem(row_index, col_index, *batch_indices)

    def _matmul(self, tensor):
        return self.base_lazy_tensor._matmul(tensor)

    def _probe_vectors_and_norms(self):
        return self.probe_vectors, self.probe_vector_norms

    def _quad_form_derivative(self, left_vecs, right_vecs):
        return self.base_lazy_tensor._quad_form_derivative(left_vecs, right_vecs)

    def _solve(self, rhs, preconditioner, num_tridiag=0):
        if num_tridiag:
            probe_vectors = rhs[(...), :num_tridiag].detach()
            if torch.equal(probe_vectors, self.probe_vectors):
                probe_vector_solves = self.probe_vector_solves
                tmats = self.probe_vector_tmats
            else:
                if settings.debug.on():
                    warnings.warn('CachedCGLazyTensor did not recognize the supplied probe vectors for tridiagonalization.', ExtraComputationWarning)
                return super(CachedCGLazyTensor, self)._solve(rhs, preconditioner, num_tridiag=num_tridiag)
        truncated_rhs = rhs[(...), num_tridiag or 0:]
        for eager_rhs, solve in zip(self.eager_rhss, self.solves):
            if torch.equal(truncated_rhs, eager_rhs):
                if num_tridiag:
                    return torch.cat([probe_vector_solves, solve], -1), tmats
                else:
                    return solve
        if settings.debug.on():
            warnings.warn('CachedCGLazyTensor had to run CG on a tensor of size {}. For best performance, this LazyTensor should pre-register all vectors to run CG against.'.format(rhs.shape), ExtraComputationWarning)
        return super(CachedCGLazyTensor, self)._solve(rhs, preconditioner, num_tridiag=num_tridiag)

    def _size(self):
        return self.base_lazy_tensor._size()

    def _t_matmul(self, tensor):
        return self.base_lazy_tensor._t_matmul(tensor)

    def _transpose_nonbatch(self):
        return self.base_lazy_tensor._transpose_nonbatch()

    def detach_(self):
        self.base_lazy_tensor.detach_()
        return self

    def inv_matmul(self, right_tensor, left_tensor=None):
        if not isinstance(self.base_lazy_tensor, CholLazyTensor):
            return super().inv_matmul(right_tensor, left_tensor=left_tensor)
        with settings.fast_computations(solves=False):
            return super().inv_matmul(right_tensor, left_tensor=left_tensor)

    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):
        if not isinstance(self.base_lazy_tensor, CholLazyTensor):
            return super().inv_quad_logdet(inv_quad_rhs=inv_quad_rhs, logdet=logdet, reduce_inv_quad=reduce_inv_quad)
        if not self.is_square:
            raise RuntimeError('inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. Got a {} of size {}.'.format(self.__class__.__name__, self.size()))
        if inv_quad_rhs is not None:
            if self.dim() == 2 and inv_quad_rhs.dim() == 1:
                if self.shape[-1] != inv_quad_rhs.numel():
                    raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, inv_quad_rhs.shape))
            elif self.dim() != inv_quad_rhs.dim():
                raise RuntimeError('LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number of dimensions.'.format(self.shape, inv_quad_rhs.shape))
            elif self.shape[-1] != inv_quad_rhs.shape[-2]:
                raise RuntimeError('LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).'.format(self.shape, inv_quad_rhs.shape))
        inv_quad_term = None
        logdet_term = None
        if inv_quad_rhs is not None:
            inv_quad_term = self.inv_quad(inv_quad_rhs, reduce_inv_quad=reduce_inv_quad)
        if logdet:
            logdet_term = self.base_lazy_tensor._chol_diag.pow(2).log().sum(-1)
        return inv_quad_term, logdet_term


class UnwhitenedVariationalStrategy(_VariationalStrategy):
    """
    Similar to :obj:`~gpytorch.variational.VariationalStrategy`, but does not perform the
    whitening operation. In almost all cases :obj:`~gpytorch.variational.VariationalStrategy`
    is preferable, with a few exceptions:

    - When the inducing points are exactly equal to the training points (i.e. :math:`\\mathbf Z = \\mathbf X`).
      Unwhitened models are faster in this case.

    - When the number of inducing points is very large (e.g. >2000). Unwhitened models can use CG for faster
      computation.

    :param ~gpytorch.models.ApproximateGP model: Model this strategy is applied to.
        Typically passed in when the VariationalStrategy is created in the
        __init__ method of the user defined model.
    :param torch.Tensor inducing_points: Tensor containing a set of inducing
        points to use for variational inference.
    :param ~gpytorch.variational.VariationalDistribution variational_distribution: A
        VariationalDistribution object that represents the form of the variational distribution :math:`q(\\mathbf u)`
    :param bool learn_inducing_points: (optional, default True): Whether or not
        the inducing point locations :math:`\\mathbf Z` should be learned (i.e. are they
        parameters of the model).
    """

    @cached(name='cholesky_factor')
    def _cholesky_factor(self, induc_induc_covar):
        L = psd_safe_cholesky(delazify(induc_induc_covar), jitter=settings.cholesky_jitter.value())
        return L

    @property
    @cached(name='prior_distribution_memo')
    def prior_distribution(self):
        out = self.model.forward(self.inducing_points)
        res = MultivariateNormal(out.mean, out.lazy_covariance_matrix.add_jitter())
        return res

    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):
        if torch.equal(x, inducing_points):
            if variational_inducing_covar is None:
                raise RuntimeError
            else:
                return MultivariateNormal(inducing_values, variational_inducing_covar)
        num_induc = inducing_points.size(-2)
        full_inputs = torch.cat([inducing_points, x], dim=-2)
        full_output = self.model.forward(full_inputs)
        full_mean, full_covar = full_output.mean, full_output.lazy_covariance_matrix
        test_mean = full_mean[(...), num_induc:]
        induc_mean = full_mean[(...), :num_induc]
        mean_diff = (inducing_values - induc_mean).unsqueeze(-1)
        induc_induc_covar = full_covar[(...), :num_induc, :num_induc].add_jitter()
        induc_data_covar = full_covar[(...), :num_induc, num_induc:].evaluate()
        data_data_covar = full_covar[(...), num_induc:, num_induc:]
        cholesky = False
        if settings.fast_computations.log_prob.off() or num_induc <= settings.max_cholesky_size.value():
            induc_induc_covar = CholLazyTensor(self._cholesky_factor(induc_induc_covar))
            cholesky = True
        if not self.training and settings.skip_posterior_variances.on():
            if not hasattr(self, '_mean_cache'):
                with settings.max_preconditioner_size(0):
                    self._mean_cache = induc_induc_covar.inv_matmul(mean_diff).detach()
            predictive_mean = torch.add(test_mean, induc_data_covar.transpose(-2, -1).matmul(self._mean_cache).squeeze(-1))
            predictive_covar = ZeroLazyTensor(test_mean.size(-1), test_mean.size(-1))
            return MultivariateNormal(predictive_mean, predictive_covar)
        shapes = [mean_diff.shape[:-1], induc_data_covar.shape[:-1], induc_induc_covar.shape[:-1]]
        if variational_inducing_covar is not None:
            root_variational_covar = variational_inducing_covar.root_decomposition().root.evaluate()
            shapes.append(root_variational_covar.shape[:-1])
        shape = _mul_broadcast_shape(*shapes)
        mean_diff = mean_diff.expand(*shape, mean_diff.size(-1))
        induc_data_covar = induc_data_covar.expand(*shape, induc_data_covar.size(-1))
        induc_induc_covar = induc_induc_covar.expand(*shape, induc_induc_covar.size(-1))
        if variational_inducing_covar is not None:
            root_variational_covar = root_variational_covar.expand(*shape, root_variational_covar.size(-1))
        with settings.max_preconditioner_size(0):
            if variational_inducing_covar is None:
                left_tensors = mean_diff
            else:
                left_tensors = torch.cat([mean_diff, root_variational_covar], -1)
            with torch.no_grad():
                eager_rhs = torch.cat([left_tensors, induc_data_covar], -1)
                solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(induc_induc_covar, eager_rhs.detach(), logdet_terms=not cholesky, include_tmats=not settings.skip_logdet_forward.on() and not cholesky)
                eager_rhss = [eager_rhs.detach(), eager_rhs[(...), left_tensors.size(-1):].detach(), eager_rhs[(...), :left_tensors.size(-1)].detach()]
                solves = [solve.detach(), solve[(...), left_tensors.size(-1):].detach(), solve[(...), :left_tensors.size(-1)].detach()]
                if settings.skip_logdet_forward.on():
                    eager_rhss.append(torch.cat([probe_vecs, left_tensors], -1))
                    solves.append(torch.cat([probe_vec_solves, solve[(...), :left_tensors.size(-1)]], -1))
            induc_induc_covar = CachedCGLazyTensor(induc_induc_covar, eager_rhss=eager_rhss, solves=solves, probe_vectors=probe_vecs, probe_vector_norms=probe_vec_norms, probe_vector_solves=probe_vec_solves, probe_vector_tmats=tmats)
        if self.training:
            self._memoize_cache['prior_distribution_memo'] = MultivariateNormal(induc_mean, induc_induc_covar)
        inv_products = induc_induc_covar.inv_matmul(induc_data_covar, left_tensors.transpose(-1, -2))
        predictive_mean = torch.add(test_mean, inv_products[(...), (0), :])
        if self.training:
            interp_data_data_var, _ = induc_induc_covar.inv_quad_logdet(induc_data_covar, logdet=False, reduce_inv_quad=False)
            data_covariance = DiagLazyTensor((data_data_covar.diag() - interp_data_data_var).clamp(0, math.inf))
        else:
            neg_induc_data_data_covar = torch.matmul(induc_data_covar.transpose(-1, -2).mul(-1), induc_induc_covar.inv_matmul(induc_data_covar))
            data_covariance = data_data_covar + neg_induc_data_data_covar
        predictive_covar = PsdSumLazyTensor(RootLazyTensor(inv_products[(...), 1:, :].transpose(-1, -2)), data_covariance)
        return MultivariateNormal(predictive_mean, predictive_covar)


class OldVersionWarning(UserWarning):
    """
    Warning thrown when loading a saved model from an outdated version of GPyTorch.
    """
    pass


def _ensure_updated_strategy_flag_set(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
    device = state_dict[list(state_dict.keys())[0]].device
    if prefix + 'updated_strategy' not in state_dict:
        state_dict[prefix + 'updated_strategy'] = torch.tensor(False, device=device)
        warnings.warn('You have loaded a variational GP model (using `VariationalStrategy`) from a previous version of GPyTorch. We have updated the parameters of your model to work with the new version of `VariationalStrategy` that uses whitened parameters.\nYour model will work as expected, but we recommend that you re-save your model.', OldVersionWarning)


class VariationalStrategy(_VariationalStrategy):
    """
    The standard variational strategy, as defined by `Hensman et al. (2015)`_.
    This strategy takes a set of :math:`m \\ll n` inducing points :math:`\\mathbf Z`
    and applies an approximate distribution :math:`q( \\mathbf u)` over their function values.
    (Here, we use the common notation :math:`\\mathbf u = f(\\mathbf Z)`.
    The approximate function distribution for any abitrary input :math:`\\mathbf X` is given by:

    .. math::

        q( f(\\mathbf X) ) = \\int p( f(\\mathbf X) \\mid \\mathbf u) q(\\mathbf u) \\: d\\mathbf u

    This variational strategy uses "whitening" to accelerate the optimization of the variational
    parameters. See `Matthews (2017)`_ for more info.

    :param ~gpytorch.models.ApproximateGP model: Model this strategy is applied to.
        Typically passed in when the VariationalStrategy is created in the
        __init__ method of the user defined model.
    :param torch.Tensor inducing_points: Tensor containing a set of inducing
        points to use for variational inference.
    :param ~gpytorch.variational.VariationalDistribution variational_distribution: A
        VariationalDistribution object that represents the form of the variational distribution :math:`q(\\mathbf u)`
    :param bool learn_inducing_points: (optional, default True): Whether or not
        the inducing point locations :math:`\\mathbf Z` should be learned (i.e. are they
        parameters of the model).

    .. _Hensman et al. (2015):
        http://proceedings.mlr.press/v38/hensman15.pdf
    .. _Matthews (2017):
        https://www.repository.cam.ac.uk/handle/1810/278022
    """

    def __init__(self, model, inducing_points, variational_distribution, learn_inducing_locations=True):
        super().__init__(model, inducing_points, variational_distribution, learn_inducing_locations)
        self.register_buffer('updated_strategy', torch.tensor(True))
        self._register_load_state_dict_pre_hook(_ensure_updated_strategy_flag_set)

    @cached(name='cholesky_factor')
    def _cholesky_factor(self, induc_induc_covar):
        L = psd_safe_cholesky(delazify(induc_induc_covar).double(), jitter=settings.cholesky_jitter.value())
        return L

    @property
    @cached(name='prior_distribution_memo')
    def prior_distribution(self):
        zeros = torch.zeros_like(self.variational_distribution.mean)
        ones = torch.ones_like(zeros)
        res = MultivariateNormal(zeros, DiagLazyTensor(ones))
        return res

    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):
        full_inputs = torch.cat([inducing_points, x], dim=-2)
        full_output = self.model.forward(full_inputs)
        full_covar = full_output.lazy_covariance_matrix
        num_induc = inducing_points.size(-2)
        test_mean = full_output.mean[(...), num_induc:]
        induc_induc_covar = full_covar[(...), :num_induc, :num_induc].add_jitter()
        induc_data_covar = full_covar[(...), :num_induc, num_induc:].evaluate()
        data_data_covar = full_covar[(...), num_induc:, num_induc:]
        L = self._cholesky_factor(induc_induc_covar)
        if L.shape != induc_induc_covar.shape:
            del self._memoize_cache['cholesky_factor']
            L = self._cholesky_factor(induc_induc_covar)
        interp_term = torch.triangular_solve(induc_data_covar.double(), L, upper=False)[0]
        predictive_mean = torch.matmul(interp_term.transpose(-1, -2), (inducing_values - self.prior_distribution.mean).unsqueeze(-1)).squeeze(-1) + test_mean
        middle_term = self.prior_distribution.lazy_covariance_matrix.mul(-1)
        if variational_inducing_covar is not None:
            middle_term = SumLazyTensor(variational_inducing_covar, middle_term)
        if trace_mode.on():
            predictive_covar = data_data_covar.add_jitter(0.0001).evaluate() + interp_term.transpose(-1, -2) @ middle_term.evaluate() @ interp_term
        else:
            predictive_covar = SumLazyTensor(data_data_covar.add_jitter(0.0001), MatmulLazyTensor(interp_term.transpose(-1, -2), middle_term @ interp_term))
        return MultivariateNormal(predictive_mean, predictive_covar)

    def __call__(self, x, prior=False):
        if not self.updated_strategy.item() and not prior:
            with torch.no_grad():
                prior_function_dist = self(self.inducing_points, prior=True)
                prior_mean = prior_function_dist.loc
                L = self._cholesky_factor(prior_function_dist.lazy_covariance_matrix.add_jitter())
                orig_mean_init_std = self._variational_distribution.mean_init_std
                self._variational_distribution.mean_init_std = 0.0
                variational_dist = self.variational_distribution
                whitened_mean = torch.triangular_solve((variational_dist.loc - prior_mean).unsqueeze(-1).double(), L, upper=False)[0].squeeze(-1)
                whitened_covar = RootLazyTensor(torch.triangular_solve(variational_dist.lazy_covariance_matrix.root_decomposition().root.evaluate().double(), L, upper=False)[0])
                whitened_variational_distribution = variational_dist.__class__(whitened_mean, whitened_covar)
                self._variational_distribution.initialize_variational_distribution(whitened_variational_distribution)
                self._variational_distribution.mean_init_std = orig_mean_init_std
                if hasattr(self, '_memoize_cache'):
                    delattr(self, '_memoize_cache')
                    self._memoize_cache = dict()
                self.updated_strategy.fill_(True)
        return super().__call__(x, prior=prior)


class WhitenedVariationalStrategy(UnwhitenedVariationalStrategy):

    def __init__(self, model, inducing_points, variational_distribution, learn_inducing_locations=True):
        warnings.warn('WhitenedVariationalStrategy is deprecated. Please use VariationalStrategy instead.', DeprecationWarning)
        super().__init__(model, inducing_points, variational_distribution, learn_inducing_locations)

    @cached(name='logdet_memo')
    def prior_covar_logdet(self):
        return -self.prior_distribution.lazy_covariance_matrix.logdet()

    @cached(name='covar_trace_memo')
    def covar_trace(self):
        variational_covar = self.variational_distribution.covariance_matrix
        prior_covar = self.prior_distribution.covariance_matrix
        batch_shape = prior_covar.shape[:-2]
        return (variational_covar * prior_covar).view(*batch_shape, -1).sum(-1)

    @cached(name='mean_diff_inv_quad_memo')
    def mean_diff_inv_quad(self):
        prior_mean = self.prior_distribution.mean
        prior_covar = self.prior_distribution.lazy_covariance_matrix
        variational_mean = self.variational_distribution.mean
        return prior_covar.inv_quad(variational_mean - prior_mean)

    def kl_divergence(self):
        variational_dist_u = self.variational_distribution
        prior_dist = self.prior_distribution
        kl_divergence = 0.5 * sum([self.prior_covar_logdet(), -variational_dist_u.lazy_covariance_matrix.logdet(), self.covar_trace(), self.mean_diff_inv_quad(), -prior_dist.event_shape.numel()])
        return kl_divergence

    def initialize_variational_dist(self):
        prior_dist = self.prior_distribution
        inv_prior_dist = torch.distributions.MultivariateNormal(prior_dist.mean, prior_dist.lazy_covariance_matrix.add_jitter().evaluate().double().inverse().type_as(prior_dist.covariance_matrix))
        self.variational_distribution.initialize_variational_distribution(inv_prior_dist)

    def forward(self, x):
        """
        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the
        inducing point function values. Specifically, forward defines how to transform a variational distribution
        over the inducing point values, :math:`q(u)`, in to a variational distribution over the function values at
        specified locations x, :math:`q(f|x)`, by integrating :math:`\\int p(f|x, u)q(u)du`

        :param torch.Tensor x: Locations x to get the variational posterior of the function values at.
        :rtype: ~gpytorch.distributions.MultivariateNormal
        :return: The distribution :math:`q(f|x)`
        """
        variational_dist = self.variational_distribution
        inducing_points = self.inducing_points
        if inducing_points.dim() < x.dim():
            inducing_points = inducing_points.expand(*x.shape[:-2], *inducing_points.shape[-2:])
        if len(variational_dist.batch_shape) < x.dim() - 2:
            variational_dist = variational_dist.expand(x.shape[:-2])
        if torch.equal(x, inducing_points):
            prior_covar = self.prior_distribution.lazy_covariance_matrix
            if isinstance(variational_dist.lazy_covariance_matrix, RootLazyTensor):
                predictive_covar = RootLazyTensor(prior_covar @ variational_dist.lazy_covariance_matrix.root.evaluate())
            else:
                predictive_covar = MatmulLazyTensor(prior_covar @ variational_dist.covariance_matrix, prior_covar)
            if self.training:
                self._mean_diff_inv_quad_memo, self._logdet_memo = prior_covar.inv_quad_logdet(variational_dist.mean - self.prior_distribution.mean, logdet=True)
            return MultivariateNormal(variational_dist.mean, predictive_covar)
        else:
            num_induc = inducing_points.size(-2)
            full_inputs = torch.cat([inducing_points, x], dim=-2)
            full_output = self.model.forward(full_inputs)
            full_mean, full_covar = full_output.mean, full_output.lazy_covariance_matrix
            test_mean = full_mean[(...), num_induc:]
            induc_mean = full_mean[(...), :num_induc]
            mean_diff = (variational_dist.mean - induc_mean).unsqueeze(-1)
            induc_induc_covar = full_covar[(...), :num_induc, :num_induc].add_jitter()
            induc_data_covar = full_covar[(...), :num_induc, num_induc:].evaluate()
            data_data_covar = full_covar[(...), num_induc:, num_induc:]
            cholesky = False
            if settings.fast_computations.log_prob.off() or num_induc <= settings.max_cholesky_size.value():
                induc_induc_covar = CholLazyTensor(induc_induc_covar.cholesky())
                cholesky = True
            with settings.max_preconditioner_size(0):
                with torch.no_grad():
                    eager_rhs = torch.cat([induc_data_covar, mean_diff], -1)
                    solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(induc_induc_covar, eager_rhs.detach(), logdet_terms=not cholesky, include_tmats=not settings.skip_logdet_forward.on() and not cholesky)
                    eager_rhss = [eager_rhs.detach()]
                    solves = [solve.detach()]
                    if settings.skip_logdet_forward.on() and self.training:
                        eager_rhss.append(torch.cat([probe_vecs, eager_rhs], -1))
                        solves.append(torch.cat([probe_vec_solves, solve[(...), :eager_rhs.size(-1)]], -1))
                    elif not self.training:
                        eager_rhss.append(eager_rhs[(...), :-1])
                        solves.append(solve[(...), :-1])
                induc_induc_covar = CachedCGLazyTensor(induc_induc_covar, eager_rhss=eager_rhss, solves=solves, probe_vectors=probe_vecs, probe_vector_norms=probe_vec_norms, probe_vector_solves=probe_vec_solves, probe_vector_tmats=tmats)
            if self.training:
                interp_data_data_var_plus_mean_diff_inv_quad, logdet = induc_induc_covar.inv_quad_logdet(torch.cat([induc_data_covar, mean_diff], -1), logdet=True, reduce_inv_quad=False)
                interp_data_data_var = interp_data_data_var_plus_mean_diff_inv_quad[(...), :-1]
                mean_diff_inv_quad = interp_data_data_var_plus_mean_diff_inv_quad[..., -1]
            predictive_mean = torch.add(test_mean, induc_induc_covar.inv_matmul(mean_diff, left_tensor=induc_data_covar.transpose(-1, -2)).squeeze(-1))
            is_root_lt = isinstance(variational_dist.lazy_covariance_matrix, RootLazyTensor)
            is_repeated_root_lt = isinstance(variational_dist.lazy_covariance_matrix, BatchRepeatLazyTensor) and isinstance(variational_dist.lazy_covariance_matrix.base_lazy_tensor, RootLazyTensor)
            if is_root_lt:
                predictive_covar = RootLazyTensor(induc_data_covar.transpose(-1, -2) @ variational_dist.lazy_covariance_matrix.root.evaluate())
            elif is_repeated_root_lt:
                predictive_covar = RootLazyTensor(induc_data_covar.transpose(-1, -2) @ variational_dist.lazy_covariance_matrix.root_decomposition().root.evaluate())
            else:
                predictive_covar = MatmulLazyTensor(induc_data_covar.transpose(-1, -2), predictive_covar @ induc_data_covar)
            if self.training:
                data_covariance = DiagLazyTensor((data_data_covar.diag() - interp_data_data_var).clamp(0, math.inf))
            else:
                neg_induc_data_data_covar = torch.matmul(induc_data_covar.transpose(-1, -2).mul(-1), induc_induc_covar.inv_matmul(induc_data_covar))
                data_covariance = data_data_covar + neg_induc_data_data_covar
            predictive_covar = PsdSumLazyTensor(predictive_covar, data_covariance)
            if self.training:
                self._memoize_cache['prior_distribution_memo'] = MultivariateNormal(induc_mean, induc_induc_covar)
                self._memoize_cache['logdet_memo'] = -logdet
                self._memoize_cache['mean_diff_inv_quad_memo'] = mean_diff_inv_quad
            return MultivariateNormal(predictive_mean, predictive_covar)

    def __call__(self, x, prior=False):
        if prior:
            return self.model.forward(x)
        if self.training:
            if hasattr(self, '_memoize_cache'):
                delattr(self, '_memoize_cache')
                self._memoize_cache = dict()
        if not self.variational_params_initialized.item():
            prior_dist = self.prior_distribution
            self._variational_distribution.initialize_variational_distribution(prior_dist)
            self.variational_params_initialized.fill_(1)
        return Module.__call__(self, x)


data_dim = 1


class SmallFeatureExtractor(nn.Sequential):

    def __init__(self):
        super(SmallFeatureExtractor, self).__init__()
        self.add_module('linear1', nn.Linear(data_dim, 10))
        self.add_module('relu3', nn.ReLU())
        self.add_module('linear4', nn.Linear(10, 1))


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (DenseNet,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 32, 32])], {}),
     False),
    (SmallFeatureExtractor,
     lambda: ([], {}),
     lambda: ([torch.rand([1, 1])], {}),
     True),
    (_DenseBlock,
     lambda: ([], {'num_layers': 1, 'num_input_features': 4, 'bn_size': 4, 'growth_rate': 4, 'drop_rate': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (_DenseLayer,
     lambda: ([], {'num_input_features': 4, 'growth_rate': 4, 'bn_size': 4, 'drop_rate': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (_Transition,
     lambda: ([], {'num_input_features': 4, 'num_output_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_cornellius_gp_gpytorch(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

