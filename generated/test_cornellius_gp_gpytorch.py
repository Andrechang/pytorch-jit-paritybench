import sys
_module = sys.modules[__name__]
del sys
conf = _module
densenet = _module
LBFGS = _module
gpytorch = _module
beta_features = _module
constraints = _module
constraints = _module
distributions = _module
delta = _module
distribution = _module
multitask_multivariate_normal = _module
multivariate_normal = _module
functions = _module
_dsmm = _module
_inv_matmul = _module
_inv_quad = _module
_inv_quad_log_det = _module
_log_normal_cdf = _module
_matmul = _module
_root_decomposition = _module
matern_covariance = _module
rbf_covariance = _module
kernels = _module
additive_structure_kernel = _module
arc_kernel = _module
cosine_kernel = _module
cylindrical_kernel = _module
grid_interpolation_kernel = _module
grid_kernel = _module
index_kernel = _module
inducing_point_kernel = _module
keops = _module
keops_kernel = _module
matern_kernel = _module
rbf_kernel = _module
kernel = _module
lcm_kernel = _module
linear_kernel = _module
multi_device_kernel = _module
multitask_kernel = _module
newton_girard_additive_kernel = _module
periodic_kernel = _module
polynomial_kernel = _module
polynomial_kernel_grad = _module
product_structure_kernel = _module
rbf_kernel_grad = _module
rff_kernel = _module
rq_kernel = _module
scale_kernel = _module
spectral_mixture_kernel = _module
lazy = _module
added_diag_lazy_tensor = _module
batch_repeat_lazy_tensor = _module
block_diag_lazy_tensor = _module
block_interleaved_lazy_tensor = _module
block_lazy_tensor = _module
cached_cg_lazy_tensor = _module
cat_lazy_tensor = _module
chol_lazy_tensor = _module
constant_mul_lazy_tensor = _module
diag_lazy_tensor = _module
interpolated_lazy_tensor = _module
keops_lazy_tensor = _module
kronecker_product_lazy_tensor = _module
lazy_evaluated_kernel_tensor = _module
lazy_tensor = _module
lazy_tensor_representation_tree = _module
matmul_lazy_tensor = _module
mul_lazy_tensor = _module
non_lazy_tensor = _module
psd_sum_lazy_tensor = _module
root_lazy_tensor = _module
sum_batch_lazy_tensor = _module
sum_lazy_tensor = _module
toeplitz_lazy_tensor = _module
zero_lazy_tensor = _module
likelihoods = _module
bernoulli_likelihood = _module
gaussian_likelihood = _module
likelihood = _module
likelihood_list = _module
multitask_gaussian_likelihood = _module
noise_models = _module
softmax_likelihood = _module
means = _module
constant_mean = _module
constant_mean_grad = _module
linear_mean = _module
mean = _module
multitask_mean = _module
zero_mean = _module
mlls = _module
_approximate_mll = _module
added_loss_term = _module
deep_approximate_mll = _module
exact_marginal_log_likelihood = _module
gamma_robust_variational_elbo = _module
inducing_point_kernel_added_loss_term = _module
marginal_log_likelihood = _module
noise_model_added_loss_term = _module
predictive_log_likelihood = _module
sum_marginal_log_likelihood = _module
variational_elbo = _module
models = _module
approximate_gp = _module
deep_gps = _module
deep_gp = _module
exact_gp = _module
exact_prediction_strategies = _module
gp = _module
model_list = _module
pyro = _module
_pyro_mixin = _module
pyro_gp = _module
module = _module
priors = _module
horseshoe_prior = _module
lkj_prior = _module
prior = _module
smoothed_box_prior = _module
torch_priors = _module
utils = _module
wishart_prior = _module
settings = _module
test = _module
base_kernel_test_case = _module
base_likelihood_test_case = _module
base_mean_test_case = _module
base_test_case = _module
lazy_tensor_test_case = _module
model_test_case = _module
variational_test_case = _module
broadcasting = _module
cholesky = _module
deprecation = _module
errors = _module
fft = _module
getitem = _module
grid = _module
interpolation = _module
lanczos = _module
linear_cg = _module
memoize = _module
pivoted_cholesky = _module
quadrature = _module
sparse = _module
stochastic_lq = _module
toeplitz = _module
transforms = _module
warnings = _module
variational = _module
_variational_distribution = _module
_variational_strategy = _module
additive_grid_interpolation_variational_strategy = _module
cholesky_variational_distribution = _module
delta_variational_distribution = _module
grid_interpolation_variational_strategy = _module
mean_field_variational_distribution = _module
multitask_variational_strategy = _module
orthogonally_decoupled_variational_strategy = _module
unwhitened_variational_strategy = _module
variational_strategy = _module
whitened_variational_strategy = _module
setup = _module
test_constraints = _module
test_delta = _module
test_multitask_multivariate_normal = _module
test_multivariate_normal = _module
examples = _module
test_batch_gp_regression = _module
test_batch_multitask_gp_regression = _module
test_batch_svgp_gp_regression = _module
test_decoupled_svgp_regression = _module
test_fixed_noise_fanatasy_updates = _module
test_grid_gp_regression = _module
test_hadamard_multitask_gp_regression = _module
test_independent_multitask_gp_regression = _module
test_kissgp_additive_classification = _module
test_kissgp_additive_regression = _module
test_kissgp_dkl_regression = _module
test_kissgp_gp_classification = _module
test_kissgp_gp_regression = _module
test_kissgp_kronecker_product_classification = _module
test_kissgp_kronecker_product_regression = _module
test_kissgp_multiplicative_regression = _module
test_kissgp_variational_regression = _module
test_kissgp_white_noise_regression = _module
test_kronecker_multitask_gp_regression = _module
test_kronecker_multitask_ski_gp_regression = _module
test_lcm_kernel_regression = _module
test_model_list_gp_regression = _module
test_pyro_integration = _module
test_sgpr_regression = _module
test_simple_gp_classification = _module
test_simple_gp_regression = _module
test_spectral_mixture_gp_regression = _module
test_svgp_gp_classification = _module
test_svgp_gp_regression = _module
test_unwhitened_svgp_regression = _module
test_white_noise_regression = _module
test_dsmm = _module
test_inv_matmul = _module
test_inv_quad = _module
test_inv_quad_log_det = _module
test_log_normal_cdf = _module
test_matern_covariance = _module
test_matmul = _module
test_rbf_covariance = _module
test_root_decomposition = _module
test_matern_kernel = _module
test_rbf_kernel = _module
test_additive_kernel = _module
test_arc_kernel = _module
test_cosine_kernel = _module
test_cylindrical_kernel = _module
test_grid_interpolation_kernel = _module
test_grid_kernel = _module
test_linear_kernel = _module
test_newton_girard_additive_kernel = _module
test_periodic_kernel = _module
test_polynomial_kernel = _module
test_polynomial_kernel_grad = _module
test_rbf_kernel = _module
test_rbf_kernel_grad = _module
test_rff_kernel = _module
test_rq_kernel = _module
test_scale_kernel = _module
test_spectral_mixture_kernel = _module
test_added_diag_lazy_tensor = _module
test_batch_repeat_lazy_tensor = _module
test_block_diag_lazy_tensor = _module
test_block_interleaved_lazy_tensor = _module
test_cached_cg_lazy_tensor = _module
test_cat_lazy_tensor = _module
test_chol_lazy_tensor = _module
test_constant_mul_lazy_tensor = _module
test_diag_lazy_tensor = _module
test_interpolated_lazy_tensor = _module
test_kronecker_product_lazy_tensor = _module
test_lazy_evaluated_kernel_tensor = _module
test_matmul_lazy_tensor = _module
test_mul_lazy_tensor = _module
test_non_lazy_tensor = _module
test_psd_sum_lazy_tensor = _module
test_root_lazy_tensor = _module
test_sum_batch_lazy_tensor = _module
test_sum_lazy_tensor = _module
test_toeplitz_lazy_tensor = _module
test_zero_lazy_tensor = _module
test_bernoulli_likelihood = _module
test_gaussian_likelihood = _module
test_general_multitask_gaussian_likelihood = _module
test_multitask_gaussian_likelihood = _module
test_softmax_likelihood = _module
test_constant_mean = _module
test_constant_mean_grad = _module
test_linear_mean = _module
test_multitask_mean = _module
test_zero_mean = _module
test_exact_gp = _module
test_model_list = _module
test_variational_gp = _module
test_gamma_prior = _module
test_horseshoe_prior = _module
test_lkj_prior = _module
test_multivariate_normal_prior = _module
test_normal_prior = _module
test_smoothed_box_prior = _module
test_cholesky = _module
test_fft = _module
test_getitem = _module
test_grid = _module
test_interpolation = _module
test_lanczos = _module
test_linear_cg = _module
test_pivoted_cholesky = _module
test_quadrature = _module
test_sparse = _module
test_toeplitz = _module
test_grid_interpolation_variational_strategy = _module
test_multitask_variational_strategy = _module
test_orthogonally_decoupled_variational_strategy = _module
test_unwhitened_variational_strategy = _module
test_variational_strategy = _module
test_whitened_variational_strategy = _module

from _paritybench_helpers import _mock_config
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
open = mock_open()
logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import re


import torch


import torch.nn as nn


import torch.nn.functional as F


from collections import OrderedDict


import math


from torch import sigmoid


from torch.nn import Module


from torch.nn.functional import softplus


from math import pi


from typing import Optional


import copy


import warnings


from abc import abstractmethod


from copy import deepcopy


from torch.nn import ModuleList


from torch.nn.parallel import DataParallel


import logging


from abc import ABC


from typing import Any


from torch import Tensor


from torch.nn import Parameter


from abc import abstractproperty


import itertools


from torch import nn


from torch.distributions import Distribution


from torch.distributions import HalfCauchy


from torch.distributions import Normal


from torch.distributions import constraints


from torch.nn import Module as TModule


from torch.distributions.utils import broadcast_all


from torch.distributions import Gamma


from torch.distributions import LogNormal


from torch.distributions import MultivariateNormal


from torch.distributions import Uniform


import numpy as np


import random


from torch import optim


class _DenseLayer(nn.Sequential):

    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super(_DenseLayer, self).__init__()
        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),
        self.add_module('relu1', nn.ReLU(inplace=True)),
        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *
            growth_rate, kernel_size=1, stride=1, bias=False)),
        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),
        self.add_module('relu2', nn.ReLU(inplace=True)),
        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate,
            growth_rate, kernel_size=3, stride=1, padding=1, bias=False)),
        self.drop_rate = drop_rate

    def forward(self, x):
        new_features = super(_DenseLayer, self).forward(x)
        if self.drop_rate > 0:
            new_features = F.dropout(new_features, p=self.drop_rate,
                training=self.training)
        return torch.cat([x, new_features], 1)


class _Transition(nn.Sequential):

    def __init__(self, num_input_features, num_output_features):
        super(_Transition, self).__init__()
        self.add_module('norm', nn.BatchNorm2d(num_input_features))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv', nn.Conv2d(num_input_features,
            num_output_features, kernel_size=1, stride=1, bias=False))
        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))


class _DenseBlock(nn.Sequential):

    def __init__(self, num_layers, num_input_features, bn_size, growth_rate,
        drop_rate):
        super(_DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = _DenseLayer(num_input_features + i * growth_rate,
                growth_rate, bn_size, drop_rate)
            self.add_module('denselayer%d' % (i + 1), layer)


class DenseNet(nn.Module):
    """Densenet-BC model class, based on
    `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
    Args:
        growth_rate (int) - how many filters to add each layer (`k` in paper)
        block_config (list of 3 or 4 ints) - how many layers in each pooling block
        num_init_features (int) - the number of filters to learn in the first convolution layer
        bn_size (int) - multiplicative factor for number of bottle neck layers
            (i.e. bn_size * k features in the bottleneck layer)
        drop_rate (float) - dropout rate after each dense layer
        num_classes (int) - number of classification classes
    """

    def __init__(self, growth_rate=12, block_config=(16, 16, 16),
        compression=0.5, num_init_features=24, bn_size=4, drop_rate=0,
        avgpool_size=8, num_classes=10):
        super(DenseNet, self).__init__()
        assert 0 < compression <= 1, 'compression of densenet should be between 0 and 1'
        self.avgpool_size = avgpool_size
        self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(3,
            num_init_features, kernel_size=3, stride=1, padding=1, bias=
            False))]))
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers=num_layers, num_input_features=
                num_features, bn_size=bn_size, growth_rate=growth_rate,
                drop_rate=drop_rate)
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = _Transition(num_input_features=num_features,
                    num_output_features=int(num_features * compression))
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = int(num_features * compression)
        self.features.add_module('norm_final', nn.BatchNorm2d(num_features))
        self.classifier = nn.Linear(num_features, num_classes)

    def forward(self, x):
        features = self.features(x)
        out = F.relu(features, inplace=True)
        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(features
            .size(0), -1)
        out = self.classifier(out)
        return out


def inv_sigmoid(x):
    return torch.log(x) - torch.log(1 - x)


def inv_softplus(x):
    return x + torch.log(-torch.expm1(-x))


TRANSFORM_REGISTRY = {torch.exp: torch.log, torch.nn.functional.softplus:
    inv_softplus, torch.sigmoid: inv_sigmoid}


def _get_inv_param_transform(param_transform, inv_param_transform=None):
    reg_inv_tf = TRANSFORM_REGISTRY.get(param_transform, None)
    if reg_inv_tf is None:
        if inv_param_transform is None:
            raise RuntimeError(
                'Must specify inv_param_transform for custom param_transforms')
        return inv_param_transform
    elif inv_param_transform is not None and reg_inv_tf != inv_param_transform:
        raise RuntimeError('TODO')
    return reg_inv_tf


class Interval(Module):

    def __init__(self, lower_bound, upper_bound, transform=sigmoid,
        inv_transform=inv_sigmoid, initial_value=None):
        """
        Defines an interval constraint for GP model parameters, specified by a lower bound and upper bound. For usage
        details, see the documentation for :meth:`~gpytorch.module.Module.register_constraint`.

        Args:
            lower_bound (float or torch.Tensor): The lower bound on the parameter.
            upper_bound (float or torch.Tensor): The upper bound on the parameter.
        """
        lower_bound = torch.as_tensor(lower_bound)
        upper_bound = torch.as_tensor(upper_bound)
        if torch.any(torch.ge(lower_bound, upper_bound)):
            raise RuntimeError('Got parameter bounds with empty intervals.')
        super().__init__()
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self._transform = transform
        self._inv_transform = inv_transform
        self._initial_value = initial_value
        if transform is not None and inv_transform is None:
            self._inv_transform = _get_inv_param_transform(transform)

    def _apply(self, fn):
        self.lower_bound = fn(self.lower_bound)
        self.upper_bound = fn(self.upper_bound)
        return super()._apply(fn)

    @property
    def enforced(self):
        return self._transform is not None

    def check(self, tensor):
        return bool(torch.all(tensor <= self.upper_bound) and torch.all(
            tensor >= self.lower_bound))

    def check_raw(self, tensor):
        return bool(torch.all(self.transform(tensor) <= self.upper_bound) and
            torch.all(self.transform(tensor) >= self.lower_bound))

    def intersect(self, other):
        """
        Returns a new Interval constraint that is the intersection of this one and another specified one.

        Args:
            other (Interval): Interval constraint to intersect with

        Returns:
            Interval: intersection if this interval with the other one.
        """
        if self.transform != other.transform:
            raise RuntimeError(
                'Cant intersect Interval constraints with conflicting transforms!'
                )
        lower_bound = torch.max(self.lower_bound, other.lower_bound)
        upper_bound = torch.min(self.upper_bound, other.upper_bound)
        return Interval(lower_bound, upper_bound)

    def transform(self, tensor):
        """
        Transforms a tensor to satisfy the specified bounds.

        If upper_bound is finite, we assume that `self.transform` saturates at 1 as tensor -> infinity. Similarly,
        if lower_bound is finite, we assume that `self.transform` saturates at 0 as tensor -> -infinity.

        Example transforms for one of the bounds being finite include torch.exp and torch.nn.functional.softplus.
        An example transform for the case where both are finite is torch.nn.functional.sigmoid.
        """
        if not self.enforced:
            return tensor
        if settings.debug.on():
            max_bound = torch.max(self.upper_bound)
            min_bound = torch.min(self.lower_bound)
            if max_bound == math.inf or min_bound == -math.inf:
                raise RuntimeError(
                    'Cannot make an Interval directly with non-finite bounds. Use a derived class like GreaterThan or LessThan instead.'
                    )
        transformed_tensor = self._transform(tensor) * (self.upper_bound -
            self.lower_bound) + self.lower_bound
        return transformed_tensor

    def inverse_transform(self, transformed_tensor):
        """
        Applies the inverse transformation.
        """
        if not self.enforced:
            return transformed_tensor
        if settings.debug.on():
            max_bound = torch.max(self.upper_bound)
            min_bound = torch.min(self.lower_bound)
            if max_bound == math.inf or min_bound == -math.inf:
                raise RuntimeError(
                    'Cannot make an Interval directly with non-finite bounds. Use a derived class like GreaterThan or LessThan instead.'
                    )
        tensor = self._inv_transform((transformed_tensor - self.lower_bound
            ) / (self.upper_bound - self.lower_bound))
        return tensor

    @property
    def initial_value(self):
        """
        The initial parameter value (if specified, None otherwise)
        """
        return self._initial_value

    def __repr__(self):
        if self.lower_bound.numel() == 1 and self.upper_bound.numel() == 1:
            return self._get_name(
                ) + f'({self.lower_bound:.3E}, {self.upper_bound:.3E})'
        else:
            return super().__repr__()

    def __iter__(self):
        yield self.lower_bound
        yield self.upper_bound


def default_postprocess_script(x):
    return x


class Distance(torch.nn.Module):

    def __init__(self, postprocess_script=default_postprocess_script):
        super().__init__()
        self._postprocess = postprocess_script

    def _sq_dist(self, x1, x2, postprocess, x1_eq_x2=False):
        adjustment = x1.mean(-2, keepdim=True)
        x1 = x1 - adjustment
        x2 = x2 - adjustment
        x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)
        x1_pad = torch.ones_like(x1_norm)
        if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:
            x2_norm, x2_pad = x1_norm, x1_pad
        else:
            x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)
            x2_pad = torch.ones_like(x2_norm)
        x1_ = torch.cat([-2.0 * x1, x1_norm, x1_pad], dim=-1)
        x2_ = torch.cat([x2, x2_pad, x2_norm], dim=-1)
        res = x1_.matmul(x2_.transpose(-2, -1))
        if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:
            res.diagonal(dim1=-2, dim2=-1).fill_(0)
        res.clamp_min_(0)
        return self._postprocess(res) if postprocess else res

    def _dist(self, x1, x2, postprocess, x1_eq_x2=False):
        res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)
        res = res.clamp_min_(1e-30).sqrt_()
        return self._postprocess(res) if postprocess else res


def is_in_cache(obj, name):
    return hasattr(obj, '_memoize_cache') and name in obj._memoize_cache


def get_from_cache(obj, name):
    """Get an item from the cache."""
    if not is_in_cache(obj, name):
        raise RuntimeError('Object does not have item {} stored in cache.'.
            format(name))
    return obj._memoize_cache[name]


def add_to_cache(obj, name, val):
    """Add a result to the cache of an object."""
    if not hasattr(obj, '_memoize_cache'):
        obj._memoize_cache = dict()
    obj._memoize_cache[name] = val
    return obj


def cached(method=None, name=None):
    """A decorator allowing for specifying the name of a cache, allowing it to be modified elsewhere."""
    if method is None:
        return functools.partial(cached, name=name)

    @functools.wraps(method)
    def g(self, *args, **kwargs):
        cache_name = name if name is not None else method
        if not is_in_cache(self, cache_name):
            add_to_cache(self, cache_name, method(self, *args, **kwargs))
        return get_from_cache(self, cache_name)
    return g


class NumericalWarning(RuntimeWarning):
    """
    Warning thrown when convergence criteria are not met, or when comptuations require extra stability.
    """
    pass


def _mul_broadcast_shape(*shapes, error_msg=None):
    """Compute dimension suggested by multiple tensor indices (supports broadcasting)"""
    num_dims = max(len(shape) for shape in shapes)
    shapes = tuple([1] * (num_dims - len(shape)) + list(shape) for shape in
        shapes)
    final_size = []
    for size_by_dim in zip(*shapes):
        non_singleton_sizes = tuple(size for size in size_by_dim if size != 1)
        if len(non_singleton_sizes):
            if any(size != non_singleton_sizes[0] for size in
                non_singleton_sizes):
                if error_msg is None:
                    raise RuntimeError(
                        'Shapes are not broadcastable for mul operation')
                else:
                    raise RuntimeError(error_msg)
            final_size.append(non_singleton_sizes[0])
        else:
            final_size.append(1)
    return torch.Size(final_size)


_noop_index = slice(None, None, None)


def _is_noop_index(index):
    """
    Determine if a given index is a noop (e.g. ":")
    """
    return isinstance(index, slice) and index == _noop_index


class LazyTensorRepresentationTree(object):

    def __init__(self, lazy_tsr):
        self._cls = lazy_tsr.__class__
        self._kwargs = lazy_tsr._kwargs
        counter = 0
        self.children = []
        for arg in lazy_tsr._args:
            if hasattr(arg, 'representation') and callable(arg.representation):
                representation_size = len(arg.representation())
                self.children.append((slice(counter, counter +
                    representation_size, None), arg.representation_tree()))
                counter += representation_size
            else:
                self.children.append((counter, None))
                counter += 1

    def __call__(self, *flattened_representation):
        unflattened_representation = []
        for index, subtree in self.children:
            if subtree is None:
                unflattened_representation.append(flattened_representation[
                    index])
            else:
                sub_representation = flattened_representation[index]
                unflattened_representation.append(subtree(*sub_representation))
        return self._cls(*unflattened_representation, **self._kwargs)


def lanczos_tridiag_to_diag(t_mat):
    """
    Given a num_init_vecs x num_batch x k x k tridiagonal matrix t_mat,
    returns a num_init_vecs x num_batch x k set of eigenvalues
    and a num_init_vecs x num_batch x k x k set of eigenvectors.

    TODO: make the eigenvalue computations done in batch mode.
    """
    orig_device = t_mat.device
    if t_mat.size(-1) < 32:
        retr = torch.symeig(t_mat.cpu(), eigenvectors=True)
    else:
        retr = torch.symeig(t_mat, eigenvectors=True)
    evals, evecs = retr
    mask = evals.ge(0)
    evecs = evecs * mask.type_as(evecs).unsqueeze(-2)
    evals = evals.masked_fill_(~mask, 1)
    return evals.to(orig_device), evecs.to(orig_device)


def lanczos_tridiag(matmul_closure, max_iter, dtype, device, matrix_shape,
    batch_shape=torch.Size(), init_vecs=None, num_init_vecs=1, tol=1e-05):
    """
    """
    multiple_init_vecs = False
    if not callable(matmul_closure):
        raise RuntimeError(
            'matmul_closure should be a function callable object that multiples a (Lazy)Tensor by a vector. Got a {} instead.'
            .format(matmul_closure.__class__.__name__))
    if init_vecs is None:
        init_vecs = torch.randn(matrix_shape[-1], num_init_vecs, dtype=
            dtype, device=device)
        init_vecs = init_vecs.expand(*batch_shape, matrix_shape[-1],
            num_init_vecs)
    else:
        if settings.debug.on():
            if dtype != init_vecs.dtype:
                raise RuntimeError(
                    'Supplied dtype {} and init_vecs.dtype {} do not agree!'
                    .format(dtype, init_vecs.dtype))
            if device != init_vecs.device:
                raise RuntimeError(
                    'Supplied device {} and init_vecs.device {} do not agree!'
                    .format(device, init_vecs.device))
            if batch_shape != init_vecs.shape[:-2]:
                raise RuntimeError(
                    'batch_shape {} and init_vecs.shape {} do not agree!'.
                    format(batch_shape, init_vecs.shape))
            if matrix_shape[-1] != init_vecs.size(-2):
                raise RuntimeError(
                    'matrix_shape {} and init_vecs.shape {} do not agree!'.
                    format(matrix_shape, init_vecs.shape))
        num_init_vecs = init_vecs.size(-1)
    num_iter = min(max_iter, matrix_shape[-1])
    dim_dimension = -2
    q_mat = torch.zeros(num_iter, *batch_shape, matrix_shape[-1],
        num_init_vecs, dtype=dtype, device=device)
    t_mat = torch.zeros(num_iter, num_iter, *batch_shape, num_init_vecs,
        dtype=dtype, device=device)
    q_0_vec = init_vecs / torch.norm(init_vecs, 2, dim=dim_dimension
        ).unsqueeze(dim_dimension)
    q_mat[0].copy_(q_0_vec)
    r_vec = matmul_closure(q_0_vec)
    alpha_0 = q_0_vec.mul(r_vec).sum(dim_dimension)
    r_vec.sub_(alpha_0.unsqueeze(dim_dimension).mul(q_0_vec))
    beta_0 = torch.norm(r_vec, 2, dim=dim_dimension)
    t_mat[0, 0].copy_(alpha_0)
    t_mat[0, 1].copy_(beta_0)
    t_mat[1, 0].copy_(beta_0)
    q_mat[1].copy_(r_vec.div_(beta_0.unsqueeze(dim_dimension)))
    for k in range(1, num_iter):
        q_prev_vec = q_mat[k - 1]
        q_curr_vec = q_mat[k]
        beta_prev = t_mat[k, k - 1].unsqueeze(dim_dimension)
        r_vec = matmul_closure(q_curr_vec) - q_prev_vec.mul(beta_prev)
        alpha_curr = q_curr_vec.mul(r_vec).sum(dim_dimension, keepdim=True)
        t_mat[k, k].copy_(alpha_curr.squeeze(dim_dimension))
        if k + 1 < num_iter:
            r_vec.sub_(alpha_curr.mul(q_curr_vec))
            correction = r_vec.unsqueeze(0).mul(q_mat[:k + 1]).sum(
                dim_dimension, keepdim=True)
            correction = q_mat[:k + 1].mul(correction).sum(0)
            r_vec.sub_(correction)
            r_vec_norm = torch.norm(r_vec, 2, dim=dim_dimension, keepdim=True)
            r_vec.div_(r_vec_norm)
            beta_curr = r_vec_norm.squeeze_(dim_dimension)
            t_mat[k, k + 1].copy_(beta_curr)
            t_mat[k + 1, k].copy_(beta_curr)
            inner_products = q_mat[:k + 1].mul(r_vec.unsqueeze(0)).sum(
                dim_dimension)
            could_reorthogonalize = False
            for _ in range(10):
                if not torch.sum(inner_products > tol):
                    could_reorthogonalize = True
                    break
                correction = r_vec.unsqueeze(0).mul(q_mat[:k + 1]).sum(
                    dim_dimension, keepdim=True)
                correction = q_mat[:k + 1].mul(correction).sum(0)
                r_vec.sub_(correction)
                r_vec_norm = torch.norm(r_vec, 2, dim=dim_dimension,
                    keepdim=True)
                r_vec.div_(r_vec_norm)
                inner_products = q_mat[:k + 1].mul(r_vec.unsqueeze(0)).sum(
                    dim_dimension)
            q_mat[k + 1].copy_(r_vec)
            if torch.sum(beta_curr.abs() > 1e-06
                ) == 0 or not could_reorthogonalize:
                break
    num_iter = k + 1
    q_mat = q_mat[:num_iter + 1].permute(-1, *range(1, 1 + len(batch_shape)
        ), -2, 0).contiguous()
    t_mat = t_mat[:num_iter + 1, :num_iter + 1].permute(-1, *range(2, 2 +
        len(batch_shape)), 0, 1).contiguous()
    if not multiple_init_vecs:
        q_mat.squeeze_(0)
        t_mat.squeeze_(0)
    return q_mat, t_mat


class StochasticLQ(object):
    """
    Implements an approximate log determinant calculation for symmetric positive definite matrices
    using stochastic Lanczos quadrature. For efficient calculation of derivatives, We additionally
    compute the trace of the inverse using the same probe vector the log determinant was computed
    with. For more details, see Dong et al. 2017 (in submission).
    """

    def __init__(self, max_iter=15, num_random_probes=10):
        """
        The nature of stochastic Lanczos quadrature is that the calculation of tr(f(A)) is both inaccurate and
        stochastic. An instance of StochasticLQ has two parameters that control these tradeoffs. Increasing either
        parameter increases the running time of the algorithm.

        Args:
            - cls - Tensor constructor - to ensure correct type (default - default tensor)
            - max_iter (scalar) - The number of Lanczos iterations to perform. Increasing this makes the estimate of
                tr(f(A)) more accurate in expectation -- that is, the average value returned has lower error.
            - num_random_probes (scalar) - The number of random probes to use in the stochastic trace estimation.
                Increasing this makes the estimate of tr(f(A)) lower variance -- that is, the value
                returned is more consistent.
        """
        self.max_iter = max_iter
        self.num_random_probes = num_random_probes

    def lanczos_batch(self, matmul_closure, rhs_vectors):
        return lanczos_tridiag(matmul_closure, self.max_iter, init_vecs=
            rhs_vectors, dtype=rhs_vectors.dtype, device=rhs_vectors.device,
            batch_shape=rhs_vectors.shape[-2:], matrix_shape=torch.Size((
            rhs_vectors.size(-2), rhs_vectors.size(-2))))

    def evaluate(self, matrix_shape, eigenvalues, eigenvectors, funcs):
        """
        Computes tr(f(A)) for an arbitrary list of functions, where f(A) is equivalent to applying the function
        elementwise to the eigenvalues of A, i.e., if A = V\\LambdaV^{T}, then f(A) = Vf(\\Lambda)V^{T}, where
        f(\\Lambda) is applied elementwise.
        Note that calling this function with a list of functions to apply is significantly more efficient than
        calling it multiple times with one function -- each additional function after the first requires negligible
        additional computation.

        Args:
            - matrix_shape (torch.Size()) - size of underlying matrix (not including batch dimensions)
            - eigenvalues (Tensor n_probes x ...batch_shape x k) - batches of eigenvalues from Lanczos tridiag mats
            - eigenvectors (Tensor n_probes x ...batch_shape x k x k) - batches of eigenvectors from " " "
            - funcs (list of closures) - A list of functions [f_1,...,f_k]. tr(f_i(A)) is computed for each function.
                Each function in the closure should expect to take a torch vector of eigenvalues as input and apply
                the function elementwise. For example, to compute logdet(A) = tr(log(A)), [lambda x: x.log()] would
                be a reasonable value of funcs.

        Returns:
            - results (list of scalars) - The trace of each supplied function applied to the matrix, e.g.,
                [tr(f_1(A)),tr(f_2(A)),...,tr(f_k(A))].
        """
        batch_shape = torch.Size(eigenvalues.shape[1:-1])
        results = [torch.zeros(batch_shape, dtype=eigenvalues.dtype, device
            =eigenvalues.device) for _ in funcs]
        num_random_probes = eigenvalues.size(0)
        for j in range(num_random_probes):
            eigenvalues_for_probe = eigenvalues[j]
            eigenvectors_for_probe = eigenvectors[j]
            for i, func in enumerate(funcs):
                eigenvecs_first_component = eigenvectors_for_probe[(...), (
                    0), :]
                func_eigenvalues = func(eigenvalues_for_probe)
                dot_products = (eigenvecs_first_component.pow(2) *
                    func_eigenvalues).sum(-1)
                results[i] = results[i] + matrix_shape[-1] / float(
                    num_random_probes) * dot_products
        return results


class InvQuadLogDet(Function):
    """
    Given a PSD matrix A (or a batch of PSD matrices A), this function computes one or both
    of the following
    - The matrix solves A^{-1} b
    - logdet(A)
    """

    @staticmethod
    def forward(ctx, representation_tree, dtype, device, matrix_shape,
        batch_shape=torch.Size(), inv_quad=False, logdet=False,
        probe_vectors=None, probe_vector_norms=None, *args):
        """
        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)
        If self.inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)
        - the RHS of the matrix solves.

        Returns:
        - (Scalar) The inverse quadratic form (or None, if self.inv_quad is False)
        - (Scalar) The log determinant (or None, self.if logdet is False)
        """
        if not (inv_quad or logdet):
            raise RuntimeError(
                'Either inv_quad or logdet must be true (or both)')
        ctx.representation_tree = representation_tree
        ctx.dtype = dtype
        ctx.device = device
        ctx.matrix_shape = matrix_shape
        ctx.batch_shape = batch_shape
        ctx.inv_quad = inv_quad
        ctx.logdet = logdet
        matrix_args = None
        inv_quad_rhs = None
        if ctx.inv_quad:
            matrix_args = args[1:]
            inv_quad_rhs = args[0]
        else:
            matrix_args = args
        lazy_tsr = ctx.representation_tree(*matrix_args)
        with torch.no_grad():
            preconditioner, precond_lt, logdet_correction = (lazy_tsr.
                _preconditioner())
        ctx.preconditioner = preconditioner
        if (probe_vectors is None or probe_vector_norms is None) and logdet:
            num_random_probes = settings.num_trace_samples.value()
            if preconditioner is None:
                if settings.deterministic_probes.on():
                    warnings.warn(
                        "Deterministic probes will currently work only if you aren't training multiple independent models simultaneously."
                        , UserWarning)
                    if settings.deterministic_probes.probe_vectors is None:
                        probe_vectors = torch.empty(matrix_shape[-1],
                            num_random_probes, dtype=dtype, device=device)
                        probe_vectors.bernoulli_().mul_(2).add_(-1)
                        settings.deterministic_probes.probe_vectors = (
                            probe_vectors)
                    else:
                        probe_vectors = (settings.deterministic_probes.
                            probe_vectors)
                else:
                    probe_vectors = torch.empty(matrix_shape[-1],
                        num_random_probes, dtype=dtype, device=device)
                    probe_vectors.bernoulli_().mul_(2).add_(-1)
                probe_vector_norms = torch.norm(probe_vectors, 2, dim=-2,
                    keepdim=True)
                if batch_shape is not None:
                    probe_vectors = probe_vectors.expand(*batch_shape,
                        matrix_shape[-1], num_random_probes)
                    probe_vector_norms = probe_vector_norms.expand(*
                        batch_shape, 1, num_random_probes)
            else:
                if precond_lt.size()[-2:] == torch.Size([1, 1]):
                    covar_root = precond_lt.evaluate().sqrt()
                else:
                    covar_root = precond_lt.root_decomposition().root
                if settings.deterministic_probes.on():
                    warnings.warn(
                        "Deterministic probes will currently work only if you aren't training multiple independent models simultaneously."
                        , UserWarning)
                    base_samples = settings.deterministic_probes.probe_vectors
                    if base_samples is None or covar_root.size(-1
                        ) != base_samples.size(-2):
                        base_samples = torch.randn(*precond_lt.batch_shape,
                            covar_root.size(-1), num_random_probes, dtype=
                            precond_lt.dtype, device=precond_lt.device)
                        settings.deterministic_probes.probe_vectors = (
                            base_samples)
                    probe_vectors = covar_root.matmul(base_samples).permute(
                        -1, *range(precond_lt.dim() - 1))
                else:
                    base_samples = torch.randn(*precond_lt.batch_shape,
                        covar_root.size(-1), num_random_probes, dtype=
                        precond_lt.dtype, device=precond_lt.device)
                    probe_vectors = precond_lt.zero_mean_mvn_samples(
                        num_random_probes)
                probe_vectors = probe_vectors.unsqueeze(-2).transpose(0, -2
                    ).squeeze(0).transpose(-2, -1).contiguous()
                probe_vector_norms = torch.norm(probe_vectors, p=2, dim=-2,
                    keepdim=True)
            probe_vectors = probe_vectors.div(probe_vector_norms)
        ctx.probe_vectors = probe_vectors
        ctx.probe_vector_norms = probe_vector_norms
        if ctx.logdet and not ctx.probe_vectors.numel():
            raise RuntimeError(
                'Probe vectors were not supplied for logdet computation')
        rhs_list = []
        num_random_probes = 0
        num_inv_quad_solves = 0
        if ctx.logdet:
            rhs_list.append(ctx.probe_vectors)
            num_random_probes = ctx.probe_vectors.size(-1)
        ctx.is_vector = False
        if ctx.inv_quad:
            if inv_quad_rhs.ndimension() == 1:
                inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)
                ctx.is_vector = True
            rhs_list.append(inv_quad_rhs)
            num_inv_quad_solves = inv_quad_rhs.size(-1)
        rhs = torch.cat(rhs_list, -1)
        t_mat = None
        if ctx.logdet and settings.skip_logdet_forward.off():
            solves, t_mat = lazy_tsr._solve(rhs, preconditioner,
                num_tridiag=num_random_probes)
        else:
            solves = lazy_tsr._solve(rhs, preconditioner, num_tridiag=0)
        logdet_term = torch.zeros(lazy_tsr.batch_shape, dtype=ctx.dtype,
            device=ctx.device)
        inv_quad_term = torch.zeros(lazy_tsr.batch_shape, dtype=ctx.dtype,
            device=ctx.device)
        if ctx.logdet and settings.skip_logdet_forward.off():
            if torch.any(torch.isnan(t_mat)).item():
                logdet_term = torch.tensor(float('nan'), dtype=ctx.dtype,
                    device=ctx.device)
            else:
                if ctx.batch_shape is None:
                    t_mat = t_mat.unsqueeze(1)
                eigenvalues, eigenvectors = lanczos_tridiag_to_diag(t_mat)
                slq = StochasticLQ()
                logdet_term, = slq.evaluate(ctx.matrix_shape, eigenvalues,
                    eigenvectors, [lambda x: x.log()])
                if logdet_correction is not None:
                    logdet_term = logdet_term + logdet_correction
        if ctx.inv_quad:
            inv_quad_solves = solves.narrow(-1, num_random_probes,
                num_inv_quad_solves)
            inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)
        ctx.num_random_probes = num_random_probes
        ctx.num_inv_quad_solves = num_inv_quad_solves
        to_save = list(matrix_args) + [solves]
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return inv_quad_term, logdet_term

    @staticmethod
    def backward(ctx, inv_quad_grad_output, logdet_grad_output):
        matrix_arg_grads = None
        inv_quad_rhs_grad = None
        compute_inv_quad_grad = inv_quad_grad_output.abs().sum(
            ) and ctx.inv_quad
        compute_logdet_grad = logdet_grad_output.abs().sum() and ctx.logdet
        matrix_args = ctx.saved_tensors[:-1]
        solves = ctx.saved_tensors[-1]
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        if ctx.inv_quad:
            inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)
        if compute_logdet_grad:
            logdet_grad_output = logdet_grad_output.unsqueeze(-1)
            logdet_grad_output.unsqueeze_(-1)
        probe_vector_solves = None
        inv_quad_solves = None
        neg_inv_quad_solves_times_grad_out = None
        if compute_logdet_grad:
            coef = 1.0 / ctx.probe_vectors.size(-1)
            probe_vector_solves = solves.narrow(-1, 0, ctx.num_random_probes
                ).mul(coef)
            probe_vector_solves.mul_(ctx.probe_vector_norms).mul_(
                logdet_grad_output)
            probe_vectors = ctx.probe_vectors.mul(ctx.probe_vector_norms)
        if ctx.inv_quad:
            inv_quad_solves = solves.narrow(-1, ctx.num_random_probes, ctx.
                num_inv_quad_solves)
            neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(
                inv_quad_grad_output).mul_(-1)
        if any(ctx.needs_input_grad):
            left_factors_list = []
            right_factors_list = []
            if compute_logdet_grad:
                left_factors_list.append(probe_vector_solves)
                if ctx.preconditioner is not None:
                    probe_vectors = ctx.preconditioner(probe_vectors)
                right_factors_list.append(probe_vectors)
            if compute_inv_quad_grad:
                left_factors_list.append(neg_inv_quad_solves_times_grad_out)
                right_factors_list.append(inv_quad_solves)
            left_factors = torch.cat(left_factors_list, -1)
            right_factors = torch.cat(right_factors_list, -1)
            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors,
                right_factors)
        if compute_inv_quad_grad and ctx.needs_input_grad[9]:
            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul_(-2)
        elif ctx.inv_quad:
            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)
        if ctx.is_vector:
            inv_quad_rhs_grad.squeeze_(-1)
        if ctx.inv_quad:
            res = [inv_quad_rhs_grad] + list(matrix_arg_grads)
        else:
            res = list(matrix_arg_grads)
        return tuple([None] * 9 + res)


def delazify(obj):
    """
    A function which ensures that `obj` is a (normal) Tensor.

    If `obj` is a Tensor, this function does nothing.
    If `obj` is a LazyTensor, this function evaluates it.
    """
    if torch.is_tensor(obj):
        return obj
    elif isinstance(obj, LazyTensor):
        return obj.evaluate()
    else:
        raise TypeError('object of class {} cannot be made into a Tensor'.
            format(obj.__class__.__name__))


def _solve(lazy_tsr, rhs):
    if settings.fast_computations.solves.off() or lazy_tsr.size(-1
        ) <= settings.max_cholesky_size.value():
        return lazy_tsr._cholesky()._cholesky_solve(rhs)
    else:
        with torch.no_grad():
            preconditioner = lazy_tsr.detach()._inv_matmul_preconditioner()
        return lazy_tsr._solve(rhs, preconditioner)


class InvMatmul(Function):

    @staticmethod
    def forward(ctx, representation_tree, has_left, *args):
        left_tensor = None
        right_tensor = None
        matrix_args = None
        ctx.representation_tree = representation_tree
        ctx.has_left = has_left
        if ctx.has_left:
            left_tensor, right_tensor, *matrix_args = args
        else:
            right_tensor, *matrix_args = args
        orig_right_tensor = right_tensor
        lazy_tsr = ctx.representation_tree(*matrix_args)
        ctx.is_vector = False
        if right_tensor.ndimension() == 1:
            right_tensor = right_tensor.unsqueeze(-1)
            ctx.is_vector = True
        if ctx.has_left:
            rhs = torch.cat([left_tensor.transpose(-1, -2), right_tensor], -1)
            solves = _solve(lazy_tsr, rhs)
            res = solves[(...), left_tensor.size(-2):]
            res = left_tensor @ res
        else:
            solves = _solve(lazy_tsr, right_tensor)
            res = solves
        if ctx.is_vector:
            res = res.squeeze(-1)
        if ctx.has_left:
            args = [solves, left_tensor, orig_right_tensor] + list(matrix_args)
        else:
            args = [solves, orig_right_tensor] + list(matrix_args)
        ctx.save_for_backward(*args)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return res

    @staticmethod
    def backward(ctx, grad_output):
        if ctx.has_left:
            solves, left_tensor, right_tensor, *matrix_args = ctx.saved_tensors
            left_solves = solves[(...), :left_tensor.size(-2)]
            right_solves = solves[(...), left_tensor.size(-2):]
        else:
            right_solves, right_tensor, *matrix_args = ctx.saved_tensors
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        arg_grads = [None] * len(matrix_args)
        left_grad = None
        right_grad = None
        if any(ctx.needs_input_grad):
            if ctx.is_vector:
                right_tensor = right_tensor.unsqueeze(-1)
                grad_output = grad_output.unsqueeze(-1)
            if not ctx.has_left:
                left_solves = InvMatmul.apply(ctx.representation_tree, 
                    False, grad_output, *matrix_args)
                if any(ctx.needs_input_grad[3:]):
                    arg_grads = lazy_tsr._quad_form_derivative(torch.cat([
                        left_solves, right_solves], -1), torch.cat([
                        right_solves, left_solves], -1).mul(-0.5))
                if ctx.needs_input_grad[2]:
                    right_grad = left_solves
                    if ctx.is_vector:
                        right_grad.squeeze_(-1)
                return tuple([None, None] + [right_grad] + list(arg_grads))
            else:
                left_solves = left_solves @ grad_output
                if ctx.needs_input_grad[3]:
                    left_grad = grad_output @ right_solves.transpose(-1, -2)
                if any(ctx.needs_input_grad[4:]):
                    arg_grads = lazy_tsr._quad_form_derivative(torch.cat([
                        left_solves, right_solves], -1), torch.cat([
                        right_solves, left_solves], -1).mul(-0.5))
                if ctx.needs_input_grad[2]:
                    right_grad = left_solves
                    if ctx.is_vector:
                        right_grad.squeeze_(-1)
                return tuple([None, None] + [left_grad, right_grad] + list(
                    arg_grads))


class NanError(RuntimeError):
    pass


def psd_safe_cholesky(A, upper=False, out=None, jitter=None):
    """Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.
    Args:
        :attr:`A` (Tensor):
            The tensor to compute the Cholesky decomposition of
        :attr:`upper` (bool, optional):
            See torch.cholesky
        :attr:`out` (Tensor, optional):
            See torch.cholesky
        :attr:`jitter` (float, optional):
            The jitter to add to the diagonal of A in case A is only p.s.d. If omitted, chosen
            as 1e-6 (float) or 1e-8 (double)
    """
    try:
        L = torch.cholesky(A, upper=upper, out=out)
        return L
    except RuntimeError as e:
        isnan = torch.isnan(A)
        if isnan.any():
            raise NanError(
                f'cholesky_cpu: {isnan.sum().item()} of {A.numel()} elements of the {A.shape} tensor are NaN.'
                )
        if jitter is None:
            jitter = 1e-06 if A.dtype == torch.float32 else 1e-08
        Aprime = A.clone()
        jitter_prev = 0
        for i in range(3):
            jitter_new = jitter * 10 ** i
            Aprime.diagonal(dim1=-2, dim2=-1).add_(jitter_new - jitter_prev)
            jitter_prev = jitter_new
            try:
                L = torch.cholesky(Aprime, upper=upper, out=out)
                warnings.warn(
                    f'A not p.d., added jitter of {jitter_new} to the diagonal'
                    , NumericalWarning)
                return L
            except RuntimeError:
                continue
        raise e


class RootDecomposition(Function):

    @staticmethod
    def forward(ctx, representation_tree, max_iter, dtype, device,
        batch_shape, matrix_shape, root, inverse, initial_vectors, *matrix_args
        ):
        """
        :param list matrix_args: The arguments representing the symmetric matrix A (or batch of PSD matrices A)

        :rtype: (torch.Tensor, torch.Tensor)
        :return: :attr:`R`, such that :math:`R R^T \\approx A`, and :attr:`R_inv`, such that
            :math:`R_{inv} R_{inv}^T \\approx A^{-1}` (will only be populated if self.inverse = True)
        """
        from ..lazy import lazify
        ctx.representation_tree = representation_tree
        ctx.device = device
        ctx.dtype = dtype
        ctx.matrix_shape = matrix_shape
        ctx.max_iter = max_iter
        ctx.batch_shape = batch_shape
        ctx.root = root
        ctx.inverse = inverse
        ctx.initial_vectors = initial_vectors
        lazy_tsr = ctx.representation_tree(*matrix_args)
        matmul_closure = lazy_tsr._matmul
        q_mat, t_mat = lanczos.lanczos_tridiag(matmul_closure, ctx.max_iter,
            dtype=ctx.dtype, device=ctx.device, matrix_shape=ctx.
            matrix_shape, batch_shape=ctx.batch_shape, init_vecs=ctx.
            initial_vectors)
        if ctx.batch_shape is None:
            q_mat = q_mat.unsqueeze(-3)
            t_mat = t_mat.unsqueeze(-3)
        if t_mat.ndimension() == 3:
            q_mat = q_mat.unsqueeze(0)
            t_mat = t_mat.unsqueeze(0)
        n_probes = t_mat.size(0)
        mins = lazify(t_mat).diag().min(dim=-1, keepdim=True)[0].unsqueeze(-1)
        jitter_mat = settings.tridiagonal_jitter.value() * mins * torch.eye(
            t_mat.size(-1), device=t_mat.device, dtype=t_mat.dtype).expand_as(
            t_mat)
        eigenvalues, eigenvectors = lanczos.lanczos_tridiag_to_diag(t_mat +
            jitter_mat)
        q_mat = q_mat.matmul(eigenvectors)
        root_evals = eigenvalues.sqrt()
        root = torch.empty(0, dtype=q_mat.dtype, device=q_mat.device)
        inverse = torch.empty(0, dtype=q_mat.dtype, device=q_mat.device)
        if ctx.inverse:
            inverse = q_mat / root_evals.unsqueeze(-2)
        if ctx.root:
            root = q_mat * root_evals.unsqueeze(-2)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        if ctx.batch_shape is None:
            root = root.squeeze(1) if root.numel() else root
            q_mat = q_mat.squeeze(1)
            t_mat = t_mat.squeeze(1)
            root_evals = root_evals.squeeze(1)
            inverse = inverse.squeeze(1) if inverse.numel() else inverse
        if n_probes == 1:
            root = root.squeeze(0) if root.numel() else root
            q_mat = q_mat.squeeze(0)
            t_mat = t_mat.squeeze(0)
            root_evals = root_evals.squeeze(0)
            inverse = inverse.squeeze(0) if inverse.numel() else inverse
        to_save = list(matrix_args) + [q_mat, root_evals, inverse]
        ctx.save_for_backward(*to_save)
        return root, inverse

    @staticmethod
    def backward(ctx, root_grad_output, inverse_grad_output):
        if any(ctx.needs_input_grad):

            def is_empty(tensor):
                return tensor.numel() == 0 or tensor.numel() == 1 and tensor[0
                    ] == 0
            if is_empty(root_grad_output):
                root_grad_output = None
            if is_empty(inverse_grad_output):
                inverse_grad_output = None
            matrix_args = ctx.saved_tensors[:-3]
            q_mat = ctx.saved_tensors[-3]
            root_evals = ctx.saved_tensors[-2]
            inverse = ctx.saved_tensors[-1]
            is_batch = False
            if root_grad_output is not None:
                if root_grad_output.ndimension() == 2 and q_mat.ndimension(
                    ) > 2:
                    root_grad_output = root_grad_output.unsqueeze(0)
                    is_batch = True
                if root_grad_output.ndimension() == 3 and q_mat.ndimension(
                    ) > 3:
                    root_grad_output = root_grad_output.unsqueeze(0)
                    is_batch = True
            if inverse_grad_output is not None:
                if inverse_grad_output.ndimension() == 2 and q_mat.ndimension(
                    ) > 2:
                    inverse_grad_output = inverse_grad_output.unsqueeze(0)
                    is_batch = True
                if inverse_grad_output.ndimension() == 3 and q_mat.ndimension(
                    ) > 3:
                    inverse_grad_output = inverse_grad_output.unsqueeze(0)
                    is_batch = True
            if hasattr(ctx, '_lazy_tsr'):
                lazy_tsr = ctx._lazy_tsr
            else:
                lazy_tsr = ctx.representation_tree(*matrix_args)
            if not ctx.inverse:
                inverse = q_mat / root_evals.unsqueeze(-2)
            left_factor = torch.zeros_like(inverse)
            if root_grad_output is not None:
                left_factor.add_(root_grad_output)
            if inverse_grad_output is not None:
                left_factor.sub_(torch.matmul(inverse, inverse_grad_output.
                    transpose(-1, -2)).matmul(inverse))
            right_factor = inverse.div(2.0)
            if is_batch:
                left_factor = left_factor.permute(1, 0, 2, 3).contiguous()
                left_factor = left_factor.view(inverse.size(1), -1,
                    left_factor.size(-1))
                right_factor = right_factor.permute(1, 0, 2, 3).contiguous()
                right_factor = right_factor.view(inverse.size(1), -1,
                    right_factor.size(-1))
            else:
                left_factor = left_factor.contiguous()
                right_factor = right_factor.contiguous()
            res = lazy_tsr._quad_form_derivative(left_factor, right_factor)
            return tuple([None] * 9 + list(res))
        else:
            pass


def _compute_getitem_size(obj, indices):
    """
    Given an object and a tuple of indices, computes the final size of the
    Indices is a tuple containing ints, slices, and tensors

    .. note::
        The length of indices must match the dimensionality of obj

    Args:
        obj - tensor or LazyTensor
        indices - tuple of ints, slices, tensors

    Returns:
        :class:`torch.Size`
    """
    if obj.dim() != len(indices):
        raise RuntimeError(
            '_compute_getitem_size assumes that obj (size: {}) and indices (len: {}) have the same dimensionality.'
            .format(obj.shape, len(indices)))
    final_shape = []
    tensor_idx = None
    tensor_idx_shape = None
    slice_after_tensor_idx = False
    for i, (size, idx) in enumerate(zip(obj.shape, indices)):
        if isinstance(idx, slice):
            if idx == _noop_index:
                final_shape.append(size)
            else:
                final_shape.append(len(range(*idx.indices(size))))
            if tensor_idx is not None:
                slice_after_tensor_idx = True
        elif isinstance(idx, int):
            if settings.debug.on():
                try:
                    range(size)[idx]
                except IndexError:
                    raise IndexError(
                        'index element {} ({}) is invalid: out of range for obj of size {}.'
                        .format(i, idx, obj.shape))
        elif torch.is_tensor(idx):
            if tensor_idx_shape is None:
                tensor_idx_shape = idx.shape
                tensor_idx = len(final_shape)
            else:
                try:
                    tensor_idx_shape = _mul_broadcast_shape(tensor_idx_shape,
                        idx.shape)
                except RuntimeError:
                    raise IndexError(
                        'Incompatible tensor indices in index - got shapes of {} .'
                        .format([idx.shape for idx in indices if torch.
                        is_tensor(idx)]))
                if slice_after_tensor_idx:
                    tensor_idx = 0
    if tensor_idx is not None:
        final_shape = final_shape[:tensor_idx] + list(tensor_idx_shape
            ) + final_shape[tensor_idx:]
    return torch.Size(final_shape)


def _is_tensor_index_moved_to_start(indices):
    """
    Given an index, determine if the indexed part of the getitem is moved to the zero'th dimension
    """
    has_tensor_index = False
    continuous_tensor_index = True
    if torch.is_tensor(indices[0]):
        return True
    for index in indices[1:]:
        if torch.is_tensor(index):
            if not has_tensor_index:
                has_tensor_index = True
            elif not continuous_tensor_index:
                return True
        elif isinstance(index, slice):
            if has_tensor_index:
                continuous_tensor_index = False
    return False


def _pad_with_singletons(obj, num_singletons_before=0, num_singletons_after=0):
    """
    Pad obj with singleton dimensions on the left and right

    Example:
        >>> x = torch.randn(10, 5)
        >>> _pad_width_singletons(x, 2, 3).shape
        >>> # [1, 1, 10, 5, 1, 1, 1]
    """
    new_shape = [1] * num_singletons_before + list(obj.shape) + [1
        ] * num_singletons_after
    return obj.view(*new_shape)


def _convert_indices_to_tensors(obj, indices):
    """
    Given an index made up of tensors/slices/ints, returns a tensor-only index that has the
    same outcome as the original index (when applied to the obj)

    .. note::
        The length of indices must match the dimensionality of obj

    Args:
        obj - tensor or LazyTensor
        indices - tuple of slices, tensors, ints

    Returns:
        tuple of tensor indices (shapes of tensors will involve broadcasting)

    Example:
        >>> x = torch.randn(3, 6, 4)
        >>> _convert_indices_to_tensors(x, (torch.tensor([0, 1]), 2, slice(None, None, None)))
        >>> # (torch.tensor([[[0]], [[1]]]), torch.tensor([[[2]]]), torch.tensor([[[0, 1, 2, 3]]]))
    """
    slice_indices = tuple(index for index in indices if isinstance(index,
        slice))
    tensor_indices = tuple(index for index in indices if torch.is_tensor(index)
        )
    tensor_index_shape = _mul_broadcast_shape(*[tensor_index.shape for
        tensor_index in tensor_indices])
    num_final_dims = len(slice_indices) + len(tensor_index_shape)
    tensor_index_moved_to_start = _is_tensor_index_moved_to_start(indices)
    num_singletons_before = len(tensor_index_shape
        ) if tensor_index_moved_to_start else 0
    num_singletons_after = num_final_dims - len(tensor_index_shape
        ) if tensor_index_moved_to_start else num_final_dims
    num_singletons_before_tensor = 0 if tensor_index_moved_to_start else None
    num_singletons_after_tensor = num_final_dims - len(tensor_index_shape
        ) if tensor_index_moved_to_start else None
    new_indices = []
    for dim, index in enumerate(indices):
        if isinstance(index, slice):
            num_singletons_after -= 1
            new_index = torch.arange(0, obj.size(dim), device=obj.device)[index
                ]
            new_index = _pad_with_singletons(new_index,
                num_singletons_before, num_singletons_after)
            num_singletons_before += 1
        elif isinstance(index, int):
            new_index = torch.tensor(index, dtype=torch.long, device=obj.device
                )
            new_index = _pad_with_singletons(new_index,
                num_singletons_before, num_singletons_after)
        elif torch.is_tensor(index):
            if num_singletons_before_tensor is None:
                num_singletons_after -= len(tensor_index_shape)
                num_singletons_before_tensor = num_singletons_before
                num_singletons_after_tensor = num_singletons_after
                num_singletons_before += len(tensor_index_shape)
            new_index = _pad_with_singletons(index,
                num_singletons_before_tensor, num_singletons_after_tensor)
        new_indices.append(new_index)
    return tuple(new_indices)


class Matmul(Function):

    @staticmethod
    def forward(ctx, representation_tree, rhs, *matrix_args):
        ctx.representation_tree = representation_tree
        orig_rhs = rhs
        if rhs.ndimension() == 1:
            is_vector = True
            rhs = rhs.unsqueeze(-1)
        else:
            is_vector = False
        lazy_tsr = ctx.representation_tree(*matrix_args)
        res = lazy_tsr._matmul(rhs)
        to_save = [orig_rhs] + list(matrix_args)
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        if is_vector:
            res = res.squeeze(-1)
        return res

    @staticmethod
    def backward(ctx, grad_output):
        rhs = ctx.saved_tensors[0]
        matrix_args = ctx.saved_tensors[1:]
        rhs_shape = rhs.shape
        rhs_grad = None
        arg_grads = [None] * len(matrix_args)
        if any(ctx.needs_input_grad[2:]):
            rhs = rhs.unsqueeze(-1) if rhs.ndimension() == 1 else rhs
            grad_output_matrix = grad_output.unsqueeze(-1
                ) if grad_output.ndimension() == 1 else grad_output
            arg_grads = ctx.representation_tree(*matrix_args
                )._quad_form_derivative(grad_output_matrix, rhs)
        if ctx.needs_input_grad[1]:
            if hasattr(ctx, '_lazy_tsr'):
                lazy_tsr = ctx._lazy_tsr
            else:
                lazy_tsr = ctx.representation_tree(*matrix_args)
            if grad_output.dim() == 1:
                rhs_grad = lazy_tsr._t_matmul(grad_output.unsqueeze(-1)
                    ).squeeze(-1)
            else:
                rhs_grad = lazy_tsr._t_matmul(grad_output)
            if rhs_grad.dim() > len(rhs_shape):
                rhs_grad = rhs_grad.reshape(-1, *rhs_shape).sum(0)
        return tuple([None] + [rhs_grad] + list(arg_grads))


class InvQuad(Function):
    """
    Given a PSD matrix A (or a batch of PSD matrices A), this function computes b A^{-1} b
    where b is a vector or batch of vectors
    """

    @staticmethod
    def forward(ctx, representation_tree, *args):
        """
        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)
        If inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)
        - the RHS of the matrix solves.

        Returns:
        - (Scalar) The inverse quadratic form (or None, if inv_quad is False)
        - (Scalar) The log determinant (or None, if logdet is False)
        """
        inv_quad_rhs, *matrix_args = args
        ctx.representation_tree = representation_tree
        lazy_tsr = ctx.representation_tree(*matrix_args)
        ctx.is_vector = False
        if inv_quad_rhs.ndimension() == 1:
            inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)
            ctx.is_vector = True
        inv_quad_solves = _solve(lazy_tsr, inv_quad_rhs)
        inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)
        to_save = matrix_args + [inv_quad_solves]
        ctx.save_for_backward(*to_save)
        if settings.memory_efficient.off():
            ctx._lazy_tsr = lazy_tsr
        return inv_quad_term

    @staticmethod
    def backward(ctx, inv_quad_grad_output):
        *matrix_args, inv_quad_solves = ctx.saved_tensors
        if hasattr(ctx, '_lazy_tsr'):
            lazy_tsr = ctx._lazy_tsr
        else:
            lazy_tsr = ctx.representation_tree(*matrix_args)
        inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)
        neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(
            inv_quad_grad_output).mul(-1)
        matrix_arg_grads = [None] * len(matrix_args)
        if any(ctx.needs_input_grad[2:]):
            left_factors = neg_inv_quad_solves_times_grad_out
            right_factors = inv_quad_solves
            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors,
                right_factors)
        if ctx.needs_input_grad[1]:
            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul(-2)
        else:
            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)
        if ctx.is_vector:
            inv_quad_rhs_grad.squeeze_(-1)
        res = tuple([None] + [inv_quad_rhs_grad] + list(matrix_arg_grads))
        return tuple(res)


def _matmul_broadcast_shape(shape_a, shape_b, error_msg=None):
    """Compute dimension of matmul operation on shapes (supports broadcasting)"""
    m, n, p = shape_a[-2], shape_a[-1], shape_b[-1]
    if len(shape_b) == 1:
        if n != p:
            if error_msg is None:
                raise RuntimeError(
                    f'Incompatible dimensions for matmul: {shape_a} and {shape_b}'
                    )
            else:
                raise RuntimeError(error_msg)
        return shape_a[:-1]
    if n != shape_b[-2]:
        if error_msg is None:
            raise RuntimeError(
                f'Incompatible dimensions for matmul: {shape_a} and {shape_b}')
        else:
            raise RuntimeError(error_msg)
    tail_shape = torch.Size([m, p])
    batch_shape_a = shape_a[:-2]
    batch_shape_b = shape_b[:-2]
    if batch_shape_a == batch_shape_b:
        bc_shape = batch_shape_a
    else:
        bc_shape = _mul_broadcast_shape(batch_shape_a, batch_shape_b)
    return bc_shape + tail_shape


class Noise(Module):
    pass


def _extract_named_constraints(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_constraints'):
        for name, constraint in module._constraints.items():
            if constraint is not None and constraint not in memo:
                memo.add(constraint)
                full_name = ('.' if prefix else '').join([prefix, name])
                yield full_name, constraint
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, constraint in _extract_named_constraints(module_, memo=
            memo, prefix=submodule_prefix):
            yield name, constraint


def _set_strict(module, value, memo=None):
    if memo is None:
        memo = set()
    if hasattr(module, '_strict_init'):
        module._strict_init = value
    for mname, module_ in module.named_children():
        _set_strict(module_, value)


def _extract_named_added_loss_terms(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_added_loss_terms'):
        for name, strategy in module._added_loss_terms.items():
            if strategy is not None and strategy not in memo:
                memo.add(strategy)
                yield prefix + ('.' if prefix else '') + name, strategy
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, strategy in _extract_named_added_loss_terms(module=
            module_, memo=memo, prefix=submodule_prefix):
            yield name, strategy


def _pyro_load_from_samples(module, samples_dict, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_priors'):
        module.local_load_samples(samples_dict, memo, prefix)
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        _pyro_load_from_samples(module_, samples_dict, memo=memo, prefix=
            submodule_prefix)


def _extract_named_priors(module, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if hasattr(module, '_priors'):
        for name, (prior, closure, inv_closure) in module._priors.items():
            if prior is not None and prior not in memo:
                memo.add(prior)
                full_name = ('.' if prefix else '').join([prefix, name])
                yield full_name, prior, closure, inv_closure
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        for name, prior, closure, inv_closure in _extract_named_priors(module_,
            memo=memo, prefix=submodule_prefix):
            yield name, prior, closure, inv_closure


class DeprecationError(Exception):
    pass


def _pyro_sample_from_prior(module, memo=None, prefix=''):
    try:
        import pyro
    except ImportError:
        raise RuntimeError(
            'Cannot call pyro_sample_from_prior without pyro installed!')
    if memo is None:
        memo = set()
    if hasattr(module, '_priors'):
        for prior_name, (prior, closure, setting_closure
            ) in module._priors.items():
            if prior is not None and prior not in memo:
                if setting_closure is None:
                    raise RuntimeError(
                        f'Cannot use Pyro for sampling without a setting_closure for each prior, but the following prior had none: {prior_name}, {prior}.'
                        )
                memo.add(prior)
                prior = prior.expand(closure().shape)
                value = pyro.sample(prefix + ('.' if prefix else '') +
                    prior_name, prior)
                setting_closure(value)
    for mname, module_ in module.named_children():
        submodule_prefix = prefix + ('.' if prefix else '') + mname
        _pyro_sample_from_prior(module=module_, memo=memo, prefix=
            submodule_prefix)


def _validate_module_outputs(outputs):
    if isinstance(outputs, tuple):
        if not all(torch.is_tensor(output) or isinstance(output,
            Distribution) or isinstance(output, LazyTensor) for output in
            outputs):
            raise RuntimeError(
                'All outputs must be a Distribution, torch.Tensor, or LazyTensor. Got {}'
                .format([output.__class__.__name__ for output in outputs]))
        if len(outputs) == 1:
            outputs = outputs[0]
        return outputs
    elif torch.is_tensor(outputs) or isinstance(outputs, Distribution
        ) or isinstance(outputs, LazyTensor):
        return outputs
    else:
        raise RuntimeError(
            'Output must be a Distribution, torch.Tensor, or LazyTensor. Got {}'
            .format(outputs.__class__.__name__))


class Prior(Distribution, Module, ABC):
    """
    Base class for Priors in GPyTorch.
    In GPyTorch, a parameter can be assigned a prior by passing it as the `prior` argument to
    :func:`~gpytorch.module.register_parameter`. GPyTorch performs internal bookkeeping of priors,
    and for each parameter with a registered prior includes the log probability of the parameter under its
    respective prior in computing the Marginal Log-Likelihood.
    """

    def transform(self, x):
        return self._transform(x) if self._transform is not None else x

    def log_prob(self, x):
        """Returns the log-probability of the parameter value under the prior."""
        return super(Prior, self).log_prob(self.transform(x))


import torch
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile

class Test_cornellius_gp_gpytorch(_paritybench_base):
    pass
    @_fails_compile()
    def test_000(self):
        self._check(_DenseBlock(*[], **{'num_layers': 1, 'num_input_features': 4, 'bn_size': 4, 'growth_rate': 4, 'drop_rate': 0.5}), [torch.rand([4, 4, 4, 4])], {})

    @_fails_compile()
    def test_001(self):
        self._check(_DenseLayer(*[], **{'num_input_features': 4, 'growth_rate': 4, 'bn_size': 4, 'drop_rate': 0.5}), [torch.rand([4, 4, 4, 4])], {})

    def test_002(self):
        self._check(_Transition(*[], **{'num_input_features': 4, 'num_output_features': 4}), [torch.rand([4, 4, 4, 4])], {})

