import sys
_module = sys.modules[__name__]
del sys
hummingbird = _module
ml = _module
_container = _module
_parse = _module
_utils = _module
convert = _module
exceptions = _module
operator_converters = _module
_gbdt_commons = _module
_tree_commons = _module
_tree_implementations = _module
constants = _module
decision_tree = _module
gbdt = _module
lightgbm = _module
xgb = _module
supported = _module
setup = _module
test_backends = _module
test_lightgbm_converters = _module
test_no_extra_install = _module
test_sklearn_decision_tree_converters = _module
test_sklearn_gbdt_converters = _module
test_sklearn_histgbdt_converters = _module
test_xgboost_converters = _module
tree_utils = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import numpy as np


import torch


import warnings


from copy import deepcopy


from enum import Enum


from abc import ABC


from abc import abstractmethod


import copy


from sklearn.ensemble import GradientBoostingClassifier


from sklearn.ensemble import GradientBoostingRegressor


from sklearn.ensemble._hist_gradient_boosting.gradient_boosting import HistGradientBoostingClassifier


class PyTorchBackendModel(torch.nn.Module):
    """
    Container for a model compiled into PyTorch.
    """

    def __init__(self, input_names, output_names, operator_map, topology, extra_config):
        """
        Args:
            input_names: The names of the input `onnxconverter_common.topology.Variable`s for this model
            output_names: The names of the output `onnxconverter_common.topology.Variable`s generated by this model
            operator_map: A dictionary of operator aliases and related PyTorch implementations
            topology: A `onnxconverter_common.topology.Topology` object representing the model graph
            extra_config: Some additional custom configuration parameter
        """
        super(PyTorchBackendModel, self).__init__()
        self.input_names = input_names
        self.output_names = output_names
        self.operator_map = torch.nn.ModuleDict(operator_map)
        self.operators = list(topology.topological_operator_iterator())
        self.extra_config = extra_config
        self.is_regression = self.operator_map[self.operators[-1].full_name].regression

    def forward(self, *inputs):
        with torch.no_grad():
            inputs = [*inputs]
            variable_map = {}
            device = next(self.parameters()).device
            for i, input_name in enumerate(self.input_names):
                if type(inputs[i]) is np.ndarray:
                    inputs[i] = torch.from_numpy(inputs[i])
                elif type(inputs[i]) is not torch.Tensor:
                    raise RuntimeError('Inputer tensor {} of not supported type {}'.format(input_name, type(inputs[i])))
                if device is not None:
                    inputs[i] = inputs[i]
                variable_map[input_name] = inputs[i]
            for operator in self.operators:
                pytorch_op = self.operator_map[operator.full_name]
                pytorch_outputs = pytorch_op(*(variable_map[input] for input in operator.input_full_names))
                if len(operator.output_full_names) == 1:
                    variable_map[operator.output_full_names[0]] = pytorch_outputs
                else:
                    for i, output in enumerate(operator.output_full_names):
                        variable_map[output] = pytorch_outputs[i]
            if len(self.output_names) == 1:
                return variable_map[self.output_names[0]]
            else:
                return list(variable_map[output_name] for output_name in self.output_names)

    def predict(self, *inputs):
        """
        Utility functions used to emulate the behavior of the Sklearn API.
        On regression returns the predicted values.
        On classification tasks returns the predicted class labels for the input data.
        """
        if self.is_regression:
            return self.forward(*inputs).cpu().numpy().flatten()
        else:
            return self.forward(*inputs)[0].cpu().numpy()

    def predict_proba(self, *inputs):
        """
        Utility functions used to emulate the behavior of the Sklearn API.
        On regression a call to this method returns a `RuntimeError`.
        On classification tasks returns the probability estimates.
        """
        if self.is_regression:
            raise RuntimeError('Predict_proba not available for regression tasks.')
        else:
            return self.forward(*inputs)[1].cpu().numpy()


class AbstracTreeImpl(ABC):
    """
    Abstract class definig the basic structure for tree-base models.
    """

    def __init__(self):
        super().__init__()

    @abstractmethod
    def aggregation(self, x):
        """
        Method defining the aggregation operation to execute after the model is evaluated.

        Args:
            x: An input tensor

        Returns:
            The tensor result of the aggregation
        """
        pass

    @abstractmethod
    def calibration(self, x):
        """
        Method implementating the calibration operation for classifiers.

        Args:
            x: An input tensor

        Returns:
            The tensor result of the calibration
        """
        pass


class AbstractPyTorchTreeImpl(AbstracTreeImpl, torch.nn.Module):
    """
    Abstract class definig the basic structure for tree-base models implemented in PyTorch.
    """

    def __init__(self, net_parameters, n_features, classes, n_classes):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
            n_classes: The total number of used classes
        """
        super(AbstractPyTorchTreeImpl, self).__init__()
        self.perform_class_select = False
        self.binary_classification = False
        self.classes = classes
        self.learning_rate = None
        self.regression = False
        self.alpha = None
        if classes is None:
            self.regression = True
            self.n_classes = 1
        else:
            self.n_classes = len(classes) if n_classes is None else n_classes
            if min(classes) != 0 or max(classes) != len(classes) - 1:
                self.classes = torch.nn.Parameter(torch.IntTensor(classes), requires_grad=False)
                self.perform_class_select = True


class GEMMTreeImpl(AbstractPyTorchTreeImpl):
    """
    Class implementing the GEMM strategy in PyTorch for tree-base models.
    """

    def __init__(self, net_parameters, n_features, classes, n_classes=None):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
            n_classes: The total number of used classes
        """
        super(GEMMTreeImpl, self).__init__(net_parameters, n_features, classes, n_classes)
        hidden_one_size = 0
        hidden_two_size = 0
        hidden_three_size = self.n_classes
        for weight, bias in net_parameters:
            hidden_one_size = max(hidden_one_size, weight[0].shape[0])
            hidden_two_size = max(hidden_two_size, weight[1].shape[0])
        n_trees = len(net_parameters)
        weight_1 = np.zeros((n_trees, hidden_one_size, n_features))
        bias_1 = np.zeros((n_trees, hidden_one_size))
        weight_2 = np.zeros((n_trees, hidden_two_size, hidden_one_size))
        bias_2 = np.zeros((n_trees, hidden_two_size))
        weight_3 = np.zeros((n_trees, hidden_three_size, hidden_two_size))
        for i, (weight, bias) in enumerate(net_parameters):
            if len(weight[0]) > 0:
                weight_1[(i), 0:weight[0].shape[0], 0:weight[0].shape[1]] = weight[0]
                bias_1[(i), 0:bias[0].shape[0]] = bias[0]
                weight_2[(i), 0:weight[1].shape[0], 0:weight[1].shape[1]] = weight[1]
                bias_2[(i), 0:bias[1].shape[0]] = bias[1]
                weight_3[(i), 0:weight[2].shape[0], 0:weight[2].shape[1]] = weight[2]
        self.n_trees = n_trees
        self.n_features = n_features
        self.hidden_one_size = hidden_one_size
        self.hidden_two_size = hidden_two_size
        self.hidden_three_size = hidden_three_size
        self.weight_1 = torch.nn.Parameter(torch.from_numpy(weight_1.reshape(-1, self.n_features).astype('float32')))
        self.bias_1 = torch.nn.Parameter(torch.from_numpy(bias_1.reshape(-1, 1).astype('float32')))
        self.weight_2 = torch.nn.Parameter(torch.from_numpy(weight_2.astype('float32')))
        self.bias_2 = torch.nn.Parameter(torch.from_numpy(bias_2.reshape(-1, 1).astype('float32')))
        self.weight_3 = torch.nn.Parameter(torch.from_numpy(weight_3.astype('float32')))

    def aggregation(self, x):
        return x

    def calibration(self, x):
        return x

    def forward(self, x):
        x = x.t()
        x = torch.mm(self.weight_1, x) < self.bias_1
        x = x.view(self.n_trees, self.hidden_one_size, -1)
        x = x.float()
        x = torch.matmul(self.weight_2, x)
        x = x.view(self.n_trees * self.hidden_two_size, -1) == self.bias_2
        x = x.view(self.n_trees, self.hidden_two_size, -1)
        x = x.float()
        x = torch.matmul(self.weight_3, x)
        x = x.view(self.n_trees, self.hidden_three_size, -1)
        x = self.aggregation(x)
        if self.learning_rate is not None:
            x *= self.learning_rate
        if self.alpha is not None:
            x += self.alpha
        if self.regression:
            return x
        x = self.calibration(x)
        if self.perform_class_select:
            return torch.index_select(self.classes, 0, torch.argmax(x, dim=1)), x
        else:
            return torch.argmax(x, dim=1), x


class TreeTraversalTreeImpl(AbstractPyTorchTreeImpl):
    """
    Class implementing the Tree Traversal strategy in PyTorch for tree-base models.
    """

    def __init__(self, tree_parameters, max_depth, n_features, classes, n_classes=None):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            max_depth: The maximum tree-depth in the model
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
            n_classes: The total number of used classes
        """
        super(TreeTraversalTreeImpl, self).__init__(tree_parameters, n_features, classes, n_classes)
        self.n_features = n_features
        self.max_tree_depth = max_depth
        self.num_trees = len(tree_parameters)
        self.num_nodes = max([len(tree_parameter[1]) for tree_parameter in tree_parameters])
        lefts = np.zeros((self.num_trees, self.num_nodes), dtype=np.float32)
        rights = np.zeros((self.num_trees, self.num_nodes), dtype=np.float32)
        features = np.zeros((self.num_trees, self.num_nodes), dtype=np.int64)
        thresholds = np.zeros((self.num_trees, self.num_nodes), dtype=np.float32)
        values = np.zeros((self.num_trees, self.num_nodes, self.n_classes), dtype=np.float32)
        for i in range(self.num_trees):
            lefts[i][:len(tree_parameters[i][0])] = tree_parameters[i][2]
            rights[i][:len(tree_parameters[i][0])] = tree_parameters[i][3]
            features[i][:len(tree_parameters[i][0])] = tree_parameters[i][4]
            thresholds[i][:len(tree_parameters[i][0])] = tree_parameters[i][5]
            values[i][:len(tree_parameters[i][0])][:] = tree_parameters[i][6]
        self.lefts = torch.nn.Parameter(torch.from_numpy(lefts).view(-1), requires_grad=False)
        self.rights = torch.nn.Parameter(torch.from_numpy(rights).view(-1), requires_grad=False)
        self.features = torch.nn.Parameter(torch.from_numpy(features).view(-1), requires_grad=False)
        self.thresholds = torch.nn.Parameter(torch.from_numpy(thresholds).view(-1))
        self.values = torch.nn.Parameter(torch.from_numpy(values).view(-1, self.n_classes))
        nodes_offset = [[(i * self.num_nodes) for i in range(self.num_trees)]]
        self.nodes_offset = torch.nn.Parameter(torch.LongTensor(nodes_offset), requires_grad=False)

    def aggregation(self, x):
        return x

    def calibration(self, x):
        return x

    def forward(self, x):
        indexes = self.nodes_offset
        indexes = indexes.expand(x.size()[0], self.num_trees)
        indexes = indexes.reshape(-1)
        for _ in range(self.max_tree_depth):
            tree_nodes = indexes
            feature_nodes = torch.index_select(self.features, 0, tree_nodes).view(-1, self.num_trees)
            feature_values = torch.gather(x, 1, feature_nodes)
            thresholds = torch.index_select(self.thresholds, 0, indexes).view(-1, self.num_trees)
            lefts = torch.index_select(self.lefts, 0, indexes).view(-1, self.num_trees)
            rights = torch.index_select(self.rights, 0, indexes).view(-1, self.num_trees)
            indexes = torch.where(torch.ge(feature_values, thresholds), rights, lefts).long()
            indexes = indexes + self.nodes_offset
            indexes = indexes.view(-1)
        output = torch.index_select(self.values, 0, indexes).view(-1, self.num_trees, self.n_classes)
        output = self.aggregation(output)
        if self.learning_rate is not None:
            output *= self.learning_rate
        if self.alpha is not None:
            output += self.alpha
        if self.regression:
            return output
        output = self.calibration(output)
        if self.perform_class_select:
            return torch.index_select(self.classes, 0, torch.argmax(output, dim=1)), output
        else:
            return torch.argmax(output, dim=1), output


class PerfectTreeTraversalTreeImpl(AbstractPyTorchTreeImpl):
    """
    Class implementing the Perfect Tree Traversal strategy in PyTorch for tree-base models.
    """

    def __init__(self, tree_parameters, max_depth, n_features, classes, n_classes=None):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            max_depth: The maximum tree-depth in the model
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
            n_classes: The total number of used classes
        """
        super(PerfectTreeTraversalTreeImpl, self).__init__(tree_parameters, n_features, classes, n_classes)
        self.max_tree_depth = max_depth
        self.num_trees = len(tree_parameters)
        self.n_features = n_features
        node_maps = [tp[0] for tp in tree_parameters]
        weight_0 = np.zeros((self.num_trees, 2 ** max_depth - 1))
        bias_0 = np.zeros((self.num_trees, 2 ** max_depth - 1))
        weight_1 = np.zeros((self.num_trees, 2 ** max_depth, self.n_classes))
        for i, node_map in enumerate(node_maps):
            self._get_weights_and_biases(node_map, max_depth, weight_0[i], weight_1[i], bias_0[i])
        node_by_levels = [set() for _ in range(max_depth)]
        self._traverse_by_level(node_by_levels, 0, -1, max_depth)
        self.root_nodes = torch.nn.Parameter(torch.from_numpy(weight_0[:, (0)].flatten().astype('int64')), requires_grad=False)
        self.root_biases = torch.nn.Parameter(-1 * torch.from_numpy(bias_0[:, (0)].astype('float32')), requires_grad=False)
        tree_indices = np.array([i for i in range(0, 2 * self.num_trees, 2)]).astype('int64')
        self.tree_indices = torch.nn.Parameter(torch.from_numpy(tree_indices), requires_grad=False)
        self.nodes = []
        self.biases = []
        for i in range(1, max_depth):
            nodes = torch.nn.Parameter(torch.from_numpy(weight_0[:, (list(sorted(node_by_levels[i])))].flatten().astype('int64')), requires_grad=False)
            biases = torch.nn.Parameter(torch.from_numpy(-1 * bias_0[:, (list(sorted(node_by_levels[i])))].flatten().astype('float32')), requires_grad=False)
            self.nodes.append(nodes)
            self.biases.append(biases)
        self.nodes = torch.nn.ParameterList(self.nodes)
        self.biases = torch.nn.ParameterList(self.biases)
        self.leaf_nodes = torch.nn.Parameter(torch.from_numpy(weight_1.reshape((-1, self.n_classes)).astype('float32')), requires_grad=False)

    def aggregation(self, x):
        return x

    def calibration(self, x):
        return x

    def forward(self, x):
        prev_indices = torch.ge(torch.index_select(x, 1, self.root_nodes), self.root_biases).long()
        prev_indices = prev_indices + self.tree_indices
        prev_indices = prev_indices.view(-1)
        factor = 2
        for nodes, biases in zip(self.nodes, self.biases):
            gather_indices = torch.index_select(nodes, 0, prev_indices).view(-1, self.num_trees)
            features = torch.gather(x, 1, gather_indices).view(-1)
            prev_indices = factor * prev_indices + torch.ge(features, torch.index_select(biases, 0, prev_indices)).long().view(-1)
        output = torch.index_select(self.leaf_nodes, 0, prev_indices.view(-1)).view(-1, self.num_trees, self.n_classes)
        output = self.aggregation(output)
        if self.learning_rate is not None:
            output *= self.learning_rate
        if self.alpha is not None:
            output += self.alpha
        if self.regression:
            return output
        output = self.calibration(output)
        if self.perform_class_select:
            return torch.index_select(self.classes, 0, torch.argmax(output, dim=1)), output
        else:
            return torch.argmax(output, dim=1), output

    def _traverse_by_level(self, node_by_levels, node_id, current_level, max_level):
        current_level += 1
        if current_level == max_level:
            return node_id
        node_by_levels[current_level].add(node_id)
        node_id += 1
        node_id = self._traverse_by_level(node_by_levels, node_id, current_level, max_level)
        node_id = self._traverse_by_level(node_by_levels, node_id, current_level, max_level)
        return node_id

    def _get_weights_and_biases(self, nodes_map, tree_depth, weight_0, weight_1, bias_0):

        def depth_f_traversal(node, current_depth, node_id, leaf_start_id):
            weight_0[node_id] = node.feature
            bias_0[node_id] = -node.threshold
            current_depth += 1
            node_id += 1
            if node.left.feature == -1:
                node_id += 2 ** (tree_depth - current_depth - 1) - 1
                v = node.left.value
                weight_1[leaf_start_id:leaf_start_id + 2 ** (tree_depth - current_depth - 1)] = np.ones((2 ** (tree_depth - current_depth - 1), self.n_classes)) * v
                leaf_start_id += 2 ** (tree_depth - current_depth - 1)
            else:
                node_id, leaf_start_id = depth_f_traversal(node.left, current_depth, node_id, leaf_start_id)
            if node.right.feature == -1:
                node_id += 2 ** (tree_depth - current_depth - 1) - 1
                v = node.right.value
                weight_1[leaf_start_id:leaf_start_id + 2 ** (tree_depth - current_depth - 1)] = np.ones((2 ** (tree_depth - current_depth - 1), self.n_classes)) * v
                leaf_start_id += 2 ** (tree_depth - current_depth - 1)
            else:
                node_id, leaf_start_id = depth_f_traversal(node.right, current_depth, node_id, leaf_start_id)
            return node_id, leaf_start_id
        depth_f_traversal(nodes_map[0], -1, 0, 0)


class GEMMDecisionTreeImpl(GEMMTreeImpl):
    """
    Class implementing the GEMM strategy in PyTorch for decision tree models.

    """

    def __init__(self, net_parameters, n_features, classes=None):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
        """
        super(GEMMDecisionTreeImpl, self).__init__(net_parameters, n_features, classes)
        self.final_probability_divider = len(net_parameters)

    def aggregation(self, x):
        output = x.sum(0).t()
        if self.final_probability_divider > 1:
            output = output / self.final_probability_divider
        return output


class TreeTraversalDecisionTreeImpl(TreeTraversalTreeImpl):
    """
    Class implementing the Tree Traversal strategy in PyTorch for decision tree models.
    """

    def __init__(self, net_parameters, max_depth, n_features, classes=None):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            max_depth: The maximum tree-depth in the model
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
        """
        super(TreeTraversalDecisionTreeImpl, self).__init__(net_parameters, max_depth, n_features, classes)
        self.final_probability_divider = len(net_parameters)

    def aggregation(self, x):
        output = x.sum(1)
        if self.final_probability_divider > 1:
            output = output / self.final_probability_divider
        return output


class PerfectTreeTraversalDecisionTreeImpl(PerfectTreeTraversalTreeImpl):
    """
    Class implementing the Perfect Tree Traversal strategy in PyTorch for decision tree models.
    """

    def __init__(self, net_parameters, max_depth, n_features, classes=None):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            max_depth: The maximum tree-depth in the model
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
        """
        super(PerfectTreeTraversalDecisionTreeImpl, self).__init__(net_parameters, max_depth, n_features, classes)
        self.final_probability_divider = len(net_parameters)

    def aggregation(self, x):
        output = x.sum(1)
        if self.final_probability_divider > 1:
            output = output / self.final_probability_divider
        return output


_constant_error = """
It usually means a constant is not available or you are trying to override a constant value.
"""


class ConstantError(TypeError):
    """
    Raised when a constant is not available or it get overwritten.
    """

    def __init__(self, msg):
        super().__init__(msg + _constant_error)


class _Constants(object):
    """
    Class enabling the proper definition of constants.
    """

    def __init__(self, constants, other_constants=None):
        for constant in dir(constants):
            if constant.isupper():
                setattr(self, constant, getattr(constants, constant))
        for constant in dir(other_constants):
            if constant.isupper():
                setattr(self, constant, getattr(other_constants, constant))

    def __setattr__(self, name, value):
        if name in self.__dict__:
            raise ConstantError('Overwriting a constant is not allowed {}'.format(name))
        self.__dict__[name] = value


class GEMMGBDTImpl(GEMMTreeImpl):
    """
    Class implementing the GEMM strategy (in PyTorch) for GBDT models.
    """

    def __init__(self, net_parameters, n_features, classes=None, extra_config={}):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
            extra_config: Extra configuration used to properly implement the source tree
        """
        super(GEMMGBDTImpl, self).__init__(net_parameters, n_features, classes, 1)
        self.n_gbdt_classes = 1
        if constants.LEARNING_RATE in extra_config:
            self.learning_rate = extra_config[constants.LEARNING_RATE]
        if constants.ALPHA in extra_config:
            self.alpha = torch.nn.Parameter(torch.FloatTensor(extra_config[constants.ALPHA]), requires_grad=False)
        if classes is not None:
            self.n_gbdt_classes = len(classes) if len(classes) > 2 else 1
            if self.n_gbdt_classes == 1:
                self.binary_classification = True
        self.n_trees_per_class = len(net_parameters) // self.n_gbdt_classes

    def aggregation(self, x):
        return torch.squeeze(x).t().view(-1, self.n_gbdt_classes, self.n_trees_per_class).sum(2)

    def calibration(self, x):
        if self.binary_classification:
            output = torch.sigmoid(x)
            return torch.cat([1 - output, output], dim=1)
        else:
            return torch.softmax(x, dim=1)


class TreeTraversalGBDTImpl(TreeTraversalTreeImpl):
    """
    Class implementing the Tree Traversal strategy in PyTorch.
    """

    def __init__(self, net_parameters, max_detph, n_features, classes=None, extra_config={}):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            max_depth: The maximum tree-depth in the model
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
            extra_config: Extra configuration used to properly implement the source tree
        """
        super(TreeTraversalGBDTImpl, self).__init__(net_parameters, max_detph, n_features, classes, 1)
        self.n_gbdt_classes = 1
        if constants.LEARNING_RATE in extra_config:
            self.learning_rate = extra_config[constants.LEARNING_RATE]
        if constants.ALPHA in extra_config:
            self.alpha = torch.nn.Parameter(torch.FloatTensor(extra_config[constants.ALPHA]), requires_grad=False)
        if classes is not None:
            self.n_gbdt_classes = len(classes) if len(classes) > 2 else 1
            if self.n_gbdt_classes == 1:
                self.binary_classification = True
        self.n_trees_per_class = len(net_parameters) // self.n_gbdt_classes

    def aggregation(self, x):
        return x.view(-1, self.n_gbdt_classes, self.n_trees_per_class).sum(2)

    def calibration(self, x):
        if self.binary_classification:
            output = torch.sigmoid(x)
            return torch.cat([1 - output, output], dim=1)
        else:
            return torch.softmax(x, dim=1)


class PerfectTreeTraversalGBDTImpl(PerfectTreeTraversalTreeImpl):
    """
    Class implementing the Perfect Tree Traversal strategy in PyTorch.
    """

    def __init__(self, net_parameters, max_depth, n_features, classes=None, extra_config={}):
        """
        Args:
            net_parameters: The parameters defining the tree structure
            max_depth: The maximum tree-depth in the model
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
            extra_config: Extra configuration used to properly implement the source tree
        """
        super(PerfectTreeTraversalGBDTImpl, self).__init__(net_parameters, max_depth, n_features, classes, 1)
        self.n_gbdt_classes = 1
        if constants.LEARNING_RATE in extra_config:
            self.learning_rate = extra_config[constants.LEARNING_RATE]
        if constants.ALPHA in extra_config:
            self.alpha = torch.nn.Parameter(torch.FloatTensor(extra_config[constants.ALPHA]), requires_grad=False)
        if classes is not None:
            self.n_gbdt_classes = len(classes) if len(classes) > 2 else 1
            if self.n_gbdt_classes == 1:
                self.binary_classification = True
        self.n_trees_per_class = len(net_parameters) // self.n_gbdt_classes

    def aggregation(self, x):
        return x.view(-1, self.n_gbdt_classes, self.n_trees_per_class).sum(2)

    def calibration(self, x):
        if self.binary_classification:
            output = torch.sigmoid(x)
            return torch.cat([1 - output, output], dim=1)
        else:
            return torch.softmax(x, dim=1)

